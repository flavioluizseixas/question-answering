[
    {
        "text": "ChapterKurose | Ross\nRedes de  \ncomputadores \ne a internet\numa abordagem top-down\n6a edição\nRedes de  \ncomputadores \ne a internet\nKurose | Ross\nRedes de  \ncomputadores \ne a internet\numa abordagem top-down\n6a edição\n2013\nDireitos exclusivos para a língua portuguesa cedidos à\nPearson Education do Brasil Ltda.,\numa empresa do grupo Pearson Education\nRua Nelson Francisco, 26\nCEP 02712-100 – São Paulo – SP – Brasil\nFone: 11 2178-8686 – Fax: 11 2178-8688\nvendas@pearson.com\nDados Internacionais de Catalogação na Publicação (CIP)\n(Câmara Brasileira do Livro, SP\n, Brasil)\nKurose, James F.\n\t\nRedes de computadores e a Internet: uma abordagem top-down/ \nJames F. Kurose, Keith W. Ross ; tradução Daniel Vieira; revisão técnica \nWagner Luiz Zucchi. – 6. ed. – São Paulo: Pearson Education do Brasil, 2013.\t\t\t\nTítulo original: Computer networking: a top-down approach \n \nBibliografia.\n \nISBN 978-85-430-1443-2\t\n1. Internet 2. Redes de computadores I. Ross,  Keith W.. II. Zucchi, \nWagner Luiz. III. Título.\n13-04218\t\nCDD-004.67\n1. Internet : Redes de computadores: \nProcessamento de dados 004.67\n©2014 by Jim F. Kurose e Keith W. Ross\nTodos os direitos reservados. Nenhuma parte desta publicação poderá ser reproduzida ou \ntransmitida de qualquer modo ou por qualquer outro meio, eletrônico ou mecânico, incluindo \nfotocópia, gravação ou qualquer outro tipo de sistema de armazenamento e transmissão de \ninformação, sem prévia autorização, por escrito, da Pearson Education do Brasil.\t\nDiretor editorial e de conteúdo\t Roger Trimer\t\nGerente editorial\t Kelly Tavares\t\nSupervisora de produção editorial\t Silvana Afonso\t\nCoordenadora de produção gráfica\t Tatiane Romano\t\nEditor de aquisições\t Vinícius Souza\t\nEditora de texto\t Daniela Braz\t\nPreparação\t Christiane Colas\t\nRevisão\t Carmen Simões Costa\t\nEditor assistente\t Luiz Salla\t\nCapa\t \u0007\nSolange Rennó \n(Sob projeto original)\t\nProjeto gráfico e Diagramação\t Casa de Ideias\t\nJim Kurose\nJim Kurose é um destacado professor universitário de Ciência da Computação na Universidade de Massachu-\nsetts, Amherst.\nDr. Kurose recebeu diversos reconhecimentos por suas atividades educacionais, incluindo o Outstanding Teacher \nAwards da National Technological University (oito vezes), na Universidade de Massachusetts e na Northeast Asso-\nciation of Graduate Schools. Recebeu a IEEE Taylor Booth Education Medal e foi reconhecido por sua liderança da \nCommonwealth Information Technology Initiative de Massachusetts. Também recebeu um GE Fellowship, um IBM \nFaculty Development Award e um Lilly Teaching Fellowship.\nFoi editor-chefe da IEEE Transactions of Communications e da IEEE/ACM Transactions on Networking. Trabalhou \nativamente nos comitês de programa para IEEE Infocom, ACM SIGCOMM, ACM Internet Measurement Conference \ne ACM SIGMETRICS por vários anos, e atendeu como copresidente de programa técnico nessas conferências. Ele é \nfellow do IEEE e da ACM. Seus interesses de pesquisa incluem protocolos e arquitetura de rede, medição de redes, \nredes de sensores, comunicação multimídia e modelagem e avaliação de desempenho. Tem doutorado em Ciência \nda Computação pela Universidade de Columbia.\nKeith Ross\nKeith Ross é professor na cátedra de Leonard J. Shustek e diretor do Departamento de Ciência da Computação \nno Instituto Politécnico da Universidade de Nova York. Antes de ingressar nesse Instituto em 2003, foi professor na \nUniversidade da Pensilvânia (13 anos) e no Eurécom Institute (5 anos). Obteve bacharelado pela Universidade Tufts, \nmestrado pela Universidade de Columbia e doutorado em Computador e Engenharia de Controle pela Universidade de \nMichigan. Keith Ross também é fundador e CEO original da Wimba, que desenvolve aplicações de multimídia on-line \npara e-learning e foi adquirida pela Blackboard em 2010.\nOs interesses de pesquisa do professor Ross estão em segurança e privacidade, redes sociais, redes P2P\n, medição \nna Internet, fluxo contínuo de vídeo, redes de distribuição de conteúdo e modelagem estocástica. É fellow do IEEE, \nrecebeu o Infocom 2009 Best Paper Award e também os Best Paper Awards de 2011 e 2008 por Comunicações em \nMultimídia (concedido pela IEEE Communications Society). Trabalhou em diversos comitês editoriais de jornal e comitês \nde programa de conferência, incluindo IEEE/ACM Transactions on Networking, ACM SIGCOMM, ACM CoNext e ACM \nInternet Measurement Conference. Ele também trabalhou como consultor de compartilhamento de arquivos P2P para \na Federal Trade Commission.\nSobre os autores\nPara Julie e nossas três preciosidades: \nChris, Charlie e Nina \nJFK\nUm grande MUITO OBRIGADO aos meus professores, colegas \ne alunos do mundo inteiro. \nKWR\nBem-vindo à sexta edição de Redes de computadores e a Internet: uma abordagem top-down. Desde a publicação \nda primeira edição, há doze anos, nosso livro foi adotado em centenas de universidades e instituições de ensino supe-\nrior, traduzido para mais de 14 idiomas e utilizado por mais de cem mil estudantes e profissionais no mundo inteiro. \nMuitos desses leitores entraram em contato conosco e ficamos extremamente satisfeitos com sua reação positiva.\nQuais são as novidades da sexta edição?\nAcreditamos que uma importante razão para esse sucesso é que o livro continua a oferecer uma abordagem \nmoderna do ensino de redes de computadores. Fizemos mudanças nesta sexta edição, mas também mantive-\nmos inalterado o que acreditamos (e os instrutores e estudantes que usaram nosso livro confirmaram) serem os \naspectos mais importantes do livro: sua abordagem top-down, seu foco na Internet e um tratamento moderno \ndas redes de computadores, sua atenção aos princípios e à prática, e seu estilo e método acessíveis em relação ao \naprendizado de redes de computadores. Apesar disso, a esta edição foi revisada e atualizada de modo substancial:\n• O site de apoio do livro foi significativamente expandido e enriquecido para incluir exercícios interativos, \nconforme discutido mais adiante neste Prefácio.\n• No Capítulo 1, o tratamento das redes de acesso foi modernizado e a descrição do ecossistema de ISP da \nInternet foi substancialmente revisada, considerando o surgimento recente das redes de provedor de con-\nteúdo, como a do Google. A apresentação da comutação de pacotes e da comutação de circuitos também \nfoi reorganizada, oferecendo uma orientação mais tópica, em vez de histórica.\n• No Capítulo 2, Python substituiu Java para a apresentação da programação de sockets. Embora ainda \nexpondo explicitamente as ideias por trás da API sockets, o código Python é mais fácil de entender para o \nprogramador iniciante. Além do mais, diferentemente do Java, ele fornece acesso a sockets brutos, permi-\ntindo que os alunos construam maior variedade de aplicações de rede. Laboratórios de programação de \nsockets baseados em Java foram substituídos por laboratórios Python correspondentes, e foi acrescentado \num novo laboratório ICMP Ping baseado em Python. Como sempre, quando um material é retirado — \ncomo aquele sobre programação de sockets baseada em Java — ele permanece disponível no site de apoio \ndo livro (ver texto mais adiante).\n• No Capítulo 3, a apresentação de um dos protocolos de transferência de dados confiável foi simplificada \ne uma nova nota em destaque sobre divisão do TCP, normalmente usada para otimizar o desempenho de \nserviços de nuvem, foi acrescentada.\nPrefácio\n   Redes de computadores e a Internet\nX\n• No Capítulo 4, a seção sobre arquiteturas de roteador foi significativamente atualizada, refletindo de-\nsenvolvimentos e práticas recentes nessa área. Incluímos diversas novas notas em destaque, envolvendo \nDNS, BGP e OSPF.\n• O Capítulo 5 foi reorganizado e simplificado, considerando a onipresença da Ethernet comutada em \nredes locais e o consequente aumento do uso da Ethernet em cenários ponto a ponto. Além disso, acres-\ncentamos uma seção sobre redes de centro de dados.\n• O Capítulo 6 foi atualizado para refletir os avanços recentes nas redes sem fio, em particular redes de \ndados por celular e serviços e arquiteturas 4G.\n• O Capítulo 7, que enfoca as redes multimídia, passou por uma grande revisão. Ele agora contém uma \ndiscussão profunda do vídeo de fluxo contínuo (streaming), incluindo fluxo contínuo adaptativo, e \numa discussão nova e modernizada de CDNs. Uma seção recém-incluída descreve os sistemas de ví-\ndeo de fluxo contínuo Netflix, YouTube e Kankan. O material que foi removido para dar espaço para \nesses novos tópicos ainda está disponível no site de apoio.\n• O Capítulo 8 agora contém uma discussão expandida sobre autenticação do ponto final.\n• Acrescentamos um novo material significativo, envolvendo problemas de fim de capítulo. Assim como \nem todas as edições anteriores, trabalhos de casa foram revisados, acrescentados e removidos.\nPúblico-alvo\nEste livro destina-se a um estudo inicial de redes de computadores. Pode ser usado em cursos de ciência da \ncomputação e de engenharia elétrica. Em termos de linguagens de programação, ele requer que os estudantes te-\nnham alguma experiência com as linguagens C, C++, Java ou Python (mesmo assim, apenas em alguns lugares). \nEmbora seja mais minucioso e analítico do que muitos outros de introdução às redes de computadores, rara-\nmente utiliza conceitos matemáticos que não sejam ensinados no ensino médio. Fizemos um esforço deliberado \npara evitar o uso de quaisquer conceitos avançados de cálculo, probabilidade ou processos estocásticos (embora \ntenhamos incluído alguns problemas para alunos com tal conhecimento avançado). Por conseguinte, o livro é \napropriado para cursos de graduação e para o primeiro ano dos cursos de pós-graduação. É também muito útil \npara os profissionais do setor de telecomunicações.\nO que há de singular neste livro?\nO assunto rede de computadores é bastante vasto e complexo, envolvendo muitos conceitos, protocolos e \ntecnologias que se entrelaçam inextricavelmente. Para dar conta desse escopo e complexidade, muitos livros so-\nbre redes são, em geral, organizados de acordo com as “camadas” de uma arquitetura de rede. Com a organização \nem camadas, os estudantes podem vislumbrar a complexidade das redes de computadores — eles aprendem os \nconceitos e os protocolos distintos de uma parte da arquitetura e, ao mesmo tempo, visualizam o grande quadro \nda interconexão entre as camadas. Do ponto de vista pedagógico, nossa experiência confirma que essa aborda-\ngem em camadas é, de fato, muito boa. Entretanto, achamos que a abordagem tradicional, a bottom-up — da \ncamada física para a camada de aplicação —, não é a melhor para um curso moderno de redes de computadores.\nUma abordagem top-down\nNa primeira edição, propusemos uma inovação adotando uma visão top-down — isto é, começando na ca-\nmada de aplicação e descendo até a camada física. O retorno que recebemos de professores e alunos confirmou \nque essa abordagem tem muitas vantagens e realmente funciona bem do ponto de vista pedagógico. Primeiro, o \nlivro dá ênfase à camada de aplicação, que tem sido a área de “grande crescimento” das redes de computadores. \nDe fato, muitas das recentes revoluções nesse ramo — incluindo a Web, o compartilhamento de arquivos P2P e \no fluxo contínuo de mídia — tiveram lugar nessa camada. A abordagem de ênfase inicial à camada de aplicação \né diferente das seguidas por muitos outros livros, que têm apenas pouco material sobre aplicações de redes, seus \nrequisitos, paradigmas da camada de aplicação (por exemplo, cliente-servidor e P2P) e interfaces de programação \nprefácio  XI \nde aplicação. Segundo, nossa experiência como professores (e a de muitos outros que utilizaram este livro) \nconfirma que ensinar aplicações de rede logo no início do curso é uma poderosa ferramenta motivadora. Os \nestudantes ficam mais entusiasmados ao aprender como funcionam as aplicações de rede — aplicações como o \ne-mail e a Web, que a maioria deles usa diariamente. Entendendo as aplicações, o estudante pode entender os \nserviços de rede necessários ao suporte de tais aplicações. Pode também, por sua vez, examinar as várias manei-\nras como esses serviços são fornecidos e executados nas camadas mais baixas. Assim, a discussão das aplicações \nlogo no início fornece a motivação necessária para os demais assuntos do livro.\nTerceiro, a abordagem top-down habilita o professor a apresentar o desenvolvimento das aplicações de rede \nno estágio inicial. Os estudantes não só veem como funcionam aplicações e protocolos populares, como também \naprendem que é fácil criar suas próprias aplicações e protocolos de aplicação de rede. Com a abordagem top­\n‑down, eles entram imediatamente em contato com as noções de programação de sockets, modelos de serviços e \nprotocolos — conceitos importantes que reaparecem em todas as camadas subsequentes. Ao apresentar exemplos \nde programação de sockets em Python, destacamos as ideias centrais sem confundir os estudantes com códigos \ncomplexos. Estudantes de engenharia elétrica e ciência da computação talvez não tenham dificuldades para en-\ntender o código Python.\nUm foco na Internet\nContinuamos a colocar a Internet em foco nesta edição do livro. Na verdade, como ela está presente em \ntoda parte, achamos que qualquer livro sobre redes deveria ter um foco significativo na Internet. Continuamos a \nutilizar a arquitetura e os protocolos da Internet como veículo primordial para estudar conceitos fundamentais \nde redes de computadores. É claro que também incluímos conceitos e protocolos de outras arquiteturas de rede. \nMas os holofotes estão claramente dirigidos à Internet, fato refletido na organização do livro, que gira em torno \nda arquitetura de cinco camadas: aplicação, transporte, rede, enlace e física.\nOutro benefício de colocá-la sob os holofotes é que a maioria dos estudantes de ciência da computação \ne de engenharia elétrica está ávida por conhecer a Internet e seus protocolos. Eles sabem que a Internet é uma \ntecnologia revolucionária e inovadora e podem constatar que ela está provocando uma profunda transformação \nem nosso mundo. Dada sua enorme relevância, os estudantes estão naturalmente curiosos em saber o que há por \ntrás dela. Assim, fica fácil para um professor manter seus alunos interessados nos princípios básicos, usando a \nInternet como guia.\nEnsinando princípios de rede\nDuas das características exclusivas deste livro — sua abordagem top-down e seu foco na Internet — apa-\nrecem no título e subtítulo. Se pudéssemos, teríamos acrescentado uma terceira palavra — princípios. O campo \ndas redes agora está maduro o suficiente para que uma quantidade de assuntos de importância fundamental \npossa ser identificada. Por exemplo, na camada de transporte, entre os temas importantes estão a comunicação \nconfiável por uma camada de rede não confiável, o estabelecimento/encerramento de conexões e mútua apre-\nsentação (handshaking), o controle de congestionamento e de fluxo e a multiplexação. Na camada de rede, dois \nassuntos muito importantes são: como determinar “bons” caminhos entre dois roteadores e como interconectar \num grande número de redes heterogêneas. Na camada de enlace, um problema fundamental é como comparti-\nlhar um canal de acesso múltiplo. Na segurança de rede, técnicas para prover sigilo, autenticação e integridade de \nmensagens são baseadas em fundamentos da criptografia. Este livro identifica as questões fundamentais de redes \ne apresenta abordagens para enfrentar tais questões. Aprendendo esses princípios, o estudante adquire conheci-\nmento de “longa validade” — muito tempo após os padrões e protocolos de rede de hoje tornarem-se obsoletos, \nos princípios que ele incorpora continuarão importantes e relevantes. Acreditamos que o uso da Internet para \napresentar o assunto aos estudantes e a ênfase dada à abordagem das questões e das soluções permitirão que os \nalunos entendam rapidamente qualquer tecnologia de rede.\n   Redes de computadores e a Internet\nXII\nCaracterísticas pedagógicas\nHá quase 20 anos damos aulas de redes de computadores. Adicionamos a este livro uma experiência agre-\ngada de mais de 50 anos de ensino para milhares de estudantes. Nesse período, também participamos ativamente \nna área de pesquisas sobre redes de computadores. (De fato, Jim e Keith se conheceram quando faziam mestrado, \nfrequentando um curso sobre redes de computadores ministrado por Mischa Schwartz, em 1979, na Universida-\nde de Colúmbia.) Achamos que isso nos dá uma boa perspectiva do que foi a rede e de qual será, provavelmente, \nseu futuro. Não obstante, resistimos às tentações de dar ao material deste livro um viés que favorecesse nossos \nprojetos de pesquisa prediletos. Se você estiver interessado em nossas pesquisas, consulte nosso site pessoal. Este \nlivro é sobre redes de computadores modernas — é sobre protocolos e tecnologias contemporâneas, bem como \nsobre os princípios subjacentes a esses protocolos e tecnologias. Também achamos que aprender (e ensinar!) re-\ndes pode ser divertido. Esperamos que algum senso de humor e a utilização de analogias e exemplos do mundo \nreal que aparecem aqui tornem o material ainda mais divertido.\nDependências de capítulo\nO primeiro capítulo apresenta um apanhado geral sobre redes de computadores. Com a introdução de \nmuitos conceitos e terminologias fundamentais, ele monta o cenário para o restante do livro. Todos os outros \ncapítulos dependem diretamente desse primeiro. Recomendamos que os professores, após o terem comple-\ntado, percorram em sequência os Capítulos 2 ao 5, seguindo nossa filosofia top-down. Cada um dos cinco \nprimeiros capítulos utiliza material dos anteriores. Após tê-los completado, o professor terá bastante flexibi-\nlidade. Não há interdependência entre os quatro últimos capítulos, de modo que eles podem ser ensinados \nem qualquer ordem. Porém, cada um dos quatro últimos capítulos depende do material nos cinco primeiros. \nMuitos professores a princípio ensinam os cinco primeiros capítulos e depois ensinam um dos quatro últimos \npara “arrematar”.\nAgradecimentos\nDesde o início deste projeto, em 1996, muitas pessoas nos deram inestimável auxílio e influenciaram nossas \nideias sobre como melhor organizar e ministrar um curso sobre redes. Nosso MUITO OBRIGADO a todos os \nque nas ajudaram desde os primeiros rascunhos deste livro, até a quinta edição. Também somos muito gratos \nsobretudo às muitas centenas de leitores de todo o mundo  estudantes, acadêmicos e profissionais  que nos \nenviaram sugestões e comentários sobre edições anteriores e sugestões para edições futuras. Nossos agradeci-\nmentos especiais para:\nAl Aho (Universidade de Columbia)\nHisham Al-Mubaid (Universidade de Houston-Clear Lake)\nPratima Akkunoor (Universidade do Estado do Arizona)\nPaul Amer (Universidade de Delaware)\nShamiul Azom (Universidade do Estado do Arizona)\nLichun Bao (Universidade da Califórnia, Irvine)\nPaul Barford (Universidade do Wisconsin)\nBobby Bhattacharjee (Universidade de Maryland)\nSteven Bellovin (Universidade de Columbia)\nPravin Bhagwat (Wibhu)\nSupratik Bhattacharyya (anteriormente na Sprint)\nErnst Biersack (Eurécom Institute)\nShahid Bokhari (Universidade de Engenharia & Tecnologia, Lahore)\nJean Bolot (Technicolor Research)\nprefácio  XIII \nDaniel Brushteyn (ex-aluno da Universidade da Pensilvânia)\nKen Calvert (Universidade do Kentucky)\nEvandro Cantu (Universidade Federal de Santa Catarina)\nJeff Case (SNMP Research International)\nJeff Chaltas (Sprint)\nVinton Cerf (Google)\nByung Kyu Choi (Universidade Tecnológica de Michigan)\nBram Cohen (BitTorrent, Inc.)\nConstantine Coutras (Universidade Pace)\nJohn Daigle (Universidade do Mississippi)\nEdmundo A. de Souza e Silva (Universidade Federal do Rio de Janeiro)\nPhilippe Decuetos (Eurécom Institute)\nChristophe Diot (Technicolor Research)\nPrithula Dhunghel (Akamai)\nDeborah Estrin (Universidade da Califórnia, Los Angeles)\nMichalis Faloutsos (Universidade da Califórnia, Riverside)\nWu-chi Feng (Oregon Graduate Institute)\nSally Floyd (ICIR, Universidade da Califórnia, Berkeley)\nPaul Francis (Max Planck Institute)\nLixin Gao (Universidade de Massachusetts)\nJJ Garcia-Luna-Aceves (Universidade da Califórnia, Santa Cruz)\nMario Gerla (Universidade da Califórnia, Los Angeles)\nDavid Goodman (NYU-Poly)\nYang Guo (Alcatel/Lucent Bell Labs)\nTim Griffin (Universidade de Cambridge)\nMax Hailperin (Gustavus Adolphus College)\nBruce Harvey (Universidade da Flórida A&M)\nCarl Hauser (Universidade do Estado de Washington)\nRachelle Heller (Universidade George Washington)\nPhillipp Hoschka (INRIA/W3C)\nWen Hsin (Universidade Park)\nAlbert Huang (ex-aluno da Universidade da Pensilvânia)\nCheng Huang (Microsoft Research)\nEsther A. Hughes (Universidade Virginia Commonwealth)\nVan Jacobson (Xerox PARC)\nPinak Jain (ex-aluno da NYU-Poly)\nJobin James (Universidade da Califórnia, Riverside)\nSugih Jamin (Universidade de Michigan)\nShivkumar Kalyanaraman (IBM Research, Índia)\nJussi Kangasharju (Universidade de Helsinki)\nSneha Kasera (Universidade de Utah)\nParviz Kermani (anteriormente da IBM Research)\nHyojin Kim (ex-aluno da Universidade da Pensilvânia)\n   Redes de computadores e a Internet\nXIV\nLeonard Kleinrock (Universidade da Califórnia, Los Angeles)\nDavid Kotz (Dartmouth College)\nBeshan Kulapala (Universidade do Estado do Arizona)\nRakesh Kumar (Bloomberg)\nMiguel A. Labrador (Universidade South Florida)\nSimon Lam (Universidade do Texas)\nSteve Lai (Universidade do Estado de Ohio)\nTom LaPorta (Universidade do Estado de Pensilvânia)\nTim-Berners Lee (World Wide Web Consortium)\nArnaud Legout (INRIA)\nLee Leitner (Universidade Drexel)\nBrian Levine (Universidade de Massachusetts)\nChunchun Li (ex-aluno da NYU-Poly)\nYong Liu (NYU-Poly)\nWilliam Liang (ex-aluno da Universidade da Pensilvânia)\nWillis Marti (Universidade Texas A&M)\nNick McKeown (Universidade Stanford)\nJosh McKinzie (Universidade Park)\nDeep Medhi (Universidade do Missouri, Kansas City)\nBob Metcalfe (International Data Group)\nSue Moon (KAIST)\nJenni Moyer (Comcast)\nErich Nahum (IBM Research)\nChristos Papadopoulos (Universidade do Estado do Colorado)\nCraig Partridge (BBN Technologies)\nRadia Perlman (Intel)\nJitendra Padhye (Microsoft Research)\nVern Paxson (Universidade da Califórnia, Berkeley)\nKevin Phillips (Sprint)\nGeorge Polyzos (Universidade de Economia e Negócios de Atenas)\nSriram Rajagopalan (Universidade do Estado do Arizona)\nRamachandran Ramjee (Microsoft Research)\nKen Reek (Rochester Institute of Technology)\nMartin Reisslein (Universidade do Estado do Arizona)\nJennifer Rexford (Universidade de Princeton)\nLeon Reznik (Rochester Institute of Technology)\nPablo Rodrigez (Telefonica)\nSumit Roy (Universidade de Washington)\nAvi Rubin (Universidade Johns Hopkins)\nDan Rubenstein (Universidade de Columbia)\nDouglas Salane (John Jay College)\nDespina Saparilla (Cisco Systems)\nJohn Schanz (Comcast)\nprefácio  XV \nHenning Schulzrinne (Universidade de Columbia)\nMischa Schwartz (Universidade de Columbia)\nArdash Sethi (Universidade de Delaware)\nHarish Sethu (Universidade Drexel)\nK. Sam Shanmugan (Universidade do Kansas)\nPrashant Shenoy (Universidade de Massachusetts)\nClay Shields (Universidade Georgetown)\nSubin Shrestra (Universidade da Pensilvânia)\nBojie Shu (ex-aluno da NYU-Poly)\nMihail L. Sichitiu (Universidade do Estado de NC)\nPeter Steenkiste (Universidade Carnegie Mellon)\nTatsuya Suda (Universidade da Califórnia, Irvine)\nKin Sun Tam (Universidade do Estado de Nova York, Albany)\nDon Towsley (Universidade de Massachusetts)\nDavid Turner (Universidade do Estado da Califórnia, San Bernardino)\nNitin Vaidya (Universidade de Illinois)\nMichele Weigle (Universidade Clemson)\nDavid Wetherall (Universidade de Washington)\nIra Winston (Universidade da Pensilvânia)\nDi Wu (Universidade Sun Yat-sen)\nShirley Wynn (NYU-Poly)\nRaj Yavatkar (Intel)\nYechiam Yemini (Universidade de Columbia)\nMing Yu (Universidade do Estado de Nova York, Binghamton)\nEllen Zegura (Instituto de Tecnologia da Geórgia)\nHonggang Zhang (Universidade Suffolk)\nHui Zhang (Universidade Carnegie Mellon)\nLixia Zhang (Universidade da Califórnia, Los Angeles)\nMeng Zhang (ex-aluno da NYU-Poly)\nShuchun Zhang (ex-aluno da Universidade da Pensilvânia)\nXiaodong Zhang (Universidade do Estado de Ohio)\nZhiLi Zhang (Universidade de Minnesota)\nPhil Zimmermann (consultor independente)\nCliff C. Zou (Universidade Central Florida)\nQueremos agradecer, também, a toda a equipe da Addison-Wesley — em particular, a Michael Hirsch, \nMarilyn Lloyd e Emma Snider — que fez um trabalho realmente notável nesta sexta edição (e que teve de \nsuportar dois autores muito complicados e quase sempre atrasados). Agradecemos aos artistas gráficos Janet \nTheurer e Patrice Rossi Calkin, pelo trabalho que executaram nas belas figuras deste livro, e a Andrea Stefa-\nnowicz e sua equipe na PreMediaGlobal, pelo maravilhoso trabalho de produção desta edição. Por fim, um \nagradecimento muito especial a Michael Hirsch, nosso editor na Addison-Wesley, e a Susan Hartman, nossa \nantiga editora. Este livro não seria o que é (e talvez nem tivesse existido) sem a administração cordial de am-\nbos, constante incentivo, paciência quase infinita, bom humor e perseverança.\n   Redes de computadores e a Internet\nXVI\nAgradecimentos — Edição brasileira\nAgradecemos a todos os profissionais envolvidos na produção desta edição no Brasil, em especial ao Prof. \nDr. Wagner Luiz Zucchi (Escola Politécnica da USP, Instituto de Pesquisas Tecnológicas — IPT — e Universidade \nNove de Julho), por sua dedicação e empenho na revisão técnica do conteúdo.\nMateriais adicionais\nA Sala Virtual (<sv.pearson.com.br>) oferece recursos adicionais que auxiliarão professores e \nalunos na exposição das aulas e no processo de aprendizagem.\nPara o professor:\n• Apresentações em PowerPoint\n• Manual de soluções (em inglês)\nPara o aluno:\n• Exercícios autocorrigíveis (10 por capítulo)\n• Exercícios interativos\n• Material de aprendizagem interativo (applets e códigos-fonte)\n• Tarefas extras de programação – Python e Java (em inglês)\n• Wireshark labs (em inglês)\n• Exemplos para implementação de laboratórios (em inglês)\nCapítulo 1\t Redes de computadores e a Internet\t\n1\n1.1\t\nO que é a Internet?\t\n2\n1.1.1\t\nUma descrição dos componentes da rede\t\n3\n1.1.2\t\nUma descrição do serviço\t\n4\n1.1.3\t\nO que é um protocolo?\t\n5\n1.2\t\nA periferia da Internet\t\n7\n1.2.1\t\nRedes de acesso\t\n8\n1.2.2\t\nMeios físicos\t\n14\n1.3\t\nO núcleo da rede\t\n16\n1.3.1\t\nComutação de pacotes\t\n16\n1.3.2\t\nComutação de circuitos\t\n20\n1.3.3\t\nUma rede de redes\t\n23\n1.4\t\nAtraso, perda e vazão em redes de comutação de pacotes\t\n26\n1.4.1\t\nUma visão geral de atraso em redes de comutação de pacotes\t\n26\n1.4.2\t\nAtraso de fila e perda de pacote\t\n29\n1.4.3\t\nAtraso fim a fim\t\n31\n1.4.4\t\nVazão nas redes de computadores\t\n32\n1.5\t\nCamadas de protocolo e seus modelos de serviço\t\n35\n1.5.1\t\nArquitetura de camadas\t\n35\n1.5.2\t\nEncapsulamento\t\n39\n1.6\t\nRedes sob ameaça\t\n41\n1.7\t\nHistória das redes de computadores e da Internet\t\n44\n1.7.1\t\nDesenvolvimento da comutação de pacotes: 1961-1972\t\n44\n1.7.2\t\nRedes proprietárias e trabalho em rede: 1972-1980\t\n46\n1.7.3\t\nProliferação de redes: 1980-1990\t\n46\n1.7.4\t\nA explosão da Internet: a década de 1990\t\n47\n1.7.5\t\nO novo milênio\t\n48\n1.8 Resumo\t\n48\nExercícios de fixação e perguntas\t\n50\t\nProblemas\t\n52\nWireshark Lab\t\n57\nEntrevista: Leonard Kleinrock\t\n58\nSumário\n   Redes de computadores e a Internet\nXVIII\nCapítulo 2\t Camada de aplicação\t\n61\n2.1\t\nPrincípios de aplicações de rede\t\n62\n2.1.1\t\nArquiteturas de aplicação de rede\t\n62\n2.1.2\t\nComunicação entre processos\t\n65\n2.1.3\t\nServiços de transporte disponíveis para aplicações\t\n66\n2.1.4\t\nServiços de transporte providos pela Internet\t\n68\n2.1.5\t\nProtocolos de camada de aplicação\t\n71\n2.1.6\t\nAplicações de rede abordadas neste livro\t\n71\n2.2\t\nA Web e o HTTP\t\n72\n2.2.1\t\nDescrição geral do HTTP\t\n72\n2.2.2\t\nConexões persistentes e não persistentes\t\n73\n2.2.3\t\nFormato da mensagem HTTP\t\n76\n2.2.4\t\nInteração usuário-servidor: cookies\t\n79\n2.2.5\t\nCaches Web\t\n81\n2.2.6\t\nGET condicional\t\n83\n2.3\t\nTransferência de arquivo: FTP\t\n85\n2.3.1\t\nComandos e respostas FTP\t\n86\n2.4\t\nCorreio eletrônico na Internet\t\n87\n2.4.1\t\nSMTP\t\n88\n2.4.2\t\nComparação com o HTTP\t\n91\n2.4.3\t\nFormatos de mensagem de correio\t\n91\n2.4.4\t\nProtocolos de acesso ao correio\t\n92\n2.5\t\nDNS: o serviço de diretório da Internet\t\n95\n2.5.1\t\nServiços fornecidos pelo DNS\t\n96\n2.5.2\t\nVisão geral do modo de funcionamento do DNS\t\n97\n2.5.3\t\nRegistros e mensagens DNS\t\n102\n2.6\t\nAplicações P2P\t\n106\n2.6.1\t\nDistribuição de arquivos P2P\t\n106\n2.6.2\t\nDistributed Hash Tables (DHTs)\t\n111\n2.7\t\nProgramação de sockets: criando aplicações de rede\t\n115\n2.7.1\t\nProgramação de sockets com UDP\t\n116\n2.7.2\t\nProgramação de sockets com TCP\t\n119\n2.8\t\nResumo\t\n123\nExercícios de fixação e perguntas\t\n124\t\nProblemas\t\n125\nTarefas de programação de sockets\t\n131\nWireshark Lab: HTTP\t\n132\nWireshark Lab: DNS\t\n132\nEntrevista: Marc Andreessen\t\n133\nCapítulo 3\t Camada de transporte\t\n135\n3.1\t\nIntrodução e serviços de camada de transporte\t\n135\n3.1.1\t\nRelação entre as camadas de transporte e de rede\t\n137\n3.1.2\t\nVisão geral da camada de transporte na Internet\t\n138\n3.2\t\nMultiplexação e demultiplexação\t\n139\n3.3\t\nTransporte não orientado para conexão: UDP\t\n145\n3.3.1\t\nEstrutura do segmento UDP\t\n147\n3.3.2\t\nSoma de verificação UDP\t\n148\n3.4\t\nPrincípios da transferência confiável de dados\t\n149\n3.4.1\t\nConstruindo um protocolo de transferência confiável de dados\t\n151\n3.4.2\t\nProtocolos de transferência confiável de dados com paralelismo\t\n159\n3.4.3\t\nGo-Back-N (GBN)\t\n161\nXIX \nsumário  \n3.4.4\t\nRepetição seletiva (SR)\t\n164\n3.5\t\nTransporte orientado para conexão: TCP\t\n168\n3.5.1\t\nA conexão TCP\t\n169\n3.5.2\t\nEstrutura do segmento TCP\t\n171\n3.5.3\t\nEstimativa do tempo de viagem de ida e volta e de esgotamento \nde temporização\t\n175\n3.5.4\t\nTransferência confiável de dados\t\n177\n3.5.5\t\nControle de fluxo\t\n184\n3.5.6\t\nGerenciamento da conexão TCP\t\n185\n3.6\t\nPrincípios de controle de congestionamento\t\n190\n3.6.1\t\nAs causas e os custos do congestionamento\t\n190\n3.6.2\t\nMecanismos de controle de congestionamento\t\n195\n3.6.3\t\nExemplo de controle de congestionamento assistido pela rede: \ncontrole de congestionamento ATM ABR\t\n196\n3.7\t\nControle de congestionamento no TCP\t\n198\n3.7.1\t\nEquidade\t\n205\n3.8\t\nResumo\t\n208\nExercícios de fixação e perguntas \t\n210\t\nProblemas\t\n212\nTarefa de programação\t\n221\nWireshark Lab: explorando TCP\t\n221\nWireshark Lab: explorando UDP\t\n221\nEntrevista: Van Jacobson\t\n222\nCapítulo 4\t A camada de rede\t\n224\n4.1\t\nIntrodução\t\n225\n4.1.1\t\nRepasse e roteamento\t\n225\n4.1.2\t\nModelos de serviço de rede\t\n228\n4.2\t\nRedes de circuitos virtuais e de datagramas\t\n230\n4.2.1\t\nRedes de circuitos virtuais\t\n231\n4.2.2\t\nRedes de datagramas\t\n233\n4.2.3\t\nOrigens das redes de circuitos virtuais e de datagramas\t\n235\n4.3\t\nO que há dentro de um roteador?\t\n235\n4.3.1\t\nProcessamento de entrada\t\n237\n4.3.2\t\nElemento de comutação\t\n239\n4.3.3\t\nProcessamento de saída\t\n241\n4.3.4\t\nOnde ocorre formação de fila?\t\n241\n4.3.5\t\nO plano de controle de roteamento\t\n243\n4.4\t\nO Protocolo da Internet (IP): repasse e endereçamento na Internet\t\n244\n4.4.1\t\nFormato do datagrama\t\n245\n4.4.2\t\nEndereçamento IPv4\t\n249\n4.4.3\t\nProtocolo de Mensagens de Controle da Internet (ICMP)\t\n260\n4.4.4\t\nIPv6\t\n263\n4.4.5\t\nUma breve investida em segurança IP\t\n267\n4.5\t\nAlgoritmos de roteamento\t\n268\n4.5.1\t\nO algoritmo de roteamento de estado de enlace (LS)\t\n271\n4.5.2\t\nO algoritmo de roteamento de vetor de distâncias (DV)\t\n274\n4.5.3\t\nRoteamento hierárquico\t\n280\n4.6\t\nRoteamento na Internet\t\n283\n4.6.1\t\nRoteamento intra-AS na Internet: RIP\t\n283\n4.6.2\t\nRoteamento intra-AS na Internet: OSPF\t\n286\n4.6.3\t\nRoteamento inter-AS: BGP\t\n288\n4.7\t\nRoteamento por difusão e para um grupo\t\n295\n   Redes de computadores e a Internet\nXX\n4.7.1\t\nAlgoritmos de roteamento por difusão (broadcast)\t\n295\n4.7.2\t\nServiço para um grupo (multicast)\t\n299\n4.8\t\nResumo\t\n305\nExercícios de fixação e perguntas\t\n306\nProblemas\t\n308\nTarefas de programação de sockets\t\n317\nTarefas de programação\t\n318\nWireshark Lab\t\n318\nEntrevista: Vinton G. Cerf\t\n319\nCapítulo 5\t Camada de enlace: enlaces, redes de acesso e redes locais\t\n321\n5.1\t\nIntrodução à camada de enlace\t\n322\n5.1.1\t\nOs serviços fornecidos pela camada de enlace\t\n323\n5.1.2\t\nOnde a camada de enlace é implementada?\t\n324\n5.2\t\nTécnicas de detecção e correção de erros\t\n325\n5.2.1\t\nVerificações de paridade\t\n326\n5.2.2\t\nMétodos de soma de verificação\t\n328\n5.2.3\t\nVerificação de redundância cíclica (CRC)\t\n328\n5.3\t\nEnlaces e protocolos de acesso múltiplo\t\n330\n5.3.1\t\nProtocolos de divisão de canal\t\n332\n5.3.2\t\nProtocolos de acesso aleatório\t\n333\n5.3.3\t\nProtocolos de revezamento\t\n340\n5.3.4\t\nDOCSIS: O protocolo da camada de enlace para acesso \nà Internet a cabo\t\n341\n5.4\t\nRedes locais comutadas\t\n342\n5.4.1\t\nEndereçamento na camada de enlace e ARP\t\n343\n5.4.2\t\nEthernet\t\n348\n5.4.3\t\nComutadores da camada de enlace\t\n352\n5.4.4\t\nRedes locais virtuais (VLANs)\t\n357\n5.5\t\nVirtualização de enlace: uma rede como camada de enlace\t\n359\n5.5.1\t\nMultiprotocol Label Switching (MPLS)\t\n360\n5.6\t\nRedes do datacenter\t\n362\n5.7\t\nUm dia na vida de uma solicitação de página Web\t\n366\n5.7.1\t\nComeçando: DHCP\n, UDP\n, IP e Ethernet\t\n367\n5.7.2\t\nAinda começando: DNS, ARP\t\n368\n5.7.3\t\nAinda começando: roteamento intradomínio ao servidor DNS\t\n369\n5.7.4\t\nInteração cliente-servidor Web: TCP e HTTP\t\n369\n5.8\t\nResumo\t\n370\nExercícios de fixação e perguntas\t\n372\t\nProblemas\t\n373\nWireshark Lab\t\n378\nEntrevista: Simon S. Lam\t\n378\nCapítulo 6\t Redes sem fio e redes móveis\t\n380\n6.1\t\nIntrodução\t\n381\n6.2\t\nCaracterísticas de enlaces e redes sem fio\t\n384\n6.2.1\t\nCDMA\t\n387\n6.3\t\nWi-Fi: LANs sem fio 802.11\t\n389\n6.3.1\t\nA arquitetura 802.11\t\n390\n6.3.2\t\nO protocolo MAC 802.11\t\n393\n6.3.3\t\nO quadro IEEE 802.11\t\n397\n6.3.4\t\nMobilidade na mesma sub-rede IP\t\n400\n6.3.5\t\nRecursos avançados em 802.11\t\n401\nXXI \nsumário  \n6.3.6\t\nRedes pessoais: Bluetooth e Zigbee\t\n402\n6.4\t\nAcesso celular à Internet\t\n404\n6.4.1\t\nVisão geral da arquitetura de rede celular\t\n405\n6.4.2\t\nRedes de dados celulares 3G: estendendo a Internet a \nassinantes de celular\t\n406\n6.4.3\t\nNo caminho para o 4G: LTE\t\n409\n6.5\t\nGerenciamento da mobilidade: princípios\t\n410\n6.5.1\t\nEndereçamento\t\n412\n6.5.2\t\nRoteamento para um nó móvel\t\n413\n6.6\t\nIP móvel\t\n418\n6.7\t\nGerenciamento de mobilidade em redes celulares\t\n421\n6.7.1\t\nRoteando chamadas para um usuário móvel\t\n421\n6.7.2\t\nTransferências (handoffs) em GSM\t\n423\n6.8\t\nRedes sem fio e mobilidade: impacto sobre protocolos de camadas superiores\t\n425\n6.9\t\nResumo\t\n427\nExercícios de fixação e perguntas\t\n427\nProblemas\t\n429\nWireshark Lab\t\n431\nEntrevista: Deborah Estrin\t\n431\nCapítulo 7\t Redes multimídia\t\n433\n7.1\t\nAplicações de rede multimídia\t\n434\n7.1.1\t\nPropriedades de vídeo\t\n434\n7.1.2\t\nPropriedades de áudio\t\n435\n7.1.3\t\nTipos de aplicações de redes multimídia\t\n436\n7.2\t\nVídeo de fluxo contínuo armazenado\t\n437\n7.2.1\t\nUDP de fluxo contínuo\t\n439\n7.2.2\t\nHTTP de fluxo contínuo\t\n439\n7.2.3\t\nFluxo contínuo adaptativo e DASH\t\n443\n7.2.4\t\nRedes de distribuição de conteúdo\t\n444\n7.2.5\t\nEstudos de caso: Netflix, YouTube e KanKan\t\n449\n7.3\t\nVoice-over-IP\t\n452\n7.3.1\t\nAs limitações de um serviço IP de melhor esforço\t\n452\n7.3.2\t\nEliminação da variação de atraso no receptor para áudio\t\n453\n7.3.3\t\nRecuperação de perda de pacotes\t\n456\n7.3.4\t\nEstudo de caso: Voip com Skype\t\n458\n7.4\t\nProtocolos para aplicações interativas em tempo real\t\n460\n7.4.1\t\nProtocolo de Tempo Real (RTP)\t\n460\n7.4.2\t\nSIP\t\n463\n7.5\t\nSuporte de rede para multimídia\t\n467\n7.5.1\t\nDimensionando redes de melhor esforço\t\n468\n7.5.2\t\nFornecendo múltiplas classes de serviço\t\n469\n7.5.3\t\nDiffserv\t\n478\n7.5.4\t\nGarantias de QoS por conexão: reserva de recurso e \nadmissão de chamada\t\n481\n7.6\t\nResumo\t\n484\nExercícios de fixação e perguntas\t\n484\nProblemas\t\n486\nTarefa de programação\t\n492\nEntrevista: Henning Schulzrinne\t\n492\nCapítulo 8\t Segurança em redes de computadores\t\n495\n8.1\t\nO que é segurança de rede?\t\n496\n8.2\t\nPrincípios de criptografia\t\n497\n   Redes de computadores e a Internet\nXXII\n8.2.1\t\nCriptografia de chaves simétricas\t\n498\n8.2.2\t\nCriptografia de chave pública\t\n503\n8.3\t\nIntegridade de mensagem e assinaturas digitais\t\n507\n8.3.1\t\nFunções de hash criptográficas\t\n508\n8.3.2\t\nCódigo de autenticação da mensagem\t\n509\n8.3.3\t\nAssinaturas digitais\t\n510\n8.4\t\nAutenticação do ponto final\t\n515\n8.4.1\t\nProtocolo de autenticação ap1.0\t\n516\n8.4.2\t\nProtocolo de autenticação ap2.0\t\n516\n8.4.3\t\nProtocolo de autenticação ap3.0\t\n517\n8.4.4\t\nProtocolo de autenticação ap3.1\t\n518\n8.4.5\t\nProtocolo de autenticação ap4.0\t\n518\n8.5\t\nProtegendo o e-mail\t\n519\n8.5.1\t\nE-mail seguro\t\n520\n8.5.2\t\nPGP\t\n522\n8.6\t\nProtegendo conexões TCP: SSL\t\n523\n8.6.1\t\nUma visão abrangente\t\n525\n8.6.2\t\nUma visão mais completa\t\n527\n8.7\t\nSegurança na camada de rede: IPsec e redes virtuais privadas\t\n528\n8.7.1\t\nIPsec e redes virtuais privadas (VPNs) \t\n529\n8.7.2\t\nOs protocolos AH e ESP\t\n530\n8.7.3\t\nAssociações de segurança\t\n530\n8.7.4\t\nO datagrama IPsec\t\n531\n8.7.5\t\nIKE: Gerenciamento de chave no IPsec\t\n533\n8.8\t\nSegurança de LANs sem fio\t\n534\n8.8.1\t\nPrivacidade Equivalente Cabeada (WEP)\t\n534\n8.8.2\t\nIEEE 802.11i\t\n536\n8.9\t\nSegurança operacional: firewalls e sistemas de detecção de invasão\t\n538\n8.9.1\t\nFirewalls \t\n538\n8.9.2\t\nSistemas de detecção de invasão\t\n544\n8.10\t Resumo\t\n546\nExercícios de fixação e perguntas\t\n547\nProblemas\t\n549\nWireshark Lab\t\n553\nIPsec Lab\t\n553\nEntrevista: Steven M. Bellovin\t\n553\nCapítulo 9\t Gerenciamento de rede\t\n555\n9.1\t\nO que é gerenciamento de rede?\t\n555\n9.2\t\nA infraestrutura do gerenciamento de rede\t\n558\n9.3\t\nA estrutura de gerenciamento padrão da Internet\t\n562\n9.3.1\t\nSMI (Estrutura de Informações de Gerenciamento)\t\n563\n9.3.2\t\nBase de informações de gerenciamento: MIB\t\n566\n9.3.3\t\nOperações do protocolo SNMP e mapeamentos de transporte\t\n568\n9.3.4\t\nSegurança e administração\t\n570\n9.4\t\nASN.1\t\n572\n9.5\t\nConclusão\t\n576\nExercícios de fixação e perguntas\t\n576\nProblemas \t\n577\nEntrevista: Jennifer Rexford\t\n578\n\t\nReferências\t\n580\n\t\nÍndice\t\n607\nA Internet de hoje é provavelmente o maior sistema de engenharia já criado pela humanidade, com cente-\nnas de milhões de computadores conectados, enlaces de comunicação e comutadores; bilhões de usuários que se \nconectam por meio de laptops, tablets e smartphones; e com uma série de dispositivos como sensores, webcams, \nconsole para jogos, quadros de imagens, e até mesmo máquinas de lavar sendo conectadas. Dado que a Internet \né tão ampla e possui inúmeros componentes e utilidades, há a possibilidade de compreender como ela funciona? \nExistem princípios de orientação e estrutura que forneçam um fundamento para a compreensão de um sistema \nsurpreendentemente complexo e abrangente? Se a resposta for sim, é possível que, nos dias de hoje, seja interes-\nsante e divertido aprender sobre rede de computadores? Felizmente, as respostas para todas essas perguntas é um \nretumbante SIM! Na verdade, nosso objetivo neste livro é fornecer uma introdução moderna ao campo dinâmico \ndas redes de computadores, apresentando os princípios e o entendimento prático necessários para utilizar não \napenas as redes de hoje, como também as de amanhã.\nO primeiro capítulo apresenta um panorama de redes de computadores e da Internet. Nosso objetivo é pin-\ntar um quadro amplo e estabelecer um contexto para o resto deste livro, para ver a floresta por entre as árvores. \nCobriremos um terreno bastante extenso neste capítulo de introdução e discutiremos várias peças de uma rede \nde computadores, sem perder de vista o quadro geral.\nO panorama geral de redes de computadores que apresentaremos neste capítulo será estruturado como segue. \nApós apresentarmos brevemente a terminologia e os conceitos fundamentais, examinaremos primeiro os compo-\nnentes básicos de hardware e software que compõem uma rede. Partiremos da periferia da rede e examinaremos os \nsistemas finais e aplicações de rede executados nela. Consideraremos os serviços de transporte fornecidos a essas \naplicações. Em seguida exploraremos o núcleo de uma rede de computadores examinando os enlaces e comutadores \nque transportam dados, bem como as redes de acesso e meios físicos que conectam sistemas finais ao núcleo da rede. \nAprenderemos que a Internet é uma rede de redes e observaremos como essas redes se conectam umas com as outras.\nApós concluirmos essa revisão sobre a periferia e o núcleo de uma rede de computadores, adotaremos uma \nvisão mais ampla e mais abstrata na segunda metade deste capítulo. Examinaremos atraso, perda e vazão de \ndados em uma rede de computadores e forneceremos modelos quantitativos simples para a vazão e o atraso fim \na fim: modelos que levam em conta atrasos de transmissão, propagação e enfileiramento. Depois apresentaremos \nalguns princípios fundamentais de arquitetura em redes de computadores, a saber: protocolos em camadas e \nmodelos de serviço. Aprenderemos, também, que as redes de computadores são vulneráveis a diferentes tipos de \nameaças; analisaremos algumas dessas ameaças e como a rede pode se tornar mais segura. Por fim, encerraremos \neste capítulo com um breve histórico da computação em rede.\n2\n1\n3 4 5 6\n8 9\n7\nc\n \na\n \np\n \ní\n \nt\n \nu\n \nl\n \no\nRedes de \ncomputadores\ne a Internet\n   Redes de computadores e a Internet\n2\n1.1  O que é a Internet?\nNeste livro, usamos a Internet pública, uma rede de computadores específica, como o veículo principal para \ndiscutir as redes de computadores e seus protocolos. Mas o que é a Internet? Há diversas maneiras de responder \na essa questão. Primeiro, podemos descrever detalhadamente os aspectos principais da Internet, ou seja, os com-\nponentes de software e hardware básicos que a formam. Segundo, podemos descrever a Internet em termos de \numa infraestrutura de redes que fornece serviços para aplicações distribuídas. Iniciaremos com a descrição dos \ncomponentes, utilizando a Figura 1.1 como ilustração para a nossa discussão.\nFigura 1.1  Alguns componentes da Internet\nKR 01.01.eps\nAW/Kurose and Ross\nLegenda:\nHost \n(ou sistema ﬁnal)\nServidor\nMóvel\nRoteador\nComutador \nda camada \nde enlace (switch)\nModem\nEstação-\n-base\nSmartphone\nTorre de \ntelefonia \ncelular\nISP nacional \nou global\nRede móvel\nISP local \nou regional\nRede corporativa\nRede doméstica\nRedes de computadores e a Internet  3 \n1.1.1  Uma descrição dos componentes da rede\nA Internet é uma rede de computadores que interconecta centenas de milhões de dispositivos de compu­\ntação ao redor do mundo. Há pouco tempo, esses dispositivos eram basicamente PCs de mesa, estações de tra-\nbalho Linux, e os assim chamados servidores que armazenam e transmitem informações, como páginas da Web \ne mensagens de e-mail. No entanto, cada vez mais sistemas finais modernos da Internet, como TVs, laptops, \nconsoles para jogos, telefones celulares, webcams, automóveis, dispositivos de sensoriamento ambiental, quadros \nde imagens, e sistemas internos elétricos e de segurança, estão sendo conectados à rede. Na verdade, o termo rede \nde computadores está começando a soar um tanto desatualizado, dados os muitos equipamentos não tradicionais \nque estão sendo ligados à Internet. No jargão da rede, todos esses equipamentos são denominados hospedeiros \nou sistemas finais. Em julho de 2011, havia cerca de 850 milhões de sistemas finais ligados à Internet [ISC, 2012], \nsem contar os smartphones, laptops e outros dispositivos que são conectados à rede de maneira intermitente. No \ntodo, estima-se que haja 2 bilhões de usuários na Internet [ITU, 2011].\nSistemas finais são conectados entre si por enlaces (links) de comunicação e comutadores (switches) de \npacotes. Na Seção 1.2, veremos que há muitos tipos de enlaces de comunicação, que são constituídos de diferen-\ntes tipos de meios físicos, entre eles cabos coaxiais, fios de cobre, fibras óticas e ondas de rádio. Enlaces diferentes \npodem transmitir dados em taxas diferentes, sendo a taxa de transmissão de um enlace medida em bits por \nsegundo. Quando um sistema final possui dados para enviar a outro sistema final, o sistema emissor segmenta \nesses dados e adiciona bytes de cabeçalho a cada segmento. Os pacotes de informações resultantes, conhecidos \ncomo pacotes no jargão de rede de computadores, são enviados através da rede ao sistema final de destino, onde \nsão remontados para os dados originais.\nUm comutador de pacotes encaminha o pacote que está chegando em um de seus enlaces de comunicação \nde entrada para um de seus enlaces de comunicação de saída. Há comutadores de pacotes de todos os tipos e \nformas, mas os dois mais proeminentes na Internet de hoje são roteadores e comutadores de camada de enla-\nce. Esses dois tipos de comutadores encaminham pacotes a seus destinos finais. Os comutadores de camada de \nenlace geralmente são utilizados em redes de acesso, enquanto os roteadores são utilizados principalmente no \nnúcleo da rede. A sequência de enlaces de comunicação e comutadores de pacotes que um pacote percorre desde \no sistema final remetente até o sistema final receptor é conhecida como rota ou caminho através da rede. É difícil \nde estimar a exata quantidade de tráfego na Internet, mas a Cisco [Cisco VNI, 2011] estima que o tráfego global \nda Internet esteve perto do 40 exabytes por mês em 2012.\nAs redes comutadas por pacotes (que transportam pacotes) são, de muitas maneiras, semelhantes às redes \nde transporte de rodovias, estradas e cruzamentos (que transportam veículos). Considere, por exemplo, uma \nfábrica que precise transportar uma quantidade de carga muito grande a algum depósito localizado a milhares \nde quilômetros. Na fábrica, a carga é dividida e carregada em uma frota de caminhões. Cada caminhão viaja, de \nmodo independente, pela rede de rodovias, estradas e cruzamentos ao depósito de destino. No depósito, a carga \né descarregada e agrupada com o resto da carga pertencente à mesma remessa. Deste modo, os pacotes se asse-\nmelham aos caminhões, os enlaces de comunicação representam rodovias e estradas, os comutadores de pacote \nseriam os cruzamentos e cada sistema final se assemelha aos depósitos. Assim como o caminhão faz o percurso \npela rede de transporte, o pacote utiliza uma rede de computadores.\nSistemas finais acessam a Internet por meio de Provedores de Serviços de Internet (Internet Service Providers \n— ISPs), entre eles ISPs residenciais como empresas de TV a cabo ou empresas de telefonia; corporativos, de univer-\nsidades e ISPs que fornecem acesso sem fio em aeroportos, hotéis, cafés e outros locais públicos. Cada ISP é uma \nrede de comutadores de pacotes e enlaces de comunicação. ISPs oferecem aos sistemas finais uma variedade de tipos \nde acesso à rede, incluindo acesso residencial de banda larga como modem a cabo ou DSL (linha digital de assinan-\nte), acesso por LAN de alta velocidade, acesso sem fio e acesso por modem discado de 56 kbits/s. ISPs também for-\nnecem acesso a provedores de conteúdo, conectando sites diretamente à Internet. Esta se interessa pela conexão \nentre os sistemas finais, portanto os ISPs que fornecem acesso a esses sistemas também devem se interconectar. Esses \nISPs de nível mais baixo são interconectados por meio de ISPs de nível mais alto, nacionais e internacionais, como \nLevel 3 Communications, AT&T, Sprint e NTT. Um ISP de nível mais alto consiste em roteadores de alta velocidade \n   Redes de computadores e a Internet\n4\ninterconectados com enlaces de fibra ótica de alta velocidade. Cada rede ISP, seja de nível mais alto ou mais baixo, é \ngerenciada de forma independente, executa o protocolo IP (ver adiante) e obedece a certas convenções de nomeação \ne endereço. Examinaremos ISPs e sua interconexão mais em detalhes na Seção 1.3.\nOs sistemas finais, os comutadores de pacotes e outras peças da Internet executam protocolos que controlam o \nenvio e o recebimento de informações. O TCP (Transmission Control Protocol — Protocolo de Controle de Trans-\nmissão) e o IP (Internet Protocol — Protocolo da Internet) são dois dos mais importantes da Internet. O protocolo \nIP especifica o formato dos pacotes que são enviados e recebidos entre roteadores e sistemas finais. Os principais \nprotocolos da Internet são conhecidos como TCP/IP. Começaremos a examinar protocolos neste capítulo de intro-\ndução. Mas isso é só um começo — grande parte deste livro trata de protocolos de redes de computadores!\nDada a importância de protocolos para a Internet, é adequado que todos concordem sobre o que cada um \ndeles faz, de modo que as pessoas possam criar sistemas e produtos que operem entre si. É aqui que os padrões \nentram em ação. Padrões da Internet são desenvolvidos pela IETF (Internet Engineering Task Force — Força \nde Trabalho de Engenharia da Internet) [IETF, 2012]. Os documentos padronizados da IETF são denominados \nRFCs (Request For Comments — pedido de comentários). Os RFCs começaram como solicitações gerais de co-\nmentários (daí o nome) para resolver problemas de arquitetura que a precursora da Internet enfrentava [Allman, \n2011]. Os RFCs costumam ser bastante técnicos e detalhados. Definem protocolos como TCP, IP, HTTP (para a \nWeb) e SMTP (para e-mail). Hoje, existem mais de 6.000 RFCs. Outros órgãos também especificam padrões para \ncomponentes de rede, principalmente para enlaces. O IEEE 802 LAN/MAN Standards Committee [IEEE 802, \n2009], por exemplo, especifica os padrões Ethernet e Wi-Fi sem fio.\n1.1.2  Uma descrição do serviço\nA discussão anterior identificou muitos dos componentes que compõem a Internet. Mas também po-\ndemos descrevê-la partindo de um ângulo completamente diferente — ou seja, como uma infraestrutura que \nprovê serviços a aplicações. Tais aplicações incluem correio eletrônico, navegação na Web, redes sociais, men-\nsagem instantânea, Voz sobre IP (VoIP), vídeo em tempo real, jogos distribuídos, compartilhamento de arquivos \npeer-to­\n‑peer (P2P), televisão pela Internet, login remoto e muito mais. Essas aplicações são conhecidas como \naplicações distribuídas, uma vez que envolvem diversos sistemas finais que trocam informações mutuamente. \nDe forma significativa, as aplicações da Internet são executadas em sistemas finais — e não em comutadores de \npacote no núcleo da rede. Embora os comutadores de pacotes facilitem a troca de dados entre os sistemas finais, \neles não estão relacionados com a aplicação, que é a origem ou o destino dos dados.\nVamos explorar um pouco mais o significado de uma infraestrutura que fornece serviços a aplicações. Nesse \nsentido, suponha que você tenha uma grande ideia para uma aplicação distribuída para a Internet, uma que possa \nbeneficiar bastante a humanidade ou que simplesmente o enriqueça e o torne famoso. Como transformar essa \nideia em uma aplicação real da Internet? Como as aplicações são executadas em sistemas finais, você precisará \ncriar programas que sejam executados em sistemas finais. Você poderia, por exemplo, criar seus programas em \nJava, C ou Python. Agora, já que você está desenvolvendo uma aplicação distribuída para a Internet, os progra-\nmas executados em diferentes sistemas finais precisarão enviar dados uns aos outros. E, aqui, chegamos ao as-\nsunto principal — o que leva ao modo alternativo de descrever a Internet como uma plataforma para aplicações. \nDe que modo um programa, executado em um sistema final, orienta a Internet a enviar dados a outro programa \nexecutado em outro sistema final?\nOs sistemas finais ligados à Internet oferecem uma Interface de Programação de Aplicação (API) que es-\npecifica como o programa que é executado no sistema final solicita à infraestrutura da Internet que envie dados \na um programa de destino específico, executado em outro sistema final. Essa API da Internet é um conjunto de \nregras que o software emissor deve cumprir para que a Internet seja capaz de enviar os dados ao programa de \ndestino. Discutiremos a API da Internet mais detalhadamente no Capítulo 2. Agora, vamos traçar uma simples \ncomparação, que será utilizada com frequência neste livro. Suponha que Alice queria enviar uma carta para Bob \nutilizando o serviço postal. Alice, é claro, não pode apenas escrever a carta (os dados) e atirá-la pela janela. \nRedes de computadores e a Internet  5 \nEm vez disso, o serviço postal necessita que ela coloque a carta em um envelope; escreva o nome completo de \nBob, endereço e CEP no centro do envelope; feche; coloque um selo no canto superior direito; e, por fim, leve \no envelope a uma agência de correio oficial. Dessa maneira, o serviço postal possui sua própria “\nAPI de serviço \npostal”\n, ou conjunto de regras, que Alice deve cumprir para que sua carta seja entregue a Bob. De um modo se-\nmelhante, a Internet possui uma API que o software emissor de dados deve seguir para que a Internet envie os \ndados para o software receptor.\nO serviço postal, claro, fornece mais de um serviço a seus clientes: entrega expressa, aviso de recebimento, \ncarta simples e muito mais. De modo semelhante, a Internet provê diversos serviços a suas aplicações. Ao de-\nsenvolver uma aplicação para a Internet, você também deve escolher um dos serviços que a rede oferece. Uma \ndescrição dos serviços será apresentada no Capítulo 2.\nAcabamos de apresentar duas descrições da Internet: uma delas diz respeito a seus componentes de \nhardware e software, e a outra, aos serviços que ela oferece a aplicações distribuídas. Mas talvez você ainda esteja \nconfuso sobre o que é a Internet. O que é comutação de pacotes e TCP/IP? O que são roteadores? Que tipos de \nenlaces de comunicação estão presentes na Internet? O que é uma aplicação distribuída? Como uma torradeira \nou um sensor de variações meteorológicas podem ser ligados à Internet? Se você está um pouco assustado com \ntudo isso agora, não se preocupe — a finalidade deste livro é lhe apresentar os mecanismos da Internet e também \nos princípios que determinam como e por que ela funciona. Explicaremos esses termos e questões importantes \nnas seções e nos capítulos subsequentes.\n1.1.3  O que é um protocolo?\nAgora que já entendemos um pouco o que é a Internet, vamos considerar outra palavra fundamental usada \nem redes de computadores: protocolo. O que é um protocolo? O que um protocolo faz?\nUma analogia humana\nTalvez seja mais fácil entender a ideia de um protocolo de rede de computadores considerando primeiro \nalgumas analogias humanas, já que executamos protocolos o tempo todo. Considere o que você faz quando quer \nperguntar as horas a alguém. Um diálogo comum é ilustrado na Figura 1.2. O protocolo humano (ou as boas \nmaneiras, ao menos) dita que, ao iniciarmos uma comunicação com outra pessoa, primeiro a cumprimentemos \n(o primeiro “oi” da Figura 1.2). A resposta comum para um “oi” é um outro “oi”\n. Implicitamente, tomamos a res-\nposta cordial “oi” como uma indicação de que podemos prosseguir e perguntar as horas. Uma reação diferente \nao “oi” inicial (tal como “Não me perturbe!”\n, “I don’t speak Portuguese!” ou alguma resposta atravessada) poderia \nindicar falta de vontade ou incapacidade de comunicação. Nesse caso, o protocolo humano seria não perguntar \nque horas são. Às vezes, não recebemos nenhuma resposta para uma pergunta, caso em que em geral desistimos \nde perguntar as horas à pessoa. Note que, no nosso protocolo humano, há mensagens específicas que enviamos \ne ações específicas que realizamos em reação às respostas recebidas ou a outros eventos (como nenhuma resposta \napós certo tempo). É claro que mensagens transmitidas e recebidas e ações realizadas quando essas mensagens \nsão enviadas ou recebidas ou quando ocorrem outros eventos desempenham um papel central em um protocolo \nhumano. Se as pessoas executarem protocolos diferentes (por exemplo, se uma pessoa tem boas maneiras, mas a \noutra não; se uma delas entende o conceito de horas, mas a outra não), os protocolos não interagem e nenhum \ntrabalho útil pode ser realizado. O mesmo é válido para redes — é preciso que duas (ou mais) entidades comuni-\ncantes executem o mesmo protocolo para que uma tarefa seja realizada.\nVamos considerar uma segunda analogia humana. Suponha que você esteja assistindo a uma aula (sobre \nredes de computadores, por exemplo). O professor está falando monotonamente sobre protocolos e você está \nconfuso. Ele para e pergunta: “Alguma dúvida?” (uma mensagem que é transmitida a todos os alunos e recebida \npor todos os que não estão dormindo). Você levanta a mão (transmitindo uma mensagem implícita ao profes-\nsor). O professor percebe e, com um sorriso, diz “Sim...” (uma mensagem transmitida, incentivando-o a fazer sua \n   Redes de computadores e a Internet\n6\npergunta — professores adoram perguntas) e você então faz a sua (isto é, transmite sua mensagem ao professor). \nEle ouve (recebe sua mensagem) e responde (transmite uma resposta a você). Mais uma vez, percebemos que a \ntransmissão e a recepção de mensagens e um conjunto de ações convencionais, realizadas quando as mensagens \nsão enviadas e recebidas, estão no centro desse protocolo de pergunta e resposta.\nProtocolos de rede\nUm protocolo de rede é semelhante a um protocolo humano; a única diferença é que as entidades que tro-\ncam mensagens e realizam ações são componentes de hardware ou software de algum dispositivo (por exemplo, \ncomputador, smartphone, tablet, roteador ou outro equipamento habilitado para rede). Todas as atividades na \nInternet que envolvem duas ou mais entidades remotas comunicantes são governadas por um protocolo. Por \nexemplo, protocolos executados no hardware de dois computadores conectados fisicamente controlam o fluxo de \nbits no “cabo” entre as duas placas de interface de rede; protocolos de controle de congestionamento em sistemas \nfinais controlam a taxa com que os pacotes são transmitidos entre a origem e o destino; protocolos em roteadores \ndeterminam o caminho de um pacote da origem ao destino. Eles estão em execução por toda a Internet e, em \nconsequência, grande parte deste livro trata de protocolos de rede de computadores.\nComo exemplo de um protocolo de rede de computadores com o qual você provavelmente está familiarizado, \nconsidere o que acontece quando fazemos uma requisição a um servidor Web, isto é, quando digitamos o URL de \numa página Web no browser. Isso é mostrado no lado direito da Figura 1.2. Primeiro, o computador enviará uma \nmensagem de requisição de conexão ao servidor Web e aguardará uma resposta. O servidor receberá essa mensa-\ngem de requisição de conexão e retornará uma mensagem de resposta de conexão. Sabendo que agora está tudo cer-\nto para requisitar o documento da Web, o computador envia então o nome da página Web que quer buscar naquele \nservidor com uma mensagem GET. Por fim, o servidor retorna a página (arquivo) para o computador.\nDados o exemplo humano e o exemplo de rede anteriores, as trocas de mensagens e as ações realizadas quando \nessas mensagens são enviadas e recebidas são os elementos fundamentais para a definição de um protocolo:\nFigura 1.2  Um protocolo humano e um protocolo de rede de computadores\nGET http://www.awl.com/kurose-ross\nSolicitação de conexão TCP\nTempo\nTempo\nResposta de conexão TCP\n<arquivo>\nOi\nQue horas são, por favor?\nTempo\nTempo\nOi\n2h00\nRedes de computadores e a Internet  7 \nUm protocolo define o formato e a ordem das mensagens trocadas entre duas ou mais entidades comuni-\ncantes, bem como as ações realizadas na transmissão e/ou no recebimento de uma mensagem ou outro evento.\nA Internet e as redes de computadores em geral fazem uso intenso de protocolos. Diferentes tipos são usados \npara realizar diferentes tarefas de comunicação. À medida que for avançando na leitura deste livro, você perceberá que \nalguns protocolos são simples e diretos, enquanto outros são complexos e intelectualmente profundos. Dominar a área \nde redes de computadores equivale a entender o que são, por que existem e como funcionam os protocolos de rede.\n1.2  A periferia da Internet\nNas seções anteriores, apresentamos uma descrição de alto nível da Internet e dos protocolos de rede. Agora \npassaremos a tratar com um pouco mais de profundidade os componentes de uma rede de computadores (e da \nInternet, em particular). Nesta seção, começamos pela periferia de uma rede e examinamos os componentes com \nos quais estamos mais familiarizados — a saber, computadores, smartphones e outros equipamentos que usamos \ndiariamente. Na seção seguinte, passaremos da periferia para o núcleo da rede e estudaremos comutação e rotea­\nmento em redes de computadores.\nComo descrito na seção anterior, no jargão de rede de computadores, os computadores e outros dispositivos \nconectados à Internet são frequentemente chamados de sistemas finais, pois se encontram na periferia da Inter-\nnet, como mostrado na Figura 1.3. Os sistemas finais da Internet incluem computadores de mesa (por exemplo, \nPCs de mesa, MACs e caixas Linux), servidores (por exemplo, servidores Web e de e-mails), e computadores \nmóveis (por exemplo, notebooks, smartphones e tablets). Além disso, diversos aparelhos alternativos estão sendo \nutilizados com a Internet como sistemas finais (veja nota em destaque).\nSistemas finais também são denominados hospedeiros (ou hosts) porque hospedam (isto é, executam) progra-\nmas de aplicação, tais como um navegador (browser) da Web, um programa servidor da Web, um programa leitor de \ne-mail ou um servidor de e-mail. Neste livro, utilizaremos os termos hospedeiros e sistemas finais como sinônimos. \nUm conjunto impressionante de sistemas finais da Internet\nNão faz muito tempo, os sistemas finais conec-\ntados à Internet eram quase sempre computadores \ntradicionais, como máquinas de mesa e servidores de \ngrande capacidade. Desde o final da década de 1990 \naté hoje, um amplo leque de equipamentos e dispo-\nsitivos interessantes, cada vez mais diversos, vem \nsendo conectado à Internet, aproveitando sua capa-\ncidade de enviar e receber dados digitais. Tendo em \nvista a onipresença da Internet, seus protocolos bem \ndefinidos (padronizados) e a disponibilidade comercial \nde hardware capacitado para ela, é natural usar sua \ntecnologia para interconectar esses dispositivos entre \nsi e a servidores conectados à Internet.\nMuitos deles parecem ter sido criados exclusiva-\nmente para diversão — consoles de videogame (por \nexemplo, Xbox da Microsoft), televisores habilitados \npara Internet, quadros de fotos digitais que baixam e \nexibem imagens digitais, máquinas de lavar, refrige-\nradores e até mesmo uma torradeira da Internet que \nbaixa informações meteorológicas de um servidor \ne grava uma imagem da previsão do tempo do dia \nem questão (por exemplo, nublado, com sol) na sua \ntorrada matinal [BBC, 2001]. Telefones celulares que \nutilizam IP com recursos de GPS permitem o uso fácil \nde serviços dependentes do local (mapas, informa-\nções sobre serviços ou pessoas nas proximidades). \nRedes de sensores incorporadas ao ambiente físico \npermitem a monitoração de prédios, pontes, ativi-\ndade sísmica, habitats da fauna selvagem, estuários \nde rios e clima. Aparelhos biomédicos podem ser in-\ncorporados e conectados em rede, numa espécie de \nrede corporal. Com tantos dispositivos diversificados \nsendo conectados em rede, a Internet está realmente \nse tornando uma “Internet de coisas” [ITU, 2005b].\nHISTÓRICO DO CASO\n   Redes de computadores e a Internet\n8\nÀs vezes, sistemas finais são ainda subdivididos em duas categorias: clientes e servidores. Informalmente, clien-\ntes costumam ser PCs de mesa ou portáteis, smartphones e assim por diante, ao passo que servidores tendem a \nser máquinas mais poderosas, que armazenam e distribuem páginas Web, vídeo em tempo real, retransmissão \nde e-mails e assim por diante. Hoje, a maioria dos servidores dos quais recebemos resultados de busca, e-mail, \npáginas e vídeos reside em grandes datacenters. Por exemplo, o Google tem 30 a 50 datacenters, com muitos \ndeles tendo mais de cem mil servidores.\n1.2.1  Redes de acesso\nTendo considerado as aplicações e sistemas finais na “periferia da Internet”\n, vamos agora considerar a rede \nde acesso — a rede física que conecta um sistema final ao primeiro roteador (também conhecido como “roteador \nFigura 1.3  Interação entre sistemas finais\nRede móvel\nKR 01.03.eps\nAW/Kurose and Ross\nComputer Networking, 6/e\nsize:  28p0 x  37p5 \n9/6/11, 10/28/11, 10/31/11\n11/21/11 rossi \nISP nacional \nou global\nISP local \nou regional\nRede corporativa\nRede doméstica\nRedes de computadores e a Internet  9 \nde borda”) de um caminho partindo de um sistema final até outro qualquer. A Figura 1.4 apresenta diversos tipos \nde redes de acesso com linhas espessas, linhas cinzas e os ambientes (doméstico, corporativo e móvel sem fio) em \nque são usadas.\nAcesso doméstico: DSL, cabo, FTTH, discado e satélite\nHoje, nos países desenvolvidos, mais de 65% dos lares possuem acesso à Internet, e, dentre eles, Coreia, Ho-\nlanda, Finlândia e Suécia lideram com mais de 80%, quase todos por meio de uma conexão de banda larga em alta \nvelocidade [ITU, 2011]. A Finlândia e a Espanha há pouco declararam que o acesso à Internet de alta velocidade \né um “direito legal”\n. Dado a esse interesse intenso no acesso doméstico, vamos começar nossa introdução às redes \nde acesso considerando como os lares se conectam à Internet.\nOs dois tipos de acesso residencial banda largas predominantes são a linha digital de assinante (DSL) ou a \ncabo. Normalmente uma residência obtém acesso DSL à Internet da mesma empresa que fornece acesso telefôni-\nco local com fio (por exemplo, a operadora local). Assim, quando a DSL é utilizada, uma operadora do cliente é \ntambém seu provedor de serviços de Internet (ISP). Como ilustrado na Figura 1.5, o modem DSL de cada cliente \nutiliza a linha telefônica existente (par de fios de cobre trançado, que discutiremos na Seção 1.2.2) para trocar \nFigura 1.4  Redes de acesso\nISP nacional \nou global\nRede móvel\nISP local \nou regional\nRede corporativa\nRede doméstica\nKR 01.04.eps \nAW/Kurose and Ross\nComputer Networking, 6/e\nsize:  28p0 x  36p7\n9/6/11, 10/28/11, 10/31/11\n   Redes de computadores e a Internet\n10\ndados com um multiplexador digital de acesso à linha do assinante (DSLAM), em geral localizado na CT da ope-\nradora. O modem DSL da casa apanha dados digitais e os traduz para sons de alta frequência, para transmissão \npelos fios de telefone até a CT; os sinais analógicos de muitas dessas residências são traduzidos de volta para o \nformato digital no DSLAM.\nA linha telefônica conduz, simultaneamente, dados e sinais telefônicos tradicionais, que são codificados em \nfrequências diferentes:\n• um canal downstream de alta velocidade, com uma banda de 50 kHz a 1 MHZ;\n• um canal upstream de velocidade média, com uma banda de 4 kHz a 50 kHz;\n• um canal de telefone bidirecional comum, com uma banda de 0 a 4 kHz.\nEssa abordagem faz que a conexão DSL pareça três conexões distintas, de modo que um telefonema e a \nconexão com a Internet podem compartilhar a DSL ao mesmo tempo. (Descreveremos essa técnica de multiple-\nxação por divisão de frequência na Seção 1.3.2.) Do lado do consumidor, para os sinais que chegam até sua casa, \num distribuidor separa os dados e os sinais telefônicos e conduz o sinal com os dados para o modem DSL. Na \noperadora, na CT, o DSLAM separa os dados e os sinais telefônicos e envia aqueles para a Internet. Centenas ou \nmesmo milhares de residências se conectam a um único DSLAM [Dischinger, 2007].\nOs padrões DSL definem taxas de transmissão de 12 Mbits/s downstream e 1,8 Mbits/s upstream \n[ITU, 1999] e 24 Mbits/s downstream e 2,5 Mbits/s upstream [ITU, 2003]. Em razão de as taxas de transmissão \ne recebimento serem diferentes, o acesso é conhecido como assimétrico. As taxas reais alcançadas podem ser \nmenores do que as indicadas anteriormente, pois o provedor de DSL pode, de modo proposital, limitar uma taxa \nresidencial quando é oferecido o serviço em camadas (diferentes taxas, disponíveis a diferentes preços), ou por-\nque a taxa máxima pode ser limitada pela distância entre a residência e a CT, pela bitola da linha de par trançado \ne pelo grau de interferência elétrica. Os engenheiros projetaram o DSL expressamente para distâncias curtas entre \na residência e a CT; quase sempre, se a residência não estiver localizada dentro de 8 a 16 quilômetros da CT, ela \nprecisa recorrer a uma forma de acesso alternativa à Internet.\nEmbora o DSL utilize a infraestrutura de telefone local da operadora, o acesso à Internet a cabo utiliza a \ninfraestrutura de TV a cabo da operadora de televisão. Uma residência obtém acesso à Internet a cabo da mesma \nempresa que fornece a televisão a cabo. Como ilustrado na Figura 1.6, as fibras óticas conectam o terminal de \ndistribuição às junções da região, sendo o cabo coaxial tradicional utilizado para chegar às casas e apartamentos \nde maneira individual. Cada junção costuma suportar de 500 a 5.000 casas. Em razão de a fibra e o cabo coaxial \nfazerem parte desse sistema, a rede é denominada híbrida fibra-coaxial (HFC).\nO acesso à Internet a cabo necessita de modems especiais, denominados modems a cabo. Como o DSL, o \nmodem a cabo é, em geral, um aparelho externo que se conecta ao computador residencial pela porta Ethernet. \n(Discutiremos Ethernet em detalhes no Capítulo 5.) No terminal de distribuição, o sistema de término do mo-\ndem a cabo (CMTS) tem uma função semelhante à do DSLAM da rede DSL — transformar o sinal analógico \nFigura 1.5  Acesso à Internet por DSL\nComputador \nresidencial\nTelefone \nresidencial\nModem \nDSL\nInternet\nRede \ntelefônica\nSplitter\nLinha telefônica existente:\ntelefone com 0-4 KHz; \ndados upstream com 4-50 KHz; \ndados downstream\ncom 50 KHZ – 1 MHz\nCentral \ntelefônica\nDSLAM\nRedes de computadores e a Internet  11 \nenviado dos modems a cabo de muitas residências downstream para o formato digital. Os modems a cabo di-\nvidem a rede HFC em dois canais, um de transmissão (downstream) e um de recebimento (upstream). Como a \ntecnologia DSL, o acesso costuma ser assimétrico, com o canal downstream recebendo uma taxa de transmissão \nmaior do que a do canal upstream. O padrão DOCSIS 2.0 define taxas downstream de até 42,8 Mbits/s e taxas \nupstream de até 30,7 Mbits/s. Como no caso das redes DSL, a taxa máxima possível de ser alcançada pode não ser \nobservada por causa de taxas de dados contratadas inferiores ou problemas na mídia.\nUma característica importante do acesso a cabo é o fato de ser um meio de transmissão compartilhado. \nEm especial, cada pacote enviado pelo terminal viaja pelos enlaces downstream até cada residência e cada pacote \nenviado por uma residência percorre o canal upstream até o terminal de transmissão. Por essa razão, se diversos \nusuários estiverem fazendo o download de um arquivo em vídeo ao mesmo tempo no canal downstream, cada \num receberá o arquivo a uma taxa bem menor do que a taxa de transmissão a cabo agregada. Por outro lado, \nse há somente alguns usuários ativos navegando, então cada um poderá receber páginas da Web a uma taxa de \ndownstream máxima, pois esses usuários raramente solicitarão uma página ao mesmo tempo. Como o canal \nupstream também é compartilhado, é necessário um protocolo de acesso múltiplo distribuído para coordenar as \ntransmissões e evitar colisões. (Discutiremos a questão de colisão no Capítulo 5.)\nEmbora as redes DSL e a cabo representem mais de 90% do acesso de banda larga residencial nos Estados \nUnidos, uma tecnologia que promete velocidades ainda mais altas é a implantação da fiber to the home (FTTH) \n[FTTH Council, 2011a]. Como o nome indica, o conceito da FTTH é simples — oferece um caminho de fibra \nótica da CT diretamente até a residência. Nos Estados Unidos, a Verizon saiu na frente com a tecnologia FTTH, \nlançando o serviço FIOS [Verizon FIOS, 2012].\nExistem várias tecnologias concorrentes para a distribuição ótica das CTs às residências. A rede mais sim-\nples é chamada fibra direta, para a qual existe uma fibra saindo da CT para cada casa. Em geral, uma fibra que sai \nda central telefônica é compartilhada por várias residências; ela é dividida em fibras individuais do cliente apenas \napós se aproximar relativamente das casas. Duas arquiteturas concorrentes de rede de distribuição ótica apre-\nsentam essa divisão: redes óticas ativas (AONs) e redes óticas passivas (PONs). A AON é na essência a Ethernet \ncomutada, assunto discutido no Capítulo 5.\nAqui, falaremos de modo breve sobre a PON, que é utilizada no serviço FIOS da Verizon. A Figura 1.7 mos-\ntra a FTTH utilizando a arquitetura de distribuição de PON. Cada residência possui um terminal de rede ótica \n(ONT), que é conectado por uma fibra ótica dedicada a um distribuidor da região. O distribuidor combina certo \nnúmero de residências (em geral menos de 100) a uma única fibra ótica compartilhada, que se liga a um terminal \nde linha ótica (OLT) na CT da operadora. O OLT, que fornece conversão entre sinais ópticos e elétricos, se co-\nFigura 1.6  Uma rede de acesso hÍbrida fibra-coaxial\nKR 01.06.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n33p2 Wide x 15p4 Deep\n10/28/11, 10/31/11,\n11/9/11, 11/16/11 rossi\nCabo de \nﬁbra ótica\nCabo coaxial\nCentenas de \nresidências\nTerminal de distribuição\nCentenas de \nresidências\nNó de \nﬁbra \nótica\nNó de \nﬁbra \nótica\nInternet\nCMTS\n   Redes de computadores e a Internet\n12\nnecta à Internet por meio de um roteador da operadora. Na residência, o usuário conecta ao ONT um roteador \nresidencial (quase sempre sem fio) pelo qual acessa a Internet. Na arquitetura de PON, todos os pacotes enviados \ndo OLT ao distribuidor são nele replicados (semelhante ao terminal de distribuição a cabo).\nA FTTH consegue potencialmente oferecer taxas de acesso à Internet na faixa de gigabits por segundo. Porém, \na maioria dos provedores de FTTH oferece diferentes taxas, das quais as mais altas custam muito mais. A velocidade \nde downstream média dos clientes FTTH nos Estados Unidos era de mais ou menos 20 Mbits/s em 2011 (em com-\nparação com 13 Mbits/s para as redes de acesso a cabo e menos de 5 Mbits/s para DSL) [FTTH Council, 2011b].\nDuas outras tecnologias também são usadas para oferecer acesso da residência à Internet. Em locais onde \nDSL, cabo e FTTH não estão disponíveis (por exemplo, em algumas propriedades rurais), um enlace de satélite \npode ser empregado para conexão em velocidades não maiores do que 1 Mbit/s; StarBand e HughesNet são dois \ndesses provedores de acesso por satélite. O acesso discado por linhas telefônicas tradicionais é baseado no mesmo \nmodelo do DSL — um modem doméstico se conecta por uma linha telefônica a um modem no ISP. Em com-\nparação com DSL e outras redes de acesso de banda larga, o acesso discado é terrivelmente lento em 56 kbits/s.\nAcesso na empresa (e na residência): Ethernet e Wi-Fi\nNos campi universitários e corporativos, e cada vez mais em residências, uma rede local (LAN) costuma ser \nusada para conectar sistemas finais ao roteador da periferia. Embora existam muitos tipos de tecnologia LAN, a \nEthernet é, de longe, a de acesso predominante nas redes universitárias, corporativas e domésticas. Como mos-\ntrado na Figura 1.8, os usuários utilizam um par de fios de cobre trançado para se conectarem a um comutador \nEthernet, uma tecnologia tratada com mais detalhes no Capítulo 5. O comutador Ethernet, ou uma rede desses \ncomutadores interconectados, é por sua vez conectado à Internet maior. Com o acesso por uma rede Ethernet, os \nusuários normalmente têm acesso de 100 Mbits/s com o comutador Ethernet, enquanto os servidores possuem \num acesso de 1 Gbit/s ou até mesmo 10 Gbits/s.\nEstá cada vez mais comum as pessoas acessarem a Internet sem fio, seja por notebooks, smartphones, tablets \nou por outros dispositivos (veja o texto “Um conjunto impressionante de sistemas finais da Internet”\n, na seção “His-\ntórico do caso”\n, p. 9). Em uma LAN sem fio, os usuários transmitem/recebem pacotes para/de um ponto de acesso \nque está conectado à rede da empresa (quase sempre incluindo Ethernet com fio) que, por sua vez, é conectada à \nInternet com fio. Um usuário de LAN sem fio deve estar no espaço de alguns metros do ponto de acesso. O acesso \nà LAN sem fio baseado na tecnologia IEEE 802.11, ou seja, Wi-Fi, está presente em todo lugar — universidades, \nempresas, cafés, aeroportos, residências e, até mesmo, em aviões. Em muitas cidades, é possível ficar na esquina de \numa rua e estar dentro da faixa de dez ou vinte estações-base (para um mapa global de estações-base 802.11 que \nforam descobertas e acessadas por pessoas que apreciam coisas do tipo, veja wigle.net [2012]). Como discutido com \ndetalhes no Capítulo 6, hoje o 802.11 fornece uma taxa de transmissão compartilhada de até 54 Mbits/s.\nFigura 1.7  ACESSO A INTERNET POR FTTH\nInternet\nCentral telefônica\nDistribuidor \nóptico\nONT\nONT\nONT\nOLT\nFibras \nópticas \nRedes de computadores e a Internet  13 \nEmbora as redes de acesso por Ethernet e Wi-Fi fossem implantadas no início em ambientes corporativos \n(empresas, universidades), elas há pouco se tornaram componentes bastante comuns das redes residenciais. Mui-\ntas casas unem o acesso residencial banda larga (ou seja, modems a cabo ou DSL) com a tecnologia LAN sem fio \na um custo acessível para criar redes residenciais potentes [Edwards, 2011]. A Figura 1.9 mostra um esquema de \numa rede doméstica típica. Ela consiste em um notebook móvel e um computador com fio; uma estação-base (o \nponto de acesso sem fio), que se comunica com o computador sem fio; um modem a cabo, fornecendo acesso \nbanda larga à Internet; e um roteador, que interconecta a estação-base e o computador fixo com o modem a cabo. \nEssa rede permite que os moradores tenham acesso banda larga à Internet com um usuário se movimentando da \ncozinha ao quintal e até os quartos.\nAcesso sem fio em longa distância: 3G e LTE\nCada vez mais, dispositivos como iPhones, BlackBerrys e dispositivos Android estão sendo usados para \nenviar e-mail, navegar na Web, tuitar e baixar música enquanto se movimentam. Esses dispositivos empregam a \nmesma infraestrutura sem fios usada para a telefonia celular para enviar/receber pacotes por uma estação-base \nque é controlada pela operadora da rede celular. Diferente do Wi-Fi, um usuário só precisa estar dentro de algu-\nmas dezenas de quilômetros (ao contrário de algumas dezenas de metros) da estação-base.\nAs empresas de telecomunicação têm investido enormemente na assim chamada terceira geração (3G) sem \nfio, que oferece acesso remoto à Internet por pacotes comutados a velocidades que ultrapassam 1 Mbit/s. Porém, \naté mesmo tecnologias de acesso remotas de maior velocidade — uma quarta geração (4G) — já estão sendo im-\nFigura 1.8  acesso a internet por ethernet\nComutador \nEthernet\nRoteador \ninstitucional\n100 Mbits/s\n100 Mbits/s\n100 Mbits/s\nServidor\nPara ISP \nda instituição\nFigura 1.9  Esquema de uma rede doméstica típica\nTerminal de \ndistribuição \na cabo\nResidência\nInternet\n   Redes de computadores e a Internet\n14\nplantadas. LTE (de “Long­\n‑Term Evolution”\n, um candidato ao prêmio de Pior Acrônimo do Ano, PAA) tem suas \nraízes na tecnologia 3G, e tem potencial para alcançar velocidades superiores a 10 Mbits/s. Taxas downstream \nLTE de muitas dezenas de Mbits/s foram relatadas em implementações comerciais. Veremos os princípios básicos \ndas redes sem fio e mobilidade, além de tecnologias Wi­\n‑Fi, 3G e LTE (e mais!) no Capítulo 6.\n1.2.2  Meios físicos\nNa subseção anterior, apresentamos uma visão geral de algumas das mais importantes tecnologias de acesso \nà Internet. Ao descrevê-las, indicamos também os meios físicos utilizados por elas. Por exemplo, dissemos que o \nHFC usa uma combinação de cabo de fibra ótica. Dissemos que DSL e Ethernet utilizam fios de cobre. Dissemos \ntambém que redes de acesso móveis usam o espectro de rádio. Nesta subseção damos uma visão geral desses e de \noutros meios de transmissão empregados na Internet.\nPara definir o que significa meio físico, vamos pensar na curta vida de um bit. Considere um bit saindo de \num sistema final, transitando por uma série de enlaces e roteadores e chegando a outro sistema final. Esse pobre \ne pequeno bit é transmitido muitas e muitas vezes. Primeiro, o sistema final originador transmite o bit e, logo \nem seguida, o primeiro roteador da série recebe­\n‑o;­\n então, o primeiro roteador envia-o para o segundo roteador \ne assim por diante. Assim, nosso bit, ao viajar da origem ao destino, passa por uma série de pares transmissor­\n‑receptor, que o recebem por meio de ondas eletromagnéticas ou pulsos ópticos que se propagam por um meio \nfísico. Com muitos aspectos e formas possíveis, o meio físico não precisa ser obrigatoriamente do mesmo tipo \npara cada par transmissor–receptor ao longo do caminho. Alguns exemplos de meios físicos são: par de fios de \ncobre trançado, cabo coaxial, cabo de fibra ótica multimodo, espectro de rádio terrestre e espectro de rádio por \nsatélite. Os meios físicos se enquadram em duas categorias: meios guiados e meios não guiados. Nos meios guia-\ndos, as ondas são dirigidas ao longo de um meio sólido, tal como um cabo de fibra ótica, um par de fios de cobre \ntrançado ou um cabo coaxial. Nos meios não guiados, as ondas se propagam na atmosfera e no espaço, como é o \ncaso de uma LAN sem fio ou de um canal digital de satélite.\nContudo, antes de examinar as características dos vários tipos de meios, vamos discutir um pouco seus custos. \nO custo real de um enlace físico (fio de cobre, cabo de fibra ótica e assim por diante) costuma ser insignificante em \ncomparação a outros. Em especial, o custo da mão de obra de instalação do enlace físico pode ser várias vezes maior \ndo que o do material. Por essa razão, muitos construtores instalam pares de fios trançados, fibra ótica e cabo coaxial em \ntodas as salas de um edifício. Mesmo que apenas um dos meios seja usado inicialmente, há uma boa probabilidade de \noutro ser usado no futuro próximo — portanto, poupa-se dinheiro por não ser preciso instalar fiação adicional depois.\nPar de fios de cobre trançado\nO meio de transmissão guiado mais barato e mais usado é o par de fios de cobre trançado, que vem sendo \nempregado há mais de cem anos nas redes de telefonia. De fato, mais de 99% da fiação que conecta aparelhos \ntelefônicos a centrais locais utilizam esse meio. Quase todos nós já vimos um em casa ou no local de trabalho: \nesse par constituído de dois fios de cobre isolados, cada um com cerca de um milímetro de espessura, enrolados \nem espiral. Os fios são trançados para reduzir a interferência elétrica de pares semelhantes que estejam próximos. \nNormalmente, uma série de pares é conjugada dentro de um cabo, isolando-se os pares com blindagem de pro-\nteção. Um par de fios constitui um único enlace de comunicação. O par trançado sem blindagem (unshielded \ntwisted pair — UTP) costuma ser usado em redes de computadores de edifícios, isto é, em LANs. Hoje, as taxas de \ntransmissão de dados para as LANs de pares trançados estão na faixa de 10 Mbits/s a 10 Gbits/s. As taxas de trans-\nmissão de dados que podem ser alcançadas dependem da bitola do fio e da distância entre transmissor e receptor.\nQuando a tecnologia da fibra ótica surgiu na década de 1980, muitos depreciaram o par trançado por suas \ntaxas de transmissão de bits relativamente baixas. Alguns até acharam que a tecnologia da fibra ótica o substitui-\nria por completo. Mas ele não desistiu assim tão facilmente. A moderna tecnologia de par trançado, tal como o \nRedes de computadores e a Internet  15 \ncabo de categoria 6a, pode alcançar taxas de transmissão de dados de 10 Gbits/s para distâncias de até algumas \ncentenas de metros. No final, o par trançado firmou-se como a solução dominante para LANs de alta velocidade.\nComo vimos anteriormente, o par trançado também é usado para acesso residencial à Internet. Vimos que \na tecnologia do modem discado possibilita taxas de acesso de até 56 kbits/s com pares trançados. Vimos também \nque a tecnologia DSL (linha digital de assinante) permitiu que usuários residenciais acessem a Internet em deze-\nnas de Mbits/s com pares de fios trançados (quando as residências estão próximas ao modem do ISP).\nCabo coaxial\nComo o par trançado, o cabo coaxial é constituído de dois condutores de cobre, porém concêntricos e não pa-\nralelos. Com essa configuração, isolamento e blindagem especiais, pode alcançar taxas altas de transmissão de dados. \nCabos coaxiais são muito comuns em sistemas de televisão a cabo. Como já comentamos, recentemente sistemas de \ntelevisão a cabo foram acoplados com modems a cabo para oferecer aos usuários residenciais acesso à Internet a ve-\nlocidades de dezenas de Mbits/s. Em televisão a cabo e acesso a cabo à Internet, o transmissor passa o sinal digital \npara uma banda de frequência específica e o sinal analógico resultante é enviado do transmissor para um ou mais \nreceptores. O cabo coaxial pode ser utilizado como um meio compartilhado guiado. Vários sistemas finais podem \nser conectados diretamente ao cabo, e todos eles recebem qualquer sinal que seja enviado pelos outros sistemas finais.\nFibras ópticas\nA fibra ótica é um meio delgado e flexível que conduz pulsos de luz, cada um deles representando um bit. \nUma única fibra ótica pode suportar taxas de transmissão elevadíssimas, de até dezenas ou mesmo centenas de \ngigabits por segundo. Fibras óticas são imunes à interferência eletromagnética, têm baixíssima atenuação de sinal \naté cem quilômetros e são muito difíceis de derivar. Essas características fizeram da fibra ótica o meio preferido \npara a transmissão guiada de grande alcance, em especial para cabos submarinos. Hoje, muitas redes telefôni-\ncas de longa distância dos Estados Unidos e de outros países usam exclusivamente fibras óticas, que também \npredominam no backbone da Internet. Contudo, o alto custo de equipamentos ópticos — como transmissores, \nreceptores e comutadores — vem impedindo sua utilização para transporte a curta distância, como em LANs ou \nem redes de acesso residenciais. As velocidades de conexão do padrão Optical Carrier (OC) variam de 51,8 Mbits/s \na 39,8 Gbits/s; essas especificações são frequentemente denominadas OC-n, em que a velocidade de conexão se \niguala a n × 51,8 Mbits/s. Os padrões usados hoje incluem OC-1, OC-3, OC-12, OC-24, OC-48, OC-96, OC-192 \ne OC-768. Mukherjee [2006] e Ramaswamy [2010] apresentam uma abordagem de vários aspectos da rede óptica.\nCanais de rádio terrestres\nCanais de rádio carregam sinais dentro do espectro eletromagnético. São um meio atraente porque sua insta-\nlação não requer cabos físicos, podem atravessar paredes, dão conectividade ao usuário móvel e, potencialmente, \nconseguem transmitir um sinal a longas distâncias. As características de um canal de rádio dependem muito do am-\nbiente de propagação e da distância pela qual o sinal deve ser transmitido. Condições ambientais determinam perda \nde sinal no caminho e atenuação por efeito de sombra (que reduz a intensidade do sinal quando­\n ele transita por \ndistâncias longas e ao redor/através de objetos interferentes), atenuação por caminhos múltiplos (devido à reflexão \ndo sinal quando atinge objetos interferentes) e interferência (por outras transmissões ou sinais eletromagnéticos).\nCanais de rádio terrestres podem ser classificados, de modo geral, em três grupos: os que operam sobre \ndistâncias muito curtas (por exemplo, com um ou dois metros); os de pequeno alcance, que funcionam em locais \npróximos, normalmente abrangendo de dez a algumas centenas de metros, e os de longo alcance, que abrangem \ndezenas de quilômetros. Dispositivos pessoais como fones sem fio, teclados e dispositivos médicos operam por \ncurtas distâncias; as tecnologias LAN sem fio, descritas na Seção 1.2.1, utilizam canais de rádio local; as tecnolo-\n   Redes de computadores e a Internet\n16\ngias de acesso em telefone celular utilizam canal de rádio de longo alcance. Abordaremos canais de rádio deta-\nlhadamente no Capítulo 6.\nCanais de rádio por satélite\nUm satélite de comunicação liga dois ou mais transmissores-receptores de micro-ondas baseados na Terra, \ndenominados estações terrestres. Ele recebe transmissões em uma faixa de frequência, gera novamente o sinal \nusando um repetidor (sobre o qual falaremos a seguir) e o transmite em outra frequência. Dois tipos de satélites \nsão usados para comunicações: satélites geoestacionários e satélites de órbita baixa (LEO).\nOs satélites geoestacionários ficam de modo permanente sobre o mesmo lugar da Terra. Essa presença estacio-\nnária é conseguida colocando-se o satélite em órbita a 36 mil quilômetros acima da superfície terrestre. Essa enorme \ndistância da estação terrestre ao satélite e de seu caminho de volta à estação terrestre traz um substancial atraso de \npropagação de sinal de 280 milissegundos. Mesmo assim, enlaces por satélite, que podem funcionar a ­\nvelocidades ­\nde \n­\ncentenas de Mbits/s, são frequentemente usados em áreas sem acesso à Internet baseado em DSL ou cabo.\nOs satélites de órbita baixa são posicionados muito mais próximos da Terra e não ficam sempre sobre um \núnico lugar. Eles giram ao redor da Terra (exatamente como a Lua) e podem se comunicar uns com os outros e \ncom estações terrestres. Para prover cobertura contínua em determinada área, é preciso colocar muitos satélites \nem órbita. Hoje, existem muitos sistemas de comunicação de baixa altitude em desenvolvimento. A página da \nWeb referente à constelação de satélites da Lloyd [Wood, 2012] fornece e coleta informações sobre esses sistemas \npara comunicações. A tecnologia de satélites de órbita baixa poderá ser utilizada para acesso à Internet no futuro.\n1.3  O núcleo da rede\nApós termos examinado a periferia da Internet, vamos agora nos aprofundar mais no núcleo da rede — a \nrede de comutadores de pacote e enlaces que interconectam os sistemas finais da Internet. Os núcleos da rede \naparecem destacados em cinza na Figura 1.10.\n1.3.1  Comutação de pacotes\nEm uma aplicação de rede, sistemas finais trocam mensagens entre si. Mensagens podem conter qualquer \ncoisa que o projetista do protocolo queira. Podem desempenhar uma função de controle (por exemplo, as men-\nsagens “oi” no nosso exemplo de comunicação na Figura 1.2) ou conter dados, tal como um e-mail, uma imagem \nJPEG ou um arquivo de áudio MP3. Para enviar uma mensagem de um sistema final de origem para um destino, \no originador fragmenta mensagens longas em porções de dados menores, denominadas pacotes. Entre origem e \ndestino, cada um deles percorre enlaces de comunicação e comutadores de pacotes (há dois tipos principais de \ncomutadores de pacotes: roteadores e comutadores de camada de enlace). Pacotes são transmitidos por cada \nenlace de comunicação a uma taxa igual à de transmissão total. Assim, se um sistema final de origem ou um \ncomutador de pacotes estiver enviando um pacote de L bits por um enlace com taxa de transmissão de R bits/s, \nentão o tempo para transmitir o pacote é L/R segundos.\nTransmissão armazena-e-reenvia\nA maioria dos comutadores de pacotes utiliza a transmissão armazena-e-reenvia (store-and-forward) nas \nentradas dos enlaces. A transmissão armazena-e-reenvia significa que o comutador de pacotes deve receber o pacote \ninteiro antes de poder começar a transmitir o primeiro bit para o enlace de saída. Para explorar a transmissão arma-\nzena-e-reenvia com mais detalhes, considere uma rede simples, consistindo em dois sistemas finais conectados por \num único roteador, conforme mostra a Figura 1.11. Um roteador em geral terá muitos enlaces incidentes, pois sua \nRedes de computadores e a Internet  17 \nfunção é comutar um pacote que chega para um enlace de ­\nsaída; neste exemplo simples, o roteador tem a tarefa de \ntransferir um pacote de um enlace (entrada) para o único outro enlace conectado. Aqui, a origem tem três pacotes, \ncada um consistindo em L bits, para enviar ao destino. No instante de tempo mostrado na Figura 1.11, a origem \ntransmitiu parte do pacote 1, e a frente do pacote 1 já chegou no roteador. Como emprega a transmissão armazena\n-e-reenvia, nesse momento, o roteador não pode transmitir os bits que recebeu; em vez disso, ele precisa primeiro \nmanter em buffer (isto é, “armazenar”) os bits do pacote. Somente depois que o roteador tiver recebido todos os bits, \npoderá começar a transmitir (isto é, “reenviar”) o pacote para o enlace de saída. Para ter uma ideia da transmissão \narmazena-e-reenvia, vamos agora calcular a quantidade de tempo decorrido desde quando a origem começa a en-\nviar até que o destino tenha recebido o pacote inteiro. (Aqui, ignoraremos o atraso de propagação — o tempo gasto \npara os bits atravessarem o fio em uma velocidade próxima à da luz —, o que será discutido na Seção 1.4.) A origem \ncomeça a transmitir no tempo 0; no tempo L/R segundos, a origem terá transmitido o pacote inteiro, que terá sido \nrecebido e armazenado no roteador (pois não há atraso de propagação). No tempo L/R segundos, como o roteador \nFigura 1.10  O núcleo da rede\nISP nacional \nou global\nISP local \nou regional\nRede corporativa\nRede doméstica\nKR01.10.eps\nAW/Kurose and Ross\nComputer Networking, 6/e\nsize:  28p0 x  36p7 \n9/6/11, 10/28/11, 10/31/11\n11/21/11 rossi \nRede móvel\n   Redes de computadores e a Internet\n18\njá terá recebido o pacote inteiro, ele pode começar a transmiti-lo para o enlace de saída, em direção ao destino; no \ntempo 2L/R, o roteador terá transmitido o pacote inteiro, e este terá sido recebido pelo destino. Assim, o atraso total \né 2L/R. Se o comutador, em vez disso, reenviasse os bits assim que chegassem (sem primeiro receber o pacote intei-\nro), então o atraso total seria L/R, pois os bits não são mantidos no roteador. Mas, conforme discutiremos na Seção \n1.4, os roteadores precisam receber, armazenar e processar o pacote inteiro antes de encaminhar.\nAgora vamos calcular a quantidade de tempo decorrido desde quando a origem começa a enviar o primeiro \npacote até que o destino tenha recebido todos os três. Como antes, no instante L/R, o roteador começa a reenviar \no primeiro pacote. Mas, também no tempo L/R, a origem começará a enviar o segundo, pois ela terá acabado de \nmandar o primeiro pacote inteiro.­\n Assim, no tempo 2L/R, o destino terá recebido o primeiro pacote e o roteador \nterá recebido o segundo. De modo semelhante, no instante 3L/R, o destino terá recebido os dois primeiros pa-\ncotes e o roteador terá recebido o terceiro. Por fim, no tempo 4L/R, o destino terá recebido todos os três pacotes!\nVamos considerar o caso geral do envio de um pacote da origem ao destino por um caminho que consiste \nem N enlaces, cada um com taxa R (assim, há N – 1 roteadores entre origem e destino). Aplicando a mesma lógica \nusada anteriormente, vemos que o atraso fim a fim é:\n\t\ndfim a fim = N L\nR\t\n(1.1)\nVocê poderá tentar determinar qual seria o atraso para P pacotes enviados por uma série de N enlaces.\nAtrasos de fila e perda de pacote\nA cada comutador de pacotes estão ligados vários enlaces. Para cada um destes, o comutador de pacotes \ntem um buffer de saída (também denominado fila de saída), que armazena pacotes prestes a serem enviados \npelo roteador para aquele enlace. Os buffers de saída desempenham um papel fundamental na comutação de \npacotes. Se um pacote que está chegando precisa ser transmitido por um enlace, mas o encontra ocupado com a \ntransmissão de outro pacote, deve aguardar no buffer de saída. Desse modo, além dos atrasos de armazenagem \ne reenvio, os pacotes sofrem atrasos de fila no buffer de saída. Esses atrasos são variáveis e dependem do grau \nde congestionamento da rede. Como o espaço do buffer é finito, um pacote que está chegando pode encontrá-lo \nlotado de outros que estão esperando transmissão. Nesse caso, ocorrerá uma perda de pacote — um pacote que \nestá chegando ou um dos que já estão na fila é descartado.\nA Figura 1.12 ilustra uma rede simples de comutação de pacotes. Como na Figura 1.11, os pacotes são repre-\nsentados por placas tridimensionais. A largura de uma placa representa o número de bits no pacote. Nessa figura, \ntodos os pacotes têm a mesma largura, portanto, o mesmo tamanho. Suponha que os hospedeiros A e B estejam \nenviando pacotes ao hospedeiro E.­\n Os hospedeiros A e B primeiro enviarão seus pacotes por enlaces Ethernet de \n10 Mbits/s até o primeiro comutador, que vai direcioná-los para o enlace de 1,5 Mbits/s. Se, durante um peque-\nno intervalo de tempo, a taxa de chegada de pacotes ao roteador (quando convertida para bits por segundo) for \nmaior do que 1,5 Mbits/s, ocorrerá congestionamento no roteador, pois os pacotes formarão uma fila no buffer \nde saída do enlace antes de ser transmitidos para o enlace. Por exemplo, se cada um dos hospedeiros A e B enviar \numa rajada de cinco pacotes de ponta a ponta ao mesmo tempo, então a maior parte deles gastará algum tempo \nFigura 1.11  Comutação de pacotes armazena-e-reenvia\nOrigem\nR bits/s\n1\n2\nDestino\nFrente do pacote 1 \narmazenado no roteador, \nesperando bits restantes \nantes de repassar\n3\nRedes de computadores e a Internet  19 \nesperando na fila. De fato, a situação é semelhante a muitas no dia a dia  — por exemplo, quando aguardamos \nna fila de um caixa de banco ou quando esperamos em uma cabine de pedágio. Vamos analisar esse atraso de fila \nmais detalhadamente na Seção 1.4.\nTabelas de repasse e protocolos de roteamento\nDissemos anteriormente que um roteador conduz um pacote que chega a um de seus enlaces de comunica-\nção para outro de seus enlaces de comunicação conectados. Mas como o roteador determina o enlace que deve \nconduzir o pacote? Na verdade, isso é feito de diversas maneiras por diferentes tipos de rede de computadores. \nAqui, descreveremos de modo resumido como isso é feito pela Internet.\nNa Internet, cada sistema final tem um endereço denominado endereço IP. Quando um sistema final de \norigem quer enviar um pacote a um destino, a origem inclui o endereço IP do destino no cabeçalho do pacote. \nComo os endereços postais, este possui uma estrutura hierárquica. Quando um pacote chega a um roteador na \nrede, este examina uma parte do endereço de destino e o conduz a um roteador adjacente. Mais especificamente, \ncada roteador possui uma tabela de encaminhamento que mapeia os endereços de destino (ou partes deles) para \nenlaces de saída desse roteador. Quando um pacote chega a um roteador, este examina o endereço e pesquisa sua \ntabela de encaminhamento, utilizando esse endereço de destino para encontrar o enlace de saída apropriado. O \nroteador, então, direciona o pacote a esse enlace de saída.\nO processo de roteamento fim a fim é semelhante a um motorista que não quer consultar o mapa, prefe-\nrindo pedir informações. Por exemplo, suponha que Joe vai dirigir da Filadélfia para 156 Lakeside Drive, em \nOrlando, Flórida. Primeiro, Joe vai ao posto de gasolina de seu bairro e pergunta como chegar a 156 Lakeside \nDrive, em Orlando, Flórida. O frentista do posto extrai a palavra Flórida do endereço e diz que Joe precisa pegar \na interestadual I-95 South, cuja entrada fica ao lado do posto. Ele também diz a Joe para pedir outras informações \nassim que chegar à Flórida. Então, Joe pega a I-95 South até chegar a Jacksonville, na Flórida, onde pede mais \ninformações a outro frentista. Este extrai a palavra Orlando do endereço a diz a Joe para continuar na I-95 até \nDaytona Beach, e lá se informar de novo. Em Daytona Beach, outro frentista também extrai a palavra Orlando \ndo endereço e pede para que ele pegue a I-4 diretamente para Orlando. Joe segue suas orientações e chega a uma \nsaída para Orlando. Ele vai até outro posto de gasolina, e dessa vez o frentista extrai a palavra Lakeside Drive \ndo endereço e diz a ele qual estrada seguir para Lakeside Drive. Assim que Joe chega a Lakeside Drive, pergunta \na uma criança andando de bicicleta como chegar a seu destino. A criança extrai o número 156 do endereço e \naponta para a casa. Joe finalmente chega enfim a seu destino. Nessa analogia, os frentistas de posto de gasolina e \nas crianças andando de bicicleta são semelhantes aos roteadores.\nFigura 1.12  Comutação de pacotes\nEthernet de 10 Mbits/s\nLegenda:\nPacotes\nA\nB\nC\nD\nE\n1,5 Mbits/s\nFila de pacotes \nesperando por um \nenlace de saída\n   Redes de computadores e a Internet\n20\nVimos que um roteador usa um endereço de destino do pacote para indexar uma tabela de encaminha-\nmento e determinar o enlace de saída apropriado. Mas essa afirmação traz ainda outra questão: como as tabelas \nde encaminhamento são montadas? Elas são configuradas manualmente em cada roteador ou a Internet utiliza \num procedimento mais automatizado? Essa questão será estudada com mais profundidade no Capítulo 4. Mas, \npara aguçar seu apetite, observe que a Internet possui uma série de protocolos de roteamento especiais, que são \nutilizados para configurar automaticamente as tabelas de encaminhamento. Um protocolo de roteamento pode, \npor exemplo, determinar o caminho mais curto de cada roteador a cada destino e utilizar os resultados para con-\nfigurar as tabelas de encaminhamento nos roteadores.\nVocê gostaria de ver a rota fim a fim que os pacotes realizam na Internet? Nós o convidamos a colocar a \nmão na massa e interagir com o programa Traceroute, visitando o site <www.traceroute.org>, escolhendo uma \norigem em um país qualquer e traçando a rota dessa origem até o seu computador. (Para obter detalhes sobre o \nTraceroute, veja a Seção 1.4.)\n1.3.2  Comutação de circuitos\nHá duas abordagens fundamentais para locomoção de dados através de uma rede de enlaces e comutadores: \ncomutação de circuitos e comutação de pacotes. Tendo visto estas últimas na subseção anterior, agora vamos \nvoltar nossa atenção às redes de comutação de circuitos.\nNessas redes, os recursos necessários ao longo de um caminho (buffers, taxa de transmissão de enlaces) \npara oferecer comunicação entre os sistemas finais são reservados pelo período da sessão de comunicação entre os \nsistemas finais. Em redes de comutação de pacotes, tais recursos não são reservados; as mensagens de uma sessão \nusam os recursos por demanda e, como consequência, poderão ter de esperar (isto é, entrar na fila) para conse-\nguir acesso a um enlace de comunicação. Como simples analogia, considere dois restaurantes — um que exige e \noutro que não exige nem aceita reserva. Se quisermos ir ao restaurante que exige reserva, teremos de passar pelo \naborrecimento de telefonar antes de sair de casa. Mas, quando chegarmos lá, poderemos, em princípio, ser logo \natendidos e servidos. No segundo restaurante, não precisaremos nos dar ao trabalho de reservar mesa, porém, \nquando lá chegarmos, talvez tenhamos de esperar para sentar.\nAs redes de telefonia tradicionais são exemplos de redes de comutação de circuitos. Considere o que acon-\ntece quando uma pessoa quer enviar a outra uma informação (por voz ou por fax) por meio de uma rede tele-\nfônica. Antes que o remetente possa enviar a informação, a rede precisa estabelecer uma conexão entre ele e o \ndestinatário. Essa é uma conexão forte, na qual os comutadores no caminho entre o remetente e o destinatário \nmantêm o estado. No jargão da telefonia, essa conexão é denominada circuito. Quando a rede estabelece o circui-\nto, também reserva uma taxa de transmissão constante nos enlaces da rede durante o período da conexão. Visto \nque foi reservada largura de banda para essa conexão remetente-destinatário, o remetente pode transferir dados \nao destinatário a uma taxa constante garantida.\nA Figura 1.13 ilustra uma rede de comutação de circuitos. Nela, os quatro comutadores de circuitos estão inter-\nconectados por quatro enlaces. Cada enlace tem quatro circuitos, de modo que cada um pode suportar quatro cone-\nxões simultâneas. Cada um dos hospedeiros (por exemplo, PCs e estações de trabalho) está conectado diretamente a \num dos circuitos. Quando dois sistemas finais querem se comunicar, a rede estabelece uma conexão fim a fim dedi-\ncada entre os dois hospedeiros. Assim, para que o sistema final A envie mensagens ao sistema final B, a rede deve pri-\nmeiro reservar um circuito em cada um dos dois enlaces. Neste exemplo, a conexão fim a fim dedicada usa o segundo \ncircuito no primeiro enlace e o quarto circuito no segundo enlace. Como cada enlace tem quatro circuitos, para cada \nenlace usado pela conexão fim a fim, esta fica com um quarto da capacidade de transmissão total durante o período \nda conexão. Assim, por exemplo, se cada enlace entre comutadores adjacentes tiver uma taxa de transmissão de \n \n1 Mbit/s, então cada conexão de comutação de circuitos fim a fim obtém 250 kbits/s de taxa de transmissão dedicada.\nAo contrário, considere o que ocorre quando um sistema final quer enviar um pacote a outro hospedeiro por \numa rede de comutação de pacotes, como a Internet. Como acontece na comutação de circuitos, o pacote é transmitido \npor uma série de enlaces de comunicação. Mas, na comutação de pacotes, ele é enviado à rede sem reservar qualquer \nRedes de computadores e a Internet  21 \nrecurso do enlace. Se um dos enlaces estiver congestionado porque outros pacotes precisam ser transmitidos ao mes-\nmo tempo, então nosso pacote terá de esperar em um buffer na extremidade de origem do enlace de transmissão e \nsofrerá um atraso. A Internet faz o melhor esforço para entregar os dados de pronto, mas não dá garantia alguma.\nMultiplexação em redes de comutação de circuitos\nUm circuito é implementado em um enlace por multiplexação por divisão de frequência (frequency-di-\nvision multiplexing — FDM) ou por multiplexação por divisão de tempo (time-division multiplexing — TDM). \nCom FDM, o espectro de frequência de um enlace é compartilhado entre as conexões estabelecidas através desse \nenlace. Ou seja, o enlace reserva uma banda de frequência para cada conexão durante o período da ligação. Em \nredes telefônicas, a largura dessa banda de frequência em geral é 4 kHz (isto é, 4 mil Hertz ou 4 mil ciclos por \nsegundo). A largura da banda é denominada, claro, largura de banda. Estações de rádio FM também usam FDM \npara compartilhar o espectro de frequência entre 88 MHz e 108 MHz, sendo atribuída para cada estação uma \nbanda de frequência específica.\nEm um enlace TDM, o tempo é dividido em quadros de duração fixa, e cada quadro é dividido em um número \nfixo de compartimentos (slots). Quando estabelece uma conexão por meio de um enlace, a rede dedica à conexão \num compartimento de tempo em cada quadro. Esses compartimentos são reservados para o uso exclusivo dessa \nconexão, e um dos compartimentos de tempo (em cada quadro) fica disponível para transmitir os dados dela.\nA Figura 1.14 ilustra as técnicas FDM e TDM para um enlace de rede que suporta até quatro circuitos. Para \nFDM, o domínio de frequência é segmentado em quatro faixas, com largura de banda de 4 kHz cada. Para TDM, \no domínio de tempo é segmentado em quadros, cada um com quatro compartimentos de tempo; a cada circuito \né designado o mesmo compartimento dedicado nos quadros sucessivos TDM. Para TDM, a taxa de transmissão \nde um circuito é igual à taxa do quadro multiplicada pelo número de bits em um compartimento. ­\nPor exemplo, \nse o enlace transmite 8 mil quadros por segundo e cada compartimento consiste em 8 bits, então a taxa de trans-\nmissão de um circuito é 64 kbits/s.\nOs defensores da comutação de pacotes sempre argumentaram que comutação de circuitos é desperdí-\ncio, porque os circuitos dedicados ficam ociosos durante períodos de silêncio. Por exemplo, quando um dos \nparticipantes de uma conversa telefônica para de falar, os recursos ociosos da rede (bandas de frequências ou \ncompartimentos nos enlaces ao longo da rota da conexão) não podem ser usados por outras conexões em curso. \nPara outro exemplo de como esses recursos podem ser subutilizados, considere um radiologista que usa uma \nrede de comutação de circuitos para acessar remotamente uma série de exames. Ele estabelece uma conexão, \nrequisita uma imagem, examina-a e, em seguida, solicita uma nova. Recursos de rede são atribuídos à conexão, \nmas não utilizados (isto é, são desperdiçados) no período em que o radiologista examina a imagem. Defensores \nFigura 1.13  \u0007\nUma rede simples de comutação de circuitos composta de quatro comutadores e \nquatro enlaces\nKR 01.13.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n17p8 Wide x 12p5 Deep\n9/6/11, 11/9/11, 11/22/11 rossi\n   Redes de computadores e a Internet\n22\nda comutação de pacotes também gostam de destacar que estabelecer circuitos e reservar larguras de banda fim \na fim é complicado e exige softwares complexos de sinalização para coordenar a operação dos comutadores ao \nlongo do caminho.\nAntes de encerrarmos esta discussão sobre comutação de circuitos, examinaremos um exemplo numérico que \ndeverá esclarecer melhor o assunto. Vamos considerar o tempo que levamos para enviar um arquivo de 640 kbits/s \ndo hospedeiro A ao hospedeiro B por uma rede de comutação de circuitos. Suponha que todos os enlaces da rede \nusem TDM de 24 compartimentos e tenham uma taxa de 1,536 Mbits/s. Suponha também que um circuito fim a \nfim leva 500 milissegundos para ser ativado antes que A possa começar a transmitir o arquivo. Em quanto tempo \no arquivo será enviado? Cada circuito tem uma taxa de transmissão de (1,536 Mbits/s)/24 = 64 kbits/s; portanto, \ndemorará (640 kbits/s)/(64 kbits/s) = 10 segundos para transmitir o arquivo. A esses 10 segundos adicionamos o \ntempo de ativação do circuito, resultando 10,5 segundos para o envio. Observe que o tempo de transmissão é in-\ndependente do número de enlaces: o tempo de transmissão seria 10 segundos se o circuito fim a fim passasse por \num ou por uma centena de enlaces. (O atraso real fim a fim também inclui um atraso de propagação; ver Seção 1.4.)\nComutação de pacotes versus comutação de circuitos\nAgora que já descrevemos comutação de pacotes e comutação de circuitos, vamos comparar as duas. Opo-\nsitores da comutação de pacotes costumam argumentar que ela não é adequada­\n para serviços de tempo real (por \nexemplo, ligações telefônicas e videoconferência) por causa de seus atrasos fim a fim variáveis e imprevisíveis \n(que se devem principalmente a variáveis e imprevisíveis atrasos de fila). Defensores da comutação de pacotes \nargumentam que (1) ela oferece melhor compartilhamento de banda do que comutação de circuitos e (2) sua \nimplementação é mais simples, mais eficiente e mais barata do que a de comutação de circuitos. Uma discussão \ninteressante sobre comutação de pacotes e comutação de circuitos pode ser encontrada em Molinero-Fernandez \n[2002]. De modo geral, quem não gosta de perder tempo fazendo reserva de mesa em restaurantes prefere comu-\ntação de pacotes à comutação de circuitos.\nPor que a comutação de pacotes é mais eficiente? Vamos examinar um exemplo simples. Suponha que usuá­\nrios compartilhem um enlace de 1 Mbit/s. Considere também que cada usuário alterne períodos de atividade, \nquando gera dados a uma taxa constante de 100 kbits/s, e de inatividade, quando não gera dados. Imagine ainda \nFigura 1.14  \u0007\nCom FDM, cada circuito dispõe continuamente de uma fração da largura de \nbanda. Com TDM, cada circuito dispõe de toda a largura de banda periodicamente, \ndurante breves intervalos de tempo (isto é, durante compartimentos de tempo)\n4KHz\nTDM\nFDM\nEnlace\nFrequência\n4KHz\nCompartimento\nLegenda:\nTodos os compartimentos \nde número “2” são dedicados a \num par transmissor/receptor especíﬁco.\nQuadro\n1\n2\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nTempo\nRedes de computadores e a Internet  23 \nque o usuário esteja ativo apenas 10% do tempo (e fique ocioso, tomando cafezinho, durante os restantes 90%). \nCom comutação de circuitos, devem ser reservados 100 kbits/s para cada usuário durante todo o tempo. Por \nexemplo, com TDM, se um quadro de um segundo for dividido em 10 compartimentos de tempo de 100 milisse-\ngundos cada, então seria alocado um compartimento de tempo por quadro a cada usuário.\nDesse modo, o enlace de comutação de circuitos pode suportar somente 10 (= 1 Mbit/s/100 kbits/s) usuá­\nrios simultaneamente. Com a comutação de pacotes, a probabilidade de haver um usuário específico ativo é 0,1 \n(isto é, 10%). Se houver 35 usuários, a probabilidade de haver 11 ou mais usuários ativos ao mesmo tempo é de \nmais ou menos 0,0004. (O Problema P8 dos Exercícios de Fixação demonstra como essa probabilidade é calcu-\nlada.) Quando houver dez ou menos usuários ativos simultâneos (a probabilidade de isso acontecer é 0,9996), a \ntaxa agregada de chegada de dados é menor ou igual a 1 Mbit/s, que é a taxa de saída do enlace. Assim, quando \nhouver dez ou menos usuários ativos, pacotes de usuários fluirão pelo enlace essencialmente sem atraso, como é \no caso na comutação de circuitos. Quando houver mais de dez usuários ativos ao mesmo tempo, a taxa agregada \nde chegada de pacotes excederá a capacidade de saída do enlace, e a fila de saída começará a crescer. (E continua­\nrá a crescer até que a velocidade agregada de entrada caia novamente para menos de 1 Mbit/s, ponto em que o \ncomprimento da fila começará a diminuir.) Como a probabilidade de haver mais de dez usuários ativos é ínfima \nnesse exemplo, a comutação de pacotes apresenta, em essência, o mesmo desempenho da comutação de circuitos, \nmas o faz para mais de três vezes o número de usuários.\nVamos considerar agora um segundo exemplo simples. Suponha que haja dez usuários e que um deles de \nrepente gere mil pacotes de mil bits, enquanto os outros nove permanecem inativos e não geram pacotes. Com \ncomutação de circuitos TDM de dez compartimentos de tempo por quadro, e cada quadro consistindo em mil \nbits, o usuário ativo poderá usar somente seu único compartimento por quadro para transmitir dados, enquanto \nos nove compartimentos restantes em cada quadro continuarão ociosos. Dez segundos se passarão antes que \ntodo o milhão de bits de dados do usuário ativo seja transmitido. No caso da comutação de pacotes, o usuário \nativo poderá enviá-los continuamente à taxa total de 1 Mbit/s, visto que não haverá outros gerando pacotes que \nprecisem ser multiplexados com os dele. Nesse caso, todos os dados do usuário ativo serão transmitidos dentro \nde 1 segundo.\nOs exemplos citados ilustram duas maneiras pelas quais o desempenho da comutação de pacotes pode ser \nsuperior à da comutação de circuitos. Também destacam a diferença crucial entre as duas formas de comparti-\nlhar a taxa de transmissão de um enlace entre vários fluxos de bits. A comutação de circuitos aloca previamente \na utilização do enlace de transmissão ­\nindependentemente da demanda, com desperdício de tempo de enlace \ndesnecessário alocado e não utilizado. A comutação de pacotes, por outro lado, aloca utilização de enlace por \ndemanda. A capacidade de transmissão do enlace será compartilhada pacote por pacote somente entre usuários \nque tenham pacotes que precisam ser transmitidos pelo enlace.\nEmbora tanto a comutação de pacotes quanto a de circuitos predominem nas redes de telecomunicação \nde hoje, a tendência é, sem dúvida, a comutação de pacotes. Até mesmo muitas das atuais redes de telefonia de \ncomutação de circuitos estão migrando aos poucos para a comutação de pacotes. Em especial, redes telefônicas \nusam comutação de pacotes na parte cara de uma chamada telefônica para o exterior.\n1.3.3 Uma rede de redes\nVimos anteriormente que sistemas finais (PCs, smartphones, servidores Web, servidores de correio eletrô-\nnico e assim por diante) conectam-se à Internet por meio de um provedor local (ISP). Este pode fornecer uma \nconectividade tanto com ou sem fio, utilizando diversas tecnologias de acesso, que incluem DSL, cabo, FTTH, \nWi-Fi e telefone celular. Observe que o provedor local não precisa ser uma operadora de telefonia ou uma empre-\nsa de TV a cabo: pode ser, por exemplo, uma universidade (que oferece acesso à Internet para os alunos, os fun-\ncionários e o corpo docente) ou uma empresa (que oferece acesso para seus funcionários). Mas conectar usuários \nfinais e provedores de conteúdo a um provedor de acesso (ISP) é apenas uma pequena peça do quebra-cabeça \n   Redes de computadores e a Internet\n24\nque é interligar os bilhões de sistemas finais que compõem a Internet. Isso é feito criando uma rede de redes — \nentender essa frase é a chave para entender a Internet.\nCom o passar dos anos, a rede de redes que forma a Internet evoluiu para uma estrutura bastante complexa. \nGrande parte dessa evolução é controlada pela política econômica e nacional, e não por considerações de desem-\npenho. Para entender a estrutura de rede da Internet de hoje, vamos criar, de modo incremental, uma série de \nestruturas de rede, com cada nova estrutura sendo uma aproximação melhor da Internet complexa que temos. \nLembre-se de que o objetivo dominante é interconectar os provedores de acesso de modo que todos os sistemas \nfinais possam enviar pacotes entre si. Um método ingênuo seria fazer que cada ISP se conectasse diretamente a \ncada outro ISP. Esse projeto em malha, é evidente, seria muito caro para os ISPs, pois exigiria que cada ISP tivesse \num enlace de comunicação separado para as centenas de milhares de outros ISPs do mundo inteiro.\nNossa primeira estrutura de rede, a Estrutura de Rede 1, interconecta todos os ISPs de acesso a um único ISP \nde trânsito global. Nosso (imaginário) ISP de trânsito global é uma rede de roteadores e enlaces de comunicação \nque não apenas se espalha pelo planeta, mas também tem pelo menos um roteador próximo de cada uma das \ncentenas de milhares de ISPs de acesso. Claro, seria muito dispendioso para o ISP global montar essa rede tão \nextensa. Para que seja lucrativo, ele naturalmente cobraria de cada um dos ISPs de acesso pela conectividade, com \no preço refletindo (mas nem sempre diretamente proporcional à) a quantidade de tráfego que um ISP de acesso \ntroca com o ISP global. Como o ISP de acesso paga ao ISP de trânsito global, ele é considerado um cliente, e o \nISP de trânsito global é considerado um provedor.\nAgora, se alguma empresa montar e operar um ISP de trânsito global que seja lucrativo, então será natural para \noutras empresas montarem seus próprios ISPs de trânsito global e competirem com o original. Isso leva à Estrutura \nde Rede 2, que consiste em centenas de milhares de ISPs de acesso e múltiplos ISPs de trânsito global. Os ISPs de \nacesso decerto preferem a Estrutura de Rede 2 à Estrutura de Rede 1, pois agora podem escolher entre os provedores \nde trânsito global concorrentes comparando seus preços e serviços. Note, porém, que os próprios ISPs de trânsito \nglobal precisam se interconectar: caso contrário, os ISPs de acesso conectados a um dos provedores de trânsito glo-\nbal não poderiam se comunicar com os ISPs de acesso conectados aos outros provedores de trânsito global.\nA Estrutura de Rede 2, que acabamos de descrever, é uma hierarquia de duas camadas com provedores \nde trânsito global residindo no nível superior e os ISPs de acesso no nível inferior. Isso considera que os ISPs \nde trânsito global não são capazes de chegar perto de todo e qualquer ISP de acesso, mas também consideram \neconomicamente desejável fazer isso. Na realidade, embora alguns ISPs tenham uma cobertura global impressio-\nnante e se conectem diretamente com muitos ISPs de acesso, nenhum tem presença em toda e qualquer cidade do \nmundo. Em vez disso, em determinada região, pode haver um ISP regional ao qual os ISPs de acesso na região \nse conectam. Cada ISP regional, então, se conecta a ISPs de nível 1. Estes são semelhantes ao nosso (imaginário) \nISP de trânsito global; mas os ISPs de nível 1, que realmente existem, não têm uma presença em cada cidade do \nmundo. Existe mais ou menos uma dúzia de ISPs de nível 1, incluindo Level 3 Communications, AT&T, Sprint \ne NTT. É interessante que nenhum grupo sanciona oficialmente o status de nível 1; como diz o ditado — se você \ntiver que perguntar se é membro de um grupo, provavelmente não é.\nRetornando a essa rede de redes, não apenas existem vários ISPs de nível 1 concorrentes, mas pode haver \nmúltiplos ISPs regionais concorrentes em uma região. Em tal hierarquia, cada ISP de acesso paga ao regional ao \nqual se conecta, e cada ISP regional paga ao ISP de nível 1 ao qual se interliga. (Um ISP de acesso também pode \nse conectar diretamente a um ISP de ­\nnível 1, quando pagará ao ISP de nível 1.) Assim, existe uma relação cliente\n-provedor em cada nível da hierarquia. Observe que os ISPs de nível 1 não pagam a ninguém, pois estão no topo. \nPara complicar as coisas ainda mais, em algumas regiões pode haver um ISP regional maior (talvez se espalhando \npor um país inteiro) ao qual os ISPs regionais menores nessa região se conectam; o ISP regional maior, então, \nse conecta a um ISP de nível 1. Por exemplo, na China existem ISPs de acesso em cada cidade, que se conectam \na ISPs provinciais, que por sua vez se ligam a ISPs nacionais, que por fim se interligam a ISPs de nível 1 [Tian, \n2012]. Chamamos a essa hierarquia multinível, que ainda é apenas uma aproximação bruta da Internet de hoje, \nEstrutura de Rede 3.\nRedes de computadores e a Internet  25 \nPara montar uma rede que se assemelhe mais à Internet de hoje, temos que acrescentar pontos de presença \n(PoPs — Points of Presence), multi-homing, emparelhamento e pontos de troca da Internet (IXPs — Internet \neXchange Points) à Estrutura de Rede 3. Existem PoPs em todos os níveis da hierarquia, exceto para o nível de \nbaixo (ISP de acesso). Um PoP é simplesmente um grupo de um ou mais roteadores (no mesmo local) na rede do \nprovedor, onde os ISPs clientes podem se conectar no ISP provedor. Para que uma rede do cliente se conecte ao \nPoP de um provedor, ele pode alugar um enlace de alta velocidade de um provedor de telecomunicações de ter-\nceiros para conectar diretamente um de seus roteadores a um roteador no PoP. Qualquer ISP (exceto os de nível \n1) pode decidir efetuar o multi-home, ou seja, conectar-se a dois ou mais ISPs provedores. Assim, por exemplo, \num ISP de acesso pode efetuar multi-home com dois ISPs regionais, ou então com dois ISPs regionais e também \ncom um ISP de nível 1. De modo semelhante, um ISP regional pode efetuar multi-home com vários ISPs de nível \n1. Quando um ISP efetua multi-home, ele pode continuar a enviar e receber pacotes na Internet, mesmo que um \nde seus provedores apresente uma falha.\nComo vimos, os ISPs clientes pagam aos seus ISPs provedores para obter interconectividade­\n global com a \nInternet. O valor que um ISP cliente paga a um ISP provedor reflete a quantidade de tráfego que ele troca com o \nprovedor. Para reduzir esses custos, um par de ISPs próximos no mesmo nível da hierarquia pode emparelhar, ou \nseja, conectar diretamente suas redes, de modo que todo o tráfego entre elas passe pela conexão direta, em vez de \npassar por intermediários mais à frente. Quando dois ISPs são emparelhados, isso em geral é feito em acordo, ou \nseja, nenhum ISP paga ao outro. Como já dissemos, os ISPs de nível 1 também são emparelhados uns com os ou-\ntros, sem taxas. Para ver uma discussão legível sobre emparelhamento e relações cliente-provedor, consulte Van \nder Berg [2008]. Nesses mesmos termos, uma empresa de terceiros pode criar um ponto de troca da Internet \n(IXP — Internet Exchange Point) — quase sempre em um prédio isolado com seus próprios comutadores —, \n \nque é um ponto de encontro onde vários ISPs podem se emparelhar. Existem cerca de 300 IXPs na Internet hoje \n[Augustin, 2009]. Referimo-nos a esse ecossistema — consistindo em ISPs de acesso, ISPs regionais, ISPs de \nnível 1, PoPs, multi-homing, emparelhamento e IXPs — como Estrutura de Rede 4.\nAgora, chegamos finalmente na Estrutura de Rede 5, que descreve a Internet de 2012. Essa estrutura, ilustrada \nna Figura 1.15, se baseia no topo da Estrutura de Rede 4 acrescentando redes de provedor de conteúdo. A Google é \num dos principais exemplos dessa rede de provedor de conteúdo. No momento, estima-se que tenha de 30 a 50 cen-\ntros de dados distribuídos na América do Norte, Europa, Ásia, América do Sul e Austrália. Alguns desses centros de \ndados acomodam mais de cem mil servidores, enquanto outros são menores, acomodando apenas centenas de servi-\ndores. Os centros de dados da Google são todos interconectados por meio de uma rede TCP/IP privativa, que se es-\npalha pelo mundo inteiro, mas apesar disso é separada da Internet pública. O importante é que essa rede privada só \ntransporta tráfego de/para servidores da Google. Como vemos na Figura 1.15, a rede privativa da Google tenta “con-\ntornar” as camadas mais altas da Internet emparelhando (sem custo) com outros ISPs de nível mais baixo, seja conec-\ntando diretamente ou interligando com eles em IXPs [Labovitz, 2010]. Entretanto, como muitos ISPs de acesso ainda \nFigura 1.15  Interconexão de ISPs\nISP de \nacesso \nISP de \nacesso \nISP de \nacesso \nISP de \nacesso \nISP de \nacesso \nISP de \nacesso \nISP de \nacesso \nISP de \nacesso \nISP \nregional\nISP de \nnível 1\nProvedor de \nconteúdo \n(p.ex., Google)\nISP de \nnível 1\nIXP\nISP \nregional\nIXP\nIXP\n   Redes de computadores e a Internet\n26\nsó podem ser alcançados transitando por redes de nível 1, a rede da Google também se conecta a ISPs de nível 1 e paga \na esses ISPs pelo tráfego que troca com eles. Criando sua própria rede, um provedor de conteúdo não apenas reduz \nseus pagamentos aos ISPs da camada mais alta, mas também tem maior controle de como seus serviços por fim são \nentregues aos usuários finais. A infraestrutura de rede da Google é descrita com mais detalhes na Seção 7.2.4.\nResumindo, a topologia da Internet é complexa, consistindo em uma dúzia ou mais de ISPs de nível 1 e cen-\ntenas de milhares de ISPs de níveis mais baixos. A cobertura dos ISPs é bastante diversificada; alguns abrangem \nvários continentes e oceanos e outros se limitam a pequenas regiões geográficas. Os ISPs de níveis mais baixos \nconectam-se a ISPs de níveis mais altos e estes se interconectam uns com os outros. Usuários e provedores de \nconteúdo são clientes de ISPs de níveis mais baixos e estes são clientes de ISPs de níveis mais altos. Nos últimos \nanos, os principais provedores de conteúdo também têm criado suas próprias redes e se conectam diretamente a \nISPs de níveis mais baixos, quando possível.\n1.4  \u0007\nAtraso, perda e vazão em redes de comutação de pacotes\nNa Seção 1.1 dissemos que a Internet pode ser vista como uma infraestrutura que fornece serviços a aplica-\nções distribuídas que são executadas nos sistemas finais. De modo ideal, gostaríamos que os serviços da Internet \ntransferissem tantos dados quanto desejamos entre dois sistemas finais, de modo instantâneo, sem nenhuma \nperda. É uma pena, mas esse é um objetivo muito elevado, algo inalcançável. Em vez disso, as redes de compu-\ntadores, necessariamente, restringem a vazão (a quantidade de dados por segundo que podem ser transferidos) \nentre sistemas finais, apresentam atrasos entre sistemas finais e podem perder pacotes. Por um lado, infelizmente \nas leis físicas da realidade introduzem atraso e perda, bem como restringem a vazão. Por outro, como as redes \nde computadores têm esses problemas, existem muitas questões fascinantes sobre como lidar com eles — ques-\ntões mais do que suficientes para preencher um curso de redes de computadores e motivar milhares de teses de \ndoutorado! Nesta seção, começaremos a examinar e quantificar atraso, perda e vazão em redes de computadores.\n1.4.1 \u0007\nUma visão geral de atraso em redes de comutação de pacotes\nLembre-se de que um pacote começa em um sistema final (a origem), passa por uma série de roteadores \ne termina sua jornada em outro sistema final (o destino). Quando um pacote viaja de um nó (sistema final ou \nroteador) ao nó subsequente (sistema final ou roteador), sofre, ao longo desse caminho, diversos tipos de atraso \nem cada nó. Os mais importantes deles são o atraso de processamento nodal, o atraso de fila, o atraso de trans-\nmissão e o atraso de propagação; juntos, eles se acumulam para formar o atraso nodal total. O desempenho de \nmuitas aplicações da Internet — como busca, navegação Web, e-mail, mapas, mensagens instantâneas e voz sobre \nIP — é bastante afetado por atrasos na rede. Para entender a fundo a comutação de pacotes e redes de computa-\ndores, é preciso entender a natureza e a importância desses atrasos.\nTipos de atraso\nVamos examinar esses atrasos no contexto da Figura 1.16. Como parte de sua rota fim a fim entre origem e \ndestino, um pacote é enviado do nó anterior por meio do roteador A até o roteador B. Nossa meta é caracterizar \no atraso nodal no roteador A. Note que este tem um enlace de saída que leva ao roteador B. Esse enlace é prece-\ndido de uma fila (também conhecida como buffer). Quando o pacote chega ao roteador A, vindo do nó anterior, \no roteador examina o cabeçalho do pacote para determinar o enlace de saída apropriado e então o direciona ao \nenlace. Nesse exemplo, o enlace de saída para o pacote é o que leva ao roteador B. Um pacote pode ser transmiti-\ndo por um enlace apenas se não houver nenhum outro sendo transmitido por ele e se não houver outros à sua \nfrente na fila. Se o enlace estiver ocupado, ou com pacotes à espera, o recém-chegado entrará na fila.\nRedes de computadores e a Internet  27 \nFigura 1.16  O atraso nodal no roteador A\nA\nB\nProcessamento \nnodal\nFila \n(esperando \npor transmissão)\nTransmissão\nPropagação\nAtraso de processamento\nO tempo exigido para examinar o cabeçalho do pacote e determinar para onde direcioná-lo é parte do \natraso de processamento, que pode também incluir outros fatores, como o tempo necessário para verificar os \nerros em bits existentes no pacote que ocorreram durante a transmissão dos bits desde o nó anterior ao roteador \nA. Atrasos de processamento em rotea­\ndores de alta velocidade em geral são da ordem de microssegundos, ou \nmenos. Depois desse processamento nodal, o roteador direciona o pacote à fila que precede o enlace com o rotea­\ndor B. (No Capítulo 4, estudaremos os detalhes da operação de um roteador.)\nAtraso de fila\nO pacote sofre um atraso de fila enquanto espera para ser transmitido no enlace. O tamanho desse atraso \ndependerá da quantidade de outros pacotes que chegarem antes e que já estiverem na fila esperando pela trans-\nmissão no enlace. Se a fila estiver vazia, e nenhum outro pacote estiver sendo transmitido naquele momento, en-\ntão o tempo de fila de nosso pacote será zero. Por outro lado, se o tráfego estiver intenso e houver muitos pacotes \ntambém esperando para ser transmitidos, o atraso de fila será longo. Em breve, veremos que o número de pacotes \nque um determinado pacote provavelmente encontrará ao chegar é uma função da intensidade e da natureza do \ntráfego que está chegando à fila. Na prática, atrasos de fila podem ser da ordem de micro a milissegundos.\nAtraso de transmissão\nAdmitindo-se que pacotes são transmitidos segundo a estratégia de “o primeiro a chegar será o pri-\nmeiro a ser processado”, como é comum em redes de comutação de pacotes, o nosso somente poderá ser \ntransmitido depois de todos os que chegaram antes terem sido enviados. Denominemos o tamanho do \npacote como L bits e a velocidade de transmissão do enlace do roteador A ao roteador B como R bits/s. Por \nexemplo, para um enlace Ethernet de 10 Mbits/s, a velocidade é R = 10 Mbits/s; para um enlace Ethernet de \n100 Mbits/s, a velocidade é R = 100 Mbits/s. O atraso de transmissão é L/R. Esta é a quantidade de tempo \nexigida para empurrar (isto é, transmitir) todos os bits do pacote para o enlace. Na prática, atrasos de trans-\nmissão costumam ser da ordem de micro a milissegundos.\nAtraso de propagação\nAssim que é lançado no enlace, um bit precisa se propagar até o roteador B. O tempo necessário para pro-\npagar o bit desde o início do enlace até o roteador B é o atraso de propagação. O bit se propaga à velocidade de \npropagação do enlace, a qual depende do meio físico (isto é, fibra ótica, par de fios de cobre trançado e assim por \ndiante) e está na faixa de\n2 ∙ 108 m/s  a  3 ∙ 108 m/s\n   Redes de computadores e a Internet\n28\nque é igual à velocidade da luz. O atraso de propagação é a distância entre dois roteadores dividida pela velocidade \nde propagação. Isto é, o atraso de propagação é d/s, sendo d a distância entre o roteador A e o roteador B, e s a \nvelocidade de propagação do enlace. Assim que o último bit do pacote se propagar até o nó B, ele e todos os outros \nbits precedentes serão armazenados no roteador B. Então, o processo inteiro continua, agora com o roteador B \nexecutando a retransmissão. Em redes WAN, os atrasos de propagação são da ordem de milissegundos.\nComparação entre atrasos de transmissão e de propagação\nOs principiantes na área de redes de computadores às vezes têm dificuldade para entender a diferença entre \natrasos de transmissão e de propagação. Ela é sutil, mas importante. O atraso de transmissão é a quantidade de \ntempo necessária para o roteador empurrar o pacote para fora; é uma função do comprimento do pacote e da taxa \nde transmissão do enlace, mas nada tem a ver com a distância entre os roteadores. O atraso de propagação, por \noutro lado, é o tempo que leva para um bit se propagar de um roteador até o seguinte; é uma função da distância \nentre os roteadores, mas nada tem a ver com o comprimento do pacote ou com a taxa de transmissão do enlace.\nPodemos esclarecer melhor as noções de atrasos de transmissão e de propagação com uma analogia. Con-\nsidere uma rodovia que tenha um posto de pedágio a cada 100 quilômetros, como mostrado na Figura 1.17. \nImagine que os trechos da rodovia entre os postos de pedágio sejam enlaces e que os postos de pedágio sejam \nroteadores. Suponha que os carros trafeguem (isto é, se propaguem) pela rodovia a uma velocidade de 100 km/h \n(isto é, quando o carro sai de um posto de pedágio, acelera instantaneamente até 100 km/h e mantém essa velo-\ncidade entre os dois postos de pedágio). Agora, considere que dez carros viajem em comboio, um atrás do outro, \nem ordem fixa. Imagine que cada carro seja um bit e que o comboio seja um pacote. Suponha ainda que cada \nposto de pedágio libere (isto é, transmita) um carro a cada 12 segundos, que seja tarde da noite e que os carros do \ncomboio sejam os únicos na estrada. Por fim, imagine que, ao chegar a um posto de pedágio, o primeiro carro do \ncomboio aguarde na entrada até que os outros nove cheguem e formem uma fila atrás dele. (Assim, o comboio \ninteiro deve ser “armazenado” no posto de pedágio antes de começar a ser “reenviado”\n.) O tempo necessário para \nque todo o comboio passe pelo posto de pedágio e volte à estrada é de (10 carros)/(5 carros/minuto) = 2 minutos, \nsemelhante ao atraso de transmissão em um roteador. O tempo necessário para um carro trafegar da saída de \num posto de pedágio até o próximo é de (100 km)/(100 km/h) = 1 hora, semelhante ao atraso de propagação. \nPortanto, o tempo decorrido entre o instante em que o comboio é “armazenado” em frente a um posto de pedágio \naté o momento em que é “armazenado” em frente ao seguinte é a soma do atraso de transmissão e do atraso de \npropagação — nesse exemplo, 62 minutos.\nVamos explorar um pouco mais essa analogia. O que aconteceria se o tempo de liberação do comboio no \nposto de pedágio fosse maior do que o tempo que um carro leva para trafegar entre dois postos? Por exemplo, su-\nponha que os carros trafeguem a uma velocidade de 1.000 km/h e que o pedágio libere um carro por minuto. En-\ntão, o atraso de trânsito entre dois postos de pedágio é de 6 minutos e o tempo de liberação do comboio no posto \nde pedágio é de 10 minutos. Nesse caso, os primeiros carros do comboio chegarão ao segundo posto de pedágio \nantes que os últimos carros saiam do primeiro posto. Essa situação também acontece em redes de comutação \nde pacotes — os primeiros bits de um pacote podem chegar a um roteador enquanto muitos dos remanescentes \nainda estão esperando para ser transmitidos pelo roteador precedente.\nFigura 1.17  Analogia do comboio\nComboio \nde dez carros\nPedágio\nPedágio\n100 km\n100 km\nRedes de computadores e a Internet  29 \nSe uma imagem vale mil palavras, então uma animação vale um milhão. O site de apoio deste livro apresen-\nta um aplicativo interativo Java que ilustra e compara o atraso de transmissão com o de propagação. Recomenda­\n‑se que o leitor visite esse aplicativo. Smith [2009] também oferece uma discussão bastante legível sobre atrasos \nde propagação, fila e transmissão.\nSe dproc, dfila, dtrans e dprop forem, respectivamente, os atrasos de processamento, de fila, de transmissão e de \npropagação, então o atraso nodal total é dado por:\ndnodal  = dproc + dfila + dtrans + dprop\nA contribuição desses componentes do atraso pode variar significativamente. Por exemplo, dprop pode ser \ndesprezível (por exemplo, dois microssegundos) para um enlace que conecta­\n dois roteadores no mesmo campus \nuniversitário; contudo, é de centenas de milissegundos para dois roteadores interconectados por um enlace de \nsatélite geoestacionário e pode ser o termo dominante no dnodal. De maneira semelhante, dtrans pode variar de des-\nprezível a significativo. Sua contribuição costuma ser desprezível para velocidades de transmissão de 10 Mbits/s \ne mais altas (por exemplo, em LANs); contudo, pode ser de centenas de milissegundos para grandes pacotes de \nInternet enviados por enlaces de modems discados de baixa velocidade. O atraso de processamento, dproc, é quase \nsempre desprezível; no entanto, tem forte influência sobre a produtividade máxima de um roteador, que é a velo-\ncidade máxima com que ele pode encaminhar pacotes.\n1.4.2  Atraso de fila e perda de pacote\nO mais complicado e interessante componente do atraso nodal é o atraso de fila, dfila. Na verdade, o atraso \nde fila é tão importante e interessante em redes de computadores que milhares de artigos e numerosos livros já \nforam escritos sobre ele [Bertsekas, 1991; Daigle, 1991; Kleinrock, 1975, 1976; Ross, 1995]. Neste livro, faremos \napenas uma discussão intuitiva, de alto nível, sobre o atraso de fila; o leitor mais curioso pode consultar alguns \ndos livros citados (ou até mesmo escrever uma tese sobre o assunto!). Diferente dos três outros atrasos (a saber, \ndproc, dtrans e dprop), o atraso de fila pode variar de pacote a pacote. Por exemplo, se dez pacotes chegarem a uma fila \nvazia ao mesmo tempo, o primeiro pacote transmitido não sofrerá nenhum atraso de fila, ao passo que o último \nsofrerá um relativamente grande (enquanto espera que os outros nove sejam transmitidos). Por conseguinte, \npara se caracterizar um atraso de fila, são utilizadas em geral medições estatísticas, tais como atraso de fila médio, \nvariância do atraso de fila e a probabilidade de que ele exceda um valor especificado.\nQuando o atraso de fila é grande e quando é insignificante? A resposta a essa pergunta depende da velo-\ncidade de transmissão do enlace, da taxa com que o tráfego chega à fila e de sua natureza, isto é, se de modo \nintermitente, em rajadas. Para entendermos melhor, vamos adotar a para representar a taxa média com que os \npacotes chegam à fila (a é medida em pacotes/segundo). Lembre-se de que R é a taxa de transmissão, isto é, a taxa \n(em bits/segundo) com que os bits são retirados da fila. Suponha também, para simplificar, que todos os pacotes \ntenham L bits. Então, a taxa média com que os bits chegam à fila é La bits/s. Por fim, imagine que a fila seja muito \nlonga, de modo que possa conter um número infinito de bits. A razão La/R, denominada intensidade de tráfe-\ngo, costuma desempenhar um papel importante na estimativa do tamanho do atraso de fila. Se La/R >1, então a \nvelocidade média com que os bits chegam à fila excederá aquela com que eles podem ser transmitidos para fora \nda fila. Nessa situação desastrosa, a fila tenderá a aumentar sem limite e o atraso de fila tenderá ao infinito! Por \nconseguinte, uma das regras de ouro da engenharia de tráfego é: projete seu sistema de modo que a intensidade de \ntráfego não seja maior do que 1.\nAgora, considere o caso em que La/R ≤ 1. Aqui, a natureza do tráfego influencia o atraso de fila. Por exem-\nplo, se pacotes chegarem periodicamente — isto é, se chegar um pacote a cada L/R segundos —, então todos os \npacotes chegarão a uma fila vazia e não haverá atraso. Por outro lado, se chegarem em rajadas, mas periodicamen-\nte, poderá haver um significativo atraso de fila médio. Por exemplo, suponha que N pacotes cheguem ao mesmo \ntempo a cada (L/R)N segundos. Então, o primeiro pacote transmitido não sofrerá atraso de fila; o segundo terá \n   Redes de computadores e a Internet\n30\num atraso de L/R segundos e, de modo mais geral, o enésimo pacote transmitido terá um atraso de fila de (n – 1)\nL/R segundos. Deixamos como exercício para o leitor o cálculo do atraso de fila médio para esse exemplo.\nOs dois exemplos de chegadas periódicas que acabamos de descrever são um tanto acadêmicos. Em geral, o \nprocesso de chegada a uma fila é aleatório — isto é, não segue um padrão e os intervalos de tempo entre os paco-\ntes são ao acaso. Nessa hipótese mais realista, a quantidade La/R quase sempre não é suficiente para caracterizar \npor completo a estatística do atraso. Não obstante, é útil para entender intuitivamente a extensão do atraso de fila. \nEm especial, se a intensidade de tráfego for próxima de zero, então as chegadas de pacotes serão poucas e bem \nespaçadas e é improvável que um pacote que esteja chegando encontre outro na fila. Consequentemente, o atraso \nde fila médio será próximo de zero. Por outro lado, quando a intensidade de tráfego for próxima de 1, haverá in-\ntervalos de tempo em que a velocidade de chegada excederá a capacidade de transmissão (por causa das variações \nna taxa de chegada do pacote) e uma fila será formada durante esses períodos; quando a taxa de chegada for \nmenor do que a capacidade de transmissão, a extensão da fila diminuirá. Todavia, à medida que a intensidade de \ntráfego se aproxima de 1, o comprimento médio da fila fica cada vez maior. A dependência qualitativa entre o \natraso de fila médio e a intensidade de tráfego é mostrada na Figura 1.18.\nUm aspecto importante a observar na Figura 1.18 é que, quando a intensidade de tráfego se aproxima de 1, \no atraso de fila médio aumenta depressa. Uma pequena porcentagem de ­\naumento na intensidade resulta em um \naumento muito maior no atraso, em termos de porcentagem. Talvez você já tenha percebido esse fenômeno na \nestrada. Se você dirige regularmente por uma estrada que costuma ser congestionada, o fato de ela estar sempre \nassim significa que a intensidade de tráfego é próxima de 1. Se algum evento causar um tráfego um pouco maior \ndo que o normal, as demoras que você sofrerá poderão ser enormes.\nPara compreender um pouco mais os atrasos de fila, visite o site de apoio do livro, que apresenta um apli-\ncativo Java interativo para uma fila. Se você aumentar a taxa de chegada do pacote o suficiente de forma que a \nintensidade do tráfego exceda 1, verá a fila aumentar ao longo do tempo.\nPerda de pacotes\nNa discussão anterior, admitimos que a fila é capaz de conter um número infinito de pacotes. Na realidade, a \ncapacidade da fila que precede um enlace é finita, embora a sua formação dependa bastante do projeto e do custo do \ncomutador. Como a capacidade da fila é finita, na verdade os atrasos de pacote não se aproximam do infinito quan-\ndo a intensidade de tráfego se aproxima de 1. O que acontece de fato é que um pacote pode chegar e encontrar uma \nfila cheia. Sem espaço disponível para armazená-lo, o roteador o descartará; isto é, ele será perdido. Esse excesso \nem uma fila pode ser observado novamente no aplicativo Java quando a intensidade do tráfego é maior do que 1.\nDo ponto de vista de um sistema final, uma perda de pacote é vista como um pacote que foi transmitido \npara o núcleo da rede, mas sem nunca ter emergido dele no destino. A fração de pacotes perdidos aumenta com \nAtraso de ﬁla médio\nLa/R           \n1\nFigura 1.18  Dependência entre atraso de fila médio e intensidade de tráfego\nRedes de computadores e a Internet  31 \no aumento da intensidade de tráfego. Por conseguinte, o desempenho em um nó costuma ser medido não ape-\nnas em termos de atraso, mas também da probabilidade de perda de pacotes. Como discutiremos nos capítulos \nsubsequentes, um pacote perdido pode ser retransmitido fim a fim para garantir que todos os dados sejam trans-\nferidos da origem ao local de destino.\n1.4.3  Atraso fim a fim\nAté o momento, nossa discussão focalizou o atraso nodal, isto é, em um único roteador. Concluiremos \nessa discussão considerando brevemente o atraso da origem ao destino. Para entender esse conceito, suponha \nque haja N – 1 roteadores entre a máquina de origem e a de destino. Imagine também que a rede não esteja \ncongestionada (e, portanto, os atrasos de fila sejam desprezíveis), que o atraso de processamento em cada \nroteador e na máquina de origem seja dproc, que a taxa de transmissão de saída de cada roteador e da máquina \nde origem seja R bits/s e que atraso de propagação em cada enlace seja dprop. Os atrasos nodais se acumulam e \nresultam em um atraso fim a fim,\n\t\ndfim a fim = N (dproc + dtrans + dprop)\t\n(1.2) \nem que, mais uma vez, dtrans = L/R e L é o tamanho do pacote. Note que a Equação 1.2 é uma generalização da \nEquação 1.1, na qual não levamos em conta os atrasos de processamento e propagação. Convidamos você a \ngeneralizar a Equação 1.2 para o caso de atrasos heterogêneos nos nós e para o caso de um atraso de fila médio \nem cada nó.\nTraceroute\nPara perceber o que é de fato o atraso em uma rede de computadores, podemos utilizar o Traceroute, pro-\ngrama de diagnóstico que pode ser executado em qualquer hospedeiro da Internet. Quando o usuário especifica \num nome de hospedeiro de destino, o programa no hospedeiro de origem envia vários pacotes especiais em dire-\nção àquele destino. Ao seguir seu caminho até o destino, esses pacotes passam por uma série de roteadores. Um \ndeles recebe um desses pacotes especiais e envia à origem uma curta mensagem, contendo o nome e o endereço \ndo roteador.\nMais especificamente, suponha que haja N – 1 roteadores entre a origem e o destino. Então, a fonte enviará \nN pacotes especiais à rede e cada um deles estará endereçado ao destino final. Esses N pacotes especiais serão \nmarcados de 1 a N, sendo a marca do primeiro pacote 1 e a do último, N. Assim que o enésimo roteador recebe o \nenésimo pacote com a marca n, não envia o pacote a seu destino, mas uma mensagem à origem. Quando o hos-\npedeiro de destino recebe o pacote N, também envia uma mensagem à origem, que registra o tempo transcorrido \nentre o envio de um pacote e o recebimento da mensagem de retorno correspondente. A origem registra também \no nome e o endereço do roteador (ou do hospedeiro de destino) que retorna a mensagem. Dessa maneira, a ori-\ngem pode reconstruir a rota tomada pelos pacotes que vão da origem ao destino e pode determinar os atrasos de \nida e volta para todos os roteadores intervenientes. Na realidade, o programa Traceroute repete o processo que \nacabamos de descrever três vezes, de modo que a fonte envia, na verdade, 3  N pacotes ao destino. O RFC 1393 \ndescreve detalhadamente o Traceroute.\nEis um exemplo de resultado do programa Traceroute, no qual a rota traçada ia do hospedeiro de origem \ngaia.cs.umass.edu (na Universidade de Massachusetts) até cis.poly.edu (na Polytechnic University no Brooklyn). \nO resultado tem seis colunas: a primeira é o valor n descrito, isto é, o número do roteador ao longo da rota; a se-\ngunda é o nome do roteador; a terceira é o endereço do roteador (na forma xxx.xxx.xxx.xxx); as últimas três são \nos atrasos de ida e volta para três tentativas. Se a fonte receber menos do que três mensagens de qualquer roteador \ndeterminado (por causa da perda de pacotes na rede), o Traceroute coloca um asterisco logo após o número do \nroteador e registra menos do que três tempos de duração de ida e volta para aquele roteador.\n   Redes de computadores e a Internet\n32\n1 cs-gw (128.119.240.254) 1.009 ms 0.899 ms 0.993 ms\n2 128.119.3.154 (128.119.3.154) 0.931 ms 0.441 ms 0.651 ms\n3 border4-rt-gi-1-3.gw.umass.edu (128.119.2.194) 1.032 ms 0.484 ms 0.451 ms\n4 acr1-ge-2-1-0.Boston.cw.net (208.172.51.129) 10.006 ms 8.150 ms 8.460 ms\n5 agr4-loopback.NewYork.cw.net (206.24.194.104) 12.272 ms 14.344 ms 13.267 ms\n6 acr2-loopback.NewYork.cw.net (206.24.194.62) 13.225 ms 12.292 ms 12.148 ms\n7 pos10-2.core2.NewYork1.Level3.net (209.244.160.133) 12.218 ms 11.823 ms 11.793 ms\n8 gige9-1-52.hsipaccess1.NewYork1.Level3.net (64.159.17.39) 13.081 ms 11.556 ms 13.297 \nms\n9 p0-0.polyu.bbnplanet.net (4.25.109.122) 12.716 ms 13.052 ms 12.786 ms\n10 cis.poly.edu (128.238.32.126) 14.080 ms 13.035 ms 12.802 ms\nNo exemplo anterior há nove roteadores entre a origem e o destino. Quase todos eles têm um nome e to-\ndos têm endereços. Por exemplo, o nome do Roteador 3 é border4-rt-gi-1-3.gw.umass.edu e seu \nendereço é 128.119.2.194. Examinando os dados apresentados para ele, verificamos que, na primeira das \ntrês tentativas, o atraso de ida e volta entre a origem e o roteador foi de 1,03 ms. Os atrasos de ida e volta para as \nduas tentativas subsequentes foram 0,48 e 0,45 ms, e incluem todos os atrasos que acabamos de discutir, ou seja, \nde transmissão, de propagação, de processamento do roteador e de fila. Como o atraso de fila varia com o tempo, \no atraso de ida e volta do pacote n enviado a um roteador n pode, às vezes, ser maior do que o do pacote n+1 \nenviado ao roteador n+1. Realmente, observamos esse fenômeno no exemplo anterior: os atrasos do roteador 6 \nsão maiores que os do roteador 7!\nVocê quer experimentar o Traceroute por conta própria? Recomendamos muito que visite o site <http://\nwww. traceroute.org>, que oferece uma interface Web para uma extensa lista de fontes para traçar rotas. Escolha \numa fonte, forneça o nome de hospedeiro para qualquer destino e o programa Traceroute fará todo o trabalho. \nExistem muitos programas de software gratuitos que apresentam uma interface gráfica para o Traceroute; um dos \nnossos favoritos é o PingPlotter [PingPlotter, 2012].\nSistema final, aplicativo e outros atrasos\nAlém dos atrasos de processamento, transmissão e de propagação, os sistemas finais podem adicionar outros \natrasos significativos. Por exemplo, um sistema final que quer transmitir um pacote para uma mídia compartilha-\nda (por exemplo, como em um cenário Wi-Fi ou modem a cabo) pode, intencionalmente, atrasar sua transmissão \ncomo parte de seu protocolo por compartilhar a mídia com outros sistemas finais; vamos analisar tais protocolos \nem detalhes no Capítulo 5. Outro importante atraso é o atraso de empacotamento de mídia, o qual está presente \nnos aplicativos VoIP (voz sobre IP). No VoIP, o remetente deve primeiro carregar um pacote com voz digitalizada \ne codificada antes de transmitir o pacote para a Internet. Esse tempo para carregar um pacote — chamado de \natraso de empacotamento — pode ser significativo e ter impacto sobre a qualidade visível pelo usuário de uma \nchamada VoIP. Esse assunto será explorado mais adiante nos exercícios de fixação no final deste capítulo.\n1.4.4  Vazão nas redes de computadores\nAlém do atraso e da perda de pacotes, outra medida de desempenho importante em redes de computadores \né a vazão fim a fim. Para definir vazão, considere a transferência de um arquivo grande do hospedeiro A para o \nhospedeiro B por uma rede de computadores. Essa transferência pode ser, por exemplo, um videoclipe extenso de \num parceiro para outro por meio do sistema de compartilhamento de arquivos P2P. A vazão instantânea a qual-\nquer momento é a taxa (em bits/s) em que o hospedeiro B está recebendo o arquivo. (Muitos aplicativos, incluin-\ndo muitos sistemas de compartilhamento P2P, exibem a vazão instantânea durante os downloads na interface do \nusuário — talvez você já tenha observado isso!) Se o arquivo consistir em F bits e a transferência levar T segun-\ndos para o hospedeiro B receber todos os F bits, então a vazão média da transferência do arquivo é F/T bits/s. \nRedes de computadores e a Internet  33 \nPara algumas aplicações, como a telefonia via Internet, é desejável ter um atraso baixo e uma vazão instantânea \nacima de algum limiar (por exemplo, superior a 24 kbits/s para aplicações de telefonia via Internet, e superior a \n \n256 kbits/s para algumas aplicações de vídeo em tempo real). Para outras aplicações, incluindo as de transferência \nde arquivo, o atraso não é importante, mas é recomendado ter a vazão mais alta possível.\nPara obter uma visão mais detalhada do importante conceito de vazão, vamos analisar alguns exemplos. A \nFigura 1.19(a) mostra dois sistemas finais, um servidor e um cliente, conectados por dois enlaces de comunicação \ne um roteador. Considere a vazão para uma transferência de arquivo do servidor para o cliente. Suponha que Rs \nseja a taxa do enlace entre o servidor e o roteador; e Rc seja a taxa do enlace entre o roteador e o cliente. Imagine \nque os únicos bits enviados na rede inteira sejam os do servidor para o cliente. Agora vem a pergunta: nesse cená-\nrio ideal, qual é a vazão servidor-para-cliente? Para responder, pense nos bits como um líquido e nos enlaces de \ncomunicação como tubo. Claro, o servidor não pode enviar os bits através de seu enlace a uma taxa mais rápida \ndo que Rs bits/s; e o roteador não pode encaminhar os bits a uma taxa mais rápida do que Rc bits/s. Se Rs < Rc, \nentão os bits enviados pelo servidor “fluirão” diretamente pelo roteador e chegarão ao cliente a uma taxa de Rs \nbits/s, gerando uma vazão de Rs bits/s. Se, por outro lado, Rc < Rs, então o roteador não poderá encaminhar os bits \ntão rápido quanto ele os recebe. Neste caso, os bits somente deixarão o roteador a uma taxa Rc, dando uma vazão \nfim a fim de Rc. (Observe também que se os bits continuarem a chegar no roteador a uma taxa Rs, e a deixá-lo a \numa taxa Rc, o acúmulo de bits esperando para transmissão ao cliente só aumentará — uma situação, na maioria \ndas vezes, indesejável!) Assim, para essa rede simples de dois enlaces, a vazão é mín{Rc, Rs}, ou seja, é a taxa de \ntransmissão do enlace de gargalo. Após determinar a vazão, agora podemos aproximar o tempo que leva para \ntransferir um arquivo grande de F bits do servidor ao cliente como F/mín{Rs, Rc,}. Para um exemplo específico, \nsuponha que você está fazendo o download de um arquivo MP3 de F = 32 milhões de bits, o servidor tem uma \ntaxa de transmissão de Rs = 2 Mbits/s, e você tem um enlace de acesso de Rc = 1 Mbit/s. O tempo necessário para \ntransferir o arquivo é, então, 32 segundos. Claro que essas expressões para tempo de vazão e de transferência são \napenas aproximações, já que elas não consideram os atrasos para armazenar-e-reenviar e de processamento, bem \ncomo assuntos relacionados a protocolos.\nA Figura 1.19(b) agora mostra uma rede com N enlaces entre o servidor e o cliente, com as taxas de trans-\nmissão R1, R2, ..., RN. Aplicando a mesma análise da rede de dois enlaces, descobrimos que a vazão para uma \ntransferência de arquivo do servidor ao cliente é mín{R1, R2, ..., RN}, a qual é novamente a taxa de transmissão do \nenlace de gargalo ao longo do caminho entre o servidor e o cliente.\nAgora considere outro exemplo motivado pela Internet de hoje. A Figura 1.20(a) mostra dois sistemas \nfinais, um servidor e um cliente, conectados a uma rede de computadores. Considere a vazão para uma transfe-\nrência de arquivo do servidor ao cliente. O servidor está conectado à rede com um enlace de acesso de taxa Rs e \no cliente está conectado à rede com um enlace de acesso de Rc. Agora suponha que todos os enlaces no núcleo \nda rede de comunicação tenham taxas de transmissão muito altas, muito maiores do que Rs e Rc. De fato, hoje, o \nServidor\nRs\nR1\nR2\nRN\nRc\nCliente\nServidor\na.\nb.\nCliente\nFigura 1.19  Vazão para uma transferência de arquivo do servidor ao cliente\n   Redes de computadores e a Internet\n34\nnúcleo da Internet está superabastecido com enlaces de alta velocidade que sofrem pouco congestionamento. Su-\nponha, também, que os únicos bits que estão sendo enviados em toda a rede sejam os do servidor para o cliente. \nComo o núcleo da rede de computadores é como um tubo largo neste exemplo, a taxa a qual os bits correm da \norigem ao destino é novamente o mínimo de Rs e Rc, ou seja, vazão = mín{Rs, Rc}. Portanto, o fator restritivo para \nvazão na Internet de hoje é, em geral, a rede de acesso.\nPara um exemplo final, considere a Figura 1.20(b), na qual existem dez servidores e dez clientes conectados ao \nnúcleo da rede de computadores. Nesse exemplo, dez downloads simultâneos estão sendo realizados, envolven-\ndo dez pares cliente-servidor. Suponha que esses downloads sejam o único tráfego na rede no momento. Como \nmostrado na figura, há um enlace no núcleo que é atravessado por todos os dez downloads. Considere R a taxa \nde transmissão desse enlace. Imagine que todos os enlaces de acesso do servidor possuem a mesma taxa Rs, todos \nos enlaces de acesso do cliente possuem a mesma taxa Rc e a taxa de transmissão de todos os enlaces no núcleo \n— com exceção de um enlace comum de taxa R — sejam muito maiores do que Rs, Rc e R. Agora perguntamos: \nquais são as vazões de download? É claro que se a taxa do enlace comum, R, é grande — digamos, cem vezes \nmaior do que Rs e Rc —, então a vazão para cada download será novamente mín{Rs, Rc}. Mas e se essa taxa for da \nmesma ordem que Rs e Rc? Qual será a vazão nesse caso? Vamos observar um exemplo específico. Suponha \nque Rs = 2 Mbits/s, Rc = 1 Mbit/s, R = 5 Mbits/s, e o enlace comum divide sua taxa de transmissão por igual en-\ntre 10 downloads. Então, o gargalo para cada download não se encontra mais na rede de acesso, mas é o enlace \ncompartilhado no núcleo, que somente fornece para cada download 500 kbits/s de vazão. Desse modo, a vazão \nfim a fim é agora reduzida a 500 kbits/s por download.\nOs exemplos nas Figuras 1.19 e 1.20(a) mostram que a vazão depende das taxas de transmissão dos enlaces \nsobre as quais os dados fluem. Vimos que quando não há tráfego interveniente, a vazão pode apenas ser aproxi-\nmada como a taxa de transmissão mínima ao longo do caminho entre a origem e o local de destino. O exemplo \nna Figura 1.20(b) mostra que, de modo geral, a vazão depende não somente das taxas de transmissão dos enla-\nces ao longo do caminho, mas também do tráfego interveniente. Em especial, um enlace com uma alta taxa de \ntransmissão pode, apesar disso, ser o enlace de gargalo para uma transferência de arquivo, caso muitos outros \nServidor\nRs\nRc\na.\nb.\nCliente\n10 clientes\n10 servidores\nEnlace de \ngargalo de \ncapacidade R\nFigura 1.20  \u0007\nVazão fim a fim: (a) O cliente baixa um arquivo do servidor; (b) 10 clientes fazem o \ndownload com 10 servidores\nRedes de computadores e a Internet  35 \nfluxos de dados estejam também passando por aquele enlace. Analisaremos em mais detalhes a vazão em redes \nde computadores nos exercícios de fixação e nos capítulos subsequentes.\n1.5  \u0007\nCamadas de protocolo e seus modelos de serviço\nAté aqui, nossa discussão demonstrou que a Internet é um sistema extremamente complicado e que possui \nmuitos componentes: inúmeras aplicações e protocolos, vários tipos de sistemas finais e conexões entre eles, \ncomutadores de pacotes, além de vários tipos de mídia em nível de enlace. Dada essa enorme complexidade, \nhá alguma esperança de organizar a arquitetura de rede ou, ao menos, nossa discussão sobre ela? Felizmente, a \nresposta a ambas as perguntas é sim.\n1.5.1 Arquitetura de camadas\nAntes de tentarmos organizar nosso raciocínio sobre a arquitetura da Internet, vamos procurar uma analo-\ngia humana. Na verdade, lidamos com sistemas complexos o tempo todo em nosso dia a dia. Imagine se alguém \npedisse que você descrevesse, por exemplo, o sistema de uma companhia aérea. Como você encontraria a estru-\ntura para descrever esse sistema complexo que tem agências de emissão de passagens, pessoal para embarcar a \nbagagem, para ficar no portão de embarque, pilotos, aviões, controle de tráfego aéreo e um sistema mundial de \nroteamento de aeronaves? Um modo poderia ser apresentar a relação de uma série de ações que você realiza (ou \nque outros executam para você) quando voa por uma empresa aérea. Você compra a passagem, despacha suas \nmalas, dirige-se ao portão de embarque e, por fim, entra no avião, que decola e segue uma rota até seu destino. \nApós a aterrissagem, você desembarca no portão designado e recupera suas malas. Se a viagem foi ruim, você \nreclama na agência que lhe vendeu a passagem (esforço em vão). Esse cenário é ilustrado na Figura 1.21.\nJá podemos notar aqui algumas analogias com redes de computadores: você está sendo despachado da ori-\ngem ao destino pela companhia aérea; um pacote é despachado da máquina de origem à máquina de destino na \nInternet. Mas essa não é exatamente a analogia que buscamos. Estamos tentando encontrar alguma estrutura na \nFigura 1.21. Observando-a, notamos que há uma função referente à passagem em cada ponta; há também uma \nfunção de bagagem para passageiros que já apresentaram o bilhete e uma de portão de embarque para os que já \napresentaram o tíquete e despacharam as malas. Para passageiros que já passaram pelo portão de embarque (isto \né, aqueles que já apresentaram a passagem, despacharam a bagagem e passaram pelo portão), há uma função de \ndecolagem e de aterrissagem e, durante o voo, uma função de roteamento do avião. Isso sugere que podemos \nexaminar a funcionalidade na Figura 1.21 na horizontal, como mostra a Figura 1.22.\nFigura 1.21  Uma viagem de avião: ações\nPassagem (comprar)\nBagagem (despachar)\nPortões (embarcar)\nDecolagem\nRoteamento da aeronave\nPassagem (reclamar)\nBagagem (recuperar)\nPortões (desembarcar)\nAterrissagem\nRoteamento da aeronave\nRoteamento da aeronave\n   Redes de computadores e a Internet\n36\nA Figura 1.22 dividiu a funcionalidade da linha aérea em camadas, provendo uma estrutura com a qual pode-\nmos discutir a viagem aérea. Note que cada camada, combinada com as que estão abaixo dela, implementa alguma \nfuncionalidade, algum serviço. Na camada da passagem aérea e abaixo dela, é realizada a transferência “balcão-\nde-linha-aérea-balcão-de-linha-aérea” de um passageiro. Na camada de bagagem e abaixo dela, é realizada a trans-\nferência “despacho-de-bagagem–recuperação-de-bagagem” de um passageiro e de suas malas. Note que a camada \nda bagagem provê esse serviço apenas para a pessoa que já apresentou o bilhete. Na camada do portão, é realizada \na transferência “portão-de-embarque-portão-de­\n‑desembarque” do viajante e de suas malas. Na camada de decola-\ngem/aterrissagem, é realizada a transferência “pista-a-pista” de passageiros e de suas bagagens. Cada camada provê \nseu serviço (1) realizando certas ações dentro dela (por exemplo, na camada do portão, embarcar e desembarcar \npessoas de um avião) e (2) utilizando os serviços da camada imediatamente inferior (por exemplo, na do portão, \naproveitando o serviço de transferência “pista-a-pista” de passageiros da camada de decolagem/aterrissagem).\nUma arquitetura de camadas nos permite discutir uma parcela específica e bem definida de um sistema \ngrande e complexo. Essa simplificação tem considerável valor intrínseco, pois provê modularidade, tornando \nmuito mais fácil modificar a execução do serviço prestado pela camada. Contanto que a camada forneça o mesmo \nserviço para a que está acima e use os mesmos serviços da que vem abaixo dela, o restante do sistema permanece \ninalterado quando a sua realização é modificada. (Note que modificar a implementação de um serviço é muito \ndiferente de mudar o serviço em si!) Por exemplo, se as funções de portão fossem modificadas (digamos que \npassassem a embarcar e desembarcar passageiros por ordem de altura), o restante do sistema da linha aérea per-\nmaneceria inalterado, já que a camada do portão continuaria a prover a mesma função (embarcar e desembarcar \npassageiros); ela apenas executaria aquela função de maneira diferente após a alteração. Para sistemas grandes e \ncomplexos que são atualizados constantemente, a capacidade de modificar a realização de um serviço sem afetar \noutros componentes do sistema é outra vantagem importante da divisão em camadas.\nCamadas de protocolo\nMas chega de linhas aéreas! Vamos agora voltar nossa atenção a protocolos de rede. Para prover uma es-\ntrutura para o projeto, projetistas de rede organizam protocolos — e o hardware e o software de rede que os \nexecutam — em camadas. Cada protocolo pertence a uma das camadas, assim como cada função na arquitetura \nde linha aérea da Figura 1.22 pertencia a uma camada. Mais uma vez estamos interessados nos serviços que uma \ncamada oferece à camada acima dela — denominado modelo de serviço. Assim como em nosso exemplo da \nlinha aérea, cada camada provê seu serviço (1) executando certas ações dentro dela e (2) utilizando os serviços \nda camada diretamente abaixo dela. Por exemplo, os serviços providos pela camada n podem incluir entrega \nconfiável de mensagens de uma extremidade da rede à outra, que pode ser implementada utilizando um serviço \nnão confiável de entrega de mensagem fim a fim da camada n – 1 e adicionando funcionalidade da camada n para \ndetectar e retransmitir mensagens perdidas.\nFigura 1.22  Camadas horizontais da funcionalidade de linha aérea\nPassagem aérea \n(comprar)\nBagagem \n(despachar)\nPortões \n(embarcar)\nDecolagem\nRoteamento \nde aeronave\nRoteamento \nde aeronave\nRoteamento \nde aeronave\nRoteamento \nde aeronave\nPassagem \n(reclamar)\nBagagem \n(recuperar)\nPortões \n(desembarcar)\nAterrissagem\nPassagem\nBagagem\nPortão\nDecolagem/Aterrissagem\nRoteamento de aeronave\nAeroporto de origem\nAeroporto de destino\nCentrais intermediárias \nde controle de tráfego aéreo\nRedes de computadores e a Internet  37 \nUma camada de protocolo pode ser executada em software, em hardware, ou em uma combinação dos dois. \nProtocolos de camada de aplicação — como HTTP e SMTP — quase sempre são realizados em software nos \nsistemas finais; o mesmo acontece com protocolos de camada de transporte. Como a camada física e as de enlace \nde dados são responsáveis pelo manuseio da comunicação por um enlace específico, em geral são executadas em \numa placa de interface de rede (por exemplo, placas de interface Ethernet ou Wi-Fi) associadas a determinado \nenlace. A camada de rede quase sempre é uma execução mista de hardware e software. Note também que, tal \ncomo as funções na arquitetura em camadas da linha aérea eram distribuídas entre os vários aeroportos e centrais \nde controle de tráfego aéreo que compunham o sistema, um protocolo de camada n é distribuído entre sistemas \nfinais, comutadores de pacote e outros componentes que formam a rede. Isto é, há sempre uma parte de um pro-\ntocolo de camada n em cada componente de rede.\nO sistema de camadas de protocolos tem vantagens conceituais e estruturais [RFC 3439]. Como vimos, a \ndivisão em camadas proporciona um modo estruturado de discutir componentes de sistemas. A modularidade \nfacilita a atualização de componentes de sistema. Devemos mencionar, no entanto, que alguns pesquisadores \ne engenheiros de rede se opõem veementemente ao sistema de camadas [Wakeman, 1992]. Uma desvantagem \npotencial é que uma camada pode duplicar a funcionalidade de uma inferior. Por exemplo, muitas pilhas de \nprotocolos oferecem serviço de recuperação de erros para cada enlace e também de fim a fim. Uma segunda \ndesvantagem é que a funcionalidade em uma camada pode necessitar de informações (por exemplo, um valor de \ncarimbo de tempo) que estão presentes somente em outra, o que infringe o objetivo de separação de camadas.\nQuando tomados em conjunto, os protocolos das várias camadas são denominados pilha de protocolos. A \npilha de protocolos da Internet é formada por cinco camadas: física, de enlace, de rede, de transporte e de apli-\ncação, como mostra a Figura 1.23(a). Se você verificar o sumário, verá que organizamos este livro utilizando as \ncamadas da pilha de protocolos da Internet. Fazemos uma abordagem top-down (de cima para baixo), primeiro \nabordando a camada de aplicação e prosseguindo para baixo.\nCamada de aplicação\nA camada de aplicação é onde residem aplicações de rede e seus protocolos. A camada de aplicação da Inter-\nnet inclui muitos protocolos, tais como o HTTP (que provê requisição e transferência de documentos pela Web), o \nSMTP (que provê transferência de mensagens de correio eletrônico) e o FTP (que provê a transferência de arquivos \nentre dois sistemas finais). Veremos que certas funções de rede, como a tradução de nomes fáceis de entender, que \nsão dados a sistemas finais da Internet (por exemplo, de <www.ietf.org> para um endereço de rede de 32 bits), tam-\nbém são executadas com a ajuda de um protocolo de camada de aplicação, no caso, o sistema de nomes de domínio \nFigura 1.23  A pilha de protocolos da Internet (a) e o modelo de referência OSI (b)\nTransporte\nAplicação\nRede\nEnlace\nFísico\na. Pilha de protocolos \nda Internet de \ncinco camadas\nTransporte\nSessão\nAplicação\nApresentação\nRede\nEnlace\nFísico\nb.  \nModelo de referência \nISO de sete camadas\n   Redes de computadores e a Internet\n38\n(domain name system — DNS). Veremos no Capítulo 2 que é muito fácil criar nossos próprios novos protocolos \nde camada de aplicação.\nUm protocolo de camada de aplicação é distribuído por diversos sistemas finais, e a aplicação em um siste-\nma final utiliza o protocolo para trocar pacotes de informação com a aplicação em outro sistema final. Chamare-\nmos de mensagem a esse pacote de informação na camada de aplicação.\nCamada de transporte\nA camada de transporte da Internet carrega mensagens da camada de aplicação entre os lados do cliente e \nservidor de uma aplicação. Há dois protocolos de transporte na Internet: TCP e UDP, e qualquer um pode levar \nmensagens da camada de aplicação. O TCP provê serviços orientados a conexão para suas aplicações. Alguns \ndesses serviços são a entrega garantida de mensagens da camada de aplicação ao destino e controle de fluxo (isto \né, compatibilização das velocidades do remetente e do receptor). O TCP também fragmenta mensagens longas \nem segmentos mais curtos e provê mecanismo de controle de congestionamento, de modo que uma origem reduz \nsua velocidade de transmissão quando a rede está congestionada. O protocolo UDP provê serviço não orientado \na conexão para suas aplicações. É um serviço econômico que fornece segurança, sem controle de fluxo e de con-\ngestionamento. Neste livro, chamaremos de segmento a um pacote da camada de transporte.\nCamada de rede\nA camada de rede da Internet é responsável pela movimentação, de um hospedeiro para outro, de pa-\ncotes da camada de rede, conhecidos como datagramas. O protocolo de camada de transporte da Internet \n(TCP ou UDP) em um hospedeiro de origem passa um segmento da camada de transporte e um endereço de \ndestino à camada de rede, exatamente como você passaria ao serviço de correios uma carta com um endereço \nde destinatário. A camada de rede então provê o serviço de entrega do segmento à camada de transporte no \nhospedeiro de destino.\nEssa camada inclui o famoso protocolo IP, que define os campos no datagrama e o modo como os sistemas \nfinais e os roteadores agem nesses campos. Existe apenas um único protocolo IP, e todos os componentes da In-\nternet que têm uma camada de rede devem executá-lo. A camada de rede da Internet também contém protocolos \nde roteamento que determinam as rotas que os datagramas seguem entre origens e destinos. A Internet tem mui-\ntos protocolos de roteamento. Como vimos na Seção 1.3, a Internet é uma rede de redes e, dentro de uma delas, o \nadministrador pode executar qualquer protocolo de roteamento. Embora a camada de rede contenha o protocolo \nIP e também numerosos outros de roteamento, ela quase sempre é denominada apenas camada IP, refletindo o \nfato de que ele é o elemento fundamental que mantém a integridade da Internet.\nCamada de enlace\nA camada de rede roteia um datagrama por meio de uma série de roteadores entre a origem e o destino. \nPara levar um pacote de um nó (hospedeiro ou roteador) ao nó seguinte na rota, a camada de rede depende dos \nserviços da camada de enlace. Em especial, em cada nó, a camada de rede passa o datagrama para a de enlace, que \no entrega, ao longo da rota, ao nó seguinte, no qual o datagrama é passado da camada de enlace para a de rede.\nOs serviços prestados pela camada de enlace dependem do protocolo específico empregado no enlace. Por \nexemplo, alguns desses protocolos proveem entrega garantida entre enlaces, isto é, desde o nó transmissor, pas-\nsando por um único enlace, até o nó receptor. Note que esse serviço confiável de entrega é diferente do de entrega \ngarantida do TCP, que provê serviço de entrega garantida de um sistema final a outro. Exemplos de protocolos \nde camadas de enlace são Ethernet, Wi-Fi e o protocolo DOCSIS da rede de acesso por cabo. Como datagramas \nnormalmente precisam transitar por diversos enlaces para irem da origem ao destino, serão manuseados por \ndiferentes protocolos de camada de enlace em diversos enlaces ao longo de sua rota, podendo ser manuseados \nRedes de computadores e a Internet  39 \npor Ethernet em um e por PPP no seguinte. A camada de rede receberá um serviço diferente de cada um dos \nvariados protocolos de camada de enlace. Neste livro, pacotes de camada de enlace serão denominados quadros.\nCamada física\nEnquanto a tarefa da camada de enlace é movimentar quadros inteiros de um elemento da rede até um \nelemento adjacente, a da camada física é movimentar os bits individuais que estão dentro do quadro de um nó \npara o seguinte. Os protocolos nessa camada de novo dependem do enlace e, além disso, do próprio meio de \ntransmissão do enlace (por exemplo, fios de cobre trançado ou fibra ótica monomodal). Por exemplo, a Ethernet \ntem muitos protocolos de camada física: um para par de fios de cobre trançado, outro para cabo coaxial, mais um \npara fibra e assim por diante. Em cada caso, o bit atravessa o enlace de um modo diferente.\nO modelo OSI\nApós discutir em detalhes a pilha de protocolos da Internet, devemos mencionar que ela não é a única exis-\ntente. No final dos anos 1970, a Organização Internacional para Padronização (ISO — International Organization \nfor Standardization) propôs que as redes de computadores fossem organizadas em, mais ou menos, sete camadas, \ndenominadas modelo de Interconexão de Sistemas Abertos (OSI — Open Systems Interconnection) [ISO, 2012]. \nO modelo OSI tomou forma quando os protocolos que iriam se tornar protocolos da Internet estavam em sua \ninfância e eram um dos muitos conjuntos em desenvolvimento; na verdade, os inventores do modelo OSI original \nprovavelmente não tinham a Internet em mente ao criá-lo. No entanto, no final dos anos 1970, muitos cursos uni-\nversitários e de treinamento obtiveram conhecimentos sobre a exigência do ISO e organizaram cursos voltados \npara o modelo de sete camadas. Em razão de seu impacto precoce na educação de redes, esse modelo continua \npresente em alguns livros sobre redes e em cursos de treinamento.\nAs sete camadas do modelo de referência OSI, mostradas na Figura 1.23(b), são: de aplicação, de apresen-\ntação, de sessão, de transporte, de rede, de enlace e camada física. A funcionalidade de cinco dessas camadas \né a mesma que seus correspondentes da Internet. Desse modo, vamos considerar as duas camadas adicionais \npresentes no modelo de referência OSI — a de apresentação e a de sessão. O papel da camada de apresentação é \nprover serviços que permitam que as aplicações de comunicação interpretem o significado dos dados trocados. \nEntre esses serviços estão a compressão e a codificação de dados (o que não precisa de explicação), assim como \na descrição de dados (que, como veremos no Capítulo 9, livram as aplicações da preocupação com o formato \ninterno no qual os dados estão sendo representados/armazenados — formatos que podem ser diferentes de um \ncomputador para o outro). A camada de sessão provê a delimitação e sincronização da troca de dados, incluindo \nos meios de construir um esquema de pontos de verificação e de recuperação.\nO fato de a Internet ser desprovida de duas camadas encontradas no modelo de referência OSI faz surgir \nduas questões: os serviços fornecidos por essas camadas são irrelevantes? E se uma aplicação precisar de um des-\nses serviços? A resposta da Internet para essas perguntas é a mesma — depende do desenvolvedor da aplicação. \nCabe a ele decidir se um serviço é importante; e se o serviço for importante, cabe ao desenvolvedor da aplicação \nconstruir essa funcionalidade para ela.\n1.5.2  Encapsulamento\nA Figura 1.24 apresenta o caminho físico que os dados percorrem: para baixo na pilha de protocolos de um \nsistema final emissor, para cima e para baixo nas pilhas de protocolos de um comutador e roteador de camada \nde enlace interveniente, e depois para cima na pilha de protocolos do sistema final receptor. Como discutiremos \nmais adiante neste livro, ambos, roteadores e comutadores de camada de enlace, são comutadores de pacotes. \nDe modo semelhante a sistemas finais, ambos organizam seu hardware e software de rede em camadas. Mas não \nimplementam todas as camadas da pilha de protocolos; em geral executam apenas as camadas de baixo. Como \n \n   Redes de computadores e a Internet\n40\nilustra a Figura 1.24, comutadores de camada de enlace realizam as camadas 1 e 2; roteadores executam as ca-\nmadas 1, 2 e 3. Isso significa, por exemplo, que roteadores da Internet são capazes de executar o protocolo IP \n(da camada 3), mas comutadores de camada de enlace não. Veremos mais adiante que, embora não reconheçam \nendereços IP, comutadores de camada­\n de enlace são capazes de reconhecer ­\nendereços de camada 2, os da Ether-\nnet. Note que os hospedeiros implementam todas as cinco camadas, o que é consistente com a noção de que a \narquitetura da Internet concentra sua complexidade na periferia da rede.\nA Figura 1.24 também ilustra o importante conceito de encapsulamento. Uma mensagem da camada de \naplicação na máquina emissora (M na Figura 1.24) é passada para a camada de transporte. No caso mais sim-\nples, esta pega a mensagem e anexa informações adicionais (denominadas informações de cabeçalho de camada \nde transporte, Ht na Figura 1.24) que serão usadas pela camada de transporte do lado receptor. A mensagem da \ncamada de aplicação e as informações de cabeçalho da camada de transporte, juntas, constituem o segmento da \ncamada de transporte, que encapsula a mensagem da camada de aplicação. As informações adicionadas podem \nincluir dados que habilitem a camada de transporte do lado do receptor a entregar a mensagem à aplicação apro-\npriada, além de bits de detecção de erro que permitem que o receptor determine se os bits da mensagem foram \nmodificados em trânsito. A camada de transporte então passa o segmento à camada de rede, que adiciona infor-\nmações de cabeçalho de camada de rede (Hn na Figura 1.24), como endereços de sistemas finais de origem e de \ndestino, criando um datagrama de camada de rede. Este é então passado para a camada de enlace, que (é claro!) \nadicionará suas próprias informações de cabeçalho e criará um quadro de camada de enlace. Assim, vemos que, \nem cada camada, um pacote possui dois tipos de campos: campos de cabeçalho e um campo de carga útil. A \ncarga útil é em geral um pacote da camada acima.\nUma analogia útil que podemos usar aqui é o envio de um memorando entre escritórios de uma empresa \npelo correio de uma filial a outra. Suponha que Alice, que está em uma filial, queira enviar um memorando a Bob, \nque está na outra filial. O memorando representa a mensagem da camada de aplicação. Alice coloca o memorando \nem um envelope de correspondência interna em cuja face são escritos o nome e o departamento de Bob. O enve-\nlope de correspondência interna representa o segmento da camada de aplicação — contém as informações de ca-\nFigura 1.24  \u0007\nhospedeiros, roteadores e comutadores de camada de enlace; cada um \ncontém um conjunto diferente de camadas, refletindo suas diferenças em \nfuncionalidade\nM\nM\nM\nM\nHt\nHt\nHt\nHn\nHn\nHl\nHt\nHn\nHl\nComutador da camada de enlace\nRoteador\nAplicação\nTransporte\nRede\nEnlace\nFísico\nMensagem\nSegmento\nDatagrama\nQuadro\nM\nM\nM\nM\nHt\nHt\nHt\nHn\nHn\nHl\nEnlace\nFísico\nOrigem\nRede\nEnlace\nFísico\nDestino\nAplicação\nTransporte\nRede\nEnlace\nFísico\nM\nHt\nHn\nHl\nM\nHt\nHn\nM\nHt\nHn\nM\nHt\nHn\nHl\nM\nHt\nHn\nHl\nM\nRedes de computadores e a Internet  41 \nbeçalho (o nome de Bob e seu departamento) e encapsula a mensagem de camada de aplicação (o memorando). \nQuando a central de correspondência do escritório emissor recebe o envelope, ele é colocado dentro de outro, \nadequado para envio pelo correio. A central de correspondência emissora também escreve o endereço postal do \nremetente e do destinatário no envelope postal. Nesse ponto, o envelope postal é análogo ao datagrama — encap-\nsula o segmento de camada de transporte (o envelope de correspondência interna), que por sua vez encapsula a \nmensagem original (o memorando). O correio entrega o envelope postal à central de correspondência do escri-\ntório destinatário. Nesse local, o processo de desencapsulamento se inicia. A central de correspondência retira o \nmemorando e o encaminha a Bob. Este, por fim, abre o envelope e retira o memorando.\nO processo de encapsulamento pode ser mais complexo do que o descrito. Por exemplo, uma mensagem \ngrande pode ser dividida em vários segmentos de camada de transporte (que também podem ser divididos em \nvários datagramas de camada de rede). Na extremidade receptora, cada segmento deve ser reconstruído a partir \ndos datagramas que o compõem.\n1.6  Redes sob ameaça\nA Internet se tornou essencial para muitas instituições, incluindo empresas grandes e pequenas, universi-\ndades e órgãos do governo. Muitas pessoas também contam com a Internet para suas atividades profissionais, \nsociais e pessoais. Mas atrás de toda essa utilidade e entusiasmo, existe o lado escuro, um lado no qual “vilões” \ntentam causar problemas em nosso cotidiano danificando nossos computadores conectados à Internet, violando \nnossa privacidade e tornando inoperantes os serviços da rede dos quais dependemos.\nA área de segurança trata de como esses vilões podem ameaçar as redes de computadores e como nós, \nfuturos especialistas no assunto, podemos defender a rede contra essas ameaças ou, melhor ainda, criar novas \narquiteturas imunes a tais riscos primeiro. Dadas a frequência e a variedade das ameaças existentes, bem como o \nperigo de novos e mais destrutivos futuros ataques, a segurança se tornou um assunto principal na área de redes \nde computadores. Um dos objetivos deste livro é trazer as questões de segurança de rede para o primeiro plano.\nVisto que ainda não temos o know-how em rede de computadores e em protocolos da Internet, começa-\nremos com uma análise de alguns dos atuais problemas mais predominantes relacionados à segurança. Isto irá \naguçar nosso apetite para discussões mais importantes nos capítulos futuros. Começamos com a pergunta: o que \npode dar errado? Como as redes de computadores são vulneráveis? Quais são alguns dos tipos de ameaças mais \npredominantes hoje?\nOs vilões podem colocar “malware” em seu hospedeiro por meio da Internet\nConectamos aparelhos à Internet porque queremos receber/enviar dados de/para a rede. Isso inclui todos \nos tipos de recursos vantajosos, como páginas da Web, mensagens de e-mail, MP3, chamadas telefônicas, vídeo \nem tempo real, resultados de mecanismo de busca etc. Porém, infelizmente, junto com esses recursos vantajosos \naparecem os maliciosos — conhecidos de modo coletivo como malware — que também podem entrar e infectar \nnossos aparelhos. Uma vez que o malware infecta nosso aparelho, ele é capaz de fazer coisas perversas, como \napagar nossos arquivos; instalar spyware que coleta informações particulares, como nosso número de cartão de \ncrédito, senhas e combinação de teclas, e as envia (pela Internet, é claro!) de volta aos vilões. Nosso hospedeiro \ncomprometido pode estar, também, envolvido em uma rede de milhares de aparelhos comprometidos, conheci-\ndos como botnet, a qual é controlada e utilizada pelos vilões para distribuição de spams ou ataques de recusa de \nserviço distribuídos (que serão discutidos em breve) contra hospedeiros direcionados.\nMuitos malwares existentes hoje são autorreprodutivos: uma vez que infectam um hospedeiro, a par-\ntir deste, ele faz a busca por entradas em outros hospedeiros pela Internet, e a dos hospedeiros recém-in-\nfectados, procura por entrada em mais hospedeiros. Dessa maneira, o malware autorreprodutivo pode se \ndisseminar rapidamente. O malware pode se espalhar na forma de um vírus ou um worm. Os vírus são \n   Redes de computadores e a Internet\n42\nmalwares que necessitam de uma interação do usuário para infectar seu aparelho. O exemplo clássico é um \nanexo de e-mail contendo um código executável malicioso. Se o usuário receber e abrir tal anexo, o malware \nserá executado em seu aparelho. Geralmente, tais vírus de e-mail se autorreproduzem: uma vez executado, \no vírus pode enviar uma mensagem idêntica, com um anexo malicioso idêntico, para, por exemplo, todos \nos contatos da lista de endereços do usuário. Worms são malwares capazes de entrar em um aparelho sem \nqualquer interação do usuário. Por exemplo, um usuário pode estar executando uma aplicação de rede frágil \npara a qual um atacante pode enviar um malware. Em alguns casos, sem a intervenção do usuário, a aplica-\nção pode aceitar o malware da Internet e executá-lo, criando um worm. Este, no aparelho recém-infectado, \nentão, varre a Internet em busca de outros hospedeiros que estejam executando a mesma aplicação de rede \nvulnerável. Ao encontrá-los, envia uma cópia de si mesmo para eles. Hoje, o malware é persuasivo e é caro \npara se criar uma proteção. À medida que trabalhar com este livro, sugerimos que pense na seguinte ques-\ntão: o que os projetistas de computadores podem fazer para proteger os aparelhos que utilizam a Internet \ncontra as ameaças de malware?\nOs vilões podem atacar servidores e infraestrutura de redes\nUm amplo grupo de ameaças à segurança pode ser classificado como ataques de recusa de serviços (DoS — \n \nDenial-of-Service). Como o nome sugere, um ataque DoS torna uma rede, hospedeiro ou outra parte da infraes-\ntrutura inutilizável por usuários verdadeiros. Servidores da Web, de e-mail e DNS (discutidos no Capítulo 2), e \nredes institucionais podem estar sujeitos aos ataques DoS. Na Internet, esses ataques são extremamente comuns, \ncom milhares deles ocorrendo todo ano [Moore, 2001; Mirkovic, 2005]. A maioria dos ataques DoS na Internet \npode ser dividida em três categorias:\n• Ataque de vulnerabilidade. Envolve o envio de algumas mensagens bem elaboradas a uma aplicação vul-\nnerável ou a um sistema operacional sendo executado em um hospedeiro direcionado. Se a sequência \ncorreta de pacotes é enviada a uma aplicação ou sistema operacional vulnerável, o serviço pode parar ou, \npior, o hospedeiro pode pifar.\n• Inundação na largura de banda. O atacante envia um grande número de pacotes ao hospedeiro dire-\ncionado — tantos pacotes que o enlace de acesso do alvo se entope, impedindo os pacotes legítimos de \nalcançarem o servidor.\n• Inundação na conexão. O atacante estabelece um grande número de conexões TCP semiabertas ou aber-\ntas (as conexões TCP são discutidas no Capítulo 3) no hospedeiro-alvo. O hospedeiro pode ficar tão \natolado com essas conexões falsas que deixa de aceitar conexões legítimas.\nVamos agora explorar mais detalhadamente o ataque de inundação na largura de banda. Lembrando de \nnossa análise sobre atraso e perda na Seção 1.4.2, é evidente que se o servidor possui uma taxa de acesso \nde R bits/s, o atacante precisará enviar tráfego a uma taxa de, mais ou menos, R bits/s para causar dano. Se R for \nmuito grande, uma fonte de ataque única pode não ser capaz de gerar tráfego suficiente para prejudicar o servi-\ndor. Além disso, se todo o tráfego emanar de uma fonte única, um roteador mais adiante pode conseguir detectar \no ataque e bloquear todo o tráfego da fonte antes que ele se aproxime do servidor. Em um ataque DoS distribuído \n(DDoS — Distributed DoS), ilustrado na Figura 1.25, o atacante controla múltiplas fontes que sobrecarregam o \nalvo. Com essa tática, a taxa de tráfego agregada por todas as fontes controladas precisa ser, aproximadamente, \nR para incapacitar o serviço. Os ataques DDoS que potencializam botnets com centenas de hospedeiros compro-\nmetidos são uma ocorrência comum hoje em dia [Mirkovic, 2005]. Os ataques DDoS são muito mais difíceis de \ndetectar e de prevenir do que um ataque DoS de um único hospedeiro.\nEncorajamos o leitor a considerar a seguinte questão à medida que trabalhar com este livro: o que os proje-\ntistas de redes de computadores podem fazer para se protegerem contra ataques DoS? Veremos que são necessá-\nrias diferentes defesas para os três tipos de ataques DoS.\nRedes de computadores e a Internet  43 \nOs vilões podem analisar pacotes\nMuitos usuários hoje acessam a Internet por meio de aparelhos sem fio, como laptops conectados à tecno-\nlogia Wi-Fi ou aparelhos portáteis com conexões à Internet via telefone celular (abordado no Capítulo 6). Em-\nbora o acesso onipresente à Internet seja de extrema conveniência e disponibilize novas aplicações sensacionais \naos usuários móveis, ele também cria uma grande vulnerabilidade de segurança — posicionando um receptor \npassivo nas proximidades do transmissor sem fio, o receptor pode obter uma cópia de cada pacote transmitido! \nEsses pacotes podem conter todo tipo de informações confidenciais, incluindo senhas, número de identificação, \nsegredos comerciais e mensagens pessoais. Um receptor passivo que grava uma cópia de cada pacote que passa é \ndenominado analisador de pacote (packet sniffer).\nOs analisadores também podem estar distribuídos em ambientes de conexão com fio. Nesses ambientes, \ncomo em muitas LANs Ethernet, um analisador de pacote pode obter cópias de todos os pacotes enviados pela \nLAN. Como descrito na Seção 1.2, as tecnologias de acesso a cabo também transmitem pacotes e são, dessa for-\nma, vulneráveis à análise. Além disso, um vilão que quer ganhar acesso ao roteador de acesso de uma instituição \nou enlace de acesso para a Internet pode instalar um analisador que faça uma cópia de cada pacote que vai para/\nde a empresa. Os pacotes farejados podem, então, ser analisados off-line em busca de informações confidenciais.\nO software para analisar pacotes está disponível gratuitamente em diversos sites da Internet e em produtos \ncomerciais. Professores que ministram um curso de redes passam exercícios que envolvem a escrita de um pro-\ngrama de reconstrução de dados da camada de aplicação e um programa analisador de pacotes. De fato, os Wire-\nshark labs [Wireshark, 2012] associados a este texto (veja o Wireshark lab introdutório ao final deste capítulo) \nutilizam exatamente tal analisador de pacotes.\nComo os analisadores de pacote são passivos — ou seja, não introduzem pacotes no canal —, eles são difíceis \nde detectar. Portanto, quando enviamos pacotes para um canal sem fio, devemos aceitar a possibilidade de que \nalguém possa estar copiando nossos pacotes. Como você deve ter imaginado, uma das melhores defesas contra a \nanálise de pacote envolve a criptografia, que será explicada no Capítulo 8, já que se aplica à segurança de rede.\nOs vilões podem se passar por alguém de sua confiança\nPor incrível que pareça, é facílimo (você saberá como fazer isso à medida que ler este livro!) criar um pacote \ncom qualquer endereço de origem, conteúdo de pacote e endereço de destino e, depois, transmiti-lo para a Inter-\nnet, que, obedientemente, o encaminhará ao destino. Imagine que um receptor inocente (digamos, um roteador \nda Internet) que recebe tal pacote acredita que o endereço de origem (falso) seja confiável e então executa um \nFigura 1.25  Um ataque de recusa de serviço distribuído (DDoS)\nAtacante\n“Iniciar \nataque”\nZumbi\nZumbi\nZumbi\nVítima\nZumbi\nZumbi\n   Redes de computadores e a Internet\n44\ncomando integrado ao conteúdo do pacote (digamos, que modifica sua base de encaminhamento). A capacidade \nde introduzir pacotes na Internet com um endereço de origem falso é conhecida como IP spoofing, e é uma das \nmuitas maneiras pelas quais o usuário pode se passar por outro.\nPara resolver esse problema, precisaremos de uma autenticação do ponto final, ou seja, um mecanismo que \nnos permita determinar com certeza se uma mensagem se origina de onde pensamos. Mais uma vez, sugerimos \nque pense em como isso pode ser feito em aplicações de rede e protocolos à medida que avança sua leitura pelos \ncapítulos deste livro. Exploraremos mais mecanismos para comprovação da fonte no Capítulo 8.\nAo encerrar esta seção, deve-se considerar como a Internet se tornou um local inseguro, antes de tudo. A \nresposta breve é que a Internet foi, a princípio, criada dessa maneira, baseada no modelo de “um grupo de usuá-\nrios de confiança mútua ligados a uma rede transparente” [Blumenthal, 2001] — um modelo no qual (por defini-\nção) não há necessidade de segurança. Muitos aspectos da arquitetura inicial da Internet refletem profundamente \nessa noção de confiança mútua. Por exemplo, a capacidade de um usuário enviar um pacote a qualquer outro é \nmais uma falha do que um recurso solicitado/concedido, e acredita-se piamente na identidade do usuário, em vez \nde ela ser autenticada como padrão.\nMas a Internet de hoje decerto não envolve “usuários de confiança mútua”\n. Contudo, os usuários atuais \nainda precisam se comunicar mesmo quando não confiam um no outro, podem querer se comunicar de modo \nanônimo, podem se comunicar indiretamente por terceiros (por exemplo, Web caches, que serão estudados no \nCapítulo 2, ou agentes móveis para assistência, que serão estudados no Capítulo 6), e podem desconfiar do \nhardware, software e até mesmo do ar pelo qual eles se comunicam. Temos agora muitos desafios relacionados \nà segurança perante nós à medida que prosseguimos com o livro: devemos buscar proteção contra a análise, \ndisfarce da origem, ataques man-in-the-middle, ataques DDoS, malware e outros. Precisamos manter em mente \nque a comunicação entre usuários de confiança mútua é mais uma exceção do que uma regra. Seja bem-vindo ao \nmundo da moderna rede de computadores!\n1.7  \u0007\nHistória das redes de computadores e da Internet\nDa Seção 1.1 à 1.6, apresentamos um panorama da tecnologia de redes de computadores e da Internet. \nAgora, você já deve saber o suficiente para impressionar sua família e amigos! Contudo, se quiser ser mesmo o \nmaior sucesso na próxima festa, você deve rechear seu discurso com pérolas da fascinante história da Internet \n[Segaller, 1998].\n1.7.1  \u0007\nDesenvolvimento da comutação de pacotes: 1961-1972\nOs primeiros passos da disciplina de redes de computadores e da Internet atual podem ser traçados desde \no início da década de 1960, quando a rede telefônica era a rede de comunicação dominante no mundo inteiro. \nLembre-se de que na Seção 1.3 dissemos que a rede de telefonia usa comutação de circuitos para transmitir in-\nformações de uma origem a um destino — uma escolha acertada, já que a voz é transmitida a uma taxa constante \nentre os pontos. Dada a importância cada vez maior dos computadores no início da década de 1960 e o advento \nde computadores com tempo compartilhado, nada seria mais natural do que considerar a questão de como inter-\nligar computadores para que pudessem ser compartilhados entre usuários geograficamente dispersos. O tráfego \ngerado por esses usuários provavelmente era feito por rajadas — períodos de atividade, como o envio de um \ncomando a um computador remoto, seguidos de períodos de inatividade, como a espera por uma resposta ou o \nexame de uma resposta recebida.\nTrês grupos de pesquisa ao redor do mundo, sem que nenhum tivesse conhecimento do trabalho do outro \n[Leiner, 1998], começaram a inventar a comutação de pacotes como uma alternativa poderosa e eficiente à co-\nmutação de circuitos. O primeiro trabalho publicado sobre técnicas de comutação de pacotes foi o de Leonard \nKleinrock [Kleinrock, 1961, 1964], que, naquela época, era um aluno de graduação no MIT. Usando a teoria de \nRedes de computadores e a Internet  45 \nfilas, seu trabalho demonstrou, com elegância, a eficácia da abordagem da comutação de pacotes para fontes de \ntráfego intermitentes (em rajadas). Em 1964, Paul Baran [Baran, 1964], do Rand Institute, começou a investigar \na utilização de comutação de pacotes na transmissão segura de voz pelas redes militares, ao mesmo tempo que \nDonald Davies e Roger Scantlebury desenvolviam suas ideias sobre esse assunto no National Physical Laboratory, \nna Inglaterra.\nOs trabalhos desenvolvidos no MIT, no Rand Institute e no NPL foram os alicerces do que hoje é a Internet. \nMas a Internet tem uma longa história de atitudes do tipo “construir e demonstrar”\n, que também data do início \nda década de 1960. J. C. R. Licklider [DEC, 1990] e Lawrence Roberts, ambos colegas de Kleinrock no MIT, foram \nadiante e lideraram o programa de ciência de computadores na ARPA (Advanced Research Projects Agency — \nAgência de Projetos de Pesquisa Avançada), nos Estados Unidos. Roberts publicou um plano geral para a ARPA-\nnet [Roberts, 1967], a primeira rede de computadores por comutação de pacotes e uma ancestral direta da Inter-\nnet pública de hoje. Em 1969, no Dia do Trabalho nos Estados Unidos, foi instalado o primeiro comutador de \npacotes na UCLA (Universidade da Califórnia em Los Angeles) sob a supervisão de Kleinrock. Pouco tempo \ndepois, foram instalados três comutadores de pacotes adicionais no Stanford Research Institute (SRI), na Univer-\nsidade da Califórnia em Santa Bárbara e na Universidade de Utah (Figura 1.26). O incipiente precursor da Inter-\nnet tinha quatro nós no final de 1969. Kleinrock recorda que a primeiríssima utilização da rede foi fazer um login \nremoto entre a UCLA e o SRI, derrubando o sistema [Kleinrock, 2004].\nEm 1972, a ARPAnet tinha cerca de 15 nós e foi apresentada publicamente pela primeira vez por Robert Kahn. \nO primeiro protocolo fim a fim entre sistemas finais da ARPAnet, conhecido como protocolo de controle de rede \n(network-control protocol — NCP), estava concluído [RFC 001]. Com um protocolo fim a fim à disposição, a escrita \nde aplicações tornou-se possível. Em 1972, Ray Tomlinson, da BBN, escreveu o primeiro programa de e-mail.\nFigura 1.26  Um dos primeiros comutadores de pacotes\n   Redes de computadores e a Internet\n46\n1.7.2  Redes proprietárias e trabalho em rede: 1972-1980\nA ARPAnet inicial era uma rede isolada, fechada. Para se comunicar com uma máquina da ARPAnet, era pre-\nciso estar ligado a um outro IMP dessa rede. Do início a meados de 1970, surgiram novas redes independentes de \ncomutação de pacotes: ALOHAnet, uma rede de micro-ondas ligando universidades das ilhas do Havaí [Abramson, \n1970], bem como as redes de pacotes por satélite [RFC 829] e por rádio [Kahn, 1978] da DARPA; Telenet, uma rede \ncomercial de comutação de pacotes da BBN baseada na tecnologia ARPAnet; Cyclades, uma rede de comutação de \npacotes pioneira na França, montada por Louis Pouzin [Think, 2002]; redes de tempo compartilhado como a Tymnet \ne a rede GE Information Services, entre outras que surgiram no final da década de 1960 e início da década de 1970 \n[Schwartz, 1977]; rede SNA da IBM (1969–1974), cujo trabalho comparava-se ao da ARPAnet [Schwartz, 1977].\nO número de redes estava crescendo. Hoje, com perfeita visão do passado, podemos perceber que aquela era \na hora certa para desenvolver uma arquitetura abrangente para conectar redes. O trabalho pioneiro de intercone-\nxão de redes, sob o patrocínio da DARPA (Defense Advanced Research Projects Agency — Agência de Projetos \nde Pesquisa Avançada de Defesa), criou basicamente uma rede de redes e foi realizado por Vinton Cerf e Robert \nKahn [Cerf, 1974]; o termo internetting foi cunhado para descrever esse trabalho.\nEsses princípios de arquitetura foram incorporados ao TCP. As primeiras versões desse protocolo, con-\ntudo, eram muito diferentes do TCP de hoje. Elas combinavam uma entrega sequencial confiável de dados via \nretransmissão por sistema final (que ainda faz parte do TCP de hoje) com funções de envio (que hoje são desem-\npenhadas pelo IP). As primeiras experiências com o TCP, combinadas com o reconhecimento da importância de \num serviço de transporte fim a fim não confiável, sem controle de fluxo, para aplicações como voz em pacotes, \nlevaram à separação entre IP e TCP e ao desenvolvimento do protocolo UDP. Os três protocolos fundamentais da \nInternet que temos hoje — TCP, UDP e IP — estavam conceitualmente disponíveis no final da década de 1970.\nAlém das pesquisas sobre a Internet realizadas pela DARPA, muitas outras atividades importantes rela-\ncionadas ao trabalho em rede estavam em andamento. No Havaí, Norman Abramson estava desenvolvendo a \nALOHAnet, uma rede de pacotes por rádio que permitia que vários lugares remotos das ilhas havaianas se co-\nmunicassem entre si. O ALOHA [Abramson, 1970] foi o primeiro protocolo de acesso múltiplo que permitiu que \nusuários geograficamente dispersos compartilhassem um único meio de comunicação broadcast (uma frequên-\ncia de rádio). Metcalfe e Boggs se basearam no trabalho de Abramson sobre protocolo de múltiplo acesso quando \ndesenvolveram o protocolo Ethernet [Metcalfe, 1976] para redes compartilhadas de transmissão broadcast por \nfio. O interessante é que o protocolo Ethernet de Metcalfe e Boggs foi motivado pela necessidade de conectar \nvários PCs, impressoras e discos compartilhados [Perkins, 1994]. Há 25 anos, bem antes da revolução do PC e da \nexplosão das redes, Metcalfe e Boggs estavam lançando as bases para as LANs de PCs de hoje.\n1.7.3  Proliferação de redes: 1980-1990\nAo final da década de 1970, cerca de 200 máquinas estavam conectadas à ARPAnet. Ao final da década de \n1980, o número de máquinas ligadas à Internet pública, uma confederação de redes muito parecida com a Inter-\nnet de hoje, alcançaria cem mil. A década de 1980 seria uma época de formidável crescimento.\nGrande parte daquele crescimento foi consequência de vários esforços distintos para criar redes de compu-\ntadores para interligar universidades. A BITNET processava e-mails e fazia transferência de arquivos entre diver-\nsas universidades do nordeste dos Estados Unidos. A CSNET (Computer Science NETwork — rede da ciência de \ncomputadores) foi formada para interligar pesquisadores de universidades que não tinham acesso à ARPAnet. \nEm 1986, foi criada a NSFNET para prover acesso a centros de supercomputação patrocinados pela NSF. Par-\ntindo de uma velocidade inicial de 56 kbits/s, ao final da década o backbone  da ­\nNSFNET estaria funcionando a \n1,5 Mbits/s e servindo como backbone  primário para a interligação de redes regionais.\nNa comunidade da ARPAnet, já estavam sendo encaixados muitos dos componentes finais da arquitetura \nda Internet de hoje. No dia 1o de janeiro de 1983, o TCP/IP foi adotado oficialmente como o novo padrão de \nRedes de computadores e a Internet  47 \nprotocolo de máquinas para a ARPAnet (em substituição ao protocolo NCP). Pela importância do evento, o dia \nda transição do NCP para o TCP/IP [RFC 801] foi marcado com antecedência — a partir daquele dia todas as \nmáquinas tiveram de adotar o TCP/IP. No final da década de 1980, foram agregadas importantes extensões ao \nTCP para implementação do controle de congestionamento baseado em hospedeiros [Jacobson, 1988]. Também \nfoi desenvolvido o sistema de nomes de domínios (DNS) utilizado para mapear nomes da Internet fáceis de en-\ntender (por exemplo, gaia.cs.umass.edu) para seus endereços IP de 32 bits [RFC 1034].\nEm paralelo ao desenvolvimento da ARPAnet (que em sua maior parte deve-se aos Estados Unidos), no iní-\ncio da década de 1980 os franceses lançaram o projeto Minitel, um plano ambicioso para levar as redes de dados \npara todos os lares. Patrocinado pelo governo francês, o sistema consistia em uma rede pública de comutação de \npacotes (baseada no conjunto de protocolos X.25, que usava circuitos virtuais), servidores Minitel e terminais \nbaratos com modems de baixa velocidade embutidos. O Minitel transformou-se em um enorme sucesso em 1984, \nquando o governo francês forneceu, gratuitamente, um terminal para toda residência francesa que quisesse. O \nsistema incluía sites de livre acesso — como o da lista telefônica — e também particulares, que cobravam uma taxa \nde cada usuário baseada no tempo de utilização. No seu auge, em meados de 1990, o Minitel oferecia mais de 20 \nmil serviços, que iam desde home banking até bancos de dados especializados para pesquisa. Estava presente em \ngrande parte dos lares franceses dez anos antes sequer de a maioria dos norte-americanos ouvir falar de Internet.\n1.7.4  A explosão da Internet: a década de 1990\nA década de 1990 estreou com vários eventos que simbolizaram a evolução contínua e a comercialização \niminente da Internet. A ARPAnet, a progenitora da Internet, deixou de existir. Em 1991, a NSFNET extinguiu as \nrestrições que impunha à sua utilização com finalidades comerciais, mas, em 1995, perderia seu mandato quando \no tráfego de backbone da Internet passou a ser carregado por provedores de serviços.\nO principal evento da década de 1990, no entanto, foi o surgimento da World Wide Web, que levou a Inter-\nnet para os lares e as empresas de milhões de pessoas no mundo inteiro. A Web serviu também como plataforma \npara a habilitação e a disponibilização de centenas de novas aplicações, inclusive busca (por exemplo, Google e \nBing), comércio pela Internet (por exemplo, Amazon e eBay) e redes sociais (por exemplo, Facebook).\nA Web foi inventada no CERN (European Center for Nuclear Physics — Centro Europeu para Física Nu-\nclear) por Tim Berners-Lee entre 1989 e 1991 [Berners-Lee, 1989], com base em ideias originadas de trabalhos \nanteriores sobre hipertexto realizados por Vannevar Bush [Bush, 1945], na década de 1940, e por Ted Nelson \n[Xanadu, 2012], na década de 1960. Berners-Lee e seus companheiros desenvolveram versões iniciais de HTML, \nHTTP, um servidor Web e um navegador (browser) — os quatro componentes fundamentais da Web. Por volta \nde 1993, havia cerca de 200 servidores Web em operação, e esse conjunto era apenas um prenúncio do que es-\ntava por vir. Nessa época, vários pesquisadores estavam desenvolvendo navegadores Web com interfaces GUI \n(Graphical User Interface — interface gráfica de usuário), ­\nentre eles Marc Andreessen, que liderou o desenvolvi-\nmento do popular navegador Mosaic, junto com Kim Clark, que formaram a Mosaic Communications, que mais \ntarde se transformou na Netscape Communications Corporation [Cusumano, 1998; Quittner, 1998]. Em 1995, \nestudantes universitários estavam usando navegadores Mosaic e Netscape para navegar na Web diariamente.­\n \nNa época, empresas — grandes e pequenas — começaram a operar servidores e a realizar transações comerciais \npela Web. Em 1996, a Microsoft começou a desenvolver navegadores, dando início à guerra entre Netscape e \nMicrosoft, vencida pela última alguns anos mais tarde [Cusumano, 1998].\nA segunda metade da década de 1990 foi um período de tremendo crescimento e inovação, com grandes \ncorporações e milhares de novas empresas criando produtos e serviços para a Internet. No final do milênio a \nInternet dava suporte a centenas de aplicações populares, entre elas quatro de enorme sucesso:\n• e-mail, incluindo anexos e correio eletrônico com acesso pela Web;\n• a Web, incluindo navegação pela Web e comércio pela Internet;\n• serviço de mensagem instantânea, com listas de contato;\n• compartilhamento peer-to-peer de arquivos MP3, cujo pioneiro foi o Napster.\n   Redes de computadores e a Internet\n48\nO interessante é que as duas primeiras dessas aplicações de sucesso arrasador vieram da comunidade de \npesquisas, ao passo que as duas últimas foram criadas por alguns jovens empreendedores.\nNo período de 1995 a 2001, a Internet realizou uma viagem vertiginosa nos mercados financeiros. Antes \nmesmo de se mostrarem lucrativas, centenas de novas empresas faziam suas ofertas públicas iniciais de ações e \ncomeçavam a ser negociadas em bolsas de valores. Muitas empresas eram avaliadas em bilhões de dólares sem ter \nnenhum fluxo significativo de receita. As ações da Internet sofreram uma queda também vertiginosa em 2000-\n2001, e muitas novas empresas fecharam. Não obstante, várias outras surgiram como grandes vencedoras no \nmundo­\n da Internet, entre elas Microsoft, Cisco, Yahoo, eBay, Google e Amazon.\n1.7.5  O novo milênio\nA inovação na área de redes de computadores continua a passos largos. Há progressos em todas as frentes, \nincluindo distribuição de roteadores mais velozes e velocidades de transmissão mais altas nas redes de acesso e \nnos backbones da rede. Mas os seguintes desenvolvimentos merecem atenção especial:\n• Desde o início do milênio, vimos a implementação agressiva do acesso à Internet por banda larga nos \nlares — não apenas modems a cabo e DSL, mas também “fiber to the home”\n, conforme discutimos na \nSeção 1.2. Esse acesso à Internet de alta velocidade preparou a cena para uma série de aplicações de ví-\ndeo, incluindo a distribuição de vídeo gerado pelo usuário (por exemplo, YouTube), fluxo contínuo por \ndemanda de filmes e shows de televisão (por exemplo, Netflix) e videoconferência entre várias pessoas \n(por exemplo, Skype).\n• A onipresença cada vez maior das redes Wi-Fi públicas de alta velocidade (54 Mbits/s e mais altas) e o \nacesso à Internet com velocidade média (até alguns Mbits/s) por redes de telefonia celular 3G e 4G não \napenas está possibilitando permanecer constantemente conectado enquanto se desloca, mas também \npermite novas aplicações específicas à localização. O número de dispositivos sem fio conectados ultra-\npassou o número de dispositivos com fio em 2011. Esse acesso sem fio em alta velocidade preparou a \ncena para o rápido surgimento de computadores portáteis (iPhones, Androids, iPads etc.), que possuem \nacesso constante e livre à Internet.\n• Redes sociais on-line, como Facebook e Twitter, criaram redes de pessoas maciças em cima da Internet. \nMuitos usuários hoje “vivem” principalmente dentro do Facebook. Através de suas APIs, as redes sociais \non-line criam plataformas para novas aplicações em rede e jogos distribuídos.\n• Conforme discutimos na Seção 1.3.3, os provedores de serviços on-line, como Google e Microsoft, im-\nplementaram suas próprias amplas redes privativas, que não apenas conectam seus centros de dados \ndistribuídos em todo o planeta, mas são usadas para evitar a Internet ao máximo possível, emparelhando \ndiretamente com ISPs de nível mais baixo. Como resultado, Google oferece resultados de busca e acesso \na e-mail quase instantaneamente, como se seus centros de dados estivessem rodando dentro do compu-\ntador de cada usuário.\n• Muitas empresas de comércio na Internet agora estão rodando suas aplicações na “nuvem” — como na EC2 \nda Amazon, na Application Engine da Google ou na Azure da Microsoft. Diversas empresas e universidades \ntambém migraram suas aplicações da Internet (por exemplo, e-mail e hospedagem de páginas Web) para a \nnuvem. Empresas de nuvem não apenas oferecem ambientes de computação e armazenamento escaláveis às \naplicações, mas também lhes oferecem acesso implícito às suas redes privativas de alto desempenho.\n1.8  Resumo\nNeste capítulo, abordamos uma quantidade imensa de assuntos. Examinamos as várias peças de hardware \ne software que compõem a Internet, em especial, e redes de computadores, em geral. Começamos pela periferia \nRedes de computadores e a Internet  49 \nda rede, examinando sistemas finais e aplicações, além do serviço de transporte fornecido às aplicações que exe-\ncutam nos sistemas finais. Também vimos as tecnologias de camada de enlace e meio físico encontrados na rede \nde acesso. Em seguida, mergulhamos no interior da rede e chegamos ao seu núcleo, identificando comutação \nde pacotes e comutação de circuitos como as duas abordagens básicas do transporte de dados por uma rede de \ntelecomunicações, expondo os pontos fortes e fracos de cada uma delas. Examinamos, então, as partes inferiores \n(do ponto de vista da arquitetura) da rede — as tecnologias de camada de enlace e os meios físicos comumente \nencontrados na rede de acesso. Estudamos também a estrutura da Internet global e aprendemos que ela é uma \nrede de redes. Vimos que a estrutura hierárquica da Internet, composta de ISPs de níveis mais altos e mais baixos, \npermitiu que ela se expandisse e incluísse milhares de redes.\nNa segunda parte deste capítulo introdutório, abordamos diversos tópicos fundamentais da área de redes \nde computadores. Primeiro examinamos as causas de atrasos e perdas de pacotes em uma rede de comutação de \npacotes. Desenvolvemos modelos quantitativos simples de atrasos de transmissão, de propagação e de fila, bem \ncomo modelos de vazão; esses modelos de atrasos serão muito usados nos problemas propostos em todo o livro. \nEm seguida examinamos camadas de protocolo e modelos de serviço, princípios fundamentais de arquitetura \nde redes aos quais voltaremos a nos referir neste livro. Analisamos também alguns dos ataques mais comuns na \nInternet. Terminamos nossa introdução sobre redes com uma breve história das redes de computadores. O pri-\nmeiro capítulo constitui um minicurso sobre redes de computadores.\nPortanto, percorremos de fato um extraordinário caminho neste primeiro capítulo! Se você estiver um \npouco assustado, não se preocupe. Abordaremos todas essas ideias em detalhes nos capítulos seguintes (é uma \npromessa, e não uma ameaça!). Por enquanto, esperamos que, ao encerrar este capítulo, você tenha adquirido \numa noção, ainda que incipiente, das partes que formam uma rede, um domínio ainda em desenvolvimento do \nvocabulário de redes (não se acanhe de voltar aqui para consulta) e um desejo cada vez maior de aprender mais \nsobre elas. Essa é a tarefa que nos espera no restante deste livro.\nO guia deste livro\nAntes de iniciarmos qualquer viagem, sempre é bom consultar um guia para nos familiarizar com as estra-\ndas principais e desvios que encontraremos pela frente. O destino final da viagem que estamos prestes a empreen­\nder é um entendimento profundo do como, do quê e do porquê das redes de computadores. Nosso guia é a \nsequência de capítulos:\n1.\t\nRedes de computadores e a Internet\n2.\t\nCamada de aplicação\n3.\t\nCamada de transporte\n4.\t\nCamada de rede\n5.\t\nCamada de enlace e redes locais (LANs)\n6.\t\nRedes sem fio e móveis\n7.\t\nRedes multimídia\n8.\t\nSegurança em redes de computadores\n9.\t\nGerenciamento de rede\nOs Capítulos 2 a 5 são os quatro capítulos centrais deste livro. Note que eles estão organizados segundo as \nquatro camadas superiores da pilha de cinco camadas de protocolos da Internet, com um capítulo para cada uma. \nNote também que nossa jornada começará no topo da pilha, a saber, a camada de aplicação, e prosseguirá daí para \nbaixo. O princípio racional que orienta essa jornada de cima para baixo é que, entendidas as aplicações, podemos \ncompreender os serviços de rede necessários para dar-lhes suporte. Então, poderemos examinar, um por um, os \nvários modos como esses serviços poderiam ser executados por uma arquitetura de rede. Assim, o estudo das \naplicações logo no início dá motivação para o restante do livro.\n   Redes de computadores e a Internet\n50\nA segunda metade — Capítulos 6 a 9 — aborda quatro tópicos de extrema importância (e de certa maneira \nindependentes) de redes modernas. No Capítulo 6, examinamos as redes sem fio e móvel, incluindo LANs sem fio \n(incluindo Wi-Fi e Bluetooth), redes de telefonia celular (GSM) e mobilidade (nas redes IP e GSM). No Capítulo \n7 (Redes multimídia), examinamos aplicações de áudio e vídeo, como telefone por Internet, videoconferência e \nfluxo contínuo de mídia armazenada. Examinamos também como uma rede de comutação de pacotes pode ser \nprojetada para prover qualidade de serviço consistente para aplicações de áudio e vídeo. No Capítulo 8 (Segu-\nrança em redes de computadores), analisamos, primeiro, os fundamentos da criptografia e da segurança de redes \ne, em seguida, de que modo a teoria básica está sendo aplicada a um amplo leque de contextos da Internet. No \núltimo capítulo (“Gerenciamento de redes”), examinamos as questões fundamentais do gerenciamento de redes, \nbem como os principais protocolos da Internet utilizados para esse fim.\nExercícios  \nde fixação e perguntas\nQuestões de revisão do Capítulo 1\nSEÇÃO 1.1\n\t\nR1.\t Qual é a diferença entre um hospedeiro e um sistema final? Cite os tipos de sistemas finais. Um servidor Web \né um sistema final?\n\t\nR2.\t A palavra protocolo é muito usada para descrever relações diplomáticas. Como a Wikipedia descreve um \nprotocolo diplomático?\n\t\nR3.\t Por que os padrões são importantes para os protocolos?\nSEÇÃO 1.2\n\t\nR4.\t Cite seis tecnologias de acesso. Classifique cada uma delas nas categorias acesso residencial, acesso corporativo \nou acesso móvel.\n\t\nR5.\t A taxa de transmissão HFC é dedicada ou é compartilhada entre usuários? É possível haver colisões na \ndireção provedor-usuário de um canal HFC? Por quê?\n\t\nR6.\t Cite as tecnologias de acesso residencial disponíveis em sua cidade. Para cada tipo de acesso, apresente a taxa \ndownstream, a taxa upstream e o preço mensal anunciados.\n\t\nR7.\t Qual é a taxa de transmissão de LANs Ethernet?\n\t\nR8.\t Cite alguns meios físicos utilizados para instalar a Ethernet.\n\t\nR9.\t Modems discados, HFC, DSL e FTTH são usados para acesso residencial. Para cada uma dessas tecnologias \nde acesso, cite uma faixa de taxas de transmissão e comente se a taxa de transmissão é compartilhada ou \ndedicada.\n\t\nR10.\t Descreva as tecnologias de acesso sem fio mais populares atualmente. Faça uma comparação entre elas.\nSEÇÃO 1.3\n\t\nR11.\t Suponha que exista exatamente um comutador de pacotes entre um computador de origem e um de destino. \nAs taxas de transmissão entre a máquina de origem e o comutador e entre este e a máquina de destino são R1 e \nR2, respectivamente. Admitindo que um roteador use comutação de pacotes do tipo armazena-e-reenvia, qual \né o atraso total fim a fim para enviar um pacote de comprimento L? (Desconsidere formação de fila, atraso de \npropagação e atraso de processamento.)\nRedes de computadores e a Internet  51 \n\t\nR12.\t Qual é a vantagem de uma rede de comutação de circuitos em relação a uma de comutação de pacotes? Quais \nsão as vantagens da TDM sobre a FDM em uma rede de comutação de circuitos?\n\t\nR13.\t Suponha que usuários compartilhem um enlace de 2 Mbits/s e que cada usuário transmita continuamente \na 1 Mbit/s, mas cada um deles transmite apenas 20% do tempo. (Veja a discussão sobre multiplexação \nestatística na Seção 1.3.)\na.\t Quando a comutação de circuitos é utilizada, quantos usuários podem ser admitidos?\nb.\t Para o restante deste problema, suponha que seja utilizada a comutação de pacotes. Por que não haverá \natraso de fila antes de um enlace se dois ou menos usuários transmitirem ao mesmo tempo? Por que \nhaverá atraso de fila se três usuários transmitirem ao mesmo tempo?\nc.\t Determine a probabilidade de um dado usuário estar transmitindo.\nd.\t Suponha agora que haja três usuários. Determine a probabilidade de, a qualquer momento, os três usuários \ntransmitirem simultaneamente. Determine a fração de tempo durante o qual a fila cresce.\n\t\nR14.\t Por que dois ISPs no mesmo nível de hierarquia farão emparelhamento? Como um IXP consegue ter lucro?\n\t\nR15.\t Alguns provedores de conteúdo criaram suas próprias redes. Descreva a rede da Google. O que motiva os \nprovedores de conteúdo a criar essas redes?\nSEÇÃO 1.4\n\t\nR16.\t Considere o envio de um pacote de uma máquina de origem a uma de destino por uma rota fixa. Relacione \nos componentes do atraso que formam o atraso fim a fim. Quais deles são constantes e quais são variáveis?\n\t\nR17.\t Visite o applet “Transmission versus Propagation Delay” no site de apoio do livro. Entre as taxas, o atraso de \npropagação e os tamanhos de pacote disponíveis, determine uma combinação para a qual o emissor termine \nde transmitir antes que o primeiro bit do pacote chegue ao receptor. Ache outra combinação para a qual o \nprimeiro bit do pacote alcança o receptor antes que o emissor termine de transmitir.\n\t\nR18.\t Quanto tempo um pacote de 1.000 bytes leva para se propagar através de um enlace de 2.500 km de distância, \ncom uma velocidade de propagação de 2,5 ∙ 108 m/s e uma taxa de transmissão de 2 Mbits/s? Em geral, quanto \ntempo um pacote de comprimento L leva para se propagar através de um enlace de distância d, velocidade de \npropagação s, e taxa de transmissão de R bits/s? Esse atraso depende do comprimento do pacote? Depende da \ntaxa de transmissão?\n\t\nR19.\t Suponha que o hospedeiro A queira enviar um arquivo grande para o hospedeiro B. O percurso de A para \nB possui três enlaces, de taxas R1 = 500 kbits/s, R2 = 2 Mbits/s, e R3 = 1 Mbit/s.\na.\t Considerando que não haja nenhum outro tráfego na rede, qual é a vazão para a transferência de arquivo?\nb.\t Suponha que o arquivo tenha 4 milhões de bytes. Dividindo o tamanho do arquivo pela vazão, quanto \ntempo levará a transferência para o hospedeiro B?\nc.\t Repita os itens “a” e “b”\n, mas agora com R2 reduzido a 100 kbits/s.\n\t\nR20.\t Suponha que o sistema final A queira enviar um arquivo grande para o sistema B. Em um nível muito alto, \ndescreva como o sistema A cria pacotes a partir do arquivo. Quando um desses arquivos chega ao comutador \nde pacote, quais informações no pacote o comutador utiliza para determinar o enlace através do qual o pacote \né encaminhado? Por que a comutação de pacotes na Internet é semelhante a dirigir de uma cidade para outra \npedindo informações ao longo do caminho?\n\t\nR21.\t Visite o applet “Queuing and Loss” no site de apoio do livro. Qual é a taxa de emissão máxima e a taxa de \ntransmissão mínima? Com essas taxas, qual é a intensidade do tráfego? Execute o applet com essas taxas e \ndetermine o tempo que leva a ocorrência de uma perda de pacote. Repita o procedimento mais uma vez e \ndetermine de novo o tempo de ocorrência para a perda de pacote. Os resultados são diferentes? Por quê? Por \nque não?\n   Redes de computadores e a Internet\n52\nSEÇÃO 1.5\n\t\nR22.\t Cite cinco tarefas que uma camada pode executar. É possível que uma (ou mais) dessas tarefas seja(m) \nrealizada(s) por duas (ou mais) camadas?\n\t\nR23.\t Quais são as cinco camadas da pilha de protocolo da Internet? Quais as principais responsabilidades de \ncada uma dessas camadas?\n\t\nR24.\t O que é uma mensagem de camada de aplicação? Um segmento de camada de transporte? Um datagrama \nde camada de rede? Um quadro de camada de enlace?\n\t\nR25.\t Que camadas da pilha do protocolo da Internet um roteador processa? Que camadas um comutador de \ncamada de enlace processa? Que camadas um sistema final processa?\nSEÇÃO 1.6\n\t\nR26.\t Qual é a diferença entre um vírus e um worm?\n\t\nR27.\t Descreva como pode ser criado uma botnet e como ela pode ser utilizada no ataque DDoS.\n\t\nR28.\t Suponha que Alice e Bob estejam enviando pacotes um para o outro por uma rede de computadores e que \nTrudy se posicione na rede para poder capturar todos os pacotes enviados por Alice e enviar o que quiser \npara Bob; ela também consegue capturar todos os pacotes enviados por Bob e enviar o que quiser para Alice. \nCite algumas atitudes maliciosas que Trudy pode fazer a partir de sua posição.\nproblemas\n\t\nP1.\t Projete e descreva um protocolo de nível de aplicação para ser usado entre um caixa eletrônico e o computador \ncentral de um banco. Esse protocolo deve permitir verificação do cartão e da senha de um usuário, consulta \ndo saldo de sua conta (que é mantido no computador central) e saque de dinheiro (isto é, entrega de dinheiro \nao usuário). As entidades do protocolo devem estar preparadas para resolver o caso comum em que não \nhá dinheiro suficiente na conta para cobrir o saque. Especifique seu protocolo relacionando as mensagens \ntrocadas e as ações realizadas pelo caixa automático ou pelo computador central do banco na transmissão \ne recepção de mensagens. Esquematize a operação de seu protocolo para o caso de um saque simples sem \nerros, usando um diagrama semelhante ao da Figura 1.2. Descreva explicitamente o que seu protocolo \nespera do serviço de transporte fim a fim.\n\t\nP2.\t A Equação 1.1 contém uma fórmula para o atraso fim a fim do envio de um pacote de comprimento L por N \nenlaces com taxa de transmissão R. Generalize essa fórmula para enviar P desses pacotes de ponta a ponta \npelos N enlaces.\n\t\nP3.\t Considere uma aplicação que transmita dados a uma taxa constante (por exemplo, a origem gera uma \nunidade de dados de N bits a cada k unidades de tempo, onde k é pequeno e fixo). Considere também \nque, quando essa aplicação começa, continuará em funcionamento por um período de tempo relativamente \nlongo. Responda às seguintes perguntas, dando uma breve justificativa para suas respostas:\na.\t O que seria mais apropriado para essa aplicação: uma rede de comutação de circuitos ou uma rede de \ncomutação de pacotes? Por quê?\nb.\t Suponha que seja usada uma rede de comutação de pacotes e que o único tráfego venha de aplicações como a \ndescrita anteriormente. Além disso, imagine que a soma das velocidades de dados da aplicação seja menor do \nque a capacidade de cada enlace. Será necessário algum tipo de controle de congestionamento? Por quê?\n\t\nP4.\t Considere a rede de comutação de circuitos da Figura 1.13. Lembre-se de que há 4 circuitos em cada enlace. \nRotule os quatro comutadores A, B, C e D, seguindo no sentido horário.\na.\t Qual é o número máximo de conexões simultâneas que podem estar em curso a qualquer instante nessa \nrede?\nRedes de computadores e a Internet  53 \nb.\t Suponha que todas as conexões sejam entre os comutadores A e C. Qual é o número máximo de conexões \nsimultâneas que podem estar em curso?\nc.\t Suponha que queiramos fazer quatro conexões entre os comutadores A e C, e outras quatro conexões \nentre os switches B e D. Podemos rotear essas chamadas pelos quatro enlaces para acomodar todas as oito \nconexões?\n\t\nP5.\t Considere novamente a analogia do comboio de carros da Seção 1.4. Admita uma velocidade de propagação \nde 100 km/h.\na.\t Suponha que o comboio viaje 150 km, começando em frente ao primeiro dos postos de pedágio, passando \npor um segundo e terminando após um terceiro. Qual é o atraso fim a fim?\nb.\t Repita o item ‘a’ admitindo agora que haja oito carros no comboio em vez de dez.\n\t\nP6.\t Este problema elementar começa a explorar atrasos de propagação e de transmissão, dois conceitos centrais \nem redes de computadores. Considere dois hospedeiros, A e B, conectados por um único enlace de taxa \nR bits/s. Suponha que eles estejam separados por m metros e que a velocidade de propagação ao longo do \nenlace seja de s metros/segundo. O hospedeiro A tem de enviar um pacote de L bits ao hospedeiro B.\na.\t Expresse o atraso de propagação, dprop, em termos de m e s.\nb.\t Determine o tempo de transmissão do pacote, dtrans, em termos de L e R.\nc.\t Ignorando os atrasos de processamento e de fila, obtenha uma expressão para o atraso fim a fim.\nd.\t Suponha que o hospedeiro A comece a transmitir o pacote no instante t = 0. No instante t = dtrans, onde \nestará o último bit do pacote?\ne.\t Imagine que dprop seja maior do que dtrans. Onde estará o primeiro bit do pacote no instante t = dtrans?\nf.\t Considere que dprop seja menor do que dtrans. Onde estará o primeiro bit do pacote no instante t = dtrans?\ng.\t Suponha que s = 2,5 ∙ 108, L = 120 bits e R = 56 kbits/s. Encontre a distância m de modo que dprop seja igual \na dtrans.\n\t\nP7.\t Neste problema, consideramos o envio de voz em tempo real do hospedeiro A para o hospedeiro B por \nmeio de uma rede de comutação de pacotes (VoIP). O hospedeiro A converte voz analógica para uma cadeia \ndigital de bits de 64 kbits/s e, em seguida, agrupa os bits em pacotes de 56 bytes. Há apenas um enlace entre \nos hospedeiros A e B; sua taxa de transmissão é de 2 Mbits/s e seu atraso de propagação, de 10 ms. Assim \nque o hospedeiro A recolhe um pacote, ele o envia ao hospedeiro B. Quando recebe um pacote completo, o \nhospedeiro B converte os bits do pacote em um sinal analógico. Quanto tempo decorre entre o momento em \nque um bit é criado (a partir do sinal analógico no hospedeiro A) e o momento em que ele é decodificado \n(como parte do sinal analógico no hospedeiro B)?\n\t\nP8.\t Suponha que usuários compartilhem um enlace de 3 Mbits/s e que cada usuário precise de 150 kbits/s para \ntransmitir, mas que transmita apenas durante 10% do tempo. (Veja a discussão sobre comutação de pacotes \nversus comutação de circuitos na Seção 1.3.)\na.\t Quando é utilizada comutação de circuitos, quantos usuários podem ser aceitos?\nb.\t Para o restante deste problema, suponha que seja usada a comutação de pacotes. Determine a probabilidade \nde que determinado usuário esteja transmitindo.\nc.\t Suponha que haja 120 usuários. Determine a probabilidade que, a um tempo dado, exatamente n usuários \nestejam transmitindo simultaneamente. (Dica: Use a distribuição binomial.)\nd.\t Determine a probabilidade de haver 21 ou mais usuários transmitindo simultaneamente.\n\t\nP9.\t Considere a discussão na Seção 1.3 sobre comutação de pacotes versus comutação de circuitos, na qual é dado \num exemplo com um enlace de 1 Mbit/s. Quando em atividade, os usuários estão gerando dados a uma taxa \nde 100 kbits/s; mas a probabilidade de estarem em atividade, gerando dados, é de p = 0,1. Suponha que o \nenlace de 1 Mbit/s seja substituído por um de 1 Gbit/s.\na.\t Qual é o número máximo de usuários, N, que pode ser suportado simultaneamente por comutação de \npacotes?\n   Redes de computadores e a Internet\n54\nb.\t Agora considere comutação de circuitos e um número M de usuários. Elabore uma fórmula (em termos \nde p, M, N) para a probabilidade de que mais de N usuários estejam enviando dados.\n\t\nP10.\t Considere um pacote de comprimento L que se inicia no sistema final A e percorre três enlaces até um \nsistema final de destino. Eles estão conectados por dois comutadores de pacotes. Suponha que di, si e Ri \nrepresentem o comprimento, a velocidade de propagação e a taxa de transmissão do enlace i, sendo i = 1, \n2, 3. O comutador de pacote atrasa cada pacote por dproc. Considerando que não haja nenhum atraso de \nfila, em relação a di, si e Ri, (i = 1, 2, 3) e L, qual é o atraso fim a fim total para o pacote? Suponha agora que \no pacote tenha 1.500 bytes, a velocidade de propagação de ambos os enlaces seja 2,5 ∙ 108 m/s, as taxas de \ntransmissão dos três enlaces sejam 2 Mbits/s, o atraso de processamento do comutador de pacotes seja de \n3 ms, o comprimento do primeiro enlace seja 5.000 km, o do segundo seja 4.000 km e do último 1.000 km. \nDados esses valores, qual é o atraso fim a fim?\n\t\nP11.\t No problema anterior, suponha que R1 = R2 = R3 = R e dproc = 0. Suponha que o comutador de pacote não \narmazene e reenvie pacotes, mas transmita imediatamente cada bit recebido antes de esperar o pacote chegar. \nQual é o atraso fim a fim?\n\t\nP12.\t Um comutador de pacotes recebe um pacote e determina o enlace de saída pelo qual deve ser enviado. Quando \no pacote chega, outro já está sendo transmitido nesse enlace de saída e outros quatro já estão esperando \npara serem transmitidos. Os pacotes são transmitidos em ordem de chegada. Suponha que todos os pacotes \ntenham 1.500 bytes e que a taxa do enlace seja 2 Mbits/s. Qual é o atraso de fila para o pacote? De modo geral, \nqual é o atraso de fila quando todos os pacotes possuem comprimento L, a taxa de transmissão é R, x bits do \npacote sendo transmitido já foram transmitidos e N pacotes já estão na fila?\n\t\nP13.\t (a) \u0007\nSuponha que N pacotes cheguem simultaneamente ao enlace no qual não há pacotes sendo transmitidos \ne nem pacotes enfileirados. Cada pacote tem L de comprimento e é transmitido à taxa R. Qual é o atraso \nmédio para os N pacotes?\n(b) Agora considere que N desses pacotes cheguem ao enlace a cada LN/R segundos. Qual é o atraso de fila \nmédio de um pacote?\n\t\nP14.\t Considere o atraso de fila em um buffer de roteador, sendo I a intensidade de tráfego; isto é, I = La/R. Suponha \nque o atraso de fila tome a forma de IL/R (1 – I) para I < 1.\na.\t Deduza uma fórmula para o atraso total, isto é, para o atraso de fila mais o atraso de transmissão.\nb.\t Faça um gráfico do atraso total como uma função de L/R.\n\t\nP15.\t Sendo a a taxa de pacotes que chegam a um enlace em pacotes/s, e μ a taxa de transmissão de enlaces em \npacotes/s, baseado na fórmula do atraso total (isto é, o atraso de fila mais o atraso de transmissão) do problema \nanterior, deduza uma fórmula para o atraso total em relação a a e μ.\n\t\nP16.\t Considere um buffer de roteador anterior a um enlace de saída. Neste problema, você usará a fórmula de \nLittle, uma famosa fórmula da teoria das filas. Considere N o número médio de pacotes no buffer mais o \npacote sendo transmitido, a a taxa de pacotes que chegam no enlace, e d o atraso total médio (isto é, o atraso \nde fila mais o atraso de transmissão) sofrido pelo pacote. Dada a fórmula de Little N = a ∙ d, suponha que, na \nmédia, o buffer contenha 10 pacotes, o atraso de fila de pacote médio seja 10 ms e a taxa de transmissão do \nenlace seja 100 pacotes/s. Utilizando tal fórmula, qual é a taxa média de chegada, considerando que não há \nperda de pacote?\n\t\nP17.\t (a) \u0007\nGeneralize a Equação 1.2 na Seção 1.4.3 para taxas de processamento heterogêneas, taxas de transmissão \ne atrasos de propagação.\n(b) Repita o item (a), mas suponha também que haja um atraso definição fila médio dfila em cada nó.\n\t\nP18.\t Execute o programa Traceroute para verificar a rota entre uma origem e um destino, no mesmo continente, \npara três horários diferentes do dia.\na.\t Determine a média e o desvio-padrão dos atrasos de ida e volta para cada um dos três horários.\nb.\t Determine o número de roteadores no caminho para cada um dos três. Os caminhos mudaram em algum \ndos horários?\nRedes de computadores e a Internet  55 \nc.\t Tente identificar o número de redes de ISP pelas quais o pacote do Traceroute passa entre origem e destino. \nRoteadores com nomes semelhantes e/ou endereços IP semelhantes devem ser considerados parte do \nmesmo ISP. Em suas respostas, os maiores atrasos ocorrem nas interfaces de formação de pares entre ISPs \nadjacentes?\nd.\t Faça o mesmo para uma origem e um destino em continentes diferentes. Compare os resultados dentro \ndo mesmo continente com os resultados entre continentes diferentes.\n\t\nP19.\t (a) \u0007\nVisite o site <www.traceroute.org> e realize traceroutes de duas cidades diferentes na França para o \nmesmo hospedeiro de destino nos Estados Unidos. Quantos enlaces são iguais nos dois traceroutes? O \nenlace transatlântico é o mesmo?\n(b) Repita (a), mas desta vez escolha uma cidade na França e outra cidade na Alemanha.\n(c) Escolha uma cidade nos Estados Unidos e realize traceroutes para dois hosts, cada um em uma cidade \ndiferente na China. Quantos enlaces são comuns nos dois traceroutes? Os dois traceroutes divergem antes \nde chegar à China?\n\t\nP20.\t Considere o exemplo de vazão correspondente à Figura 1.20 (b). Agora imagine que haja M pares de cliente-\nservidor em vez de 10. Rs, Rc e R representam as taxas do enlace do servidor, enlaces do cliente e enlace da \nrede. Suponha que os outros enlaces possuam capacidade abundante e que não haja outro tráfego na rede \nalém daquele gerado pelos M pares cliente-servidor. Deduza uma expressão geral para a vazão em relação a \nRs, Rc, R e M.\n\t\nP21.\t Considere a Figura 1.19(b). Agora suponha que haja M percursos entre o servidor e o cliente. Dois percursos \nnunca compartilham qualquer enlace. O percurso k (k = 1, ..., M) consiste em N enlaces com taxas de \ntransmissão Rk\n1, Rk\n2...; RK\nN. Se o servidor pode usar somente um percurso para enviar dados ao cliente, qual é a \nvazão máxima que ele pode atingir? Se o servidor pode usar todos os M percursos para enviar dados, qual é \na vazão máxima que ele pode atingir?\n\t\nP22.\t Considere a Figura 1.19(b). Suponha que cada enlace entre o servidor e o cliente possua uma probabilidade \nde perda de pacote p, e que as probabilidades de perda de pacote para esses enlaces sejam independentes. \nQual é a probabilidade de um pacote (enviado pelo servidor) ser recebido com sucesso pelo receptor? Se o \npacote se perder no percurso do servidor para o cliente, então o servidor retransmitirá o pacote. Na média, \nquantas vezes o servidor retransmitirá o pacote para que o cliente o receba com sucesso?\n\t\nP23.\t Considere a Figura 1.19(a). Suponha que o enlace de gargalo ao longo do percurso do servidor para o cliente \nseja o primeiro com a taxa Rs bits/s. Imagine que enviemos um par de pacotes um após o outro do servidor \npara o cliente, e que não haja outro tráfego nesse percurso. Suponha também que cada pacote de tamanho L \nbits e os dois enlaces tenham o mesmo atraso de propagação dprop.\na.\t Qual é o tempo entre chegadas do pacote ao destino? Isto é, quanto tempo transcorre desde quando o \núltimo bit do primeiro pacote chega até quando o último bit do segundo pacote chega?\nb.\t Agora suponha que o segundo enlace seja o de gargalo (isto é, Rc < Rs). É possível que o segundo pacote \nentre na fila de entrada do segundo enlace? Explique. Agora imagine que o servidor envie o segundo \npacote T segundos após enviar o primeiro. Qual deverá ser o tamanho de T para garantir que não haja \numa fila antes do segundo enlace? Explique.\n\t\nP24.\t Imagine que você queira enviar, com urgência, 40 terabytes de dados de Boston para Los Angeles. Você tem \ndisponível um enlace dedicado de 100 Mbits/s para transferência de dados. Escolheria transmitir os dados \npor meio desse enlace ou usar um serviço de entrega em 24 horas? Explique.\n\t\nP25.\t Suponha que dois hospedeiros, A e B, estejam separados por uma distância de 20 mil quilômetros e \nconectados por um enlace direto de R = 2 Mbits/s. Suponha que a velocidade de propagação pelo enlace \nseja de 2,5 ∙ 108 m/s.\na.\t Calcule o produto largura de banda-atraso R ∙ dprop.\n   Redes de computadores e a Internet\n56\nb.\t Considere o envio de um arquivo de 800 mil bits do hospedeiro A para o hospedeiro B. Suponha que o arquivo \nseja enviado continuamente, como se fosse uma única grande mensagem. Qual é o número máximo de bits que \nestará no enlace a qualquer dado instante?\nc.\t Interprete o produto largura de banda × atraso.\nd.\t Qual é o comprimento (em metros) de um bit no enlace? É maior do que o de um campo de futebol?\ne.\t Derive uma expressão geral para o comprimento de um bit em termos da velocidade de propagação s, da \nvelocidade de transmissão R e do comprimento do enlace m.\n\t\nP26.\t Com referência ao Problema P25, suponha que possamos modificar R. Para qual valor de R o comprimento \nde um bit será o mesmo que o comprimento do enlace?\n\t\nP27.\t Considere o Problema P25, mas agora com um enlace de R = 1 Gbit/s.\na.\t Calcule o produto largura de banda × atraso, R × dprop.\nb.\t Considere o envio de um arquivo de 800 mil bits do hospedeiro A para o computador B. Suponha que o \narquivo seja enviado continuamente, como se fosse uma única grande mensagem. Qual será o número \nmáximo de bits que estará no enlace a qualquer dado instante?\nc.\t Qual é o comprimento (em metros) de um bit no enlace?\n\t\nP28.\t Novamente com referência ao Problema P25.\na.\t Quanto tempo demora para mandar o arquivo, admitindo que ele seja enviado continuamente?\nb.\t Suponha agora que o arquivo seja fragmentado em 20 pacotes e que cada um contenha 40 mil bits. Imagine \nque cada pacote seja verificado pelo receptor e que o tempo de transmissão de uma verificação de pacote \nseja desprezível. Por fim, admita que o emissor não possa enviar um pacote até que o anterior tenha sido \nreconhecido. Quanto tempo demorará para enviar o arquivo?\nc.\t Compare os resultados de ‘a’ e ‘b’\n.\n\t\nP29.\t Suponha que haja um enlace de micro-ondas de 10 Mbits/s entre um satélite geoestacionário e sua estação-\nbase na Terra. A cada minuto o satélite tira uma foto digital e a envia à estação-base. Considere uma velocidade \nde propagação de 2,4 ∙ 108 m/s.\na.\t Qual é o atraso de propagação do enlace?\nb.\t Qual é o produto largura de banda-atraso, R ∙ dprop? \nc.\t Seja x o tamanho da foto. Qual é o valor mínimo de x para que o enlace de micro­\n‑ondas transmita \ncontinuamente?\n\t\nP30.\t Considere a analogia da viagem aérea que utilizamos em nossa discussão sobre camadas na Seção 1.5, e o \nacréscimo de cabeçalhos a unidades de dados de protocolo enquanto passam pela pilha. Existe uma noção \nequivalente de acréscimo de informações de cabeçalho à movimentação de passageiros e suas malas pela pilha \nde protocolos da linha aérea?\n\t\nP31.\t Em redes modernas de comutação de pacotes, inclusive a Internet, o hospedeiro de origem segmenta \nmensagens longas de camada de aplicação (por exemplo, uma imagem ou um arquivo de música) em pacotes \nmenores e os envia pela rede. O destinatário, então, monta novamente os pacotes restaurando a mensagem \noriginal. Denominamos esse processo segmentação de mensagem. A Figura 1.27 ilustra o transporte fim a fim \nde uma mensagem com e sem segmentação. Considere que uma mensagem de 8 × 106 bits de comprimento \ntenha de ser enviada da origem ao destino na Figura 1.27. Suponha que a velocidade de cada enlace da figura \nseja 2 Mbits/s. Ignore atrasos de propagação, de fila e de processamento.\na.\t Considere o envio da mensagem da origem ao destino sem segmentação. Quanto tempo essa mensagem \nlevará para ir do hospedeiro de origem até o primeiro comutador de pacotes? Tendo em mente que cada \ncomutador usa comutação de pacotes do tipo armazena-e-reenvia, qual é o tempo total para levar a \nmensagem do hospedeiro de origem ao hospedeiro de destino?\nb.\t Agora suponha que a mensagem seja segmentada em 800 pacotes, cada um com 10.000 bits de comprimento. \nQuanto tempo demorará para o primeiro pacote ir do hospedeiro de origem até o primeiro comutador? \nRedes de computadores e a Internet  57 \nQuando o primeiro pacote está sendo enviado do primeiro ao segundo comutador, o segundo pacote está \nsendo enviado da máquina de origem ao primeiro comutador. Em que instante o segundo pacote terá sido \ncompletamente recebido no primeiro comutador?\nc.\t Quanto tempo demorará para movimentar o arquivo do hospedeiro de origem até o hospedeiro de \ndestino quando é usada segmentação de mensagem? Compare este resultado com sua resposta no item ‘a’ \ne comente.\nd.\t Além de reduzir o atraso, quais são as razões para usar a segmentação de mensagem?\ne.\t Discuta as desvantagens da segmentação de mensagem.\n\t\nP32.\t Experimente o applet “Message Segmentation” apresentado no site deste livro. Os atrasos no applet \ncorrespondem aos atrasos obtidos no problema anterior? Como os atrasos de propagação no enlace afetam \no atraso total fim a fim na comutação de pacotes (com segmentação de mensagem) e na comutação de \nmensagens?\n\t\nP33.\t Considere o envio de um arquivo grande de F bits do hospedeiro A para o hospedeiro B. Há dois enlaces \n(e dois comutadores) entre A e B, e os enlaces não estão congestionados (isto é, não há atrasos de fila). \nO hospedeiro A fragmenta o arquivo em segmentos de S bits cada e adiciona 80 bits de cabeçalho a cada \nsegmento, formando pacotes de L =80 + S bits. Cada enlace tem uma taxa de transmissão de R bits/s. Qual é \no valor de S que minimiza o atraso para levar o arquivo de A para B? Desconsidere o atraso de propagação.\n\t\nP34.\t O Skype oferece um serviço que lhe permite fazer uma ligação telefônica de um PC para um telefone comum. \nIsso significa que a chamada de voz precisa passar pela Internet e por uma rede telefônica. Discuta como isso \npoderia ser feito.\nWireshark Lab\n“Conte-me e eu esquecerei. Mostre-me e eu lembrarei. \n \nEnvolva-me e eu entenderei.”\nProvérbio chinês\nA compreensão de protocolos de rede pode ser muito mais profunda se os virmos em ação e interagirmos \ncom eles — observando a sequência de mensagens trocadas entre duas entidades de protocolo, pesquisando de-\ntalhes de sua operação, fazendo que eles executem determinadas ações e observando tais ações e suas consequên­\ncias. Isso pode ser feito em cenários simulados ou em um ambiente real de rede, tal como a Internet. Os applets \nJava apresentados (em inglês) no site deste livro adotam a primeira abordagem. Nos Wireshark labs adotaremos \na última. Você executará aplicações de rede em vários cenários utilizando seu computador no escritório, em casa \nFigura 1.27  \u0007\nTransporte fim a fim de mensagem: (a) sem segmentação de mensagem; (b) com \nsegmentação de mensagem\nOrigem\na.\nComutador \nde pacotes\nComutador \nde pacotes\nDestino\nMensagem\nOrigem\nb.\nComutador \nde pacotes\nComutador \nde pacotes\nPacote\nDestino\n   Redes de computadores e a Internet\n58\nou em um laboratório e observará também os protocolos de rede interagindo e trocando mensagens com entida-\ndes de protocolo que estão executando em outros lugares da Internet. Assim, você e seu computador serão partes \nintegrantes desses laboratórios ao vivo. Você observará — e aprenderá — fazendo.\nA ferramenta básica para observar as mensagens trocadas entre entidades de protocolos em execução é deno-\nminada analisador de pacotes (packet sniffer). Como o nome sugere, um analisador de pacotes copia (fareja) passi-\nvamente mensagens enviadas e recebidas por seu computador; também exibe o conteúdo dos vários campos de \nprotocolo das mensagens que captura. Uma tela do analisador de pacotes Wireshark é mostrada na Figura 1.28. O \nWireshark é um analisador de pacotes gratuito que funciona em computadores com sistemas operacionais Win-\ndows, Linux/Unix e Mac. Por todo o livro, você encontrará Wireshark labs que o habilitarão a explorar vários dos \nprotocolos estudados em cada capítulo. Neste primeiro Wireshark lab, você obterá e instalará uma cópia do progra-\nma, acessará um site e examinará as mensagens de protocolo trocadas entre seu navegador e o servidor Web.\nVocê encontrará detalhes completos, em inglês, sobre este primeiro Wireshark Lab (incluindo instruções \nsobre como obter e instalar o programa) no site de apoio deste livro.\nFigura 1.28  \u0007\nUma amostra de tela do programa Wireshark (amostra de tela do Wireshark \nreimpressa com permissão da Wireshark Foundation)\nMenu de \ncomandos\nListagem \nde pacotes \ncapturados\nDetalhes do \ncabeçalho \ndo pacote \nselecionado\nConteúdo \ndo pacote em \nhexadecimal \ne ASCII\nKR 01.28.eps\nAW/Kurose and Ross\nComputer Networking 6/e\nsize:  36p0  x  20p8\n10/28/11, 11/23/11 rossi \nLeonard Kleinrock\nLeonard Kleinrock é professor de ciência da computação da Universidade da Califórnia \nem Los Angeles. Em 1969, seu computador na UCLA se tornou o primeiro nó da Internet. \nEle criou os princípios da comutação de pacotes em 1961, que se tornou a tecnologia bási-\nca da Internet. Ele é bacharel em engenharia elétrica pela City College of New York (CCNY) \ne mestre e doutor em engenharia elétrica pelo Instituto de Tecnologia de Massachusetts \n(MIT).\nENTREVISTA\nRedes de computadores e a Internet  59 \nO que o fez se decidir pela especialização em tec-\nnologia de redes/Internet?\nComo doutorando do MIT em 1959, percebi que \na maioria dos meus colegas de turma estava fazendo \npesquisas nas áreas de teoria da informação e de teoria \nda codificação. Havia no MIT o grande pesquisador \nClaude Shannon, que já tinha proposto estudos nes-\nsas áreas e resolvido a maior parte dos problemas im-\nportantes. Os problemas que restaram para pesquisar \neram difíceis e de menor importância. Portanto, deci-\ndi propor uma nova área na qual até então ninguém \ntinha pensado. Lembre-se de que no MIT eu estava \ncercado de computadores, e era evidente para mim \nque em breve aquelas máquinas teriam de se comuni-\ncar umas com as outras. Na época, não havia nenhum \nmeio eficaz de fazer isso; portanto, decidi desenvolver \na tecnologia que permitiria a criação de redes de dados \neficientes e confiáveis.\nQual foi seu primeiro emprego no setor de compu-\ntação? O que ele envolvia?\nFrequentei o curso noturno de bacharelado em en-\ngenharia elétrica da CCNY de 1951 a 1957. Durante \no dia, trabalhei de início como técnico e depois como \nengenheiro em uma pequena empresa de eletrônica \nindustrial chamada Photobell. Enquanto trabalhava \nlá, introduzi tecnologia digital na linha de produtos \nda empresa. Basicamente, estávamos usando equipa-\nmentos fotoelétricos para detectar a presença de certos \nitens (caixas, pessoas etc.) e a utilização de um circuito \nconhecido na época como multivibrador biestável era \nexatamente o tipo de tecnologia de que precisávamos \npara levar o processamento digital a esse campo da de-\ntecção. Acontece que esses circuitos são os blocos de \nconstrução básicos dos computadores e vieram a ser \nconhecidos como flip-flops ou chaves na linguagem co-\nloquial de hoje.\nO que passou por sua cabeça quando enviou a \nprimeira mensagem computador a computador \n(da UCLA para o Stanford Research Institute)?\nFrancamente, não fazíamos a menor ideia da impor-\ntância daquele acontecimento. Não havíamos prepa-\nrado uma mensagem de significância história, como \nmuitos criadores do passado o fizeram (Samuel Mor-\nse com “Que obra fez Deus.”\n, ou Alexandre Graham \nBell, com “Watson, venha cá! Preciso de você.”\n, ou \nNeil Armstrong com “Este é um pequeno passo para \no homem, mas um grande salto para a humanidade.”). \nEsses caras eram inteligentes! Eles entendiam de meios \nde comunicação e relações públicas. Nosso objetivo foi \nnos conectar ao computador do SRI. Então digitamos a \nletra “L”\n, que foi aceita corretamente, digitamos a letra \n“o”\n, que foi aceita, e depois digitamos a letra “g”\n, o que \nfez o hospedeiro no SRI pifar! Então, nossa mensagem \nacabou sendo curta e, talvez, a mais profética de todas, \nou seja, “Lo!”\n, como em “Lo and behold” (Pasmem!).\nAntes, naquele mesmo ano, fui citado em um comu-\nnicado de imprensa da UCLA por ter dito que, logo que \na rede estivesse pronta e em funcio­\nnamento, seria pos-\nsível ter acesso a outros ­\ncomputadores a partir de nossa \ncasa e escritório tão facilmente quanto tínhamos acesso à \neletricidade e ao telefone. Portanto, a visão que eu tinha \nda Internet naquela época era que ela seria onipresente, \nestaria sempre em funcionamento e sempre disponível, \nque qualquer pessoa que possuísse qualquer equipamen-\nto poderia se conectar com ela de qualquer lugar e que ela \nseria invisível. Mas jamais imaginei que minha mãe, aos \n99 anos de idade, usaria a Internet — e ela de fato usou!\nEm sua opinião, qual é o futuro das redes?\nA parte fácil da visão é predizer a infraestrutura por \nsi mesma. Eu antecipo que vemos uma implantação \nconsiderável de computação nômade, aparelhos móveis \ne espaços inteligentes. Realmente, a disponibilidade da \ncomputação portátil, de alto desempenho, acessível e \nleve, e dos aparelhos de comunicação (mais a onipre-\nsença da Internet) que permitiu que nos tornássemos \nnômades. A computação nômade refere-se à tecnologia \nque permite aos usuários finais, que viajam de um lugar \npara o outro, ganhar acesso aos serviços da Internet de \nmaneira transparente, não importando para onde vão \ne qual aparelho possuem ou ganham acesso. O difícil é \npredizer as aplicações e serviços, que nos surpreende-\nram consistentemente de formas dramáticas (e-mail, \ntecnologias de busca, a World Wide Web, blogs, redes \nsociais, geração de usuários e compartilhamento de \nmúsica, fotos, vídeos etc.). Estamos na margem de uma \nnova categoria de aplicações móveis, inovadoras e sur-\npreendentes, presentes em nossos aparelhos portáteis.\nO passo seguinte vai nos capacitar a sair do mun-\ndo misterioso do ciberespaço para o mundo físico dos \nespaços inteligentes. A tecnologia dará vida a nossos \nambientes (mesas, paredes, veículos, relógios e cintos, \nentre outros) por meio de atuadores, sensores, lógica, \nprocessamento, armazenagem, câmeras, microfones, al-\nto-falantes, monitores e comunicação. Essa tecnologia \n   Redes de computadores e a Internet\n60\nembutida permitirá que nosso ambiente forneça os \nserviços IP que quisermos. Quando eu entrar em uma \nsala, ela saberá que entrei. Poderei me comunicar com \nmeu ambiente naturalmente, como se estivesse falan-\ndo o meu idioma nativo; minhas solicitações gerarão \nrespostas apresentadas como páginas Web em painéis \nde parede, por meus óculos, por voz, por hologramas \ne assim por diante.\nAnalisando um panorama mais longínquo, vejo um \nfuturo para as redes que inclui componentes funda-\nmentais que ainda virão. Vejo agentes inteligentes de \nsoftware distribuídos por toda a rede, cuja função é fa-\nzer mineração de dados, agir sobre esses dados, obser-\nvar tendências e realizar tarefas de formas dinâmicas \ne adaptativas. Vejo tráfego de rede consideravelmente \nmaior gerado não tanto por seres humanos, mas por \nesses equipamentos embutidos e agentes inteligentes \nde software. Vejo grandes conjuntos de sistemas au-\nto-organizáveis controlando essa rede imensa e veloz. \nVejo quantidades enormes de informações zunindo \npor essa rede instantaneamente e passando por proces-\nsamento e filtragem extraordinários. A Internet será, \nbasicamente, um sistema nervoso de presença global. \nVejo tudo isso e mais enquanto entramos de cabeça no \nséculo XXI.\nQue pessoas o inspiraram profissionalmente?\nSem dúvida alguma, quem mais me inspirou foi \nClaude Shannon, do MIT, um brilhante pesquisador \nque tinha a capacidade de relacionar suas ideias mate-\nmáticas com o mundo físico de modo muitíssimo in-\ntuitivo. Ele fazia parte da banca examinadora de minha \ntese de doutorado.\nVocê pode dar algum conselho aos estudantes \nque estão ingressando na área de redes/Internet?\nA Internet, e tudo o que ela habilita, é uma vasta \nfronteira nova, cheia de desafios surpreendentes. Há \nespaço para grandes inovações. Não fiquem limitados \nà tecnologia existente hoje. Soltem sua imaginação, \npensem no que poderia acontecer e transformem isso \nem realidade.\nAplicações de rede são a razão de ser de uma rede de computadores. Se não fosse possível inventar apli-\ncações úteis, não haveria necessidade de projetar protocolos de rede para suportá-las. Desde o surgimento da \nInternet, foram criadas numerosas aplicações úteis e divertidas. Elas têm sido a força motriz por trás do sucesso \nda Internet, motivando pessoas em lares, escolas, governos e empresas a tornarem a rede uma parte integral de \nsuas atividades diárias.\nEntre as aplicações da Internet estão as aplicações clássicas de texto, que se tornaram populares nas décadas \nde 1970 e 1980: correio eletrônico, acesso a computadores remotos, transferência de arquivo e grupos de discus-\nsão. Também há uma aplicação que alcançou estrondoso sucesso em meados da década de 1990: a World Wide \nWeb, abrangendo a navegação na Web, busca e o comércio eletrônico. Duas aplicações de enorme sucesso também \nsurgiram no final do milênio — mensagem instantânea e compartilhamento de arquivos P2P. Desde 2000, temos \nvisto uma explosão de aplicações populares de voz e vídeo, incluindo: voz sobre IP (VoIP) e videoconferência \nsobre IP, como Skype; distribuição de vídeo gerada pelo usuário, como YouTube; e filmes por demanda, como \nNetflix. Durante esse mesmo período, também vimos aparecerem jogos on-line com vários jogadores, bastante \natraentes, como Second Life e World of Warcraft. Mais recentemente, vimos o surgimento de uma nova geração \nde aplicações de rede social, como Facebook e Twitter, que criaram redes humanas atraentes em cima da rede de \nroteadores e enlaces de comunicação da Internet. É evidente que não tem havido redução de aplicações novas e \ninteressantes para Internet. Talvez alguns dos leitores deste texto criem a próxima geração de aplicações quentes \npara Internet!\nNeste capítulo estudaremos os aspectos conceituais e de implementação de aplicações de rede. Começa-\nremos definindo conceitos fundamentais de camada de aplicação, incluindo serviços de rede exigidos por apli-\ncações, clientes e servidores, processos e interfaces de camada de transporte. Vamos examinar detalhadamente \nvárias aplicações de rede, entre elas a Web, e-mail, DNS e distribuição de arquivos P2P (o Capítulo 8 focaliza as \naplicações multimídia, incluindo o vídeo por demanda e VoIP). Em seguida, abordaremos o desenvolvimento \nde aplicação de rede por TCP e também por UDP. Em particular, vamos estudar o API socket e examinar al-\ngumas aplicações cliente-servidor simples em Python. Apresentaremos também vários exercícios divertidos e \ninteressantes de programação de aplicações no final do capítulo.\nA camada de aplicação é um lugar particularmente bom para iniciarmos o estudo de protocolos. É terreno \nfamiliar, pois conhecemos muitas das aplicações que dependem dos que estudaremos. Ela nos dará uma boa \nideia do que são protocolos e nos apresentará muitos assuntos que encontraremos de novo quando estudarmos \nprotocolos de camadas de transporte, de rede e de enlace.\ncamada de\naplicação\n3 4 5 6\n8 9\n7\nc\n \na\n \np\n \ní\n \nt\n \nu\n \nl\n \no\n12\n   Redes de computadores e a Internet\n62\n2.1  Princípios de aplicações de rede\nSuponha que você tenha uma grande ideia para uma nova aplicação de rede. Essa aplicação será, talvez, \num grande serviço para a humanidade, ou agradará a seu professor, ou fará de você uma pessoa rica; ou apenas \nserá divertido desenvolvê-la. Seja qual for sua motivação, vamos examinar agora como transformar a ideia em \numa aplicação do mundo real.\nO núcleo do desenvolvimento de aplicação de rede é escrever programas que rodem em sistemas finais \ndiferentes e se comuniquem entre si. Por exemplo, na aplicação Web há dois programas distintos que se comuni-\ncam um com o outro: o do navegador, que roda no hospedeiro do usuário (computador de mesa, laptop, tablet, \nsmartphone e assim por diante); e o do servidor Web, que roda na máquina deste. Outro exemplo é um sistema de \ncompartilhamento de arquivos P2P no qual há um programa em cada máquina que participa da comunidade de \ncompartilhamento de arquivos. Nesse caso, os programas de cada máquina podem ser semelhantes ou idênticos.\nPortanto, ao desenvolver sua nova aplicação, você precisará escrever um software que rode em vários \nsistemas finais. Esse software poderia ser criado, por exemplo, em C, Java ou Python. Importante: você não \nprecisará escrever programas que executem nos elementos do núcleo de rede, como roteadores e comuta-\ndores. Mesmo se quisesse, não poderia desenvolver programas para esses elementos. Como aprendemos \nno Capítulo 1 e mostramos na Figura 1.24, equipamentos de núcleo de rede não funcionam na camada de \naplicação, mas em camadas mais baixas, em especial na de rede e abaixo dela. Esse projeto básico — a saber, \nconfinar o software de aplicação nos sistemas finais —, como mostra a Figura 2.1, facilitou o desenvolvi-\nmento e a proliferação rápidos de uma vasta gama de aplicações de rede.\n2.1.1 Arquiteturas de aplicação de rede\nAntes de mergulhar na codificação do software, você deverá elaborar um plano geral para a arquitetura da \nsua aplicação. Tenha sempre em mente que a arquitetura de uma aplicação é bastante diferente da arquitetura da \nrede (por exemplo, a arquitetura em cinco camadas da Internet que discutimos no Capítulo 1). Do ponto de vista \ndo profissional que desenvolve a aplicação, a arquitetura de rede é fixa e provê um conjunto específico de serviços. \nPor outro lado, a arquitetura da aplicação é projetada pelo programador e determina como a aplicação é orga-\nnizada nos vários sistemas finais. Ao escolher a arquitetura da aplicação, é provável que o programador aproveite \numa das duas arquiteturas mais utilizadas em aplicações modernas de rede: cliente-servidor ou P2P.\nEm uma arquitetura cliente-servidor há um hospedeiro sempre em funcionamento, denominado servidor, \nque atende a requisições de muitos outros hospedeiros, denominados clientes. Um exemplo clássico é a aplicação \nWeb na qual um servidor Web que está sempre em funcionamento atende a solicitações de navegadores de hos-\npedeiros clientes. Quando recebe uma requisição de um objeto de um hospedeiro cliente, um servidor Web res-\nponde enviando o objeto solicitado. Observe que, na arquitetura cliente-servidor, os clientes não se comunicam \ndiretamente uns com os outros; por exemplo, na aplicação Web, dois navegadores não se comunicam de modo \ndireto. Outra característica dessa arquitetura é que o servidor tem um endereço fixo, bem conhecido, denomi-\nnado endereço IP (que discutiremos em breve). Por causa dessa característica do servidor e pelo fato de ele estar \nsempre em funcionamento, um cliente sempre pode contatá-lo, enviando um pacote ao endereço do servidor. \nAlgumas das aplicações mais conhecidas que empregam a arquitetura cliente-servidor são Web, FTP, Telnet e \ne-mail. Essa arquitetura é mostrada na Figura 2.2(a).\nEm aplicações cliente-servidor, muitas vezes acontece de um único hospedeiro servidor ser incapaz de \natender a todas as requisições de seus clientes. Por exemplo, um site popular de redes sociais pode ficar logo sa-\nturado se tiver apenas um servidor para atender a todas as solicitações. Por essa razão, um datacenter, acomo-\ndando um grande número de hospedeiros, é usado com frequência para criar um servidor virtual poderoso. \nOs serviços de Internet mais populares — como mecanismos de busca (por exemplo, Google e Bing), comércio \nvia Internet (por exemplo, Amazon e eBay), e-mail baseado na Web (por exemplo, Gmail e Yahoo Mail), rede \nsocial (por exemplo, Facebook e Twitter) — empregam um ou mais centros de dados. Conforme discutimos \nCAMADA  de APLICAÇÃO  63 \nna Seção 1.3.3, o Google tem de 30 a 50 datacenters distribuídos no mundo inteiro, que em conjunto tratam \nde busca, YouTube, Gmail e outros serviços. Um datacenter pode ter centenas de milhares de servidores, que \nprecisam ser alimentados e mantidos. Além disso, os provedores de serviços têm de pagar pelos custos de in-\nterconexão recorrente e largura de banda para o envio de dados a partir de seus datacenters.\nEm uma arquitetura P2P, há uma confiança mínima (ou nenhuma) nos servidores dedicados nos cen-\ntros de dados. Em vez disso, a aplicação utiliza a comunicação direta entre duplas de hospedeiros conectados \nalternadamente, denominados pares. Eles não são de propriedade dos provedores de serviço, mas são contro-\nlados por usuários de computadores de mesa e laptops, cuja maioria se aloja em residências, universidades e \nescritórios. Como os pares se comunicam sem passar por nenhum servidor dedicado, a arquitetura é deno-\nminada par a par (peer-to-peer — P2P). Muitas das aplicações de hoje mais populares e de intenso tráfego são \nRede móvel\nTransporte\nRede\nEnlace\nFísica\nAplicação\nKR 02.01.eps\nAW/Kurose and Ross\nComputer Networking, 6/e\nsize:  33p6  x  39p0\n9/6/11, 10/28/11, 10/31/11\n11/21/11 rossi \nISP nacional \nou global\nISP local \nou regional\nEnterprise Network\nRede doméstica\nTransporte\nRede\nEnlace\nAplicação\nFísica\nTransporte\nRede\nEnlace\nFísica\nAplicação\nFigura 2.1  \u0007\nA comunicação de uma aplicação de rede ocorre entre sistemas finais na camada \nde aplicação\n   Redes de computadores e a Internet\n64\nbaseadas nas arquiteturas P2P, incluindo compartilhamento de arquivos (por exemplo, BitTorrent), aceleração \nde download assistida por par (por exemplo, Xunlei), telefonia por Internet (por exemplo, Skype) e IPTV (por \nexemplo, KanKan e PPstream). Essa arquitetura está ilustrada na Figura 2.2(b). Mencionamos que algumas \naplicações possuem arquiteturas híbridas, combinando elementos cliente-servidor e P2P. Para muitas apli-\ncações de mensagem instantânea, os servidores costumam rastrear o endereço IP dos usuários, mas as men-\nsagens entre usuários são enviadas diretamente entre os hospedeiros do usuário (sem passar por servidores \nintermediários).\nUma das características mais fortes da arquitetura P2P é sua autoescalabilidade. Por exemplo, em uma \naplicação de compartilhamento de arquivos P2P, embora cada par gere uma carga de trabalho solicitando ar-\nquivos, também acrescenta capacidade de serviço ao sistema distribuindo arquivos a outros pares. As arquite-\nturas P2P também possuem uma boa relação custo-benefício, visto que em geral não requerem infraestrutura \ne largura de banda de servidor significativas (ao contrário de projetos cliente-servidor com centros de dados). \nEntretanto, as futuras aplicações P2P estão diante de três principais desafios:\n1.\t ISP Amigável. A maioria dos ISPs residenciais (incluindo o DSL e os ISPs a cabo) foi dimensionada para \nuso de largura de banda “assimétrica”\n, ou seja, para muito mais tráfego de entrada do que de saída. Mas a \ntransmissão de vídeo P2P e as aplicações de distribuição de vídeo transferem o tráfego de saída dos ser-\nvidores para ISPs residenciais, colocando, assim, uma pressão significativa nos ISPs. As futuras aplicações \nP2P precisam ser criadas para que sejam amigáveis aos ISPs [Xie, 2008].\n2.\t Segurança. Em razão de sua natureza altamente distribuída e exposta, as aplicações P2P podem ser um de-\nsafio para proteger [Doucer, 2002; Yu, 2006; Liang, 2006; Naoumov, 2006; Dhungel, 2008; LeBlond 2011].\n3.\t Incentivos. O sucesso das futuras aplicações P2P também depende de usuários participativos para ofere-\ncer largura de banda, armazenamento e recursos da computação às aplicações, um projeto desafiador de \nincentivo [Feldman, 2005; Piatek, 2008; Aperjis, 2008; Liu, 2010].\nKR 02.02.eps \nAW/Kurose and Ross\nComputer Networking 6/e\na. Arquitetura cliente-servidor\nb. Arquitetura P2P\nFigura 2.2  (a) Arquitetura cliente-servidor; (b) arquitetura P2P\nCAMADA  de APLICAÇÃO  65 \n2.1.2 Comunicação entre processos\nAntes de construir sua aplicação de rede, você também precisará ter um entendimento básico de como pro-\ngramas que rodam em vários sistemas finais comunicam-se entre si. No jargão de sistemas operacionais, na verdade \nnão são programas, mas processos que se comunicam. Um processo pode ser imaginado como um programa que \nestá rodando dentro de um sistema final. Quando os processos rodam no mesmo sistema final, comunicam-se \nusando comunicação interprocessos, cujas regras são determinadas pelo sistema operacional do sistema final. Po-\nrém, neste livro, não estamos interessados na comunicação entre processos do mesmo hospedeiro, mas em como \nse comunicam os que rodam em sistemas finais diferentes (com sistemas operacionais potencialmente diferentes).\nOs processos em dois sistemas finais diferentes se comunicam trocando mensagens por meio da rede de \ncomputadores. Um processo originador cria e envia mensagens para a rede; um processo destinatário recebe-as e \nresponde, devolvendo outras. A Figura 2.1 mostra que processos se comunicam usando a camada de aplicação da \npilha de cinco camadas da arquitetura.\nProcessos clientes e processos servidores\nUma aplicação de rede consiste em pares de processos que enviam mensagens uns para os outros por meio \nde uma rede. Por exemplo, na aplicação Web, o processo navegador de um cliente troca mensagens com o de um \nservidor Web. Em um sistema de compartilhamento de arquivos P2P, um arquivo é transferido de um processo \nque está em um par para um que está em outro par. Para cada par de processos comunicantes normalmente ro-\ntulamos um dos dois processos de cliente e o outro, de servidor. Na Web, um navegador é um processo cliente \ne um servidor Web é um processo servidor. No compartilhamento de arquivos P2P, o par que envia o arquivo é \nrotulado de cliente e o que recebe, de servidor.\nTalvez você já tenha observado que, em algumas aplicações, tal como compartilhamento de arquivos P2P, \num processo pode ser ambos, cliente e servidor. De fato, um processo em um sistema de compartilhamento de \narquivos P2P pode carregar e descarregar arquivos. Mesmo assim, no contexto de qualquer dada sessão entre um \npar de processos, ainda podemos rotular um processo de cliente e o outro de servidor. Definimos os processos \ncliente e servidor como segue:\nNo contexto de uma sessão de comunicação entre um par de processos, aquele que inicia a comunicação \n(isto é, o primeiro a contatar o outro no início da sessão) é rotulado de cliente. O que espera ser contatado para \niniciar a sessão é o servidor.\nNa Web, um processo do navegador inicia o contato com um processo do servidor Web; por conseguinte, o \nprocesso do navegador é o cliente e o do servidor Web é o servidor. No compartilhamento de arquivos P2P, quan-\ndo o Par A solicita ao Par B o envio de um arquivo específico, o Par A é o cliente enquanto o Par B é o servidor no \ncontexto dessa sessão de comunicação. Quando não houver possibilidade de confusão, às vezes usaremos também \na terminologia “lado cliente e lado servidor de uma aplicação”\n. No final deste capítulo examinaremos passo a passo \num código simples para ambos os lados de aplicações de rede: o lado cliente e o lado servidor.\nA interface entre o processo e a rede de computadores\nComo dissemos anteriormente, a maioria das aplicações consiste em pares de processos comunicantes, e \nos dois processos de cada par enviam mensagens um para o outro. Qualquer mensagem enviada de um processo \npara outro tem de passar pela rede subjacente. Um processo envia mensagens para a rede e recebe mensagens \ndela através de uma interface de software denominada socket. Vamos considerar uma analogia que nos auxiliará \na entender processos e sockets. Um processo é semelhante a uma casa e seu socket, à porta da casa. Quando um \nprocesso quer enviar uma mensagem a outro processo em outro hospedeiro, ele empurra a mensagem pela porta \n(socket). O emissor admite que exista uma infraestrutura de transporte do outro lado de sua porta que transpor-\ntará a mensagem pela rede até a porta do processo destinatário. Ao chegar ao hospedeiro destinatário, a mensa-\ngem passa pela porta (socket) do processo receptor, que então executa alguma ação sobre a mensagem.\n   Redes de computadores e a Internet\n66\nA Figura 2.3 ilustra a comunicação por socket entre dois processos que se comunicam pela Internet. (A \nFigura 2.3 admite que o protocolo de transporte subjacente usado pelos processos é o TCP.) Como mostra essa \nfigura, um socket é a interface entre a camada de aplicação e a de transporte dentro de um hospedeiro. É também \ndenominado interface de programação da aplicação (application programming interface — API) entre a aplica-\nção e a rede, visto que é a interface de programação pela qual as aplicações de rede são criadas. O programador da \naplicação controla tudo o que existe no lado da camada de aplicação do socket, mas tem pouco controle do lado de \nsua camada de transporte. Os únicos controles que o programador da aplicação tem do lado da camada de trans-\nporte são: (1) a escolha do protocolo de transporte e (2), talvez, a capacidade de determinar alguns parâmetros, \ntais como tamanho máximo de buffer e de segmentos (a serem abordados no Capítulo 3). Uma vez escolhido um \nprotocolo de transporte, (se houver escolha) o programador constrói a aplicação usando os serviços da camada \nde transporte oferecidos por esse protocolo. Examinaremos sockets mais detalhadamente na Seção 2.7.\nFigura 2.3  Processos de aplicação, sockets e protocolo de transporte subjacente\nProcesso\nHospedeiro \nou servidor\nHospedeiro \nou servidor\nControlado \npelo programador \nda aplicação\nControlado \npelo programador \nda aplicação\nProcesso\nTCP com \nbuffers, \nvariáveis\nInternet\nControlado \npelo sistema \noperacional\nControlado \npelo sistema \noperacional\nTCP com \nbuffers, \nvariáveis\nSocket\nSocket\nEndereçando processos\nPara enviar correspondência postal a determinado destino, este precisa ter um endereço. De modo semelhan-\nte, para que um processo rodando em um hospedeiro envie pacotes a um processo rodando em outro hospedeiro, \no receptor precisa ter um endereço. Para identificar o processo receptor, duas informações devem ser especificadas: \n(1) o endereço do hospedeiro e (2) um identificador que especifica o processo receptor no hospedeiro de destino.\nNa Internet, o hospedeiro é identificado por seu endereço IP. Discutiremos os endereços IP com mais deta-\nlhes no Capítulo 4. Por enquanto, tudo o que precisamos saber é que um endereço IP é uma quantidade de 32 bits \nque podemos pensar como identificando um hospedeiro de forma exclusiva. Além de saber o endereço do hospe-\ndeiro ao qual a mensagem é destinada, o processo de envio também precisa identificar o processo receptor (mais \nespecificamente, o socket receptor) executando no hospedeiro. Essa informação é necessária porque, em geral, um \nhospedeiro poderia estar executando muitas aplicações de rede. Um número de porta de destino atende a essa fina-\nlidade. Aplicações populares receberam números de porta específicos. Por exemplo, um servidor Web é identificado \npelo número de porta 80. Um processo servidor de correio (usando o protocolo SMTP) é identificado pelo número \nde porta 25. Uma lista dos números de porta conhecidos para todos os protocolos-padrão da Internet poderá ser \nencontrada em <http://www.iana.org>. Vamos examinar os números de porta em detalhes no Capítulo 3.\n2.1.3  Serviços de transporte disponíveis para aplicações\nLembre-se de que um socket é a interface entre o processo da aplicação e o protocolo de camada de \ntransporte. A aplicação do lado remetente envia mensagens por meio do socket. Do outro lado, o protocolo \nCAMADA  de APLICAÇÃO  67 \nde camada de transporte tem a responsabilidade de levar as mensagens pela rede até o socket do processo \ndestinatário.\nMuitas redes, inclusive a Internet, oferecem mais de um protocolo de camada de transporte. Ao desenvolver \numa aplicação, você deve escolher um dos protocolos de camada de transporte disponíveis. Como fazer essa es-\ncolha? O mais provável é que você avalie os serviços e escolha o protocolo que melhor atenda às necessidades de \nsua aplicação. A situação é semelhante a decidir entre ônibus ou avião como meio de transporte entre duas cida-\ndes. Você tem de optar, e cada modalidade de transporte oferece serviços diferentes. (Por exemplo, o ônibus ofe-\nrece a facilidade da partida e da chegada no centro da cidade, ao passo que o avião tem menor tempo de viagem.)\nQuais são os serviços que um protocolo da camada de transporte pode oferecer às aplicações que o cha-\nmem? Podemos classificar, de maneira geral, os possíveis serviços segundo quatro dimensões: transferência con-\nfiável de dados, vazão, temporização e segurança.\nTransferência confiável de dados\nComo discutido no Capítulo 1, os pacotes podem se perder dentro de uma rede de computadores. Um pacote \npode, por exemplo, esgotar um buffer em um roteador, ou ser descartado por um hospedeiro ou um roteador após \nalguns de seus bits terem sido corrompidos. Para muitas aplicações — como correio eletrônico, transferência de \narquivo, acesso a hospedeiro remoto, transferências de documentos da Web e aplicações financeiras — a perda de \ndados pode ter consequências devastadoras (no último caso, para o banco e para o cliente!). Assim, para suportar \nessas aplicações, algo deve ser feito para garantir que os dados enviados por uma extremidade da aplicação sejam \ntransmitidos correta e completamente para a outra ponta. Se um protocolo fornecer um serviço de recebimento de \ndados garantido, ele fornecerá uma transferência confiável de dados. Um importante serviço que o protocolo da \ncamada de transporte pode oferecer para uma aplicação é a transferência confiável de dados processo a processo. \nQuando um protocolo de transporte oferece esse serviço, o processo remetente pode apenas passar seus dados para \num socket e saber com absoluta confiança que eles chegarão sem erro ao processo destinatário.\nQuando um protocolo da camada de transporte não oferece uma transferência confiável de dados, os dados \nenviados pelo remetente talvez nunca cheguem ao destinatário. Isso pode ser aceitável para aplicações tolerantes \na perda, em especial as de multimídia como áudio/vídeo em tempo real ou áudio/vídeo armazenado, que podem \ntolerar alguma perda de dados. Nessas aplicações, dados perdidos podem resultar em uma pequena falha durante \na execução do áudio/vídeo — o que não é um prejuízo crucial.\nVazão\nNo Capítulo 1 apresentamos o conceito de vazão disponível, que, no contexto de sessão da comunicação \nentre dois processos ao longo de um caminho de rede, é a taxa pela qual o processo remetente pode enviar bits \nao processo destinatário. Como outras sessões compartilharão a largura de banda no caminho da rede e estão \nindo e voltando, a vazão disponível pode oscilar com o tempo. Essas observações levam a outro serviço natural \nque um protocolo da camada de transporte pode oferecer, ou seja, uma vazão disponível garantida a uma taxa \nespecífica. Com tal serviço, a aplicação pode solicitar uma vazão garantida de r bits/s, e o protocolo de transporte \ngarante, então, que a vazão disponível seja sempre r bits/s, pelo menos. Tal serviço de vazão garantida seria atra-\nente para muitas aplicações. Por exemplo, se uma aplicação de telefonia por Internet codifica voz a 32 kbits/s, ela \nprecisa enviar dados para a rede e fazer que sejam entregues na aplicação receptora à mesma taxa. Se o protocolo \nde transporte não puder fornecer essa vazão, a aplicação precisará codificar a uma taxa menor (e receber vazão \nsuficiente para sustentar essa taxa de codificação mais baixa) ou então desistir, já que receber, digamos, metade \nda vazão de que precisa de nada ou pouco adianta para essa aplicação de telefonia por Internet. Aplicações que \npossuam necessidade de vazão são conhecidas como aplicações sensíveis à largura de banda. Muitas aplicações \nde multimídia existentes são sensíveis à largura de banda, embora algumas possam usar técnicas adaptativas para \ncodificar a uma taxa que corresponda à vazão disponível na ocasião.\n   Redes de computadores e a Internet\n68\nEmbora aplicações sensíveis à largura de banda possuam necessidades específicas de vazão, aplicações elás-\nticas podem usar qualquer quantidade mínima ou máxima que por acaso esteja disponível. Correio eletrônico, \ntransferência de arquivos e transferências Web são todas aplicações elásticas. Claro, quanto mais vazão, melhor. \nHá um ditado que diz que “dinheiro nunca é demais”; nesse caso, podemos dizer que vazão nunca é demais!\nTemporização\nUm protocolo da camada de transporte pode também oferecer garantias de temporização. Como nas ga-\nrantias de vazão, as de temporização podem surgir em diversos aspectos e modos. Citamos como exemplo o fato \nde que cada bit que o remetente insere no socket chega ao socket destinatário em menos de 100 ms depois. Esse \nserviço seria atrativo para aplicações interativas em tempo real, como a telefonia por Internet, ambientes virtuais, \nteleconferência e jogos multijogadores, que exigem restrições de temporização no envio de dados para garantir \neficácia. (Veja Capítulo 7, [Gauthier, 1999; Ramjee, 1994].) Longos atrasos na telefonia por Internet, por exemplo, \ntendem a resultar em pausas artificiais na conversação; em um jogo multiusuário ou ambiente virtual interativo, \num longo atraso entre realizar uma ação e ver a reação do ambiente (por exemplo, a reação de outro jogador na \noutra extremidade de uma conexão fim a fim) faz a aplicação parecer menos realista. Para aplicações que não \nsão em tempo real, é sempre preferível um atraso menor a um maior, mas não há nenhuma limitação estrita aos \natrasos fim a fim.\nSegurança\nPor fim, um protocolo de transporte pode oferecer um ou mais serviços de segurança a uma aplicação. Por \nexemplo, no hospedeiro remetente, um protocolo de transporte é capaz de codificar todos os dados transmitidos \npelo processo remetente e, no hospedeiro destinatário, o protocolo da camada de transporte pode codificar os da-\ndos antes de enviá-los ao destinatário. Tal serviço pode oferecer sigilo entre os dois, mesmo que os dados sejam, \nde algum modo, observados entre os processos remetente e destinatário. Um protocolo de transporte consegue, \nalém do sigilo, fornecer outros serviços de segurança, incluindo integridade dos dados e autenticação do ponto \nterminal, assuntos que serão abordados em detalhes no Capítulo 8.\n2.1.4  Serviços de transporte providos pela Internet\nAté aqui, consideramos serviços de transportes que uma rede de computadores poderia oferecer em geral. \nVamos agora nos aprofundar mais no assunto e analisar o tipo de suporte de aplicação provido pela Internet. A \nInternet (e, em um amplo sentido, as redes TCP/IP) disponibiliza dois protocolos de transporte para aplicações, \no UDP e o TCP. Quando você (como um criador de aplicação) cria uma nova aplicação de rede para a Internet, \numa das primeiras decisões a ser tomada é usar o UDP ou o TCP. Cada um deles oferece um conjunto diferente \nde serviços para as aplicações solicitantes. A Figura 2.4 mostra os requisitos do serviço para algumas aplicações.\nFigura 2.4  Requisitos de aplicações de rede selecionadas\nAplicação\nPerda de dados\nVazão\nSensibilidade ao tempo\nTransferência / download de arquivo\nSem perda\nElástica\nNão\nE-mail\nSem perda\nElástica\nNão\nDocumentos Web\nSem perda\nElástica (alguns kbits/s)\nNão\nTelefonia via Internet/\nvideoconferência\nTolerante à perda\nÁudio: alguns kbits/s – 1Mbit/s \nVídeo: 10 kbits/s – 5 Mbits/s\nSim: décimos de segundo\nÁudio/vídeo armazenado\nTolerante à perda\nIgual acima\nSim: alguns segundos\nJogos interativos\nTolerante à perda\nPoucos kbits/s – 10 kbits/s\nSim: décimos de segundo\nMensagem instantânea\nSem perda\nElástico\nSim e não\nCAMADA  de APLICAÇÃO  69 \nServiços do TCP\nO modelo de serviço TCP inclui um serviço orientado para conexão e um serviço confiá­\nvel de transferência \nde dados. Quando uma aplicação solicita o TCP como seu protocolo de transporte, recebe dele ambos os serviços.\n• Serviço orientado para conexão. O TCP faz o cliente e o servidor trocarem informações de controle de ca-\nmada de transporte antes que as mensagens de camada de aplicação comecem a fluir. Esse procedimento \nde apresentação, por assim dizer, alerta o cliente e o servidor, permitindo que eles se preparem para uma \nenxurrada de pacotes. Após a fase de apresentação, dizemos que existe uma conexão TCP entre os sockets \ndos dois processos. A conexão é full-duplex (simultânea), visto que os dois processos podem enviar men-\nsagens um ao outro pela conexão ao mesmo tempo. Quando termina de enviar mensagens, a aplicação \ndeve interromper a conexão. No Capítulo 3, discutiremos em detalhes serviço orientado para conexão e \nexaminaremos como ele é implementado.\n• Serviço confiável de transporte. Os processos comunicantes podem contar com o TCP para a entrega de \ntodos os dados enviados sem erro e na ordem correta. Quando um lado da aplicação passa uma cadeia de \nbytes para dentro de um socket, pode contar com o TCP para entregar a mesma cadeia de dados ao socket \nreceptor, sem falta de bytes nem bytes duplicados.\nO TCP também inclui um mecanismo de controle de congestionamento, um serviço voltado ao bem-estar \ngeral da Internet e não ao benefício direto dos processos comunicantes. O mecanismo de controle de congestio-\nnamento do TCP limita a capacidade de transmissão de um processo (cliente ou servidor) quando a rede está \ncongestionada entre remetente e destinatário. Como veremos no Capítulo 3, o controle de congestionamento do \nTCP tenta limitar cada conexão do TCP à sua justa porção de largura de banda de rede.\nServiços do UDP\nO UDP é um protocolo de transporte simplificado, leve, com um modelo de serviço minimalista. É um \nserviço não orientado para conexão; portanto, não há apresentação antes que os dois processos comecem a se \ncomunicar. O UDP provê um serviço não confiável de transferência de dados — isto é, quando um processo \nenvia uma mensagem para dentro de um socket UDP, o protocolo não oferece garantias de que a mensagem \nchegará ao processo receptor. Além do mais, mensagens que chegam de fato ao processo receptor podem che-\ngar fora de ordem.\nO UDP não inclui um mecanismo de controle de congestionamento; portanto, um processo originador \npode bombear dados para dentro de uma camada abaixo (a de rede) à taxa que quiser. (Observe, entretanto, que a \nvazão fim a fim real pode ser menor do que essa taxa por causa da capacidade de transmissão limitada de enlaces \nintervenientes ou pelo congestionamento.)\nServiços não providos pelos protocolos de transporte da Internet\nOrganizamos os serviços do protocolo de transporte em quatro dimensões: transferência confiável de da-\ndos, vazão, temporização e segurança. Quais deles são providos pelo TCP e pelo UDP? Já vimos que o TCP forne-\nce a transferência confiável de dados fim a fim, e sabemos também que ele pode ser aprimorado com facilidade na \ncamada de aplicação com o SSL para oferecer serviços de segurança. Mas em nossa breve descrição sobre o TCP e \no UDP faltou mencionar as garantias de vazão e de temporização — serviços não fornecidos pelos protocolos de \ntransporte da Internet de hoje. Isso significa que as aplicações sensíveis ao tempo, como a telefonia por Internet, \nnão podem rodar na rede atual? A resposta decerto é negativa — a Internet tem recebido essas aplicações por \nmuitos anos. Tais aplicações muitas vezes funcionam bem, por terem sido desenvolvidas para lidar, na medida \ndo possível, com a falta de garantia. Analisaremos vários desses truques de projeto no Capítulo 7. No entanto, o \nprojeto inteligente possui suas limitações quando o atraso é excessivo, ou quando a vazão fim a fim é limitada. \n   Redes de computadores e a Internet\n70\nEm resumo, a Internet hoje pode oferecer serviços satisfatórios a aplicações sensíveis ao tempo, mas não garantias \nde temporização ou de largura de banda.\nA Figura 2.5 mostra os protocolos de transporte usados por algumas aplicações populares da Internet. Vemos \nque e-mail, acesso a terminais remotos, a Web e transferência de arquivos usam o TCP. Essas aplicações escolheram \no TCP principalmente porque ele oferece um serviço confiável de transferência de dados, garantindo que todos \neles mais cedo ou mais tarde cheguem a seu destino. Como as aplicações de telefonia por Internet (como Skype) \nmuitas vezes toleram alguma perda, mas exigem uma taxa mínima para que sejam eficazes, seus programadores \nem geral preferem rodá-las em cima do UDP, contornando assim o mecanismo de controle de congestionamento \ndo TCP e evitando pacotes extras. Porém, como muitos firewalls são configurados para bloquear (quase todo) o \ntráfego UDP, as aplicações de telefonia por Internet quase sempre são projetadas para usar TCP como um apoio se \na comunicação por UDP falhar.\nFigura 2.5  \u0007\nAplicações populares da Internet, seus protocolos de camada de aplicação e seus \nprotocolos de transporte subjacentes\nAplicação\nProtocolo de camada de aplicação\nProtocolo de transporte subjacente\nCorreio eletrônico\nSMTP [RFC 5321]\nTCP\nAcesso a terminal remoto\nTelnet [RFC 854]\nTCP\nWeb\nHTTP [RFC 2616]\nTCP\nTransferência de arquivos\nFTP [RFC 959]\nTCP\nMultimídia em fluxo contínuo\nHTTP (por exemplo, YouTube)\nTCP\nTelefonia por Internet\nSIP [RFC 3261], RTP [RFC 3550] ou proprietária \n(por exemplo, Skype)\nUDP ou TCP\nProtegendo o TCP\nNem o TCP ou o UDP fornecem qualquer codifi-\ncação — os dados que o processo remetente trans-\nfere para seu socket são os mesmos que percorrem \na rede até o processo destinatário. Então, por exem-\nplo, se o processo destinatário enviar uma senha em \ntexto claro (ou seja, não codificado) para seu socket, \nela percorrerá por todos os enlaces entre o remeten-\nte e o destinatário, sendo analisada e descoberta em \nqualquer um dos enlaces intervenientes. Em razão de \na privacidade e outras questões de segurança terem \nse tornado importantes para muitas aplicações, a \ncomunidade da Internet desenvolveu um aperfeiçoa­\nmento para o TCP\n, denominado Camada de Socket \nSeguros (Secure Sockets Layer — SSL). O aperfei-\nçoamento SSL para o TCP não só faz tudo o que o \nTCP tradicional faz, como também oferece serviços \nimportantes de segurança processo a processo, in-\ncluindo codificação, integridade dos dados e autenti-\ncação do ponto de chegada. Enfatizamos que o SSL \nnão é um terceiro protocolo da Internet, no mesmo \nnível do TCP e do UDP\n, mas um aperfeiçoamento do \nTCP executado na camada de aplicação. Em parti-\ncular, se uma aplicação quiser utilizar o serviço do \nSSL, é preciso incluir o código SSL (disponível na \nforma de classes e bibliotecas altamente otimizadas) \nda aplicação em ambas as partes cliente e servidor. \nO SSL possui sua própria API de socket que é se-\nmelhante à tradicional API de socket TCP\n. Quando \numa aplicação utiliza o SSL, o processo remetente \ntransfere dados em texto claro para o socket SSL; no \nhospedeiro emissor, então, o SSL codifica os dados \ne os passa para o socket TCP\n. Os dados codificados \npercorrem a Internet até o socket TCP no processo \ndestinatário. O socket destinatário passa os dados \ncodificados ao SSL, que os decodifica. Por fim, o \nSSL passa os dados em texto claro por seu socket \nSSL até o processo destinatário. Abordaremos o SSL \nem mais detalhes no Capítulo 8.\nSegurança em foco\nCAMADA  de APLICAÇÃO  71 \n2.1.5  Protocolos de camada de aplicação\nAcabamos de aprender que processos de rede comunicam-se entre si enviando mensagens para dentro de \nsockets. Mas, como essas mensagens são estruturadas? O que significam os vários campos nas mensagens? Quan-\ndo os processos enviam as mensagens? Essas perguntas nos transportam para o mundo dos protocolos de camada \nde aplicação. Um protocolo de camada de aplicação define como processos de uma aplicação, que funcionam \nem sistemas finais diferentes, passam mensagens entre si. Em particular, um protocolo de camada de aplicação \ndefine:\n• Os tipos de mensagens trocadas, por exemplo, de requisição e de resposta.\n• A sintaxe dos vários tipos de mensagens, tais como os campos da mensagem e como os campos são de-\nlineados.\n• A semântica dos campos, isto é, o significado da informação nos campos.\n• Regras para determinar quando e como um processo envia mensagens e responde a mensagens.\nAlguns protocolos de camada de aplicação estão especificados em RFCs e, portanto, são de domínio públi-\nco. Por exemplo, o protocolo de camada de aplicação da Web, HTTP (HyperText Transfer Protocol [RFC 2616]), \nestá à disposição como um RFC. Se um programador de navegador seguir as regras do RFC do HTTP, o nave-\ngador estará habilitado a extrair páginas de qualquer servidor que também tenha seguido essas mesmas regras. \nMuitos outros protocolos de camada de aplicação são próprios e, de modo intencional, não estão disponíveis ao \npúblico, por exemplo, o Skype.\nÉ importante distinguir aplicações de rede de protocolos de camada de aplicação, os quais são apenas um \npedaço de aplicação de rede (embora muito importante, do nosso ponto de vista!). Examinemos alguns exemplos. \nA Web é uma aplicação cliente-servidor que permite aos usuários obter documentos de servidores por demanda. \nA aplicação Web consiste em muitos componentes, entre eles um padrão para formato de documentos (isto é, \nHTML), navegadores Web (por exemplo, Firefox e Microsoft Internet Explorer), servidores Web (por exemplo, \nApache e Microsoft) e um protocolo de camada de aplicação. O HTTP, protocolo de camada de aplicação da Web, \ndefine o formato e a sequência das mensagens que são passadas entre o navegador e o servidor. Assim, ele é ape-\nnas um pedaço (embora importante) da aplicação Web. Como outro exemplo, a aplicação de correio eletrônico \nda Internet que também tem muitos componentes, entre eles servidores de correio que armazenam caixas postais \nde usuários, leitores de correio (como o Microsoft Outlook) que permitem aos usuários ler e criar mensagens, um \npadrão que define como elas são passadas entre servidores e entre servidores e leitores de correio, e como deve \nser interpretado o conteúdo de cabeçalhos de mensagem. O principal protocolo de camada de aplicação para o \ncorreio eletrônico é o SMTP (Simple Mail Transfer Protocol) [RFC 5321]. Assim, o SMTP é apenas um pedaço \n(embora importante) da aplicação de correio eletrônico.\n2.1.6  Aplicações de rede abordadas neste livro\nNovas aplicações de Internet de domínio público e proprietárias são desenvolvidas todos os dias. Em \nvez de tratarmos de um grande número dessas aplicações de maneira enciclopédica, preferimos focalizar um \npequeno número ao mesmo tempo importantes e populares. Neste capítulo, discutiremos cinco aplicações \npopulares: a Web, a transferência de arquivos, o correio eletrônico, o serviço de diretório e aplicações P2P. \nDiscutiremos primeiro a Web não apenas porque ela é uma aplicação de imensa popularidade, mas também \nporque seu protocolo de camada de aplicação, HTTP, é direto e fácil de entender. Após estudarmos a Web, \nexaminaremos brevemente o FTP porque ele oferece um ótimo contraste com o HTTP. Em seguida, discuti-\nremos o correio eletrônico, a primeira aplicação de enorme sucesso da Internet. O correio eletrônico é mais \ncomplexo do que a Web, pois usa não somente um, mas vários protocolos de camada de aplicação. Após \no e-mail, estudaremos o DNS, que provê um serviço de diretório para a Internet. A maioria dos usuários \nnão interage direto com o DNS; em vez disso, eles o chamam indiretamente por meio de outras aplicações \n(inclusive a Web, a transferência de arquivos e o correio eletrônico). O DNS ilustra de maneira primorosa \n   Redes de computadores e a Internet\n72\ncomo um componente de funcionalidade do núcleo da rede (tradução de nome de rede para endereço de \nrede) pode ser implementado na camada de aplicação da Internet. Por fim, discutiremos várias aplicações \nP2P, incluindo aplicações de compartilhamento de arquivos e serviços de busca distribuída. No Capítulo 7, \nveremos as aplicações de multimídia, incluindo vídeo de fluxo contínuo e voz sobre IP (VoIP).\n2.2  A Web e o HTTP\nAté a década de 1990, a Internet era usada principalmente por pesquisadores, acadêmicos e estudantes \nuniversitários para efetuar login em hospedeiros remotos, transferir arquivos de hospedeiros locais para remo-\ntos e vice-versa, enviar e receber notícias e correio eletrônico. Embora essas aplicações fossem (e continuem a \nser) de extrema utilidade, a Internet era desconhecida fora das comunidades acadêmicas e de pesquisa. Então, \nno início da década de 1990, entrou em cena uma nova aplicação importantíssima — a World Wide Web [Ber-\nners-Lee, 1994]. A Web é a aplicação que chamou a atenção do público em geral. Ela transformou drasticamen-\nte a maneira como pessoas interagem dentro e fora de seus ambientes de trabalho. Alçou a Internet de apenas \nmais uma entre muitas para, na essência, a única rede de dados.\nTalvez o que mais atraia a maioria dos usuários da Web é que ela funciona por demanda. Usuários recebem \no que querem, quando querem, o que é diferente da transmissão de rádio e de televisão, que obriga a sintonizar \nquando o provedor disponibiliza o conteúdo. Além de funcionar por demanda, a Web tem muitas outras caracte-\nrísticas maravilhosas que as pessoas adoram. É muito fácil para qualquer indivíduo fazer que informações fiquem \ndisponíveis na Web — todo mundo pode se transformar em editor a um custo baixíssimo. Hiperenlaces e busca-\ndores nos ajudam a navegar pelo oceano dos sites. Dispositivos gráficos estimulam nossos sentidos. Formulários, \napplets Java e muitos outros recursos nos habilitam a interagir com páginas e sites. E a Web serve como uma \nplataforma para muitas aplicações de sucesso que surgiram após 2003, incluindo YouTube, Gmail e Facebook.\n2.2.1  Descrição geral do HTTP\nO HTTP — Protocolo de Transferência de Hipertexto (HyperText Transfer Protocol) —, \n \no protocolo da camada de aplicação da Web, está no coração da Web e é definido no [RFC 1945] e no [RFC 2616]. O \nHTTP é executado em dois programas: um cliente e outro servidor. Os dois, executados em sistemas finais dife-\nrentes, conversam entre si por meio da troca de mensagens HTTP. O HTTP define a estrutura dessas mensagens \ne o modo como o cliente e o servidor as trocam. Antes de explicarmos em detalhes o HTTP, devemos revisar a \nterminologia da Web.\nUma página Web (também denominada documento) é constituída de objetos. Um objeto é apenas um arqui-\nvo — tal como um arquivo HTML, uma imagem JPEG, um applet Java, ou um clipe de vídeo — que se pode acessar \ncom um único URL. A maioria das páginas Web é constituída de um arquivo-base HTML e diversos objetos refe-\nrenciados. Por exemplo, se uma página contiver um texto HTML e cinco imagens JPEG, então ela terá seis objetos: \no arquivo-base HTML e mais as cinco imagens. O arquivo-base HTML referencia os outros objetos na página com \nos URLs dos objetos. Cada URL tem dois componentes: o nome de hospedeiro (hostname) do servidor que abriga o \nobjeto e o nome do caminho do objeto. Por exemplo, no URL\nhttp://www.someSchool.edu/someDepartment/picture.gif\nwww.someSchool.edu é o nome de hospedeiro e /someDepartment/picture.gif é o nome do cami-\nnho. Como navegadores Web (por exemplo, Internet Explorer e Firefox) também executam o lado cliente do \nHTTP, usaremos as palavras navegador e cliente indiferentemente nesse contexto. Os servidores Web, que exe-\ncutam o lado servidor do HTTP, abrigam objetos Web, cada um endereçado por um URL. São servidores Web \npopulares o Apache e o Microsoft Internet Information Server.\nCAMADA  de APLICAÇÃO  73 \nO HTTP define como os clientes requisitam páginas aos servidores e como eles as transferem aos clientes. \nDiscutiremos em detalhes a interação entre cliente e servidor mais adiante, mas a ideia geral está ilustrada na Fi-\ngura 2.6. Quando um usuário requisita uma página Web (por exemplo, clica sobre um hiperenlace), o navegador \nenvia ao servidor mensagens de requisição HTTP para os objetos da página. O servidor recebe as requisições e \nresponde com mensagens de resposta HTTP que contêm os objetos.\nO HTTP usa o TCP como seu protocolo de transporte subjacente (em vez de rodar em cima do UDP). \nO cliente HTTP primeiro inicia uma conexão TCP com o servidor. Uma vez estabelecida, os processos do \nnavegador e do servidor acessam o TCP por meio de suas interfaces de socket. Como descrito na Seção 2.1, no \nlado cliente a interface socket é a porta entre o processo cliente e a conexão TCP; no lado servidor, ela é a porta \nentre o processo servidor e a conexão TCP. O cliente envia mensagens de requisição HTTP para sua interface \nsocket e recebe mensagens de resposta HTTP de sua interface socket. De maneira semelhante, o servidor HTTP \nrecebe mensagens de requisição de sua interface socket e envia mensagens de resposta para sua interface socket. \nAssim que o cliente envia uma mensagem para sua interface socket, a mensagem sai de suas mãos e passa a \nestar “nas mãos” do TCP. Lembre-se de que na Seção 2.1 dissemos que o TCP oferece ao HTTP um serviço \nconfiável de transferência de dados, o que implica que toda mensagem de requisição HTTP emitida por um \nprocesso cliente chegará intacta ao servidor. De maneira semelhante, toda mensagem de resposta HTTP emi-\ntida pelo processo servidor chegará intacta ao cliente. Percebemos, nesse ponto, uma das grandes vantagens \nde uma arquitetura de camadas — o HTTP não precisa se preocupar com dados perdidos ou com detalhes de \ncomo o TCP se recupera da perda de dados ou os reordena dentro da rede. Essa é a tarefa do TCP e dos proto-\ncolos das camadas mais inferiores da pilha de protocolos.\nÉ importante notar que o servidor envia ao cliente os arquivos solicitados sem armazenar qualquer infor-\nmação de estado sobre o cliente. Se determinado cliente solicita o mesmo objeto duas vezes em um período de \npoucos segundos, o servidor não responde dizendo que acabou de enviá-lo; em vez disso, manda de novo o obje-\nto, pois já esqueceu por completo o que fez antes. Como o servidor HTTP não mantém informação alguma sobre \nclientes, o HTTP é denominado um protocolo sem estado. Salientamos também que a Web usa a arquitetura de \naplicação cliente-servidor, como descrito na Seção 2.1. Um servidor Web está sempre em funcionamento, tem \num endereço IP fixo e atende requisições de potencialmente milhões de navegadores diferentes.\n2.2.2  Conexões persistentes e não persistentes\nEm muitas aplicações da Internet, o cliente e o servidor se comunicam por um período prolongado de \ntempo, em que o cliente faz uma série de requisições e o servidor responde a cada uma. Dependendo da aplica-\nção e de como ela está sendo usada, a série de requisições pode ser feita de forma consecutiva, periodicamente \nFigura 2.6  Comportamento de requisição-resposta do HTTP\nRequisição HTTP\nResposta HTTP\nResposta HTTP\nRequisição HTTP\nPC executando \nInternet Explorer\nLinux executando \nFirefox\nServidor executando \no servidor Web Apache\n   Redes de computadores e a Internet\n74\nem intervalos regulares ou de modo esporádico. Quando a interação cliente-servidor acontece por meio de \nconexão TCP, o programador da aplicação precisa tomar uma importante decisão — cada par de requisição/\nresposta deve ser enviado por uma conexão TCP distinta ou todas as requisições e suas respostas devem ser en-\nviadas por uma mesma conexão TCP? Na abordagem anterior, a aplicação utiliza conexões não persistentes; e \nna última abordagem, conexões persistentes. Para entender melhor este assunto, vamos analisar as vantagens \ne desvantagens das conexões não persistentes e das conexões persistentes no contexto de uma aplicação espe-\ncífica, o HTTP, que pode utilizar as duas. Embora o HTTP utilize conexões persistentes em seu modo padrão, \nos clientes e servidores HTTP podem ser configurados para utilizar a não persistente.\nO HTTP com conexões não persistentes\nVamos percorrer as etapas da transferência de uma página de um servidor para um cliente para o caso de \nconexões não persistentes. Suponhamos que uma página consista em um arquivo-base HTML e em dez imagens \nJPEG e que todos esses 11 objetos residam no mesmo servidor. Suponha também que o URL para o arquivo-base \nHTTP seja\nhttp://www.someSchool.edu/someDepartment/home.index\nEis o que acontece:\n1.\t O processo cliente HTTP inicia uma conexão TCP para o servidor www.someSchool.edu na porta \nnúmero 80, que é o número de porta default para o HTTP. Associados à conexão TCP, haverá um socket \nno cliente e um socket no servidor.\n2.\t O cliente HTTP envia uma mensagem de requisição HTTP ao servidor por meio de seu socket. Essa men-\nsagem inclui o nome de caminho /someDepartment/home.index. (Discutiremos mensagens HTTP \nmais detalhadamente logo adiante.)\n3.\t O processo servidor HTTP recebe a mensagem de requisição por meio de seu socket, extrai o objeto \n \n/someDepartment/home.index de seu armazenamento (RAM ou disco), encapsula-o em uma men-\nsagem de resposta HTTP e a envia ao cliente pelo socket.\n4.\t O processo servidor HTTP ordena ao TCP que encerre a conexão TCP. (Mas, na realidade, o TCP só a \nencerrará quando tiver certeza de que o cliente recebeu a mensagem de resposta intacta.)\n5.\t O cliente HTTP recebe a mensagem de resposta e a conexão TCP é encerrada. A mensagem indica que o \nobjeto encapsulado é um arquivo HTML. O cliente extrai o arquivo da mensagem de resposta, analisa o \narquivo HTML e encontra referências aos dez objetos JPEG.\n6.\t As primeiras quatro etapas são repetidas para cada um dos objetos JPEG referenciados.\nÀ medida que recebe a página Web, o navegador a apresenta ao usuário. Dois navegadores diferentes \npodem interpretar (isto é, exibir ao usuário) uma página de modos um pouco diferentes. O HTTP não tem \nnada a ver com o modo como uma página Web é interpretada por um cliente. As especificações do HTTP \n([RFC 1945] e [RFC 2616]) definem apenas o protocolo de comunicação entre o programa cliente HTTP e o \nprograma servidor HTTP.\nAs etapas apresentadas ilustram a utilização de conexões não persistentes, nas quais cada conexão TCP é \nencerrada após o servidor enviar o objeto — a conexão não persiste para outros objetos. Note que cada conexão \nTCP transporta exatamente uma mensagem de requisição e uma mensagem de resposta. Assim, nesse exemplo, \nquando um usuário solicita a página Web, são geradas 11 conexões TCP.\nNos passos descritos, fomos intencionalmente vagos sobre se os clientes obtêm as dez JPEGs por meio de \ndez conexões TCP em série ou se algumas delas são recebidas por conexões TCP paralelas. Na verdade, usuários \npodem configurar navegadores modernos para controlar o grau de paralelismo. Nos modos default, a maioria \ndos navegadores abre de cinco a dez conexões TCP paralelas e cada uma manipula uma transação requisição/\nresposta. Se o usuário preferir, o número máximo de conexões paralelas poderá ser fixado em um, caso em que as \nCAMADA  de APLICAÇÃO  75 \ndez conexões são estabelecidas em série. Como veremos no próximo capítulo, a utilização de conexões paralelas \nreduz o tempo de resposta.\nAntes de continuar, vamos fazer um rascunho de cálculo para estimar o tempo que transcorre entre a re-\nquisição e o recebimento de um arquivo-base HTTP por um cliente. Para essa finalidade, definimos o tempo \nde viagem de ida e volta (round-trip time — RTT), ou seja, o tempo que leva para um pequeno pacote viajar \ndo cliente ao servidor e de volta ao cliente. O RTT inclui atrasos de propagação de pacotes, de fila de pacotes \nem roteadores e comutadores intermediários e de processamento de pacotes. (Esses atrasos foram discutidos \nna Seção 1.4.) Considere, agora, o que acontece quando um usuário clica sobre um hiperenlace. Como ilustra-\ndo na Figura 2.7, isso faz com que o navegador inicie uma conexão TCP entre ele e o servidor, o que envolve \numa “apresentação de três vias” — o cliente envia um pequeno segmento TCP ao servidor, este o reconhece e \nresponde com um pequeno segmento ao cliente que, por fim, o reconhece novamente para o servidor. As \nduas primeiras partes da apresentação de três vias representam um RTT. Após concluí-las, o cliente envia \na mensagem de requisição HTTP combinada com a terceira parte da apresentação de três vias (o reconhe-\ncimento) por meio da conexão TCP. Assim que a mensagem de requisição chega ao servidor, este envia o \narquivo HTML por meio da conexão TCP. Essa requisição/resposta HTTP consome outro RTT. Assim, de \nmodo aproximado, o tempo total de resposta são dois RTTs mais o tempo de transmissão do arquivo HTML \nno servidor.\nO HTTP com conexões persistentes\nConexões não persistentes têm algumas desvantagens. Primeiro, uma nova conexão deve ser estabelecida e \nmantida para cada objeto solicitado. Para cada conexão, devem ser alocados buffers TCP e conservadas variáveis \nTCP tanto no cliente quanto no servidor. Isso pode sobrecarregar seriamente o servidor Web, que poderá estar pro-\ncessando requisições de centenas de diferentes clientes ao mesmo tempo. Segundo, como acabamos de descrever, \ncada objeto sofre dois RTTs — um RTT para estabelecer a conexão TCP e outro para solicitar e receber um objeto.\nTempo \nno cliente\nTempo \nno servidor\nIniciar conexão \nTCP\nRTT\nRequisitar \narquivo\nRTT\nArquivo \ncompletamente \nrecebido\nTempo para \ntransmitir arquivo\nFigura 2.7  \u0007\nCálculo simples para o tempo necessário para solicitar e receber um arquivo \nHTML\n   Redes de computadores e a Internet\n76\nEm conexões persistentes, o servidor deixa a conexão TCP aberta após enviar resposta. Requisições e \nrespostas subsequentes entre os mesmos cliente e servidor podem ser enviadas por meio da mesma conexão. \nEm particular, uma página Web inteira (no exemplo anterior, o arquivo-base HTML e as dez imagens) pode \nser enviada mediante uma única conexão TCP persistente. Além do mais, várias páginas residindo no mesmo \nservidor podem ser enviadas ao mesmo cliente por uma única conexão TCP persistentes. Essas requisições por \nobjetos podem ser feitas em sequência sem ter de esperar por respostas a requisições pendentes (paralelismo). \nEm geral, o servidor HTTP fecha uma conexão quando ela não é usada durante certo tempo (um intervalo \nde pausa configurável). Quando o servidor recebe requisições consecutivas, os objetos são enviados de forma \nininterrupta. O modo default do HTTP usa conexões persistentes com paralelismo. Faremos uma comparação \nquantitativa entre os desempenhos de conexões persistentes e não persistentes nos exercícios de fixação dos \nCapítulos 2 e 3. Aconselhamos o leitor interessado a consultar Heidemann [1997] e Nielsen [1997].\n2.2.3  Formato da mensagem HTTP\nAs especificações do HTTP [RFC 1945; RFC 2616] incluem as definições dos formatos das mensagens \nHTTP. Há dois tipos delas: de requisição e de resposta, ambas discutidas a seguir.\nMensagem de requisição HTTP\nApresentamos a seguir uma mensagem de requisição HTTP típica:\nGET /somedir/page.html HTTP/1.1\nHost: www.someschool.edu\nConnection: close\nUser-agent: Mozilla/5.0\nAccept-language: fr\nPodemos aprender bastante examinando essa simples mensagem de requisição. Primeiro, vemos que ela \nestá escrita em texto ASCII comum, de modo que pode ser lida por qualquer um que conheça computadores. \nSegundo, vemos que ela é constituída de cinco linhas, cada qual seguida de um ‘carriage return’ e ‘line feed’ (fim \nde linha). A última linha é seguida de um comando adicional de ‘carriage return’ e ‘line feed’\n. Embora essa tenha \ncinco linhas, uma mensagem de requisição pode ter muito mais e também menos do que isso, podendo conter até \nmesmo uma única linha. A primeira linha é denominada linha de requisição; as subsequentes são denominadas \nlinhas de cabeçalho. A linha de requisição tem três campos: o do método, o do URL e o da versão do HTTP. O \ncampo do método pode assumir vários valores diferentes, entre eles GET, POST, HEAD, PUT e DELETE. A grande \nmaioria das mensagens de requisição HTTP emprega o método GET, o qual é usado quando o navegador requi-\nsita um objeto e este é identificado no campo do URL. Nesse exemplo, o navegador está requisitando o objeto \n \n/somedir/page.html. A versão é autoexplicativa. Nesse exemplo, o navegador executa a versão HTTP/1.1.\nVamos agora examinar as linhas de cabeçalho do exemplo. A linha de cabeçalho Host:www.some-school.\nedu especifica o hospedeiro no qual o objeto reside. Talvez você ache que ela é desnecessária, pois já existe \numa conexão TCP para o hospedeiro. Mas, como veremos na Seção 2.2.5, a informação fornecida pela linha \nde cabeçalho do hospedeiro é exigida por caches proxy da Web. Ao incluir a linha de cabeçalho Connection: \nclose, o navegador está dizendo ao servidor que não quer usar conexões persistentes; quer que o servidor \nfeche a conexão após o envio do objeto requisitado. A linha de cabeçalho User-agent: especifica o agente de \nusuário, isto é, o tipo de navegador que está fazendo a requisição ao servidor. Neste caso, o agente de usuário é \no Mozilla/5.0, um navegador Firefox. Essa linha de cabeçalho é útil porque, na verdade, o servidor pode enviar \nversões diferentes do mesmo objeto a tipos diferentes de agentes de usuário. (Cada versão é endereçada pelo \nmesmo URL.) Por fim, o cabeçalho Accept-language: mostra que o usuário prefere receber uma versão em \nfrancês do objeto se este existir no servidor; se não existir, o servidor deve enviar a versão default. O cabeçalho \nAccept-language: é apenas um dos muitos de negociação de conteúdo disponíveis no HTTP.\nCAMADA  de APLICAÇÃO  77 \nApós examinar um exemplo, vamos agora analisar o formato geral de uma mensagem de requisição, ilus-\ntrado na Figura 2.8. Vemos que tal formato é muito parecido com nosso exemplo anterior. Contudo, você pro-\nvavelmente notou que, após as linhas de cabeçalho (e após a linha adicional com “carriage return” e “line feed”), \nhá um “corpo de entidade”\n. O corpo de entidade fica vazio com o método GET, mas é utilizado com o método \nPOST. Um cliente HTTP em geral usa o método POST quando o usuário preenche um formulário — por exemplo, \n \nquando fornece palavras de busca a um site buscador. Com uma mensagem POST, o usuário continua solicitando \numa página Web ao servidor, mas seu conteúdo depende do que ele escreveu nos campos do formulário. Se o valor \ndo campo de método for POST, então o corpo de entidade conterá o que o usuário digitou nos campos do formulário.\nSeríamos omissos se não mencionássemos que uma requisição gerada com um formulário não utiliza ne-\ncessariamente o método POST. Ao contrário, formulários HTML costumam empregar o método GET e incluem \nos dados digitados (nos campos do formulário) no URL requisitado. Por exemplo, se um formulário usar o mé-\ntodo GET, tiver dois campos e as entradas desses dois campos forem monkeys e bananas, então a estrutura do \nURL será www.somesite.com/animalsearch?monkeys&bananas. Ao navegar normalmente pela Web, \nvocê talvez já tenha notado URLs extensos como esse.\nO método HEAD é semelhante ao GET. Quando um servidor recebe uma requisição com o método HEAD, \nresponde com uma mensagem HTTP, mas deixa de fora o objeto requisitado. Esse método é usado com frequência \npelos programadores de aplicação para depuração. O método PUT é muito usado junto com ferramentas de edição \nda Web. Permite que um usuário carregue um objeto para um caminho (diretório) específico em um servidor Web \nespecífico. O método PUT também é usado por aplicações que precisam carregar objetos para servidores Web. O \nmétodo DELETE permite que um usuário, ou uma aplicação, elimine um objeto em um servidor Web.\nMensagem de resposta HTTP\nApresentamos a seguir uma mensagem de resposta HTTP típica. Essa mensagem poderia ser a resposta ao \nexemplo de mensagem de requisição que acabamos de discutir.\nHTTP/1.1 200 OK\nConnection: close\nDate: Tue, 09 Aug 2011 15:44:04 GMT\nServer: Apache/2.2.3 (CentOS)\nLast-Modified: Tue, 09 Aug 2011 15:11:03 GMT\nContent-Length: 6821\nContent-Type: text/html\n(dados dados dados dados dados ...)\nFigura 2.8  Formato geral de uma mensagem de requisição HTTP\nMétodo\nsp\nsp\ncr\nlf\ncr\nlf\nnome do campo \nde cabeçalho:\nLinhas de \ncabeçalho\nLinha em \nbranco\nCorpo da \nentidade\nLinha de \nrequisição\nvalor\nsp\ncr\nlf\ncr\nlf\nvalor\nsp\nURL\nVersão\nnome do campo \nde cabeçalho:\n   Redes de computadores e a Internet\n78\nVamos examinar com cuidado essa mensagem de resposta. Ela tem três seções: uma linha inicial ou linha \nde estado, seis linhas de cabeçalho e, em seguida, o corpo da entidade, que é a parte principal da mensagem — \ncontém o objeto solicitado (representado por dados dados dados dados dados ...). A linha de estado tem \ntrês campos: o de versão do protocolo, um código de estado e uma mensagem de estado correspondente. Neste \nexemplo, ela mostra que o servidor está usando o HTTP/1.1 e que está tudo OK (isto é, o servidor encontrou e \nestá enviando o objeto solicitado).\nAgora, vamos ver as linhas de cabeçalho. O servidor usa Connection: close para informar ao cliente \nque fechará a conexão TCP após enviar a mensagem. A linha de cabeçalho Date: indica a hora e a data em que \na resposta HTTP foi criada e enviada pelo servidor. Note que esse não é o horário em que o objeto foi criado \nnem o de sua modificação mais recente; é a hora em que o servidor extraiu o objeto de seu sistema de arquivos, \ninseriu-o na mensagem de resposta e a enviou. A linha de cabeçalho Server: mostra que a mensagem foi \ngerada por um servidor Web Apache, semelhante à linha de cabeçalho User-agent: na mensagem de requi-\nsição HTTP. A linha de cabeçalho Last-Modified: indica a hora e a data em que o objeto foi criado ou sofreu \na última modificação. Esse cabeçalho, que logo estudaremos em mais detalhes, é fundamental para fazer \ncache do objeto, tanto no cliente local quanto em servidores de cache da rede (também conhecidos como ser-\nvidores proxy). A linha de cabeçalho Content-Length: indica o número de bytes do objeto que está sendo \nenviado e a linha Content-Type: mostra que o objeto presente no corpo da mensagem é um texto HTML. \n(O tipo do objeto é oficialmente indicado pelo cabeçalho Content-Type: e não pela extensão do arquivo.)\nApós examinar um exemplo, vamos agora analisar o formato geral de uma mensagem de resposta, ilustrado \nna Figura 2.9. Esse formato geral de mensagem de resposta condiz com o exemplo anterior de uma mensagem de \nresposta. Mas falemos um pouco mais sobre códigos de estado e suas frases, que indicam o resultado da requisi-\nção. Eis alguns códigos de estado e frases associadas comuns:\n• 200 OK: requisição bem-sucedida e a informação é entregue com a resposta.\n• 301 Moved Permanently: objeto requisitado foi removido em definitivo; novo URL é especificado \nno cabeçalho Location: da mensagem de resposta. O software do cliente recuperará automaticamente o \nnovo URL.\n• 400 Bad Request: código genérico de erro que indica que a requisição não pôde ser entendida pelo \nservidor.\n• 404 Not Found: o documento requisitado não existe no servidor.\n• 505 HTTP Version Not Supported: a versão do protocolo HTTP requisitada não é suportada \npelo servidor.\nFigura 2.9  Formato geral de uma mensagem de resposta HTTP\nversão\nsp\nsp\ncr\nlf\ncr\nlf\nnome do campo \nde cabeçalho:\nLinhas de \ncabeçalho\nLinha em \nbranco\nCorpo da \nentidade\nLinha de \nestado\nvalor\ncr\nsp\nsp\nlf\ncr\nlf\nvalor\ncódigo \nde estado\nfrase\nnome do campo \nde cabeçalho:\nCAMADA  de APLICAÇÃO  79 \nVocê gostaria de ver uma mensagem de resposta HTTP real? É muito recomendável e muito fácil! Pri-\nmeiro, dê um comando Telnet em seu servidor favorito. Em seguida, digite uma mensagem de requisição de \numa linha solicitando algum objeto abrigado no servidor. Por exemplo, se você tem acesso a um prompt de \ncomando, digite:\ntelnet cis.poly.edu 80\nGET /~ross/ HTTP/1.1\nHost: cis.poly.edu\n(Pressione duas vezes a tecla “Enter” após digitar a última linha.) Essa sequência de comandos abre uma conexão \nTCP para a porta número 80 do hospedeiro cis.poly.edu e, em seguida, envia a mensagem de requisição \nHTTP. Deverá aparecer uma mensagem de resposta que inclui o arquivo-base HTML da homepage do professor \nRoss. Se você preferir apenas ver as linhas da mensagem HTTP e não receber o objeto em si, substitua GET por \nHEAD. Finalmente, substitua /~ross/ por /~banana/ e veja que tipo de mensagem você obtém.\nNesta seção, discutimos várias linhas de cabeçalho que podem ser usadas em mensagens de requisição e \nde resposta HTTP. A especificação do HTTP define muitas outras linhas de cabeçalho que podem ser inseridas \npor navegadores, servidores Web e servidores de cache da rede. Vimos apenas um pouco do total de linhas de \ncabeçalho. Examinaremos mais algumas a seguir e mais um pouco quando discutirmos armazenagem Web na \nSeção 2.2.5. Uma discussão muito abrangente e fácil de ler sobre o protocolo HTTP, seus cabeçalhos e códigos \nde estado, pode ser encontrada em Krishnamurty [2001].\nComo um navegador decide quais linhas de cabeçalho serão incluídas em uma mensagem de requisição? \nComo um servidor Web decide quais linhas de cabeçalho serão incluídas em uma mensagem de resposta? Um \nnavegador vai gerar linhas de cabeçalho em função de seu tipo e versão (por exemplo, um navegador HTTP/1.0 \nnão vai gerar nenhuma linha de cabeçalho 1.1), da configuração do usuário para o navegador (por exemplo, \nidioma preferido) e se o navegador tem uma versão do objeto possivelmente desatualizada em sua memória. \nServidores Web se comportam de maneira semelhante: há diferentes produtos, versões e configurações, e todos \ninfluenciam as linhas de cabeçalho que são incluídas nas mensagens de resposta.\n2.2.4  Interação usuário-servidor: cookies\nMencionamos anteriormente que um servidor HTTP não tem estado, o que simplifica o projeto do servi-\ndor e vem permitindo que engenheiros desenvolvam servidores Web de alto desempenho que podem manipular \nmilhares de conexões TCP simultâneas. No entanto, é sempre bom que um site identifique usuários, seja porque \no servidor deseja restringir acesso, seja porque quer apresentar conteúdo em função da identidade do usuário. \nPara essas finalidades, o HTTP usa cookies. Cookies, definidos no [RFC 6265], permitem que sites monitorem \nseus usuários. Hoje, a maioria dos sites comerciais utiliza cookies.\nComo ilustrado na Figura 2.10, a tecnologia dos cookies tem quatro componentes: (1) uma linha de cabeçalho \nde cookie na mensagem de resposta HTTP; (2) uma linha de cabeçalho de cookie na mensagem de requisição HTTP; \n(3) um arquivo de cookie mantido no sistema final do usuário e gerenciado pelo navegador do usuário; (4) um banco \nde dados de apoio no site. Utilizando a Figura 2.10, vamos esmiuçar um exemplo de como os cookies são usados. \nSuponha que Susan, que sempre acessa a Web usando o Internet Explorer de seu PC, acesse o Amazon.com pela pri-\nmeira vez, e que, no passado, ela já tenha visitado o site da eBay. Quando a requisição chega ao servidor da Amazon, \nele cria um número de identificação exclusivo e uma entrada no seu banco de dados de apoio, que é indexado pelo \nnúmero de identificação. Então, o servidor da Amazon responde ao navegador de Susan, incluindo na resposta HTTP \num cabeçalho Set-cookie: que contém o número de identificação. Por exemplo, a linha de cabeçalho poderia ser:\nSet-cookie: 1678\nQuando recebe a mensagem de resposta HTTP, o navegador de Susan vê o cabeçalho Set-cookie: e, então, \nanexa uma linha ao arquivo especial de cookies que ele gerencia. Essa linha inclui o nome de hospedeiro do \n   Redes de computadores e a Internet\n80\nservidor e seu número de identificação no cabeçalho. Observe que o arquivo de cookie já possui um entrada para \no eBay, pois Susan já visitou esse site no passado. Toda vez que ela requisita uma página enquanto navega pelo \nsite da Amazon, seu navegador consulta seu arquivo de cookies, extrai seu número de identificação para esse site \ne insere na requisição HTTP uma linha de cabeçalho de cookie que inclui o número de identificação. Especifica-\nmente, cada uma de suas requisições HTTP ao servidor da Amazon inclui a linha de cabeçalho:\nCookie: 1678\nDessa maneira, o servidor da Amazon pode monitorar a atividade de Susan em seu site e, embora não \nsaiba necessariamente que o nome dela é Susan, sabe com exatidão quais páginas o usuário 1678 visitou, em \nque ordem e em que horários! Então, pode utilizar cookies para oferecer um serviço de carrinho de compra \n— a Amazon pode manter uma lista de todas as compras de Susan, de modo que ela possa pagar por todas \nelas ao mesmo tempo, no final da sessão.\nSe Susan voltar ao site da Amazon, digamos, uma semana depois, seu navegador continuará a inserir a \nlinha de cabeçalho Cookie: 1678 nas mensagens de requisição. A Amazon pode recomendar produtos com \nbase nas páginas que Susan visitou anteriormente. Se ela também se registrar no site — fornecendo seu nome \ncompleto, endereço de e-mail, endereço postal e informações de cartão de crédito — a Amazon pode incluir \nessas informações no banco de dados e, assim, associar o nome de Susan com seu número de identificação (e \nFigura 2.10  Mantendo o estado do usuário com cookies\nHospedeiro do cliente\nHospedeiro do servidor\nmensagem normal de requisição http\nmensagem normal \nde resposta http\nSet-cookie: 1678\nmensagem normal \nde requisição http\ncookie: 1678\nmensagem normal \nde resposta http\nmensagem normal \nde requisição http\ncookie: 1678\nmensagem normal \nde resposta http\nTempo\nUma semana depois\nebay: 8734\nO servidor cria \nID 1678 para \no usuário\nTempo\nArquivo de cookies\nLegenda:\namazon: 1678\nebay: 8734\namazon: 1678\nebay: 8734\nAção especíﬁca \ndo cookie\nacesso\nacesso\nEntrada no banco \nde dados de apoio\nAção especíﬁca \ndo cookie\nCAMADA  de APLICAÇÃO  81 \ncom todas as páginas que ela consultou em suas visitas anteriores). É assim que a Amazon e outros sites de \ncomércio eletrônico oferecem “compras com um só clique” — quando quiser comprar um item em uma visita \nposterior, Susan não precisará digitar de novo seu nome, número de cartão de crédito, nem endereço.\nEssa discussão nos mostrou que cookies podem ser usados para identificar um usuário. Quando visitar um \nsite pela primeira vez, um usuário pode fornecer dados de identificação (possivelmente seu nome). No decorrer \ndas próximas sessões, o navegador passa um cabeçalho de cookie ao servidor durante todas as visitas subsequen-\ntes ao site, identificando, desse modo, o usuário ao servidor. Assim, vemos que os cookies podem ser usados para \ncriar uma camada de sessão de usuário sobre HTTP sem estado. Por exemplo, quando um usuário acessa uma \naplicação de e-mail baseada na Web (como o Hotmail), o navegador envia informações de cookie ao servidor, \npermitindo que o servidor identifique o usuário por meio da sessão deste com a aplicação.\nEmbora cookies quase sempre simplifiquem a experiência de compra pela Internet, continuam provo-\ncando muita controvérsia porque também podem ser considerados invasão da privacidade do usuário. Como \nacabamos de ver, usando uma combinação de cookies e informações de conta fornecidas pelo usuário, um site \npode ficar sabendo muita coisa sobre esse usuário e, potencialmente, vender o que sabe para algum terceiro. O \nCookie Central [Cookie Central, 2012] inclui muitas informações sobre a controvérsia dos cookies.\n2.2.5  Caches Web\nUm cache Web — também denominado servidor proxy — é uma entidade da rede que atende re-\nquisições HTTP em nome de um servidor Web de origem. O cache Web tem seu próprio disco de arma-\nzenagem e mantém, dentro dele, cópias de objetos recentemente requisitados. Como ilustrado na Figu-\nra 2.11, o navegador de um usuário pode ser configurado de modo que todas as suas requisições HTTP \nsejam dirigidas primeiro ao cache Web. Uma vez que esteja configurado um navegador, cada uma das \nrequisições de um objeto que o navegador faz é primeiro dirigida ao cache Web. Como exemplo, supo-\nnha que um navegador esteja requisitando o objeto http://www.someschool.edu/campus.gif. \n \nEis o que acontece:\n1.\t O navegador estabelece uma conexão TCP com o cache Web e envia a ele uma requisição HTTP para o \nobjeto.\n2.\t O cache Web verifica se tem uma cópia do objeto armazenada localmente. Se tiver, envia o objeto ao na-\nvegador do cliente, dentro de uma mensagem de resposta HTTP.\n3.\t Se não tiver o objeto, o cache Web abre uma conexão TCP com o servidor de origem, isto é, com www.\nsomeschool.edu. Então, envia uma requisição HTTP do objeto para a conexão TCP. Após recebê-la, \no servidor de origem envia o objeto ao cache Web, dentro de uma resposta HTTP.\nFigura 2.11  Clientes requisitando objetos por meio de um cache Web\nrequisição HTTP\nresposta HTTP\nrequisição HTTP\nresposta HTTP\nrequisição HTTP\nresposta HTTP\nrequisição HTTP\nresposta HTTP\nCliente\nServidor \nde origem\nServidor \nde origem\nCliente\nServidor \nproxy\n   Redes de computadores e a Internet\n82\n4.\t Quando recebe o objeto, o cache Web guarda uma cópia em seu armazenamento local e envia outra, den-\ntro de uma mensagem de resposta HTTP, ao navegador do cliente (pela conexão TCP existente entre o \nnavegador do cliente e o cache Web).\nNote que um cache é, ao mesmo tempo, um servidor e um cliente. Quando recebe requisições de um na-\nvegador e lhe envia respostas, é um servidor. Quando envia requisições para um servidor de origem e recebe \nrespostas dele, é um cliente.\nEm geral, é um ISP que compra e instala um cache Web. Por exemplo, uma universidade poderia instalar \num cache na rede de seu campus e configurar todos os navegadores apontando para esse cache. Ou um importante \nISP residencial (como a AOL) poderia instalar um ou mais caches em sua rede e configurar antecipadamente os \nnavegadores que fornece apontando para os caches instalados.\nO cache na Web tem tido ampla utilização na Internet por duas razões. Primeiro, pode reduzir substancial-\nmente o tempo de resposta para a requisição de um cliente, em particular se o gargalo da largura de banda entre \no cliente e o servidor de origem for muito menor do que aquele entre o cliente e o cache. Se houver uma conexão \nde alta velocidade entre o cliente e o cache, como em geral é o caso, e se este tiver o objeto requisitado, então ele \npoderá entregar com rapidez o objeto ao cliente. Segundo, como logo ilustraremos com um exemplo, caches Web \npodem reduzir de modo substancial o tráfego no enlace de acesso de uma instituição qualquer à Internet. Com \na redução do tráfego, a instituição (por exemplo, uma empresa ou uma universidade) não precisa ampliar sua \nlargura de banda tão rapidamente, o que diminui os custos. Além disso, caches Web podem reduzir bastante o \ntráfego na Internet como um todo, melhorando, assim, o desempenho para todas as aplicações.\nPara entender melhor os benefícios dos caches, vamos considerar um exemplo no contexto da Figura 2.12. \nEssa figura mostra duas redes: uma rede institucional e a Internet pública. A rede institucional é uma LAN de \nalta velocidade. Um roteador da rede institucional e um roteador da Internet estão ligados por um enlace \nde 15 Mbits/s. Os servidores de origem estão todos ligados à Internet, mas localizados pelo mundo todo. Supo-\nnha que o tamanho médio do objeto seja 1 Mbit/s e que a taxa média de requisição dos navegadores da instituição \naté os servidores de origem seja de 15 requisições por segundo. Imagine também que o tamanho das mensagens \nde requisição HTTP seja insignificante e, portanto, elas não criem tráfego nas redes ou no enlace de acesso (do \nroteador da instituição até o da Internet). Suponha ainda que o tempo entre o envio de uma requisição HTTP \n(dentro de um datagrama IP) pelo roteador do lado da Internet do enlace de acesso mostrado na Figura 2.12 e o \nrecebimento da resposta (em geral, dentro de muitos datagramas IPs) seja de 2 s em média. Esse último atraso é \ndenominado informalmente “atraso da Internet”\n.\nO tempo de resposta total — isto é, aquele transcorrido entre a requisição de um objeto feita pelo nave-\ngador e o recebimento dele — é a soma do atraso da LAN, do atraso de acesso (isto é, o atraso entre os dois ro-\nteadores) e do atraso da Internet. Vamos fazer agora um cálculo bastante rudimentar para estimar esse atraso. \nA intensidade de tráfego na LAN (veja a Seção 1.4.2) é\n\t\n\t\n(15 requisições/s) ∙ (1 Mbit/s/requisição)/(100 Mbits/s) = 0,15\nao passo que a intensidade de tráfego no enlace de acesso (do roteador da Internet ao da instituição) é\n\t\n\t\n(15 requisições/s) = (1 Mbit/s/requisição)/(15 Mbits/s) = 1\nUma intensidade de tráfego de 0,15 em uma LAN resulta em, no máximo, dezenas de milissegundos de atraso; \npor conseguinte, podemos desprezar o atraso da LAN. Contudo, como discutimos na Seção 1.4.2, à medida que a \nintensidade de tráfego se aproxima de 1 (como é o caso do enlace de acesso da Figura 2.12), o atraso em um enlace se \ntorna muito grande e cresce sem limites. Assim, o tempo médio de resposta para atender requisições será da ordem \nde minutos, se não for maior, o que é inaceitável para os usuários da instituição. Claro, algo precisa ser feito.\nUma possível solução seria aumentar a velocidade de acesso de 15 Mbits/s para, digamos, 100 Mbits/s. \nIsso reduziria a intensidade de tráfego no enlace de acesso a 0,15, o que se traduziria em atrasos desprezíveis \nentre os dois roteadores. Nesse caso, o tempo total de resposta seria de mais ou menos 2 s, isto é, o atraso da \nInternet. Mas essa solução também significa que a instituição tem de atualizar seu enlace de acesso de 15 Mbits/s \npara 100 Mbits/s, o que pode ser muito dispendioso.\nCAMADA  de APLICAÇÃO  83 \nConsidere agora a solução alternativa de não atualizar o enlace de acesso, mas, em vez disso, instalar um \ncache Web na rede institucional. Essa solução é ilustrada na Figura 2.13. A taxa de resposta local — a fração de \nrequisições atendidas por um cache — em geral fica na faixa de 0,2 a 0,7 na prática. Para fins de ilustração, vamos \nsupor que a taxa de resposta local do cache dessa instituição seja 0,4. Como os clientes e o cache estão conectados \nà mesma LAN de alta velocidade, 40% das requisições serão atendidas quase de imediato pelo cache, digamos, \nem 10 ms. Mesmo assim, os demais 60% das requisições ainda precisam ser atendidos pelos servidores de origem. \nMas, com apenas 60% dos objetos requisitados passando pelo enlace de acesso, a intensidade de tráfego neste di-\nminui de 1,0 para 0,6. Em geral, uma intensidade de tráfego menor do que 0,8 corresponde a um atraso pequeno, \ndigamos, dezenas de milissegundos, no caso de um enlace de 15 Mbits/s. Esse atraso é desprezível se comparado \naos 2 s do atraso da Internet. Dadas essas considerações, o atraso médio é, portanto,\n0,4 ∙ (0,01 s) + 0,6 ∙ (2,01 s)\nque é ligeiramente maior do que 1,2 s. Assim, essa segunda solução resulta em tempo de resposta até menor do \nque o da primeira e não requer que a instituição atualize seu enlace com a Internet. Evidentemente, a instituição \nterá de comprar e instalar um cache Web. Mas esse custo é baixo — muitos caches usam softwares de domínio \npúblico que rodam em PCs baratos.\nCom o uso de redes de distribuição de conteúdo (CDNs), caches Web estão cada vez mais desempenhando \num papel importante na Internet. Uma empresa de CDN instala muitos caches geograficamente dispersos pela \nInternet, localizando assim grande parte do tráfego. Existem CDNs compartilhadas (como Akamai e Limelight) \ne CDNs dedicadas (como Google e Microsoft). Discutiremos as CDNs mais em detalhes no Capítulo 7.\n2.2.6  GET condicional\nEmbora possa reduzir os tempos de resposta do ponto de vista do usuário, fazer cache introduz um novo \nproblema — a cópia de um objeto existente no cache pode estar desatualizada. Em outras palavras, o objeto \nFigura 2.12  Gargalo entre uma rede institucional e a Internet\nInternet pública\nRede institucional\nEnlace de acesso de 1,5 Mbits/s\nLAN de 10 Mbits/s\nServidor de origem\n   Redes de computadores e a Internet\n84\nabrigado no servidor pode ter sido modificado desde a data em que a cópia entrou no cache no cliente. Fe-\nlizmente, o HTTP tem um mecanismo que permite que um cache verifique se seus objetos estão atualizados. \nEsse mecanismo é denominado GET condicional (conditional GET). Uma mensagem de requisição HTTP é \ndenominada uma mensagem GET condicional se (1) usar o método GET e (2) possuir uma linha de cabeçalho \nIf-Modified-Since:.\nPara ilustrar como o GET condicional funciona, vamos examinar um exemplo. Primeiro, um cache proxy \nenvia uma mensagem de requisição a um servidor em nome de um navegador requisitante:\nGET /fruit/kiwi.gif HTTP/1.1\nHost: www.exotiquecuisine.com\nSegundo, o servidor Web envia ao cache uma mensagem de resposta com o objeto requisitado:\nHTTP/1.1 200 OK\nDate: Sat, 8 Oct 2011 15:39:29\nServer: Apache/1.3.0 (Unix)\nLast-Modified: Wed, 7 Sep 2011 09:23:24\nContent-Type: image/gif\n(dados dados dados dados dados ...)\nO cache encaminha o objeto ao navegador requisitante, mas também o guarda em sua memória cache local. \nO importante é que ele também guarda, junto com o objeto, a data da última modificação. Terceiro, uma semana \ndepois, outro navegador requisita ao cache o mesmo objeto, que ainda está ali. Como esse objeto pode ter sido \nmodificado no servidor na semana anterior, o navegador realiza uma verificação de atualização emitindo um \nGET condicional. Especificamente, o cache envia:\nFigura 2.13  Acrescentando um cache à rede institucional\nKR 02.13.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n18p8 Wide x 25p Deep\n9/13/11, 10/28/11, 11/9/11 rossi\nInternet pública\nRede institucional\nEnlace de acesso de 1,5 Mbits/s\nCache\ninstitucional\nLAN de 10 Mbits/s\nServidor de origem\nCAMADA  de APLICAÇÃO  85 \nGET /fruit/kiwi.gif HTTP/1.1\nHost: www.exotiquecuisine.com\nIf-modified-since: Wed, 7 Sep 2011 09:23:24\nNote que o valor da linha de cabeçalho If-modified-since: é idêntico ao da linha de cabeçalho \nLast-Modified: que foi enviada pelo servidor há uma semana. Esse GET condicional está dizendo ao \nservidor para enviar o objeto somente se ele tiver sido modificado desde a data especificada. Suponha que \no objeto não tenha sofrido modificação desde 7 set. 2011 09:23:24. Então, em quarto lugar, o servidor Web \nenvia uma mensagem de resposta ao cache:\nHTTP/1.1 304 Not Modified\nDate: Sat, 15 Oct 2011 15:39:29\nServer: Apache/1.3.0 (Unix)\n(corpo de mensagem vazio)\nVemos que, em resposta ao GET condicional, o servidor ainda envia uma mensagem de resposta, mas não \ninclui nela o objeto requisitado, o que apenas desperdiçaria largura de banda e aumentaria o tempo de resposta \ndo ponto de vista do usuário, em particular se o objeto fosse grande. Note que, na linha de estado dessa última \nmensagem de resposta está inserido 304 Not Modified, que informa ao cache que ele pode seguir e transmitir \nao navegador requisitante a cópia do objeto que está contida nele (no cache proxy).\nAssim, finalizamos nossa discussão sobre HTTP, o primeiro protocolo da Internet (um protocolo da ca-\nmada de aplicação) que estudamos em detalhes. Vimos o formato das mensagens HTTP e as ações tomadas \npelo cliente e servidor Web quando essas mensagens são enviadas e recebidas. Também estudamos um pouco \nda infraestrutura da aplicação da Web, incluindo caches, cookies e banco de dados de apoio, todos associados, de \nalgum modo, ao protocolo HTTP.\n2.3  Transferência de arquivo: FTP\nEm uma sessão FTP típica, o usuário, sentado à frente de um hospedeiro (o local), quer transferir arquivos de \nou para um hospedeiro remoto. Para acessar a conta remota, o usuário deve fornecer uma identificação e uma senha. \nApós fornecer essas informações de autorização, pode transferir arquivos do sistema local de arquivos para o sistema \nremoto e vice-versa. Como mostra a Figura 2.14, o usuário interage com o FTP por meio de um agente de usuário \nFTP\n. Primeiro, ele fornece o nome do hospedeiro remoto, o que faz com que o processo cliente FTP do hospedeiro \nlocal estabeleça uma conexão TCP com o processo servidor FTP do hospedeiro remoto. O usuário então fornece sua \nidentificação e senha, que são enviadas pela conexão TCP como parte dos comandos FTP\n. Assim que autorizado pelo \nservidor, o usuário copia um ou mais arquivos armazenados no sistema de arquivo local para o sistema de arquivo \nremoto (ou vice-versa).\nHTTP e FTP são protocolos de transferência de arquivos e têm muitas características em comum; por \nexemplo, ambos utilizam o TCP. Contudo, esses dois protocolos de camada de aplicação têm algumas diferen-\nças importantes. A mais notável é que o FTP usa duas conexões TCP paralelas para transferir um arquivo: uma \nconexão de controle e uma conexão de dados. A primeira é usada para enviar informações de controle entre os \ndois hospedeiros — como identificação de usuário, senha, comandos para trocar diretório remoto e comandos de \n“enviar” (put) e “receber” (get) arquivos. A conexão de dados é a usada para enviar de fato um arquivo. Como o \nFTP usa uma conexão de controle separada, dizemos que ele envia suas informações de controle fora da banda. \nO HTTP, como você deve recordar, envia linhas de cabeçalho de requisição e de resposta pela mesma conexão \nTCP que carrega o próprio arquivo transferido. Por essa razão, dizemos que o HTTP envia suas informações de \ncontrole na banda. Na próxima seção, veremos que o SMTP, o principal protocolo para correio eletrônico, tam-\nbém envia suas informações de controle na banda. As conexões de controle e de dados do FTP estão ilustradas \nna Figura 2.15.\n   Redes de computadores e a Internet\n86\nFigura 2.14  FTP transporta arquivos entre sistemas de arquivo local e remoto\nInterface \nFTP do \nusuário\nSistema de \narquivo local\nUsuário ou \nhospedeiro\nSistema de \narquivo remoto\nCliente FTP\nServidor \nFTP\nTransferência de arquivo\nQuando um usuário inicia uma sessão FTP com um hospedeiro remoto, o lado cliente do FTP (usuário) \ninicia primeiro uma conexão TCP de controle com o lado servidor (hospedeiro remoto) na porta número 21 do \nservidor e envia por essa conexão de controle a identificação e a senha do usuário, além de comandos para mudar \no diretório remoto. Quando o lado servidor recebe, pela conexão de controle, um comando para uma transfe-\nrência de arquivo (de ou para o hospedeiro remoto), abre uma conexão TCP de dados para o lado cliente. O FTP \nenvia exatamente um arquivo pela conexão de dados e em seguida fecha-a. Se, durante a mesma sessão, o usuário \nquiser transferir outro arquivo, o FTP abrirá outra conexão de dados. Assim, com FTP, a conexão de controle \npermanece aberta durante toda a sessão do usuário, mas uma nova conexão de dados é criada para cada arquivo \ntransferido dentro de uma sessão (ou seja, a conexão de dados é não persistente).\nDurante uma sessão, o servidor FTP deve manter informações de estado sobre o usuário. Em particular, o \nservidor deve associar a conexão de controle com uma conta de usuário específica e também deve monitorar o \ndiretório corrente do usuário enquanto este passeia pela árvore do diretório remoto. Monitorar essas informações \nde estado para cada sessão de usuário em curso limita de modo significativo o número total de sessões que o FTP \npode manter simultaneamente. Lembre-se de que o HTTP, por outro lado, é sem estado — não tem de monitorar \no estado de nenhum usuário.\n2.3.1  Camadas e respostas FTP\nEncerraremos esta seção com uma breve discussão sobre alguns dos comandos mais comuns do FTP e \nsuas respostas. Os comandos, do cliente para o servidor, e as respostas, do servidor para o cliente, são enviados \npor meio da conexão de controle no formato ASCII de 7 bits. Assim, tal como comandos HTTP, comandos FTP \ntambém podem ser lidos pelas pessoas. Para separar comandos sucessivos, um “carriage return” e um “line feed” \nencerram cada um. Um comando é constituído de quatro caracteres ASCII maiúsculos, alguns com argumentos \nopcionais. Alguns dos comandos mais comuns são descritos a seguir:\n• USER username: usado para enviar identificação do usuário ao servidor.\n• PASS password: usado para enviar a senha do usuário ao servidor.\nFigura 2.15  Conexões de controle e de dados\nConexão de controle TCP porta 21\nConexão de dados TCP porta 20\nCliente \nFTP\nServidor \nFTP\nCAMADA  de APLICAÇÃO  87 \n• LIST: usado para pedir ao servidor que envie uma lista com todos os arquivos existentes no atual dire-\ntório remoto. A lista de arquivos é enviada por meio de uma conexão de dados (nova e não persistente), \ne não pela conexão TCP de controle.\n• RETR filename: usado para extrair (isto é, obter) um arquivo do diretório atual do hospedeiro remo-\nto. Ativa o hospedeiro remoto para que abra uma conexão de dados e envia o arquivo requisitado por \nessa conexão.\n• STOR filename: usado para armazenar (isto é, inserir) um arquivo no diretório atual do hospedeiro \nremoto.\nHá, em particular, uma correspondência direta entre o comando que o usuário gera e o comando FTP en-\nviado pela conexão de controle. Cada comando é seguido de uma resposta, que é enviada do servidor ao cliente. \nAs respostas são números de três dígitos com uma mensagem opcional após o número. Elas se assemelham, em \nestrutura, ao código de estado e à frase da linha de estado da mensagem de resposta HTTP. Algumas respostas \ntípicas, junto com suas possíveis mensagens, são as seguintes:\n• 331 Nome de usuário OK, senha requisitada\n• 125 Conexão de dados já aberta; iniciando transferência\n• 425 Não é possível abrir a conexão de dados\n• 452 Erro ao escrever o arquivo\nPara saber mais sobre outros comandos e respostas FTP, o leitor interessado pode consultar o RFC 959.\n2.4  Correio eletrônico na Internet\nO correio eletrônico existe desde o início da Internet. Era uma das aplicações mais populares quando ela \nainda estava na infância [Segaller, 1998], e ficou mais e mais elaborado e poderoso ao longo dos anos. É uma das \naplicações mais importantes e de maior uso na Internet.\nTal como o correio normal, o e-mail é um meio de comunicação assíncrono — as pessoas enviam e rece-\nbem mensagens quando for conveniente para elas, sem ter de estar coordenadas com o horário das outras. Ao \ncontrário do correio normal, que anda a passos lentos, o eletrônico é rápido, fácil de distribuir e barato. O correio \neletrônico moderno tem muitas características poderosas, incluindo mensagens com anexos, hiperlinks, textos \nformatados em HTML e fotos embutidas.\nNesta seção, examinaremos os protocolos de camada de aplicação que estão no cerne do correio eletrônico \nda Internet. Mas, antes de entrarmos nessa discussão, vamos tomar uma visão geral do sistema de correio da \nInternet e de seus componentes principais.\nA Figura 2.16 apresenta uma visão do sistema de correio da Internet. Vemos, por esse diagrama, que há três \ncomponentes principais: agentes de usuário, servidores de correio e o SMTP (Simple Mail Transfer Protocol). \nDescreveremos agora cada um deles no contexto de um remetente, Alice, enviando uma mensagem de e-mail \npara um destinatário, Bob. Agentes de usuários permitem que usuários leiam, respondam, encaminhem, salvem \ne componham mensagens. Microsoft Outlook e Apple Mail são alguns desses agentes. Quando Alice termina de \ncompor sua mensagem, seu agente de usuário envia a mensagem para seu servidor de correio, onde a mensagem \né colocada em sua fila de mensagens de saída. Quando Bob quer ler uma mensagem, seu agente de usuário apa-\nnha a mensagem de sua caixa de correio, em seu servidor de correio.\nServidores de correio formam o núcleo da infraestrutura do e-mail. Cada destinatário, como Bob, tem \numa caixa postal localizada em um desses servidores. A de Bob administra e guarda as mensagens que foram \nenviadas a ele. Uma mensagem típica inicia sua jornada no agente de usuário do remetente, vai até seu ser-\nvidor de correio e viaja até o do destinatário, onde é depositada na caixa postal. Quando Bob quer acessar as \nmensagens de sua caixa postal, o servidor de correio que contém sua caixa postal o autentica (com nome de \nusuário e senha). O servidor de correio de Alice também deve cuidar das falhas no servidor de correio de Bob. \nSe o servidor de correio dela não puder entregar a correspondência ao dele, manterá a mensagem em uma fila \n   Redes de computadores e a Internet\n88\nde mensagens e tentará transferi-la mais tarde. Em geral, novas tentativas serão feitas a cada 30 minutos mais \nou menos; se não obtiver sucesso após alguns dias, o servidor removerá a mensagem e notificará o remetente \n(Alice) por meio de uma mensagem de correio.\nO SMTP é o principal protocolo de camada de aplicação do correio eletrônico da Internet. Usa o serviço \nconfiável de transferência de dados do TCP para transferir mensagens do servidor de correio do remetente para \no do destinatário. Como acontece com a maioria dos protocolos de camada de aplicação, o SMTP tem dois lados: \num lado cliente, que funciona no servidor de correio do remetente, e um lado servidor, que funciona no servidor \nde correio do destinatário. Ambos funcionam em todos os servidores de correio. Quando um servidor de correio \nenvia correspondência para outros, age como um cliente SMTP. Quando o servidor de correio recebe correspon-\ndência de outros, age como um servidor SMTP.\n2.4.1  SMTP\nO SMTP, definido no RFC 5321, está no cerne do correio eletrônico da Internet. Como já dissemos, esse \nprotocolo transfere mensagens de servidores de correio remetentes para servidores de correio destinatários. O \nSMTP é muito mais antigo que o HTTP. (O RFC original do SMTP data de 1982, e ele já existia muito antes \ndisso.) Embora tenha inúmeras qualidades maravilhosas, como evidencia sua ubiquidade na Internet, o SMTP é \numa tecnologia antiga que possui certas características arcaicas. Por exemplo, restringe o corpo (e não apenas o \ncabeçalho) de todas as mensagens de correio ao simples formato ASCII de 7 bits. Essa restrição tinha sentido no \ncomeço da década de 1980, quando a capacidade de transmissão era escassa e ninguém enviava correio eletrônico \ncom anexos volumosos nem arquivos grandes com imagens, áudio ou vídeo. Mas, hoje, na era da multimídia, a \nFigura 2.16  Uma visão do sistema de e-mail da Internet\nFila de \nmensagem de saída\nLegenda:\nCaixa de entrada do usuário\nSMTP\nAgente de \nusuário\nAgente de \nusuário\nServidor de \ncorreio\nServidor de \ncorreio\nSMTP\nSMTP\nAgente de \nusuário\nAgente de \nusuário\nAgente de \nusuário\nAgente de \nusuário\nServidor de \ncorreio\nCAMADA  de APLICAÇÃO  89 \nrestrição do ASCII de 7 bits é um tanto incômoda — exige que os dados binários de multimídia sejam codifica-\ndos em ASCII antes de ser enviados pelo SMTP e que a mensagem correspondente em ASCII seja decodificada \nnovamente para o sistema binário depois do transporte pelo SMTP. Lembre-se da Seção 2.2, na qual dissemos que \no HTTP não exige que os dados de multimídia sejam codificados em ASCII antes da transferência.\nPara ilustrar essa operação básica do SMTP, vamos percorrer um cenário comum. Suponha que Alice queira \nenviar a Bob uma simples mensagem ASCII.\n1.\t Alice chama seu agente de usuário para e-mail, fornece o endereço de Bob (por exemplo, bob@somes-\nchool.edu), compõe uma mensagem e instrui o agente de usuário a enviá-la.\n2.\t O agente de usuário de Alice envia a mensagem para seu servidor de correio, onde ela é colocada em uma \nfila de mensagens.\n3.\t O lado cliente do SMTP, que funciona no servidor de correio de Alice, vê a mensagem na fila e abre uma \nconexão TCP para um servidor SMTP, que funciona no servidor de correio de Bob.\n4.\t Após alguns procedimentos iniciais de apresentação (handshaking), o cliente SMTP envia a mensagem de \nAlice pela conexão TCP.\n5.\t No servidor de correio de Bob, o lado servidor do SMTP recebe a mensagem e a coloca na caixa postal \ndele.\n6.\t Bob chama seu agente de usuário para ler a mensagem quando for mais conveniente para ele.\nEsse cenário está resumido na Figura 2.17.\nÉ importante observar que o SMTP em geral não usa servidores de correio intermediários para enviar \ncorrespondência, mesmo quando os dois servidores estão localizados em lados opostos do mundo. Se o servidor \nde Alice está em Hong Kong, e o de Bob, em St. Louis, a conexão TCP é uma conexão direta entre os servidores \nem Hong Kong e St. Louis. Em particular, se o servidor de correio de Bob não estiver em funcionamento, a men-\nsagem permanece no de Alice esperando por uma nova tentativa — a mensagem não é colocada em nenhum \nservidor de correio intermediário.\nE-mail da Web\nEm dezembro de 1995, apenas alguns anos de-\npois da “invenção” da Web, Sabeer Bhatia e Jack \nSmith fizeram uma visita a Draper Fisher Jurvetson, \num investidor em empreendimentos de Internet, e \npropuseram desenvolver um sistema de e-mail de \nlivre acesso baseado na Web. A ideia era oferecer \numa conta de e-mail grátis a quem quisesse e tornar \ntais contas acessíveis pela Web. Em troca de 15% \nda empresa, Draper Fisher Jurvetson financiou Bha-\ntia e Smith, que formaram uma empresa denominada \nHotmail. Com três funcionários em tempo integral e \nmais 14 em tempo parcial, que trabalhavam em tro-\nca de opções de compra de ações da empresa, eles \nconseguiram desenvolver e lançar o serviço em julho \nde 1996. Um mês após o lançamento, a Hotmail tinha \ncem mil assinantes. Em dezembro de 1997, menos \nde 18 meses depois, a Hotmail, já com mais de 12 \nmilhões de assinantes, foi adquirida pela Microsoft, \nao que se saiba, por 400 milhões de dólares. O su-\ncesso da Hotmail é muitas vezes atribuído à vanta-\ngem de ela ter sido a primeira a entrar no mercado e \nao inerente “marketing viral” do e-mail. (Talvez alguns \ndos estudantes que leem este livro estarão entre os \nnovos empreendedores que conceberão e desenvol-\nverão serviços de Internet pioneiros no mercado e \ncom marketing viral.)\nO e-mail da Web continua a prosperar, tornando­\n‑se, a cada ano, mais sofisticado e potente. Um dos \nserviços mais populares de hoje é o gmail do Google, \nque oferece livre armazenagem de gigabytes, filtro de \n \nspam e detector de vírus avançados, codificação \n \nde e-mail opcional (utilizando SSL), busca de correio de \nserviços de terceiros e uma interface orientada a busca. \nServiços de mensagens assíncronas dentro de redes \nsociais, como Facebook, também se tornaram popu-\nlares nos últimos anos.\nHistória\n   Redes de computadores e a Internet\n90\nVamos agora examinar mais de perto como o SMTP transfere uma mensagem de um servidor de correio re-\nmetente para um servidor de correio destinatário. Veremos que o protocolo SMTP tem muitas semelhanças com \nprotocolos usados na interação humana cara a cara. Primeiro, o cliente SMTP (que funciona no hospedeiro do \nservidor de correio remetente) faz o TCP estabelecer uma conexão na porta 25 com o servidor SMTP (que fun-\nciona no hospedeiro do servidor de correio destinatário). Se o servidor não estiver em funcionamento, o cliente \ntenta de novo mais tarde. Uma vez estabelecida a conexão, o servidor e o cliente trocam alguns procedimentos de \napresentação de camada de aplicação — exatamente como os seres humanos, que costumam se apresentar antes \nde transferir informações, clientes e servidores SMTP também se apresentam antes de transferir informações. \nDurante essa fase, o cliente SMTP indica os endereços de e-mail do remetente (a pessoa que gerou a mensagem) \ne do destinatário. Assim que o cliente e o servidor SMTP terminam de se apresentar, o cliente envia a mensagem. \nO SMTP pode contar com o serviço confiável de transferência de dados do TCP para entregar a mensagem ao \nservidor sem erros. Então, o cliente repetirá esse processo, na mesma conexão TCP, se houver outras mensagens \na enviar ao servidor; caso contrário, dará uma instrução ao TCP para encerrar a conexão.\nVamos analisar um exemplo de troca de mensagens entre um cliente (C) e um servidor SMTP (S). O \nnome do hospedeiro do cliente é crepes.fr e o nome do hospedeiro do servidor é hamburger.edu. \nAs linhas de texto ASCII iniciadas com C: são exatamente as linhas que o cliente envia para dentro de seu \nsocket TCP e as iniciadas com S: são exatamente as linhas que o servidor envia para dentro de seu socket TCP. A \ntranscrição a seguir começa assim que a conexão TCP é estabelecida:\nS: 220 hamburger.edu\nC: HELO crepes.fr\nS: 250 Hello crepes.fr, pleased to meet you\nC: MAIL FROM: <alice@crepes.fr>\nS: 250 alice@crepes.fr ... Sender ok\nC: RCPT TO: <bob@hamburger.edu>\nS: 250 bob@hamburger.edu ... Recipient ok\nC: DATA\nS: 354 Enter mail, end with “.” on a line by itself\nC: Do you like ketchup?\nC: How about pickles?\nC: .\nS: 250 Message accepted for delivery\nC: QUIT\nS: 221 hamburger.edu closing connection\nNeste exemplo, o cliente enviou uma mensagem (“Do you like ketchup? How about pickles?”) \ndo servidor de correio crepes.fr ao servidor de correio hamburger.edu. Como parte do diálogo, o cliente \nemitiu cinco comandos: HELO (uma abreviação de HELLO), MAIL FROM, RCPT TO, DATA e QUIT. Esses \ncomandos são autoexplicativos. O cliente também enviou uma linha consistindo em um único ponto final, que \nindica o final da mensagem para o servidor. (No jargão ASCII, cada mensagem termina com CRLF.CRLF, em \nFigura 2.17  Alice envia uma mensagem a Bob\nSMTP\nServidor de \ncorreio de Alice\nServidor de \ncorreio de Bob\nAgente de \nAlice\nAgente de \nBob\n1\n2\n4\n6\n5\nFila de mensagem\nLegenda:\nCaixa postal do usuário\n3\nCAMADA  de APLICAÇÃO  91 \nque CR significa ‘carriage return’ e LF significa ‘line feed’.) O servidor emite respostas a cada comando, e cada \nresposta tem uma codificação de resposta e algumas explicações (opcionais) em inglês. Mencionamos aqui que \no SMTP usa conexões persistentes: se o servidor de correio remetente tiver diversas mensagens para enviar ao \nmesmo servidor de correio destinatário, poderá enviar todas pela mesma conexão TCP. Para cada mensagem, \no cliente inicia o processo com um novo MAIL FROM: crepes.fr, indica o final da mensagem com um \nponto final isolado e emite QUIT somente após todas as mensagens terem sido enviadas.\nRecomendamos vivamente que você utilize o Telnet para executar um diálogo direto com um servidor \nSMTP. Para fazer isso digite\ntelnet serverName 25\nem que serverName é o nome de um servidor de correio local. Ao fazer isso, você está apenas estabelecendo uma \nconexão TCP entre seu hospedeiro local e o servidor de correio. Após digitar essa linha, você deverá receber ime-\ndiatamente do servidor a resposta 220. Digite, então, os comandos HELO, MAIL FROM, RCPT TO, DATA, CRLF.\nCRLF e QUIT nos momentos apropriados. Também recomendamos que você faça a Tarefa de Programação 2 no \nfinal deste capítulo. Nela, você construirá um agente de usuário simples que executa o lado cliente do SMTP. Esse \nagente permitirá que você envie uma mensagem de e-mail a um destinatário qualquer, por meio de um servidor \nde correio local.\n2.4.2  Comparação com o HTTP\nAgora, vamos fazer uma breve comparação entre o SMTP e o HTTP. Ambos os protocolos são usados para \ntransferir arquivos de um hospedeiro para outro. O HTTP transfere arquivos (também denominados objetos) \nde um servidor para um cliente Web (em geral um navegador). O SMTP transfere arquivos (isto é, mensagens \nde e-mail) de um servidor de correio para outro. Ao transferir os arquivos, o HTTP persistente e o SMTP usam \nconexões persistentes. Assim, os dois protocolos têm características em comum. Existem, todavia, diferenças \nimportantes. A primeira é que o HTTP é, principalmente, um protocolo de recuperação de informações (pull \nprotocol) — alguém carrega informações em um servidor Web e os usuários utilizam o HTTP para recuperá-las \nquando quiserem. Em particular, a conexão TCP é ativada pela máquina que quer receber o arquivo. O SMTP, \npor sua vez, é, primordialmente, um protocolo de envio de informações (push protocol) — o servidor de correio \nremetente envia o arquivo para o servidor de correio destinatário. Em particular, a conexão TCP é ativada pela \nmáquina que quer enviar o arquivo.\nA segunda diferença, à qual aludimos anteriormente, é que o SMTP exige que cada mensagem, inclusive o \ncorpo, esteja no formato ASCII de 7 bits. Se ela contiver caracteres que não estejam nesse formato (por exemplo, \ncaracteres em francês, com acento) ou dados binários (como um arquivo de imagem), terá de ser codificada em \nASCII de 7 bits. Dados HTTP não impõem esta restrição.\nA terceira diferença importante refere-se ao modo como um documento que contém texto e imagem (jun-\ntamente com outros tipos possíveis de mídia) é manipulado. Como vimos na Seção 2.2, o HTTP encapsula cada \nobjeto em sua própria mensagem HTTP. O correio pela Internet, como discutiremos com maiores detalhes mais \nadiante, coloca todos os objetos de mensagem em uma única mensagem.\n2.4.3  Formatos de mensagem de correio\nQuando Alice escreve uma carta a Bob e a envia pelo correio normal, ela pode incluir todos os tipos de \ninformações periféricas no cabeçalho da carta, como seu próprio endereço, o endereço de Bob e a data. De \nmodo semelhante, quando uma mensagem de e-mail é enviada, um cabeçalho contendo informações periféricas \nantecede o corpo da mensagem em si. Essas informações periféricas estão contidas em uma série de linhas de \ncabeçalho definidas no RFC 5322. As linhas de cabeçalho e o corpo da mensagem são separados por uma linha \n   Redes de computadores e a Internet\n92\nem branco (isto é, por CRLF). O RFC 5322 especifica o formato exato das linhas de cabeçalho das mensagens, \nbem como suas interpretações semânticas. Como acontece com o HTTP, cada linha de cabeçalho contém um \ntexto legível, consistindo em uma palavra-chave seguida de dois-pontos e de um valor. Algumas palavras-chave \nsão obrigatórias e outras, opcionais. Cada cabeçalho deve ter uma linha de cabeçalho From: e uma To: e pode \nincluir também uma Subject: bem como outras opcionais. É importante notar que essas linhas de cabeçalho \nsão diferentes dos comandos SMTP que estudamos na Seção 2.4.1 (ainda que contenham algumas palavras em \ncomum, como ‘from’ e ‘to’). Os comandos daquela seção faziam parte do protocolo de apresentação SMTP; as \nlinhas de cabeçalho examinadas nesta seção fazem parte da própria mensagem de correio.\nUm cabeçalho de mensagem típico é semelhante a:\nFrom: alice@crepes.fr\nTo: bob@hamburger.edu\nSubject: Searching for the meaning of life.\nApós o cabeçalho da mensagem, vem uma linha em branco e, em seguida, o corpo da mensagem (em AS-\nCII). Você pode usar o Telnet para enviar a um servidor de correio uma mensagem que contenha algumas linhas \nde cabeçalho, inclusive Subject:. Para tal, utilize o comando telnet serverName 25, como discutido na \nSeção 2.4.1.\n2.4.4  Protocolos de acesso ao correio\nQuando o SMTP entrega a mensagem do servidor de correio de Alice ao de Bob, ela é colocada na caixa \npostal de Bob. Durante toda essa discussão, ficou tacitamente subentendido que Bob lê sua correspondência ao \nentrar no hospedeiro servidor e, em seguida, executa o leitor de correio que roda naquela máquina. Até o início \nda década de 1990, este era o modo padronizado de acessar o correio. Mas hoje o acesso ao correio usa uma arqui-\ntetura cliente­\n‑servidor — o usuário típico lê e-mails com um cliente que funciona em seu sistema final, por exem-\nplo, um PC no escritório, um laptop ou um smartphone. Quando executam um cliente de correio em PC local, \nusuários desfrutam de uma série de propriedades, inclusive a capacidade de ver mensagens e anexos multimídia.\nDado que Bob (o destinatário) executa seu agente de usuário em seu PC local, é natural que ele considere a \ninstalação de um servidor de correio também em seu PC local. Adotando essa abordagem, o servidor de correio de \nAlice dialogaria diretamente com o PC de Bob. Porém, há um problema com essa abordagem. Lembre-se de que um \nservidor de correio gerencia caixas postais e executa os lados cliente e servidor do SMTP. Se o servidor de correio de \nBob residisse em seu PC local, este teria de ficar sempre em funcionamento e ligado na Internet para poder receber \nnovas correspondências que poderiam chegar a qualquer hora, o que não é prático para muitos usuários. Em vez \ndisso, um usuário típico executa um agente de usuário no PC local, mas acessa sua caixa postal armazenada em um \nservidor de correio compartilhado que está sempre em funcionamento. Esse servidor de correio é compartilhado \ncom outros usuários e, em geral, é mantido pelo ISP do usuário (por exemplo, universidade ou empresa).\nAgora, vamos considerar o caminho que uma mensagem percorre quando é enviada de Alice para Bob. Aca-\nbamos de aprender que, em algum ponto do percurso, a mensagem de e-mail precisa ser depositada no servidor \nde correio de Bob. Essa tarefa poderia ser realizada simplesmente fazendo o agente de usuário de Alice enviar a \nmensagem diretamente ao servidor de correio de Bob, o que pode ser feito com o SMTP — de fato, o SMTP foi pro-\njetado para enviar e-mail de um hospedeiro para outro. Contudo, em geral o agente de usuário do remetente não \ndialoga diretamente com o servidor de correio do destinatário. Em vez disso, como mostra a Figura 2.18, o agente \nde usuário de Alice usa SMTP para enviar a mensagem de e-mail a seu servidor de correio. Em seguida, esse ser-\nvidor usa SMTP (como um cliente SMTP) para retransmitir a mensagem de e-mail ao servidor de correio de Bob. \nPor que esse procedimento em duas etapas? Primordialmente porque, sem a retransmissão pelo servidor de cor-\nreio de Alice, o agente de usuário dela não dispõe de nenhum recurso para um servidor de correio de destinatário que \nnão pode ser alcançado. Fazendo que Alice primeiro deposite o e-mail em seu próprio servidor de correio, este \npode tentar, várias vezes, enviar a mensagem ao servidor de correio de Bob, digamos, a cada 30 minutos, até que \nesse servidor entre em operação. (E, se o servidor de correio de Alice não estiver funcionando, ela terá o recurso \n \nCAMADA  de APLICAÇÃO  93 \nde se queixar ao administrador do seu sistema!) O RFC do SMTP define como os comandos do \n \nSMTP podem ser usados para retransmitir uma mensagem por vários servidores SMTP.\nMas ainda falta uma peça do quebra-cabeça! De que forma um destinatário como Bob, que executa um \nagente de usuário em seu PC local, obtém suas mensagens que estão em um servidor de correio dentro do seu \nISP? Note que o agente de usuário de Bob não pode usar SMTP para obter as mensagens porque essa operação \né de recuperação (pull), e o SMTP é um protocolo de envio (push). O quebra-cabeça é concluído com a introdu-\nção de um protocolo especial de acesso ao correio que transfere mensagens do servidor de correio de Bob para \nseu PC local. Hoje, há vários protocolos populares de acesso a correio, entre eles POP3 (Post Office Protocol \nversão 3), IMAP (Internet Mail Access Protocol) e HTTP.\nA Figura 2.18 apresenta um resumo dos protocolos usados no correio da Internet: o SMTP é utilizado para \ntransferir correspondência do servidor de correio remetente para o servidor de correio destinatário; também \npara transferir correspondência do agente de usuário remetente para o servidor de correio remetente. Um pro-\ntocolo de acesso de correio, como o POP3, é utilizado para transferir correspondência do servidor de correio \ndestinatário para o agente de usuário destinatário.\nPOP3\nO POP3 é um protocolo de acesso de correio de extrema simplicidade. É definido no [RFC 1939], que é \ncurto e bem fácil de ler. Por ser tão simples, sua funcionalidade é bastante limitada. O POP3 começa quando o \nagente de usuário (o cliente) abre uma conexão TCP com o servidor de correio (o servidor) na porta 110. Com a \nconexão TCP ativada, o protocolo passa por três fases: autorização, transação e atualização. Durante a primeira \nfase, autorização, o agente de usuário envia um nome de usuário e uma senha (às claras) para autenticar o usuá-\nrio. Na segunda fase, transação, recupera mensagens; é também nessa etapa que o agente pode marcar mensagens \nque devem ser apagadas, remover essas marcas e obter estatísticas de correio. A terceira, atualização, ocorre após \no cliente ter dado o comando quit que encerra a sessão POP3. Nesse momento, o servidor de correio apaga as \nmensagens que foram marcadas.\nEm uma transação POP3, o agente de usuário emite comandos e o servidor, uma resposta para cada um de-\nles. Há duas respostas possíveis: +OK (às vezes seguida de dados do servidor para o cliente), usada pelo servidor \npara indicar que correu tudo bem com o comando anterior e –ERR, que o servidor usa para informar que houve \nalgo errado com o comando anterior.\nA fase de autorização tem dois comandos principais: user <username> e pass <password>. Para \nilustrá-los, sugerimos que você realize uma sessão Telnet diretamente com um servidor POP3, usando a porta \n110, e emita os dois comandos. Suponha que mailServer seja o nome de seu servidor de correio. O que você \nverá será algo parecido com:\ntelnet mailServer 110 \n+OK POP3 server ready \nuser bob\n+OK\npass hungry\n+OK user successfully logged on\nSMTP\nServidor de \ncorreio de Alice\nServidor de \ncorreio de Bob\nAgente \nde Alice\nAgente \nde Bob\nSMTP\nPOP3,\nIMAP\n, ou\nHTTP\nFigura 2.18\t\nProtocolos de e-mail e suas entidades comunicantes\n   Redes de computadores e a Internet\n94\nSe você escrever um comando errado, o POP3 responderá com uma mensagem –ERR.\nAgora, vamos examinar a fase de transação. Um agente de usuário que utiliza POP3 com frequência pode \nser configurado (pelo usuário) para “ler-e-apagar” ou para “ler-e-guardar”\n. A sequência de comandos emitida por \num agente de usuário POP3 depende do modo em que o agente de usuário estiver operando. No modo ler-e-apa-\ngar, o agente de usuário emite os comandos list, retr e dele. Como exemplo, suponha que o usuário tenha \nduas mensagens em sua caixa postal. No diálogo abaixo, C: (que representa o cliente) é o agente de usuário e S: \n(que representa o servidor), o servidor de correio. A transação será mais ou menos assim:\nC: list\nS: 1 498\nS: 2 912\nS: .\nC: retr 1\nS: (blah blah ...\nS: .................\nS: ..........blah)\nS: .\nC: dele 1\nC: retr 2\nS: (blah blah ...\nS: .................\nS: ..........blah)\nS: .\nC: dele 2\nC: quit\nS: +OK POP3 server signing off\nO agente de usuário primeiro pede ao servidor de correio que apresente o tamanho de cada mensagem arma-\nzenada. Então, recupera e apaga cada mensagem do servidor. Note que, após a fase de autorização, o agente de usuá­\nrio empregou apenas quatro comandos: list, retr, dele e quit. A sintaxe para eles é definida no RFC 1939. \nDepois de processar o comando de saída (quit), o servidor POP3 entra na fase de atualização e remove as mensagens \n1 e 2 da caixa postal.\nHá um problema com o modo ler-e-apagar: o destinatário, Bob, pode ser nômade e querer acessar seu cor-\nreio de muitas máquinas, por exemplo, do PC de seu escritório, do PC de sua casa e de seu computador portátil. \nO modo ler-e-apagar reparte as mensagens de correio de Bob entre essas três máquinas; em particular, se ele ler \nprimeiro uma mensagem no PC de seu escritório, não poderá lê-la de novo mais tarde em seu computador por-\ntátil. No modo ler-e-guardar, o agente de usuário deixa as mensagens no servidor de correio após descarregá-las. \nNesse caso, Bob pode reler mensagens em máquinas diferentes; pode acessar uma mensagem em seu local de \ntrabalho e, uma semana depois, acessá-la novamente em casa.\nDurante uma sessão POP3 entre um agente de usuário e o servidor de correio, o servidor POP3 mantém \nalguma informação de estado; em particular, monitora as mensagens do usuá–rio marcadas para apagar. Con-\ntudo, não mantém informação de estado entre sessões POP3. Essa falta de informação simplifica a execução de \num servidor POP3.\nIMAP\nUsando POP3, assim que baixar suas mensagens na máquina local, Bob pode criar pastas de correspondência \ne transferir as mensagens baixadas para elas. Em seguida, consegue apagar as mensagens, mudá-las de pastas e \nprocurar mensagens (por nome de remetente ou assunto). Mas esse paradigma — pastas e mensagens na máquina \nlocal — apresenta um problema para o usuário nômade que gostaria de manter uma hierarquia de pastas em um \nservidor remoto que possa ser acessado de qualquer computador: com o POP3, isso não é possível. Esse protocolo \nnão provê nenhum meio para um usuário criar pastas remotas e designar mensagens a pastas.\nCAMADA  de APLICAÇÃO  95 \nPara resolver esse e outros problemas, foi inventado o protocolo IMAP, definido no [RFC 3501]. Como o \nPOP3, o IMAP é um protocolo de acesso a correio, porém com mais recursos, mas é também significativamente \nmais complexo. (E, portanto, também as implementações dos lados cliente e servidor são muito mais complexas.)\nUm servidor IMAP associa cada mensagem a uma pasta. Quando uma mensagem chega a um servidor pela \nprimeira vez, é associada com a pasta INBOX do destinatário, que, então, pode transferi-la para uma nova pasta \ncriada por ele, lê-la, apagá-la e assim por diante. O protocolo IMAP provê comandos que permitem aos usuários \ncriarem pastas e transferir mensagens de uma para outra. Também provê comandos que os usuários podem usar \npara pesquisar pastas remotas em busca de mensagens que obedeçam a critérios específicos. Note que, ao contrá-\nrio do POP3, um servidor IMAP mantém informação de estado de usuário entre sessões IMAP — por exemplo, \nos nomes das pastas e quais mensagens estão associadas a elas.\nOutra característica importante do IMAP é que ele tem comandos que permitem que um agente de usuário \nobtenha componentes de mensagens. Por exemplo, um agente de usuário pode obter apenas o cabeçalho ou so-\nmente uma das partes de uma mensagem MIME multiparte. Essa característica é útil quando há uma conexão de \nlargura de banda estreita (por exemplo, uma conexão de modem em baixa velocidade) entre o agente de usuário e \nseu servidor de correio. Com uma conexão de pequena largura de banda, o usuário pode decidir não baixar todas \nas mensagens de sua caixa postal, evitando, em particular, mensagens longas que possam conter, por exemplo, \num clipe de áudio ou de vídeo.\nE-mail pela Web\nHoje, um número cada vez maior de usuários está enviando e acessando e-mails por meio de seus nave-\ngadores Web. O Hotmail lançou o e-mail com acesso pela Web em meados da década de 1990; agora, esse tipo \nde acesso também é fornecido por Google, Yahoo!, bem como universidades e empresas importantes. Com esse \nserviço, o agente de usuário é um navegador Web comum e o usuário se comunica com sua caixa postal remota \nvia HTTP. Quando um destinatário, por exemplo, Bob, quer acessar uma mensagem em sua caixa postal, ela é \nenviada do servidor de correio para o navegador de Bob usando o protocolo HTTP, e não os protocolos POP3 \nou IMAP. Quando um remetente, por exemplo, Alice, quer enviar uma mensagem de e-mail, esta é enviada do \nnavegador de Alice para seu servidor de correio por HTTP, e não por SMTP. O servidor de correio de Alice, \ncontudo, ainda envia mensagens para outros servidores de correio e recebe mensagens de outros servidores de \ncorreio usando o SMTP.\n2.5  DNS: o serviço de diretório da Internet\nNós, seres humanos, podemos ser identificados por diversas maneiras. Por exemplo, podemos ser identifi-\ncados pelo nome que aparece em nossa certidão de nascimento, pelo número do RG ou da carteira de motorista. \nEmbora cada um desses números possa ser usado para identificar pessoas, em um dado contexto um pode ser \nmais adequado que outro. Por exemplo, os computadores da Receita Federal preferem usar o número do CPF (de \ntamanho fixo) ao nome que consta em nossa certidão de nascimento. Por outro lado, pessoas comuns preferem \nnosso nome de batismo, mais fácil de lembrar, ao número do CPF. (Realmente, você pode se imaginar dizendo: \n“Oi, meu nome é 132-67-9875. Este é meu marido, 178-87-1146”?)\nAssim como seres humanos podem ser identificados de muitas maneiras, o mesmo acontece com hos-\npedeiros da Internet. Um identificador é seu nome de hospedeiro (hostname). Nomes de hospedeiro — como \ncnn.com, www.yahoo.com, gaia.cs.umass.edu e cis.poly.edu — são fáceis de lembrar e, portanto, \napreciados pelos seres humanos. Todavia, eles fornecem pouca — se é que alguma — informação sobre a locali-\nzação de um hospedeiro na Internet. (Um nome como www.eurecom.fr, que termina com o código do país \n.fr, nos informa que o hospedeiro deve estar na França, mas não diz muito mais do que isso.) Além disso, como \nnomes de hospedeiros podem consistir em caracteres alfanuméricos de comprimento variável, seriam difíceis de \nser processados por roteadores. Por essas razões, hospedeiros também são identificados pelo que denominamos \nendereços IP.\n   Redes de computadores e a Internet\n96\nDiscutiremos endereços IP mais detalhadamente no Capítulo 4, mas é importante falar um pouco sobre \neles agora. Um endereço IP é constituído de 4 bytes e sua estrutura hierárquica é rígida. Ele é semelhante a \n121.7.106.83, no qual cada ponto separa um dos bytes expressos em notação decimal de 0 a 255. Um endere-\nço IP é hierárquico porque, ao examiná-lo da esquerda para a direita, obtemos gradativamente mais informações \nespecíficas sobre onde o hospedeiro está localizado na Internet (isto é, em qual rede, dentre as muitas que com-\npõem a Internet). De maneira semelhante, quando examinamos um endereço postal de cima para baixo, obtemos \ninformações cada vez mais específicas sobre a localização do destinatário.\n2.5.1  Serviços fornecidos pelo DNS\nAcabamos de ver que há duas maneiras de identificar um hospedeiro — por um nome de hospedeiro e por \num endereço IP. As pessoas preferem o identificador nome de hospedeiro por ser mais fácil de lembrar, ao passo \nque roteadores preferem endereços IP de comprimento fixo e estruturados hierarquicamente. Para conciliar essas \npreferências, é necessário um serviço de diretório que traduza nomes de hospedeiro para endereços IP. Esta é a \ntarefa principal do DNS (domain name system — sistema de nomes de domínio) da Internet. O DNS é (1) um \nbanco de dados distribuído executado em uma hierarquia de servidores de DNS, e (2) um protocolo de camada \nde aplicação que permite que hospedeiros consultem o banco de dados distribuído. Os servidores DNS são muitas \nvezes máquinas UNIX que executam o software BIND (Berkeley Internet Name Domain) [BIND, 2012]. O proto-\ncolo DNS utiliza UDP e usa a porta 53.\nO DNS costuma ser empregado por outras entidades da camada de aplicação — inclusive HTTP, SMTP e \nFTP — para traduzir nomes de hospedeiros fornecidos por usuários para endereços IP. Como exemplo, considere \no que acontece quando um navegador (isto é, um cliente HTTP), que executa na máquina de algum usuário, re-\nquisita o URL www.someschool.edu/index.html. Para que a máquina do usuário possa enviar uma men-\nsagem de requisição HTTP ao servidor Web www.someschool.edu, ela precisa primeiro obter o endereço IP. \nIsso é feito da seguinte maneira:\n1.\t A própria máquina do usuário executa o lado cliente da aplicação DNS.\n2.\t O navegador extrai o nome de hospedeiro, www.someschool.edu, do URL e passa o nome para o lado \ncliente da aplicação DNS.\n3.\t O cliente DNS envia uma consulta contendo o nome do hospedeiro para um servidor DNS.\n4.\t O cliente DNS por fim recebe uma resposta, que inclui o endereço IP correspondente ao nome de hospe-\ndeiro.\n5.\t Tão logo o navegador receba o endereço do DNS, pode abrir uma conexão TCP com o processo servidor \nHTTP localizado na porta 80 naquele endereço IP.\nVemos, por esse exemplo, que o DNS adiciona mais um atraso — às vezes substancial — às aplicações de Inter-\nnet que o usam. Felizmente, como discutiremos mais adiante, o endereço IP procurado quase sempre está no cache \nde um servidor DNS “próximo”\n, o que ajuda a reduzir o tráfego DNS na rede, bem como o atraso médio do DNS.\nO DNS provê alguns outros serviços importantes além da tradução de nomes de hospedeiro para endereços IP:\n• Apelidos (aliasing) de hospedeiro. Um hospedeiro com nome complicado pode ter um ou mais ape-\nlidos. Um nome como relay1.west-coast.enterprise.com pode ter, por exemplo, dois ape-\nlidos, como enterprise.com e www.enterprise.com. Nesse caso, o nome de hospedeiro re-\nlay1.west-coast.enterprise.com é denominado nome canônico. Apelidos, quando existem, \nsão em geral mais fáceis de lembrar do que o nome canônico. O DNS pode ser chamado por uma \naplicação para obter o nome canônico correspondente a um apelido fornecido, bem como para obter o \nendereço IP do hospedeiro.\n• Apelidos de servidor de correio. Por razões óbvias, é adequado que endereços de e-mail sejam fáceis de \nlembrar. Por exemplo, se Bob tem uma conta no Hotmail, seu endereço de e-mail pode ser simplesmente \nbob@hotmail.com. Contudo, o nome de hospedeiro do servidor do Hotmail é mais complicado e \nCAMADA  de APLICAÇÃO  97 \nmuito mais difícil de lembrar do que apenas hotmail.com (por exemplo, o nome canônico pode ser algo \nparecido com relay1.west-coast.hotmail.com). O DNS pode ser chamado por uma aplicação \nde correio para obter o nome canônico a partir de um apelido fornecido, bem como o endereço IP do \nhospedeiro. Na verdade, o registro MX (veja adiante) permite que o servidor de correio e o servidor Web \nde uma empresa tenham nomes (apelidos) idênticos; por exemplo, o servidor Web e o servidor de correio \nde uma empresa podem ambos ser denominados enterprise.com.\n• Distribuição de carga. O DNS também é usado para realizar distribuição de carga entre servidores re-\nplicados, tais como os servidores Web replicados. Sites movimentados, como cnn.com, são replicados \nem vários servidores, cada qual rodando em um sistema final e com um endereço IP diferentes. Assim, \nno caso de servidores Web replicados, um conjunto de endereços IP fica associado a um único nome \ncanônico e contido no banco de dados do DNS. Quando clientes consultam um nome mapeado para \num conjunto de endereços, o DNS responde com o conjunto inteiro de endereços IP, mas faz um rodízio \nda ordem deles dentro de cada resposta. Como um cliente em geral envia sua mensagem de requisição \nHTTP ao endereço IP que ocupa o primeiro lugar no conjunto, o rodízio de DNS distribui o tráfego entre \nos servidores replicados. O rodízio de DNS também é usado para e-mail, de modo que vários servidores \nde correio podem ter o mesmo apelido. Há pouco, empresas distribuidoras de conteúdo como a Akamai \npassaram a usar o DNS de maneira mais sofisticada [Dilley, 2002] para prover distribuição de conteúdo \nna Web (veja Capítulo 7).\nO DNS está especificado no RFC 1034 e no RFC 1035 e atualizado em diversos RFCs adicionais. É um sistema \ncomplexo e, neste livro, apenas mencionamos os aspectos fundamentais de sua operação. O leitor interessado pode \nconsultar os RFCs citados, o livro escrito por Abitz e Liu [Abitz, 1993] e também o artigo de Mockapetris [1988], que \napresenta uma retrospectiva e uma ótima descrição do que e do porquê do DNS, e Mockapetris [2005].\n2.5.2  Visão geral do modo de funcionamento do DNS\nApresentaremos, agora, uma visão panorâmica do modo de funcionamento do DNS. Nossa discussão foca-\nlizará o serviço de tradução de nome de hospedeiro para endereço IP.\nSuponha que certa aplicação (como um navegador Web ou um leitor de correio), que executa na máquina de um \nusuário, precise traduzir um nome de hospedeiro para um endereço IP\n. A aplicação chamará o lado cliente do DNS, \nespecificando o nome de hospedeiro que precisa ser traduzido. (Em muitas máquinas UNIX, gethostbyname() é \na chamada de função que uma aplicação invoca para realizar a tradução.) A partir daí, o DNS do hospedeiro do \nusuá­\nrio assume o controle, enviando uma mensagem de consulta para a rede. Todas as mensagens de consulta e \nFunções decisivas de rede via paradigma cliente-servidor\nComo HTTP\n, FTP e SMTP\n, o DNS é um protocolo \nda camada de aplicação, já que (1) roda entre siste-\nmas finais comunicantes usando o paradigma cliente-\nservidor e (2) depende de um protocolo de transporte \nfim a fim subjacente para transferir mensagens DNS \nentre sistemas finais comunicantes. Em outro sen-\ntido, contudo, o papel do DNS é bastante diferente \ndas aplicações Web, da transferência de arquivo e do \ne-mail. Ao contrário delas, o DNS não é uma aplica-\nção com a qual o usuário interage diretamente. Em \nvez disso, fornece uma função interna da Internet — a \nsaber, a tradução de nomes de hospedeiro para seus \nendereços IP subjacentes, para aplicações de usuá-\nrio e outros softwares da Internet. Notamos, na Seção \n1.2, que grande parte da complexidade da arquitetu-\nra da Internet está localizada na “periferia” da rede. \nO DNS, que executa o processo crucial de tradução \nde nome para endereço usando clientes e servidores \nlocalizados nas bordas da rede, é mais um exemplo \ndessa filosofia de projeto.\nPrincípios na prática\n   Redes de computadores e a Internet\n98\nde resposta do DNS são enviadas dentro de datagramas UDP à porta 53. Após um atraso na faixa de milissegun-\ndos a segundos, o DNS no hospedeiro do usuário recebe uma mensagem de resposta DNS fornecendo o mapea­\nmento desejado, que é, então, passado para a aplicação que está interessada. Portanto, do ponto de vista dessa \naplicação, que está na máquina do cliente, o DNS é uma caixa-preta que provê um serviço de tradução simples \ne direto. Mas, na realidade, a caixa-preta que executa o serviço é complexa, consistindo em um grande número \nde servidores DNS distribuídos ao redor do mundo, bem como em um protocolo de camada de aplicação que \nespecifica como se comunicam os servidores DNS e os hospedeiros que fazem a consulta.\nUm arranjo simples para DNS seria ter um servidor DNS contendo todos os mapeamentos. Nesse projeto \ncentralizado, os clientes apenas dirigiriam todas as consultas a esse único servidor DNS, que responderia dire-\ntamente aos clientes que estão fazendo a consulta. Embora a simplicidade desse arranjo seja atraente, ele não \né adequado para a Internet de hoje com seu vasto (e crescente) número de hospedeiros. Dentre os problemas \nde um arranjo centralizado, estão:\n• Um único ponto de falha. Se o servidor DNS quebrar, a Internet inteira quebrará!\n• Volume de tráfego. Um único servidor DNS teria de manipular todas as consultas DNS (para todas as \nrequisições HTTP e mensagens de e-mail geradas por centenas de milhões de hospedeiros).\n• Banco de dados centralizado distante. Um único servidor DNS nunca poderia estar “próximo” de to-\ndos os clientes que fazem consultas. Se colocarmos o único servidor DNS na cidade de Nova York, todas \nas buscas provenientes da Austrália terão de viajar até o outro lado do globo, talvez por linhas lentas e \ncongestionadas, o que pode resultar em atrasos significativos.\n• Manutenção. O único servidor DNS teria de manter registros de todos os hospedeiros da Internet. \nEsse banco de dados não só seria enorme, mas também precisaria ser atualizado frequentemente para \natender a todos os novos hospedeiros.\nResumindo, um banco de dados centralizado em um único servidor DNS simplesmente não é escalá-\nvel. Por conseguinte, o DNS é distribuído por conceito de projeto. Na verdade, ele é um ótimo exemplo de \ncomo um banco de dados distribuído pode ser executado na Internet.\nUm banco de dados distribuído e  hierárquico\nPara tratar da questão da escala, o DNS usa um grande número de servidores, organizados de maneira hie-\nrárquica e distribuídos por todo o mundo. Nenhum servidor DNS isolado tem todos os mapeamentos para todos \nos hospedeiros da Internet. Em vez disso, os mapeamentos são distribuídos pelos servidores DNS. Como uma \nprimeira aproximação, há três classes de servidores DNS: raiz, de domínio de alto nível (top-level domain — TLD) \ne servidores DNS autoritativos — organizados em uma hierarquia, como mostra a Figura 2.19. Para entender \ncomo essas três classes interagem, suponha que um cliente DNS queira determinar o endereço IP para o nome de \nhospedeiro www.amazon.com. Como uma primeira aproximação, ocorrerão os seguintes eventos. Primeiro, o \ncliente contatará um dos servidores raiz, que retornará endereços IP dos servidores TLD para o domínio de alto \nnível com. Então, o cliente contatará um desses servidores TLD, que retornará o endereço IP de um servidor au-\ntoritativo para amazon.com. Por fim, o cliente contatará um dos servidores autoritativos para amazon.com, que \nretornará o endereço IP para o nome de hospedeiro www.amazon.com. Mais adiante, analisaremos em detalhes \nesse processo de consulta DNS. Mas, primeiro, vamos examinar mais de perto as três classes de servidores DNS:\n• Servidores DNS raiz. Na Internet há 13 servidores DNS raiz (denominados de A a M) e a maior parte \ndeles está localizada na América do Norte. Um mapa de servidores DNS raiz de outubro de 2006 é mos-\ntrado na Figura 2.20; uma lista dos servidores DNS raiz existentes hoje está disponível em Root-servers \n[2012]. Embora tenhamos nos referido a cada um dos 13 servidores DNS raiz como se fossem um servi-\ndor único, na realidade, cada um é um conglomerado de servidores replicados, para fins de segurança e \nconfiabilidade. No total, havia 247 servidores raiz no segundo semestre de 2011.\n• Servidores DNS de Domínio de Alto Nível (TLD). Esses servidores são responsáveis por domínios de \nalto nível como com, org, net, edu e gov, e por todos os domínios de alto nível de países, tais como uk, fr, \nCAMADA  de APLICAÇÃO  99 \nca e jp. A empresa Verisign Global Registry Services mantém os servidores TLD para o domínio de alto \nnível com e a Educause mantém os servidores TLD para o domínio de alto nível edu. Veja em IANA TLD \n[2012] uma lista de todos os domínios de alto nível.\n• Servidores DNS autoritativos. Toda organização que tiver hospedeiros que possam ser acessados publi-\ncamente na Internet (como servidores Web e servidores de correio) deve fornecer registros DNS também \nacessíveis publicamente que mapeiem os nomes desses hospedeiros para endereços IP. Um servidor DNS \nautoritativo de uma organização abriga esses registros. Uma organização pode preferir executar seu próprio \nservidor DNS autoritativo para abrigar esses registros ou, como alternativa, pagar para armazená-los em \num servidor DNS autoritativo de algum provedor de serviço. A maioria das universidades e empresas de \ngrande porte executa e mantém seus próprios servidores DNS primário e secundário (backup) autoritativos.\nOs servidores DNS raiz, TLD e autoritativo pertencem à hierarquia de servidores DNS, como mostra a \nFigura 2.19. Há mais um tipo importante de DNS, denominado servidor DNS local, que não pertence, estrita-\nmente, à hierarquia de servidores, mas, mesmo assim, é central para a arquitetura DNS. Cada ISP — como o de \numa universidade, de um departamento acadêmico, de uma empresa ou de uma residência — tem um servidor \nDNS local (também denominado servidor DNS default). Quando um hospedeiro se conecta com um ISP, este \nfornece os endereços IP de um ou mais de seus servidores DNS locais (em geral por DHCP, que será discutido \nno Capítulo 4). Determinar o endereço IP do seu servidor DNS local é fácil: basta acessar as janelas de estado \nservidores \nDNS edu\nservidores DNS \norganização\nservidores \nDNS com\nservidores DNS \npoly.edu\nservidores DNS \nyahoo.com\nservidores DNS \namazon.com\nservidores DNS \npbs.org\nservidores DNS \numass.edu\nservidores \nDNS raiz\nFigura 2.19  Parte da hierarquia de servidores DNS\nFigura 2.20  Servidores DNS raiz em 2012 (nome, organização, localização)\nc.\nd.\nh.\nj.\nCogent, Herndon, VA (5 outros locais)\nU Maryland College Park, MD\nARL Aberdeen, MD\nVerisign, Dulles VA (69 outros locais)\ni. Netnod, Stockholm\n(37 outros locais)\nk. RIPE London \n(17 outros locais)\nm. WIDE Tokyo\n     (5 outros locais)\ng. US DoD Columbus, OH\n     (5 outros locais)\ne.\nf.\nNASA Mt View, CA\nInternet Software C.\nPalo Alto, CA\n(e 48 outros locais)\na.\nb.\n l.\nVerisign, Los Angeles CA \n(5 outros locais)\nUSC-ISI Marina del Rey, CA\nICANN Los Angeles, CA\n(41 outros locais)\n   Redes de computadores e a Internet\n100\nda rede no Windows ou UNIX. O servidor DNS local de um hospedeiro costuma estar “próximo” dele. No caso \nde um ISP institucional, pode estar na mesma LAN do hospedeiro; já no caso de um ISP residencial, em geral \no servidor DNS está separado do hospedeiro por não mais do que alguns roteadores. Quando um hospedeiro \nfaz uma consulta ao DNS, ela é enviada ao servidor DNS local, que age como proxy e a retransmite para a hie-\nrarquia do servidor DNS, como discutiremos mais detalhadamente a seguir.\nVamos examinar um exemplo simples. Suponha que o hospedeiro cis.poly.edu deseje o endereço IP \nde gaia.cs.umass.edu. Imagine também que o servidor DNS local da Polytechnic seja denominado dns.\npoly.edu e que um servidor DNS autoritativo para gaia.cs.umass.edu seja denominado dns.umass.\nedu. Como mostra a Figura 2.21, o hospedeiro cis.poly.edu primeiro envia uma mensagem de consulta DNS \na seu servidor DNS local dns.poly.edu. A mensagem de consulta contém o nome de hospedeiro a ser traduzi-\ndo, isto é, gaia.cs.umass.edu. O servidor DNS local transmite a mensagem de consulta a um servidor DNS \nraiz, que percebe o sufixo edu e retorna ao servidor DNS local uma lista de endereços IP contendo servidores TLD \nresponsáveis por edu. Então, o servidor DNS local retransmite a mensagem de consulta a um desses servidores \nTLD que, por sua vez, percebe o sufixo umass.edu e responde com o endereço IP do servidor DNS autorizado \npara a University of Massachusetts, a saber, dns.umass.edu. Por fim, o servidor DNS local reenvia a mensagem \nde consulta diretamente a dns.umass.edu, que responde com o endereço IP de gaia.cs.umass.edu. \nNote que, neste exemplo, para poder obter o mapeamento para um único nome de hospedeiro, foram enviadas \noito mensagens DNS: quatro mensagens de consulta e quatro de resposta! Em breve veremos como o cache de \n \nDNS reduz esse tráfego de consultas.\nNosso exemplo anterior considerou que o servidor TLD conhece o servidor DNS autoritativo para o nome \nde hospedeiro, o que em geral não acontece. Ele pode conhecer apenas um servidor DNS intermediário que, por \nsua vez, conhece o servidor DNS autoritativo para o nome de hospedeiro. Por exemplo, suponha de novo que a \nUniversidade de Massachusetts tenha um servidor DNS para a universidade denominado dns.umass.edu. \nFigura 2.21  Interação dos diversos servidores DNS\nHospedeiro requisitante\ncis.poly.edu\nServidor DNS local\nServidor DNS TLD\ndns.poly.edu\nServidor DNS raiz\n1\n8\n2\n7\n4\n5\n3\n6\nServidor DNS autoritativo\ndns.umass.edu\ngaia.cs.umass.edu\nCAMADA  de APLICAÇÃO  101 \nImagine também que cada departamento da universidade tenha seu próprio servidor DNS e que cada servidor \nDNS departamental seja um servidor DNS autoritativo para todos os hospedeiros do departamento. Nesse caso, \nquando o servidor DNS intermediário dns.umass.edu receber uma consulta para um hospedeiro cujo nome \ntermina com cs.umass.edu, ele retornará a dns.poly.edu o endereço IP de dns.cs.umass.edu, que \ntem autoridade para todos os nomes de hospedeiro que terminam com cs.umass.edu. Então, o servidor DNS \nlocal dns.poly.edu enviará a consulta ao servidor DNS autoritativo, que retornará o mapeamento desejado \npara o servidor DNS local e que, por sua vez, o repassará ao hospedeiro requisitante. Nesse caso, serão enviadas \ndez mensagens DNS no total!\nO exemplo mostrado na Figura 2.21 usa consultas recursivas e consultas iterativas. A consulta enviada de \ncis.poly.edu para dns.poly.edu é recursiva, visto que pede a dns.poly.edu que obtenha o mapeamento \nem seu nome. Mas as três consultas subsequentes são iterativas, visto que todas as respostas são retornadas di-\nretamente a dns.poly.edu. Em teoria, qualquer consulta DNS pode ser iterativa ou recursiva. Por exemplo, a \nFigura 2.22 mostra uma cadeia de consultas DNS na qual todas são recursivas. Na prática, as consultas em geral \nseguem o padrão mostrado na Figura 2.21: a consulta do hospedeiro requisitante ao servidor DNS local é recur-\nsiva e todas as outras são iterativas.\nCache DNS\nAté aqui, nossa discussão ignorou o cache DNS, uma característica muito importante do sistema DNS. \nNa realidade, o DNS explora extensivamente o cache para melhorar o desempenho quanto ao atraso e reduzir o \nnúmero de mensagens DNS que dispara pela Internet. A ideia por trás do cache DNS é muito simples. Em uma \ncadeia de consultas, quando um servidor DNS recebe uma resposta DNS (contendo, por exemplo, o mapea-\nFigura 2.22  Consultas recursivas em DNS\nHospedeiro requisitante\ncis.poly.edu\nServidor DNS local\nServidor DNS TLD\ndns.poly.edu\nServidor DNS raiz\n1\n8\n5\n4\n2\n7\nServidor DNS autoritativo\ndns.umass.edu\ngaia.cs.umass.edu\n6\n3\n   Redes de computadores e a Internet\n102\nmento de um nome de hospedeiro para um endereço IP), pode fazer cache das informações da resposta em sua \nmemória local. Por exemplo, na Figura 2.21, toda vez que o servidor DNS local dns.poly.edu recebe uma \nresposta de algum servidor DNS, pode fazer cache de qualquer informação contida na resposta. Se um par nome \nde hospedeiro/endereço IP estiver no cache de um servidor DNS e outra consulta chegar ao mesmo servidor para \no mesmo nome de hospedeiro, o servidor DNS poderá fornecer o endereço IP desejado, mesmo que não tenha \nautoridade para esse nome. Como hospedeiros e mapeamentos entre hospedeiros e endereços IP não são, de \nmodo algum, permanentes, após um período de tempo (quase sempre dois dias), os servidores DNS descartam \nas informações armazenadas em seus caches.\nComo exemplo, imagine que um hospedeiro apricot.poly.edu consulte dns.poly.edu para o \nendereço IP da máquina cnn.com. Além disso, suponha que algumas horas mais tarde outra máquina da \n \nPolytechnic University, digamos, kiwi.poly.fr também consulte dns.poly.edu para o mesmo nome de hos-\npedeiro. Por causa do cache, o servidor local poderá imediatamente retornar o endereço IP de cnn.com a esse \nsegundo hospedeiro requisitante, sem ter de consultar quaisquer outros servidores DNS. Um servidor DNS local \ntambém pode fazer cache de endereços IP de servidores TLD, permitindo, assim, que servidores DNS locais evi-\ntem os servidores DNS raiz em uma cadeia de consultas (isso acontece bastante).\n2.5.3  Registros e mensagens DNS\nOs servidores DNS que juntos executam o banco de dados distribuído do DNS armazenam registros de re-\ncursos (RR) que fornecem mapeamentos de nomes de hospedeiros para endereços IP. Cada mensagem de resposta \nDNS carrega um ou mais registros de recursos. Nesta seção e na subsequente, apresentaremos uma breve visão geral \ndos registros de recursos e mensagens DNS. Para mais detalhes, consulte Abitz [1993] ou RFC 1034; RFC 1035.\nUm registro de recurso é uma tupla de quatro elementos que contém os seguintes campos:\n(Name, Value, Type, TTL)\nTTL é o tempo de vida útil do registro de recurso; determina quando um recurso deve ser removido de \num cache. Nos exemplos de registros dados a seguir, ignoramos o campo TTL. Os significados de Name e Value \ndependem de Type:\n• Se Type=A, então Name é um nome de hospedeiro e Value é o endereço IP para o nome de hospedeiro. \nAssim, um registro Type A fornece o mapeamento-padrão entre nomes de hospedeiros e endereços IP. \nComo exemplo, (relay1.bar.foo.com, 145.37.93.126, A) é um registro com Type igual a A.\n• Se Type=NS, então Name é um domínio (como foo.com) e Value é o nome de um servidor DNS \nautoritativo que sabe como obter os endereços IP para hospedeiros do domínio. Esse registro é usado \npara encaminhar consultas DNS ao longo da cadeia de consultas. Como exemplo, (foo.com, dns.\nfoo.com, NS) é um registro com Type igual a NS.\n• Se Type=CNAME, então Value é um nome canônico de hospedeiro para o apelido de hospedeiro contido \nem Name. Esse registro pode fornecer aos hospedeiros consultantes o nome canônico correspondente a um \napelido de hospedeiro. Como exemplo, (foo.com, relay1.bar.foo.com, CNAME) é um registro \nCNAME.\n• Se Type=MX, então Value é o nome canônico de um servidor de correio cujo apelido de hospedeiro \nestá contido em Name. Como exemplo, (foo.com, mail.bar.foo.com, MX) é um registro MX. \nRegistros MX permitem que os nomes de hospedeiros de servidores de correio tenham apelidos sim-\nples. Note que, usando o registro MX, uma empresa pode ter o mesmo apelido para seu servidor de \narquivo e para um de seus outros servidores (tal como seu servidor Web). Para obter o nome canôni-\nco do servidor de correio, um cliente DNS consultaria um registro MX; para obter o nome canônico \ndo outro servidor, o cliente DNS consultaria o registro CNAME.\nCAMADA  de APLICAÇÃO  103 \nSe um servidor DNS tiver autoridade para determinado nome de hospedeiro, então conterá um regis-\ntro Type A para o nome de hospedeiro. (Mesmo que não tenha autoridade, o servidor DNS pode conter um \nregistro Type A em seu cache.) Se um servidor não tiver autoridade para um nome de hospedeiro, conterá \num registro Type NS para o domínio que inclui o nome e um registro Type A que fornece o endereço IP do \nservidor DNS no campo Value do registro NS. Como exemplo, suponha que um servidor TLD edu não tenha \nautoridade para o hospedeiro gaia.cs.umass.edu. Nesse caso, esse servidor conterá um registro para um \ndomínio que inclui o hospedeiro gaia.cs.umass.edu, por exemplo (umass.edu, dns.umass.edu, \nNS). O servidor TLD edu conterá também um registro Type A, que mapeia o servidor DNS dns.umass.edu \npara um endereço IP, por exemplo (dns.umass.edu, 128.119.40.111, A).\nMensagens DNS\nAbordamos anteriormente nesta seção mensagens de consulta e de resposta DNS, que são as duas únicas \nespécies de mensagem DNS. Além disso, tanto as mensagens de consulta como as de resposta têm o mesmo for-\nmato, como ilustra a Figura 2.23. A semântica dos vários campos de uma mensagem DNS é a seguinte:\n• Os primeiros 12 bytes formam a seção de cabeçalho, que tem vários campos. O primeiro campo é um \nnúmero de 16 bits que identif﻿﻿ica a consulta. Esse identificador é copiado para a mensagem de resposta a \numa consulta, permitindo que o cliente combine respostas recebidas com consultas enviadas. Há várias \nflags no campo de flag. Uma flag de consulta/resposta de 1 bit indica se a mensagem é uma consulta (0) \nou uma resposta (1). Uma flag de autoridade de 1 bit é marcada em uma mensagem de resposta quando \no servidor DNS é um servidor autoritativo para um nome consultado. Uma flag de recursão desejada de \n1 bit é estabelecida quando um cliente (hospedeiro ou servidor DNS) quer que um servidor DNS proce-\nda recursivamente sempre que não tem o registro. Um campo de recursão disponível de 1 bit é marcado \nem uma resposta se o servidor DNS suporta recursão. No cabeçalho, há também quatro campos de \n“tamanho”\n. Eles indicam o número de ocorrências dos quatro tipos de seção de dados que se seguem ao \ncabeçalho.\n• A seção de pergunta contém informações sobre a consulta que está sendo feita. Essa seção inclui (1) um \ncampo de nome que contém o nome que está sendo consultado e (2) um campo de tipo que indica o tipo \nIdentiﬁcação\nNúmero de perguntas\nNúmero de RRs autoritativos\nNome, campos de \ntipo para uma consulta\n12 bytes\nRRs de resposta à consulta\nRegistros para servidores \ncom autoridade\nInformação adicional ‘útil’, \nque pode ser usada\nFlags\nNúmero de RRs de resposta\nNúmero de RRs adicionais\nAutoridade\n(número variável de registros de recursos)\nInformação adicional\n(número variável de registros de recursos)\nRespostas\n(número variável de registros de recursos)\nPerguntas\n(número variável de perguntas)\nFigura 2.23  Formato da mensagem DNS\n   Redes de computadores e a Internet\n104\nda pergunta que está sendo feita sobre o nome — por exemplo, um endereço de hospedeiro associado a \num nome (Type A) ou o servidor de correio para um nome (Type MX).\n• Em uma resposta de um servidor DNS, a seção de resposta contém os registros de recursos para o nome \nque foi consultado originalmente. Lembre-se de que em cada registro de recurso há o Type (por exem-\nplo, A, NS, CSNAME e MX), o Value e o TTL. Uma resposta pode retornar vários RRs, já que um nome \nde hospedeiro pode ter diversos endereços IP (por exemplo, para servidores Web replicados, como já \ndiscutimos anteriormente nesta seção).\n• A seção de autoridade contém registros de outros servidores autoritativos.\n• A seção adicional contém outros registros úteis. Por exemplo, o campo resposta em uma resposta a uma \nconsulta MX conterá um registro de recurso que informa o nome canônico de um servidor de correio. A \nseção adicional conterá um registro Type A que fornece o endereço IP para o nome canônico do servidor \nde correio.\nVocê gostaria de enviar uma mensagem de consulta DNS direto de sua máquina a algum servidor DNS? Isso \npode ser feito facilmente com o programa nslookup, que está disponível na maioria das plataformas Windows \ne UNIX. Por exemplo, se um hospedeiro executar Windows, abra o Prompt de comando e chame o programa \nnslookup apenas digitando “nslookup”\n. Depois de chamar o programa, você pode enviar uma consulta DNS a \nqualquer servidor DNS (raiz, TLD ou autoritativo). Após receber a mensagem de resposta do servidor DNS, o \nnslookup apresentará os registros incluídos na resposta (em formato que pode ser lido normalmente). Como al-\nternativa para executar nslookup na sua própria máquina, você pode visitar um dos muitos sites que permitem o \nemprego remoto do programa. (Basta digitar “nslookup” em um buscador e você será levado a um desses sites.) \nO Wireshark lab DNS, ao final deste capítulo, lhe permitirá explorar o DNS com muito mais detalhes.\nInserindo registros no banco de dados do DNS\nA discussão anterior focalizou como são extraídos registros do banco de dados DNS. É possível que você esteja \nse perguntando como os registros entraram no banco de dados em primeiro lugar. Vamos examinar como isso é \nfeito no contexto de um exemplo específico. Imagine que você acabou de criar uma nova empresa muito interessante \ndenominada Network Utopia. A primeira coisa que você certamente deverá fazer é registrar o nome de domínio \nnetworkutopia.com em uma entidade registradora. Uma entidade registradora é uma organização comercial \nque verifica se o nome de domínio é exclusivo, registra-o no banco de dados do DNS (como discutiremos mais \nadiante) e cobra uma pequena taxa por seus serviços. Antes de 1999, uma única entidade registradora, a Network \nSolutions, detinha o monopólio do registro de nomes para os domínios com, net e org. Mas agora existem muitas \nentidades registradoras credenciadas pela Internet Corporation for Assigned Names and Numbers (ICANN) com-\npetindo por clientes. Uma lista completa dessas entidades está disponível em http://www.internic.net.\nAo registrar o nome de domínio networkutopia.com, você também precisará informar os nomes e endereços \nIP dos seus servidores DNS com autoridade, primários e secundários. Suponha que os nomes e endereços IP se-\njam dns1.networkutopia.com, dns2.networkutopia.com, 212.212.212.1 e 212.212.212.2. A \nentidade registradora ficará encarregada de providenciar a inserção dos registros Type NS e Type A nos servidores \nTLD do domínio com para cada um desses dois servidores de nomes autorizados. Em especial, para o servidor \nprimário com autoridade networkutopia.com, a autoridade registradora inseriria no sistema DNS os dois re-\ngistros de recursos a seguir:\n(networkutopia.com, dns1.networkutopia.com, NS)\n(dns1.networkutopia.com, 212.212.212.1, A)\nNão se esqueça de providenciar também a inserção em seus servidores de nomes com autoridade do regis-\ntro de recurso Type A para seu servidor Web www.networkutopia.com e o registro de recurso Type MX para \nseu servidor de correio mail.networkutopia.com. (Até há pouco tempo, o conteúdo de cada servidor DNS \nera configurado estaticamente, por exemplo, a partir de um arquivo de configuração criado por um gerenciador \nCAMADA  de APLICAÇÃO  105 \nde sistema. Mais recentemente, foi acrescentada ao protocolo DNS uma opção UPDATE, que permite que dados \nsejam dinamicamente acrescentados no banco de dados ou apagados deles por meio de mensagens DNS. O [RFC \n2136] e o [RFC 3007] especificam atualizações dinâmicas do DNS.)\nQuando todas essas etapas estiverem concluídas, o público em geral poderá visitar seu site e enviar e-mails aos \nempregados de sua empresa. Vamos concluir nossa discussão do DNS verificando que essa afirmação é verdadeira, o \nVulnerabilidades do DNS\nVimos que o DNS é um componente fundamen-\ntal da infraestrutura da Internet, com muitos serviços \nimportantes — incluindo a Web e o e-mail — simples-\nmente incapazes de funcionar sem ele. Dessa ma-\nneira, perguntamos: como o DNS pode ser atacado? \nO DNS é um alvo esperando para ser atingido, pois \ncausa dano à maioria das aplicações da Internet junto \ncom ele?\nO primeiro tipo de ataque que vem à mente é a \ninundação na largura de banda DDoS (veja a Seção \n1.6) contra servidores DNS. Por exemplo, um ata-\ncante pode tentar enviar para cada servidor DNS raiz \numa inundação de pacotes, fazendo a maioria das \nconsultas DNS legítimas nunca ser respondida. Um \nataque DDoS em larga escala contra servidores DNS \nraiz aconteceu em 21 de outubro de 2002. Os atacan-\ntes aproveitaram um botnet para enviar centenas de \nmensagens ping para os 13 servidores DNS raiz. (As \nmensagens ICMP são discutidas no Capítulo 4. Por \nenquanto, basta saber que os pacotes ICMP são ti-\npos especiais de datagramas IP\n.) Felizmente, esse \nataque em larga escala causou um dano mínimo, \ntendo um pequeno ou nenhum impacto sobre a ex-\nperiência dos usuários com a Internet. Os atacantes \nobtiveram êxito ao direcionar centenas de pacotes \naos servidores raiz. Mas muitos dos servidores DNS \nraiz foram protegidos por filtros de pacotes, confi-\ngurados para sempre bloquear todas as mensagens \nping ICMP encaminhadas aos servidores raiz. As-\nsim, esses servidores protegidos foram poupados \ne funcionaram normalmente. Além disso, a maioria \ndos servidores DNS locais oculta os endereços IP \ndos servidores de domínio de nível superior, permi-\ntindo que o processo de consulta ultrapasse com \nfrequência os servidores DNS raiz.\nUm ataque DDoS potencialmente mais eficaz \ncontra o DNS seria enviar uma inundação de consul-\ntas DNS aos servidores de domínio de alto nível, por \nexemplo, para todos os que lidam com o domínio .com. \nSeria mais difícil filtrar as consultas DNS direcionadas \naos servidores DNS; e os servidores de domínio de alto \nnível não são ultrapassados com tanta facilidade quan-\nto os raiz. Mas a gravidade do ataque poderia ser em \nparte amenizada pelo cache nos servidores DNS locais.\nO DNS poderia ser atacado de outras maneiras. \nEm um ataque de homem no meio, o atacante in-\ntercepta consultas do hospedeiro e retorna respos-\ntas falsas. No de envenenamento, o atacante envia \nrespostas falsas a um servidor DNS, fazendo-o ar-\nmazenar os registros falsos em sua cache. Ambos \nos ataques podem ser utilizados, por exemplo, para \nredirecionar um usuário da Web inocente ao site Web \ndo atacante. Esses ataques, entretanto, são difíceis \nde implementar, uma vez que requerem a intercep-\nção de pacotes ou o estrangulamento de servidores \n[Skoudis, 2006].\nOutro ataque DNS importante não é um ataque \nao serviço DNS por si mesmo, mas, em vez disso, \nse aproveitar da infraestrutura do DNS para lançar \num ataque DDoS contra um hospedeiro-alvo (por \nexemplo, o servidor de mensagens de sua universi-\ndade). Nesse caso, o atacante envia consultas DNS \npara muitos servidores DNS autoritativos, com cada \nconsulta tendo o endereço-fonte falsificado do hos-\npedeiro-alvo. Os servidores DNS, então, enviam suas \nrespostas diretamente para o hospedeiro-alvo. Se as \nconsultas puderem ser realizadas de tal maneira que \numa resposta seja muito maior (em bytes) do que \numa consulta (denominada amplificação), então o \natacante pode entupir o alvo sem ter que criar muito \nde seu próprio tráfego. Tais ataques de reflexão que \nexploram o DNS possuem um sucesso limitado até \nhoje [Mirkovic, 2005].\nEm resumo, não houve um ataque que tenha \ninterrompido o serviço DNS com sucesso. Houve \nataques de reflexão bem-sucedidos; entretanto, \neles podem ser (e estão sendo) abordados por uma \nconfiguração apropriada de servidores DNS.\nSegurança em foco\n   Redes de computadores e a Internet\n106\nque também ajudará a solidificar aquilo que aprendemos sobre o DNS. Suponha que Alice, que está na Austrália, queira \nconsultar a página www.networkutopia.com. Como discutimos, seu hospedeiro primeiro enviará uma consulta \nDNS a seu servidor de nomes local, que então contatará um servidor TLD do domínio com. (O servidor de nomes local \ntambém terá de contatar um servidor de nomes raiz caso não tenha em seu cache o endereço de um servidor TLD com.) \nEsse servidor TLD contém os registros de recursos Type NS e Type A já citados, porque a entidade registradora já os tinha \n \ninserido em todos os servidores TLD com. O servidor TLD com envia uma resposta ao servidor de nomes lo-\ncal de Alice, contendo os dois registros de recursos. Então, o servidor de nomes local envia uma consulta DNS \na 212.212.212.1, solicitando o registro Type A correspondente a www.networkutopia.com. Este registro \noferece o endereço IP do servidor Web desejado, digamos, 212.212.71.4, que o servidor local de nomes trans-\nmite para o hospedeiro de Alice. Agora, o navegador de Alice pode iniciar uma conexão TCP com o hospedeiro \n212.212.71.4 e enviar uma requisição HTTP pela conexão. Ufa! Acontecem muito mais coisas do que perce-\nbemos quando navegamos na Web!\n2.6  Aplicações P2P\nTodas as aplicações descritas neste capítulo até agora — inclusive a Web, e-mail e DNS — empregam ar-\nquiteturas cliente-servidor com dependência significativa em servidores com infraestrutura que sempre perma-\nnecem ligados. Como consta na Seção 2.1.1, com uma arquitetura P2P, há dependência mínima (se houver) de \nservidores com infraestrutura que permanecem sempre ligados. Em vez disso, duplas de hospedeiros intermi-\ntentemente conectados, chamados pares, comunicam-se direto entre si. Os pares não são de propriedade de um \nprovedor de serviços, mas sim de desktops e laptops controlados por usuários.\nNesta seção, examinaremos três diferentes aplicações que são particularmente bem apropriadas a projetos \nP2P. A primeira é a distribuição de arquivos, em que a aplicação distribui um arquivo a partir de uma única fonte \npara um grande número de pares. A distribuição de arquivos é um bom local para iniciar a investigação de P2P, \nvisto que expõe com clareza a autoescalabilidade de arquiteturas P2P. Como exemplo específico para distribuição \nde arquivos, descreveremos o popular sistema BitTorrent. A segunda aplicação P2P que examinaremos é um \nbanco de dados distribuído em uma grande comunidade de pares. Para essa aplicação, exploraremos o conceito \nde uma Distributed Hash Table (DHT).\n2.6.1  Distribuição de arquivos P2P\nComeçaremos nossa investida em P2P considerando uma aplicação bastante natural, ou seja, a distribui-\nção de um grande arquivo a partir de um único servidor a grande número de hospedeiros (chamados pares). O \narquivo pode ser uma nova versão do sistema operacional Linux, um patch de software para um sistema opera-\ncional ou aplicação, um arquivo de música MP3 ou um arquivo de vídeo MPEG. Em uma distribuição de arquivo \ncliente-servidor, o servidor deve enviar uma cópia para cada par — colocando um enorme fardo sobre o servidor \ne consumindo grande quantidade de banda deste. Na distribuição de arquivos P2P, cada par pode redistribuir \nqualquer parte do arquivo recebido para outros pares, auxiliando, assim, o servidor no processo de distribuição. \nHoje (em 2013), o protocolo de distribuição de arquivos P2P mais popular é o BitTorrent. Desenvolvido no iní-\ncio por Bram Cohen, há agora muitos diferentes clientes independentes de BitTorrent conformes ao protocolo \ndo BitTorrent, assim como há diversos clientes de navegadores Web conformes ao protocolo HTTP. Nesta sub-\nseção, examinaremos primeiro a autoescalabilidade de arquiteturas P2P no contexto de distribuição de arquivos. \nEntão, descreveremos o BitTorrent em certo nível de detalhes, destacando suas características mais importantes.\nEscalabilidade de arquiteturas P2P\nPara comparar arquiteturas cliente-servidor com arquiteturas P2P, e ilustrar a inerente autoescalabilidade \nde P2P, consideraremos um modelo quantitativo simples para a distribuição de um arquivo a um conjunto fixo \nCAMADA  de APLICAÇÃO  107 \nde pares para ambos os tipos de arquitetura. Conforme demonstrado na Figura 2.24, o servidor e os pares são \nconectados por enlaces de acesso da Internet. A taxa de upload do enlace de acesso do servidor é denotada por \nus, e a de upload do enlace de acesso do par i é denotada por ui, e a taxa de download do enlace de acesso do \npar i é denotada por di. O tamanho do arquivo a ser distribuído (em bits) é indicado por F e o número de pares \nque querem obter uma cópia do arquivo, por N. O tempo de distribuição é o tempo necessário para que todos \nos N pares obtenham uma cópia do arquivo. Em nossa análise do tempo de distribuição a seguir, tanto para a \narquitetura cliente-servidor como para a arquitetura P2P, fazemos a hipótese simplificada (e em geral precisa \n[Akella, 2003]) de que o núcleo da Internet tem largura de banda abundante, o que implica que todos os garga-\nlos encontram-se no acesso à rede. Suponha também que o servidor e os clientes não participam de nenhuma \noutra aplicação de rede, para que toda sua largura de banda de acesso de upload e download possa ser totalmente \ndedicada à distribuição do arquivo.\nDeterminemos primeiro o tempo de distribuição para a arquitetura cliente-servidor, que indicaremos por \nDcs. Na arquitetura cliente-servidor, nenhum dos pares auxilia na distribuição do arquivo. Fazemos as observa-\nções a seguir:\nO servidor deve transmitir uma cópia do arquivo a cada um dos N pares. Assim, o servidor deve transmitir \nNF bits. Como a taxa de upload do servidor é de us, o tempo para distribuição do arquivo deve ser de pelo menos \nNF/us.\nSuponha que dmin indique a taxa de download do par com menor taxa de download, ou seja, dmin = min{d1, \ndp, ..., dN}. O par com a menor taxa de download não pode obter todos os bits F do arquivo em menos de F/dmin \nsegundos. Assim, o tempo de distribuição mínimo é de pelo menos F/dmin.\nReunindo essas observações, temos:\nbandwidth, implying that all of the bottlenecks are in access networks. We also sup-\npose that the server and clients are not participating in any other network applica-\ntions, so that all of their upload and download access bandwidth can be fully\ndevoted to distributing this file.\nLet’s first determine the distribution time for the client-server architecture,\nwhich we denote by Dcs. In the client-server architecture, none of the peers aids in\ndistributing the file. We make the following observations:\n•\nThe server must transmit one copy of the file to each of the N peers. Thus the\nserver must transmit NF bits. Since the server’s upload rate is us, the time to dis-\ntribute the file must be at least NF/us.\n•\nLet dmin denote the download rate of the peer with the lowest download rate, that\nis, dmin = min{d1,dp,...,dN}. The peer with the lowest download rate cannot\nobtain all F bits of the file in less than F/dmin seconds. Thus the minimum distri-\nbution time is at least F/dmin.\nPutting these two observations together, we obtain\n.\nDcs Ú maxb NF\nus\n, F\ndmin\nr\n146\nCHAPTER 2\n•\nAPPLICATION LAYER\nInternet\nFile: F\nServer\nus\nu1\nu2\nu3\nd1\nd2\nd3\nu4\nu5\nu6\nd4\nd5\nd6\nuN\ndN\n \nFigure 2.24 \u001f An illustrative file distribution problem\nURO6201_06_SE_C02_PP2.qxd  1/10/12  9:45 AM  Page 146\nIsso proporciona um limite inferior para o tempo mínimo de distribuição para a arquitetura cliente-servi-\ndor. Nos problemas do final do capítulo, você deverá demonstrar que o servidor pode programar suas transmis-\nFigura 2.24  Um problema ilustrativo de distribuição de arquivo\nInternet\nArquivo: F\nServidor\nus\nu1\nu2\nu3\nd1\nd2\nd3\nu4\nu5\nu6\nd4\nd5\nd6\nuN\ndN\n   Redes de computadores e a Internet\n108\nsões de forma que o limite inferior seja sempre alcançado. Portanto, consideraremos esse limite inferior fornecido \nantes como o tempo real de distribuição, ou seja,\n\t\nThis provides a lower bound on the minimum distribution time for the client-server\narchitecture. In the homework problems you will be asked to show that the server\ncan schedule its transmissions so that the lower bound is actually achieved. So let’s\ntake this lower bound provided above as the actual distribution time, that is,\n(2.1)\nWe see from Equation 2.1 that for N large enough, the client-server distribution time\nis given by NF/us. Thus, the distribution time increases linearly with the number of\npeers N. So, for example, if the number of peers from one week to the next increases\na thousand-fold from a thousand to a million, the time required to distribute the file\nto all peers increases by 1,000.\nLet’s now go through a similar analysis for the P2P architecture, where each\npeer can assist the server in distributing the file. In particular, when a peer receives\nsome file data, it can use its own upload capacity to redistribute the data to other\npeers. Calculating the distribution time for the P2P architecture is somewhat more\ncomplicated than for the client-server architecture, since the distribution time\ndepends on how each peer distributes portions of the file to the other peers. Never-\ntheless, a simple expression for the minimal distribution time can be obtained\n[Kumar 2006]. To this end, we first make the following observations:\n•\nAt the beginning of the distribution, only the server has the file. To get this file\ninto the community of peers, the server must send each bit of the file at least once\ninto its access link. Thus, the minimum distribution time is at least F/us. (Unlike\nthe client-server scheme, a bit sent once by the server may not have to be sent by\nthe server again, as the peers may redistribute the bit among themselves.)\n•\nAs with the client-server architecture, the peer with the lowest download rate\ncannot obtain all F bits of the file in less than F/dmin seconds. Thus the minimum\ndistribution time is at least F/dmin.\n•\nFinally, observe that the total upload capacity of the system as a whole is equal\nto the upload rate of the server plus the upload rates of each of the individual\npeers, that is, utotal = us + u1 + … + uN. The system must deliver (upload) F bits\nto each of the N peers, thus delivering a total of NF bits. This cannot be done at a\nrate faster than utotal. Thus, the minimum distribution time is also at least\nNF/(us + u1 + … + uN).\nPutting these three observations together, we obtain the minimum distribution time\nfor P2P, denoted by DP2P.\n(2.2)\nDP2P Ú max c\nF\nus\n, F\ndmin\n, \nNF\nus + a\nN\ni=1\nui\ns\nDcs = maxb NF\nus\n, F\ndmin\nr\n2.6\n•\nPEER-TO-PEER APPLICATIONS\n147\n\t\n\t\n(2.1)\nVemos, a partir da Equação 2.1, que para N grande o suficiente, o tempo de distribuição cliente-servidor é \ndado por NF/us. Assim, o tempo de distribuição aumenta linearmente com o número de pares N. Portanto, por \nexemplo, se o número de pares de uma semana para a outra for multiplicado por mil, de mil para um milhão, o \ntempo necessário para distribuir o arquivo para todos os pares aumentará mil vezes.\nPassemos agora para uma análise semelhante para a arquitetura P2P, em que cada par pode auxiliar o ser-\nvidor na distribuição do arquivo. Em particular, quando um par recebe alguns dados do arquivo, ele pode usar \nsua própria capacidade de upload para redistribuir os dados a outros pares. Calcular o tempo de distribuição \npara a arquitetura P2P é, de certa forma, mais complicado do que para a arquitetura cliente-servidor, visto que o \ntempo de distribuição depende de como cada par distribui parcelas do arquivo aos outros pares. Não obstante, \numa simples expressão para o tempo mínimo de distribuição pode ser obtida [Kumar, 2006]. Para essa finalidade, \nfaremos as observações a seguir:\n• No início da distribuição, apenas o servidor tem o arquivo. Para levar esse arquivo à comunidade de \npares, o servidor deve enviar cada bit do arquivo pelo menos uma vez para seu enlace de acesso. Assim, \no tempo de distribuição mínimo é de pelo menos F/us. (Diferente do esquema cliente-servidor, um bit \nenviado uma vez pelo servidor pode não precisar ser enviado novamente, visto que os pares podem re-\ndistribuir entre si esse bit.)\n• Assim como na arquitetura cliente-servidor, o par com a menor taxa de download não pode obter todos \nos bits F do arquivo em menos de F/dmin segundos. Assim, o tempo mínimo de distribuição é de pelo \nmenos F/dmin.\n• Por fim, observemos que a capacidade de upload total do sistema como um todo é igual à taxa de upload \ndo servidor mais as taxas de upload de cada par, ou seja, utotal = us + u1 + ... + uN. O sistema deve entregar \n(fazer o upload de) F bits para cada um dos N pares, entregando assim um total de NF bits. Isso não pode \nser feito em uma taxa mais rápida do que utotal. Assim, o tempo mínimo de distribuição é também de pelo \nmenos NF/(us + u1 + ... + uN).\nJuntando as três observações, obtemos o tempo mínimo de distribuição para P2P, indicado por DP2P.\n\t\nThis provides a lower bound on the minimum distribution time for the client-server\narchitecture. In the homework problems you will be asked to show that the server\ncan schedule its transmissions so that the lower bound is actually achieved. So let’s\ntake this lower bound provided above as the actual distribution time, that is,\n(2.1)\nWe see from Equation 2.1 that for N large enough, the client-server distribution time\nis given by NF/us. Thus, the distribution time increases linearly with the number of\npeers N. So, for example, if the number of peers from one week to the next increases\na thousand-fold from a thousand to a million, the time required to distribute the file\nto all peers increases by 1,000.\nLet’s now go through a similar analysis for the P2P architecture, where each\npeer can assist the server in distributing the file. In particular, when a peer receives\nsome file data, it can use its own upload capacity to redistribute the data to other\npeers. Calculating the distribution time for the P2P architecture is somewhat more\ncomplicated than for the client-server architecture, since the distribution time\ndepends on how each peer distributes portions of the file to the other peers. Never-\ntheless, a simple expression for the minimal distribution time can be obtained\n[Kumar 2006]. To this end, we first make the following observations:\n•\nAt the beginning of the distribution, only the server has the file. To get this file\ninto the community of peers, the server must send each bit of the file at least once\ninto its access link. Thus, the minimum distribution time is at least F/us. (Unlike\nthe client-server scheme, a bit sent once by the server may not have to be sent by\nthe server again, as the peers may redistribute the bit among themselves.)\n•\nAs with the client-server architecture, the peer with the lowest download rate\ncannot obtain all F bits of the file in less than F/dmin seconds. Thus the minimum\ndistribution time is at least F/dmin.\n•\nFinally, observe that the total upload capacity of the system as a whole is equal\nto the upload rate of the server plus the upload rates of each of the individual\npeers, that is, utotal = us + u1 + … + uN. The system must deliver (upload) F bits\nto each of the N peers, thus delivering a total of NF bits. This cannot be done at a\nrate faster than utotal. Thus, the minimum distribution time is also at least\nNF/(us + u1 + … + uN).\nPutting these three observations together, we obtain the minimum distribution time\nfor P2P, denoted by DP2P.\n(2.2)\nDP2P Ú max c\nF\nus\n, F\ndmin\n, \nNF\nus + a\nN\ni=1\nui\ns\nDcs = maxb NF\nus\n, F\ndmin\nr\n2.6\n•\nPEER-TO-PEER APPLICATIONS\n147\n\t\n(2.2)\nA Equação 2.2 fornece um limite inferior para o tempo mínimo de distribuição para a arquitetura P2P. \nOcorre que, se imaginarmos que cada par pode redistribuir um bit assim que o recebe, há um esquema de redis-\ntribuição que, de fato, alcança esse limite inferior [Kumar, 2006] (Provaremos um caso especial desse resultado \nnos exercícios.) Na realidade, quando blocos do arquivo são redistribuídos, em vez de bits individuais, a Equação \n2.2 serve como uma boa aproximação do tempo mínimo real de distribuição. Assim, peguemos o limite inferior \nfornecido pela Equação 2.2 como o tempo mínimo real de distribuição, que é:\nCAMADA  de APLICAÇÃO  109 \n\t\nsoon as it receives the bit, then there is a redistribution scheme that actually achieves\nthis lower bound [Kumar 2006]. (We will prove a special case of this result in the\nhomework.) In reality, where chunks of the file are redistributed rather than individ-\nual bits, Equation 2.2 serves as a good approximation of the actual minimum distri-\nbution time. Thus, let’s take the lower bound provided by Equation 2.2 as the actual\nminimum distribution time, that is,\n(2.3)\nFigure 2.25 compares the minimum distribution time for the client-server and\nP2P architectures assuming that all peers have the same upload rate u. In Figure\n2.25, we have set F/u = 1 hour, us = 10u, and dmin ≥us. Thus, a peer can transmit the\nentire file in one hour, the server transmission rate is 10 times the peer upload rate,\nand (for simplicity) the peer download rates are set large enough so as not to have\nan effect. We see from Figure 2.25 that for the client-server architecture, the dis-\ntribution time increases linearly and without bound as the number of peers\nincreases. However, for the P2P architecture, the minimal distribution time is not\nonly always less than the distribution time of the client-server architecture; it is also\nless than one hour for any number of peers N. Thus, applications with the P2P\narchitecture can be self-scaling. This scalability is a direct consequence of peers\nbeing redistributors as well as consumers of bits.\nDP2P = max c\nF\nus\n, F\ndmin\n, \nNF\nus + a\nN\ni=1\nui\ns\n0\n5\n10\n15\n20\n25\n30\n0\nN\nMinimum distributioin tiime\n35\n0.5\n1.5\n2.5\n1.0\n3.0\n2.0\n3.5\nClient-Server\nP2P\nFigure 2.25 \u001f Distribution time for P2P and client-server architectures\n\t\n(2.3)\nA Figura 2.25 compara o tempo mínimo de distribuição para as arquiteturas cliente-servidor e P2P, pressu-\npondo que todos os pares têm a mesma taxa de upload u. Na Figura 2.25, definimos que F/u = 1 hora, uS = 10u e \ndmín ≥ uS. Assim, um par pode transmitir todo o arquivo em uma hora, sendo a taxa de transmissão do servidor 10 \nvezes a taxa de upload do par, e (para simplificar) as taxas de download de par são definidas grandes o suficiente \nde forma a não ter efeito. Vemos na Figura 2.25 que, para a arquitetura cliente-servidor, o tempo de distribuição \naumenta linearmente e sem limite, conforme cresce o número de pares. No entanto, para a arquitetura P2P, o \ntempo mínimo de distribuição não é apenas sempre menor do que o tempo de distribuição da arquitetura clien-\nte-servidor; é também de menos do que uma hora para qualquer número de pares N. Assim, aplicações com \na arquitetura P2P podem ter autoescalabilidade. Tal escalabilidade é uma consequência direta de pares sendo \nredistribuidores, bem como consumidores de bits.\nBitTorrent\nO BitTorrent é um protocolo P2P popular para distribuição de arquivos [Chao, 2011]. No jargão do BitTor-\nrent, a coleção de todos os pares que participam da distribuição de um determinado arquivo é chamada de tor-\nrent. Os pares em um torrent fazem o download de blocos de tamanho igual do arquivo entre si, com um tamanho \ntípico de bloco de 256 KBytes. Quando um par entra em um torrent, ele não tem nenhum bloco. Com o tempo, \nele acumula mais blocos. Enquanto ele faz o download de blocos, faz também uploads de blocos para outros pares. \nUma vez que um par adquire todo o arquivo, ele pode (de forma egoísta) sair do torrent ou (de forma altruísta) \npermanecer e continuar fazendo o upload a outros pares. Além disso, qualquer par pode sair do torrent a qual-\nquer momento com apenas um subconjunto de blocos, e depois voltar.\nObservemos agora, mais atentamente, como opera o BitTorrent. Como é um protocolo e sistema compli-\ncado, descreveremos apenas seus mecanismos mais importantes, ignorando alguns detalhes; isso nos permitirá \nver a floresta através das árvores. Cada torrent tem um nó de infraestrutura chamado rastreador. Quando um par \nchega em um torrent, ele se registra com o rastreador e periodicamente informa ao rastreador que ainda está lá. \nFigura 2.25  Tempo de distribuição para arquiteturas P2P e cliente-servidor\n0\n5\n10\n15\n20\n25\n30\n0\nN\nTempo mínimo de distribuição\n35\n0,5\n1,5\n2,5\n1,0\n3,0\n2,0\n3,5\nCliente-Servidor\nP2P\n   Redes de computadores e a Internet\n110\nDessa forma, o rastreador mantém um registro dos pares que participam do torrent. Um determinado torrent \npode ter menos de dez ou mais de mil pares participando a qualquer momento.\nComo demonstrado na Figura 2.26, quando um novo par, Alice, chega, o rastreador seleciona aleatoriamen-\nte um subconjunto de pares (para dados concretos, digamos que sejam 50) do conjunto de pares participantes, e \nenvia os endereços IP desses 50 pares a Alice. Com a lista de pares, ela tenta estabelecer conexões TCP simultâ-\nneas com todos. Chamaremos todos os pares com quem Alice consiga estabelecer uma conexão TCP de “pares \nvizinhos”\n. (Na Figura 2.26, Alice é representada com apenas três pares vizinhos. Normalmente, ela teria muito \nmais.) Com o tempo, alguns desses pares podem sair e outros pares (fora dos 50 iniciais) podem tentar estabele-\ncer conexões TCP com Alice. Portanto, os pares vizinhos de um par podem flutuar com o tempo.\nA qualquer momento, cada par terá um subconjunto de blocos do arquivo, com pares diferentes com sub-\nconjuntos diferentes. De tempos em tempos, Alice pedirá a cada um de seus pares vizinhos (nas conexões TCP) \na lista de quais blocos eles têm. Caso Alice tenha L vizinhos diferentes, ela obterá L listas de blocos. Com essa \ninformação, Alice emitirá solicitações (novamente, nas conexões TCP) de blocos que ela não tem.\nPortanto, a qualquer momento, Alice terá um subconjunto de blocos e saberá quais blocos seus vizinhos \ntêm. Com essa informação, ela terá duas decisões importantes a fazer. Primeiro, quais blocos deve solicitar de iní-\ncio a seus vizinhos, e segundo, a quais vizinhos deve enviar os blocos solicitados. Ao decidir quais blocos solicitar, \nAlice usa uma técnica chamada rarest first (o mais raro primeiro). A ideia é determinar, dentre os blocos que \nela não tem, quais são os mais raros dentre seus vizinhos (ou seja, os blocos que têm o menor número de cópias \nrepetidas) e então solicitar esses blocos mais raros primeiro. Dessa forma, os blocos mais raros são redistribuídos \nmais depressa, procurando (grosso modo) equalizar os números de cópias de cada bloco no torrent.\nPara determinar a quais pedidos atender, o BitTorrent usa um algoritmo de troca inteligente. A ideia bá-\nsica é Alice dar prioridade aos vizinhos que estejam fornecendo seus dados com a maior taxa. Especificamente, \npara cada vizinho, Alice mede de maneira contínua a taxa em que recebe bits e determina os quatro pares que \nlhe fornecem na taxa mais alta. Então, ela reciprocamente envia blocos a esses mesmos quatro pares. A cada \n10 s, ela recalcula as taxas e talvez modifique o conjunto de quatro pares. No jargão do BitTorrent, esses quatro \npares são chamados de unchoked (não sufocado). É importante informar que a cada 30 s ela também escolhe \nFigura 2.26  Distribuição de arquivos com o BitTorrent\nRastreador\nTroca de blocos\nPar\nObter \nlista de \npares\nAlice\nCAMADA  de APLICAÇÃO  111 \num vizinho adicional ao acaso e envia blocos a ele. Chamaremos o vizinho escolhido de Bob. No jargão de Bit-\nTorrent, Bob é chamado de otimisticamente não sufocado. Como Alice envia dados a Bob, ela pode se tornar \num dos quatro melhores transmissores para Bob, caso em que ele começaria a enviar dados para Alice. Caso \na taxa em que Bob envie dados seja alta o suficiente, ele pode, em troca, tornar-se um dos quatro melhores \ntransmissores para Alice. Em outras palavras, a cada 30 s, Alice escolherá ao acaso um novo parceiro de troca \ne a começará com ele. Caso os dois pares estejam satisfeitos com a troca, eles colocarão um ao outro nas suas \nlistas de quatro melhores pares e continuarão a troca até que um dos pares encontre um parceiro melhor. O \nefeito é que pares capazes de fazer uploads em taxas compatíveis tendem a se encontrar. A seleção aleatória de \nvizinho também permite que novos pares obtenham blocos, de forma que possam ter algo para trocar. Todos \nos pares vizinhos, além desses cinco pares (quatro pares “top” e um em experiência) estão “sufocados”\n, ou seja, \nnão recebem nenhum bloco de Alice. O BitTorrent tem diversos mecanismos interessantes não discutidos \naqui, incluindo pedaços (miniblocos), pipelining (tubulação), primeira seleção aleatória, modo endgame (fim \nde jogo) e anti-snubbing (antirrejeição) [Cohen 2003].\nO mecanismo de incentivo para troca descrito costuma ser chamado de tit-for-tat (olho por olho) [Chen, \n2003]. Demonstrou-se que esse esquema de incentivo pode ser burlado [Liogkas, 2006; Locher, 2006; Piatek, \n2007]. Não obstante, o ecossistema do BitTorrent é muito bem-sucedido, com milhões de pares simultâneos com-\npartilhando arquivos ativamente em centenas de milhares de torrents. Caso o BitTorrent tivesse sido projetado \nsem o tit-for-tat (ou uma variante), mas com o restante da mesma maneira, ele talvez nem existisse mais, visto \nque a maioria dos usuários são pessoas que apenas querem obter as coisas de graça [Saroiu, 2002].\nVariantes interessantes do protocolo BitTorrent são propostas [Guo, 2005; Piatek, 2007]. Além disso, \nmuitas das aplicações de transmissão em tempo real P2P, como PPLive e ppstream, foram inspiradas pelo \nBitTorrent [Hei, 2007].\n2.6.2  Distributed Hash Tables (DHTs)\nNesta seção, vamos considerar como realizar um banco de dados simples em uma rede P2P. Começamos \ndescrevendo uma versão centralizada desse banco de dados simples, que terá apenas pares (chave, valor). Por \nexemplo, as chaves podem ser números de seguridade social e os valores podem ser nomes humanos correspon-\ndentes; nesse caso, um exemplo de dupla chave-valor é (156-45-7081, Johnny Wu). Ou as duplas podem ser no-\nmes de conteúdo (por exemplo, nomes de filmes, álbuns e software), e os valores podem ser endereços IP onde o \nconteúdo está armazenado; nesse caso, um exemplo de par chave-valor é (Led Zeppelin IV, 203.17.123.38). Pares \nconsultam nossos bancos de dados fornecendo a chave: caso haja duplas (chave, valor) em seus bancos de dados \nque correspondam à chave, o banco de dados retorna as duplas correspondentes ao par solicitante. Portanto, por \nexemplo, se o banco de dados armazenar números de seguridade social e seus nomes humanos correspondentes, \num par pode consultar um número de seguridade social e o banco de dados retornará o nome do humano que \npossui aquele número. Ou então, se o banco de dados armazenar os nomes de conteúdo e seus endereços IP \ncorrespondentes, podemos consultar um nome de conteúdo e o banco de dados retornará os endereços IP que \narmazenam aquele conteúdo.\nBasear em tal banco de dados é simples com a arquitetura cliente-servidor que armazena todos os pares \n(chave, valor) em um servidor central. Assim, nesta seção, vamos considerar, em vez disso, como montar uma \nversão distribuída, P2P, desse banco de dados, que guardará os pares (chave, valor) por milhões. No sistema P2P, \ncada par só manterá um pequeno subconjunto da totalidade (chave, valor). Permitiremos que qualquer par con-\nsulte o banco de dados distribuído com uma chave em particular. O banco de dados distribuído, então, localizará \nos pares que possuem os pares (chave, valor) correspondentes e retornará os pares chave­\n‑valor ao consultante. \nQualquer par também poderá inserir novos pares chave-valor no banco de dados. Esse banco de dados distribuído \né considerado como uma tabela hash distribuída (DHT — Distributed Hash Table).\nAntes de descrever como podemos criar um DHT, primeiro vamos apresentar um exemplo específico de \nserviço DHT no contexto do compartilhamento de arquivos P2P. Neste caso, uma chave é o nome de conteúdo e \n   Redes de computadores e a Internet\n112\no valor é o endereço IP de um par que tem uma cópia do conteúdo. Assim, se Bob e Charlie tiverem cada um uma \ncópia da distribuição Linux mais recente, então o banco de dados DHT incluirá as seguintes duplas de chave-va-\nlor: (Linux, IPBob) e (Linux, IPCharlie). Mais especificamente, como o banco de dados DHT é distribuído pelos pares, \nalgum deles, digamos Dave, será responsável pela chave “Linux” e terá as duplas chave-valor correspondentes. \nAgora, suponha que Alice queira obter uma cópia do Linux. É claro, ela precisa saber primeiro quais pares têm \numa cópia do Linux antes que possa começar a baixá-lo. Para essa finalidade, ela consulta o DHT com “Linux” \ncomo chave. O DHT, então, determina que Dave é responsável pela chave. O DHT entra em contato com Dave, \nobtém dele as duplas chave-valor (Linux, IPBob) e (Linux, IPCharlie), e os passa a Alice. Ela pode, então, baixar a \ndistribuição Linux mais recente a partir de IPBob ou de IPCharlie.\nAgora, vamos retornar ao problema de projetar um DHT para duplas gerais de chave­\n‑valor. Uma técnica \ningênua para a criação de um DHT é espalhar ao acaso as duplas (chave, valor) por todos os pares e fazer cada um \nmanter uma lista dos endereços IP de todos os pares. Nesse esquema, o par consultante envia sua consulta a todos os \noutros, e aqueles contendo as duplas (chave, valor) que combinam com a chave podem responder com suas duplas \ncorrespondentes. Essa técnica é totalmente não escalável, é claro, pois exigiria que cada par não apenas soubesse \nsobre todos os outros (talvez milhões deles!), mas, pior ainda, cada consulta deveria ser enviada a todos os pares.\nAgora descreveremos uma abordagem elegante para o projeto de um DHT. Para isso, primeiro designare-\nmos um identificador a cada par, em que cada identificador é um número inteiro na faixa [0, 2n\n – 1] de algum \nn fixo. Observe que cada identificador pode ser expresso por uma representação com n bits. Vamos também \nexigir que cada chave seja um número inteiro na mesma faixa. O leitor atento pode ter observado que as cha-\nves de exemplo descritas (números de seguridade social e nomes de conteúdo) não são números inteiros. Para \ncriar números inteiros a partir delas, precisaremos usar uma função hash que mapeie cada chave (por exemplo, \nnúmero de seguridade social) em um número inteiro na faixa [0, 2n\n – 1]. Uma função de hash é uma função de \nmuitos-para-um para a qual duas entradas diferentes podem ter a mesma saída (mesmo número inteiro), mas \na probabilidade de terem a mesma saída é extremamente pequena. (Leitores não familiarizados com funções de \nhash podem querer consultar o Capítulo 8, que as discute em detalhes.) A função de hash é considerada publi-\ncamente disponível a todos os pares no sistema. Daqui em diante, quando nos referirmos à “chave”\n, estaremos \nnos referindo ao hash da chave original. Portanto, por exemplo, caso a chave original seja “Led Zeppelin IV”\n, a \nchave usada no DHT será o número inteiro que corresponda ao hash de “Led Zeppelin IV”\n. Como você já deve \nter percebido, é por isso que “Hash” é usado no termo “Distributed Hash Table”\n.\nConsideraremos agora o problema de armazenar as duplas (chave, valor) no DHT. A questão central \naqui é definir uma regra para designar chaves a pares. Considerando que cada par tenha um identificador \nde número inteiro e cada chave também seja um número inteiro na mesma faixa, uma abordagem natural é \ndesignar cada dupla (chave, valor) ao par cujo identificador está mais próximo da chave. Para executar esse es-\nquema, precisaremos definir o que significa “mais próximo”, o que admite muitas convenções. Por conveniên-\ncia, definiremos que o par mais próximo é o sucessor imediato da chave. Para entender melhor, observaremos \num exemplo. Considere que n = 4, portanto, todos os identificadores de par e chave estarão na faixa de [0, 15]. \nSuponha ainda que haja oito pares no sistema com identificadores 1, 3, 4, 5, 8, 10, 12 e 15. Por fim, imagine \nque queiramos armazenar a dupla chave-valor (11, Johnny Wu) em um dos oito pares. Mas em qual? Usando \nnossa convenção de mais próximo, como o par 12 é o sucessor imediato da chave 11, armazenaremos, por-\ntanto, a dupla (11, Johnny Wu) no par 12. [Para concluir nossa definição de mais próximo, caso a chave seja \nidêntica a um dos identificadores do par, armazenaremos a dupla (chave-valor) em um par correspondente; \ne caso seja maior do que todos os identificadores de par, usaremos uma convenção módulo-2n, que armazena \na dupla (chave-valor) no par com o menor identificador.]\nSuponha agora que um par, Alice, queira inserir uma dupla (chave-valor) no DHT. Na concepção, é um \nprocesso objetivo: ela primeiro determina o par cujo identificador é o mais próximo da chave; então envia uma \nmensagem a esse par, instruindo-o a armazenar a dupla (chave, valor). Mas como Alice determina o par mais \npróximo da chave? Se ela rastreasse todos os pares no sistema (IDs de par e endereços IP correspondentes), pode-\nria determinar localmente o par mais próximo. Mas essa abordagem requer que cada par rastreie todos os outros \npares no DHT — o que é completamente impraticável para um sistema de grande escala com milhões de pares.\nCAMADA  de APLICAÇÃO  113 \nDHT circular\nPara tratar desse problema de escala, consideraremos agora organizar os pares em um círculo. Nessa dis-\nposição circular, cada par rastreia apenas seu sucessor e predecessor imediatos (módulo 2n). Um exemplo desse \ncírculo é exibido na Figura 2.27(a). Nesse exemplo, n é novamente 4 e há os mesmos oito pares do exemplo ante-\nrior. Cada par está ciente apenas de seu sucessor e predecessor imediatos; por exemplo, o par 5 sabe o endereço \nIP e o identificador do par 8, mas talvez não saiba nada sobre quaisquer outros pares no DHT. Essa disposição \ncircular dos pares é um caso especial de uma rede sobreposta. Nesse tipo de rede, os pares formam uma rede \nlógica abstrata que reside acima da de computadores “inferior” que consiste de enlaces físicos, roteadores e hos-\npedeiros. Os enlaces em uma rede sobreposta não são físicos, mas enlaces virtuais entre duplas de pares. Na rede \nde sobreposição da Figura 2.27(a), há oito pares e oito enlaces sobrepostos; na sobreposição da Figura 2.27(b) \nhá oito pares e 16 enlaces sobrepostos. Um único enlace de sobreposição em geral usa muitas ligações físicas e \nroteadores físicos na rede inferior.\nUsando a rede de sobreposição circular da Figura 2.27(a), suponha agora que o par 3 deseje determinar \nqual par no DHT é responsável pela chave 11. Usando a rede sobreposta circular, o par de origem (par 3) cria \numa mensagem que pergunta “Quem é responsável pela chave 11?” e a envia no sentido horário ao redor do \ncírculo. Sempre que um par recebe essa mensagem, como conhece o identificador de seu sucessor e predeces-\nsor, pode determinar se é responsável pela (ou seja, mais próximo da) chave em questão. Caso um par não \nseja responsável pela chave, ele apenas envia a mensagem a seu sucessor. Portanto, por exemplo, quando o par \n4 recebe a mensagem perguntando sobre a chave 11, ele determina que não é responsável pela chave (porque \nseu sucessor está mais perto dela), portanto, ele passa a mensagem a seu sucessor, ou seja, o par 5. O processo \ncontinua até que a mensagem chegue ao par 12, que determina que é o mais próximo da chave 11. A essa altura, \no par 12 pode enviar uma mensagem de volta à origem, o par 3, indicando que é responsável pela chave 11.\nO DHT circular oferece uma solução bastante elegante para reduzir a quantidade de informação sobre-\nposta que cada par deve gerenciar. Em particular, cada par está ciente apenas de dois outros, seu sucessor e seu \npredecessor imediato. Porém, essa solução ainda introduz um novo problema. Embora cada par esteja ciente de \ndois pares vizinhos, para encontrar o nó responsável por uma chave (no pior das hipóteses), todos os N nós no \nDHT deverão encaminhar uma mensagem pelo círculo; N/2 mensagens são enviadas em média.\nAssim, no projeto de um DHT, há uma escolha entre o número de vizinhos que cada par tem de rastrear \ne o número de mensagens que o DHT precisa enviar para resolver uma única solicitação. Por um lado, se cada \npar rastrear todos os outros (sobreposições de malha), apenas uma mensagem será enviada por solicitação, \nmas cada par deverá rastrear N pares. Por outro lado, com um DHT circular, cada par está ciente apenas de \ndois, mas N/2 mensagens são enviadas em média por solicitação. Felizmente, podemos refinar nossos proje-\nFigura 2.27  \u0007\n(a) Um DHT circular. O par 3 quer determinar quem É responsável pela chave 11. (b) \nUm DHT circular com atalhos\n1\n3\nQuem é \nresponsável \npela chave 11?\n4\n5\n8\na.\nb.\n10\n12\n15\n1\n3\n4\n5\n8\n10\n12\n15\n   Redes de computadores e a Internet\n114\ntos de DHTs de forma que o número de vizinhos por par, bem como o número de mensagens por solicitação \nseja mantido em um tamanho aceitável. Um desses refinamentos é usar a rede sobreposta circular como fun-\ndação, mas adicionar “atalhos” de forma que cada par não apenas rastreie seu sucessor imediato, mas também \num número relativamente pequeno de pares espalhados pelo círculo. Um exemplo de DHT circular com al-\nguns atalhos é mostrado na Figura 2.27(b). Atalhos são usados para expedir o roteamento das mensagens de \nsolicitação. Especificamente, quando um par recebe uma mensagem que solicita uma chave, ele encaminha a \nmensagem ao vizinho (sucessor ou um dos vizinhos via atalho) que está mais perto da chave. Assim, na Figu-\nra 2.27(b), quando o par 4 recebe a mensagem solicitando a chave 11, ele determina que o par mais próximo \n(entre seus vizinhos) é seu vizinho no atalho 10 e então envia a mensagem diretamente ao par 10. É claro que \natalhos podem reduzir de modo significativo o número de mensagens usado para processar uma solicitação.\nA próxima questão natural é “Quantos vizinhos no atalho cada par deve ter, e quais pares devem ser esses \nvizinhos de atalho?”\n. A pergunta recebeu atenção significativa da comunidade de pesquisa [Balakrishnan, 2003; \nAndroutsellis-Theotokis, 2004]. De forma importante, demonstrou-se que o DHT pode ser projetado para que \ntanto o número de vizinhos como o de mensagens por solicitação seja da ordem de O(log N), em que N é o \nnúmero de pares. Esses projetos obtêm um compromisso satisfatório entre as soluções extremas de se usar to-\npologias de sobreposição circular e de malha.\nPeer Churn\nEm sistemas P2P, um par pode vir ou ir sem aviso. Assim, no projeto de um DHT, devemos nos preocupar \nem manter a sobreposição de DHT na presença desse peer churn. Para termos uma compreensão abrangente \nde como isso pode ser realizado, consideraremos mais uma vez o DHT circular da Figura 2.27(a). Para tratar \ndo peer churn, exigiremos que cada par rastreie (ou seja, saiba o endereço IP de) seu primeiro e segundo su-\ncessores; por exemplo, o par 4 agora rastreia tanto o 5 como o 8. Exigiremos também que cada par verifique de \ntempos em tempos se seus dois sucessores estão vivos (por exemplo, enviando mensagens de ping e pedindo \nrespostas). Consideraremos agora como o DHT é mantido quando um par sai bruscamente. Por exemplo, su-\nponha que o par 5 da Figura 2.27(a) saia de modo abrupto. Nesse caso, os dois pares precedentes ao que saiu (4 \ne 3) saberão que o par saiu, pois não responde mais às mensagens de ping. Os pares 4 e 3 precisam, portanto, \natualizar as informações do estado de seu sucessor. Consideraremos agora como o par 4 atualiza seu estado:\n1.\t O par 4 substitui seu primeiro sucessor (par 5) por seu segundo sucessor (par 8).\n2.\t O par 4, então, pergunta a seu novo primeiro sucessor (par 8) o identificador e o endereço IP de seu su-\ncessor imediato (par 10). O par 4, então, torna o par 10 seu segundo sucessor.\nNos problemas, você deverá determinar como o par 3 atualiza suas informações de determinação de ro-\nteamento de sobreposição.\nTendo abordado de modo sucinto o que deve ser feito quando um par sai, consideraremos agora o que \nacontece quando um par quer entrar no DHT. Digamos que um par com identificador 13 quer entrar, e quando \nentra, sabe apenas da existência do par 1 no DHT. O par 13 primeiro envia ao par 1 uma mensagem, perguntan-\ndo “quem serão o predecessor e o sucessor do par 13?”\n. Essa mensagem é encaminhada pelo DHT até alcançar \no par 12, que percebe que será o predecessor do par 13, e que seu sucessor, o par 15, será o sucessor do par 13. \nEm seguida, o par 12 envia as informações de sucessor e predecessor ao par 13. Este, então, pode entrar no DHT, \ntornando o par 15 seu sucessor e notificando ao par 12 que deve mudar seu sucessor imediato para 13.\nDHTs têm amplo uso na prática. Por exemplo, o BitTorrent usa o DHT Kademlia para criar um rastreador distri-\nbuído. No BitTorrent, a chave é o identificador do torrent e o valor é o endereço IP dos pares que atualmente participam \ndos torrents [Falkner, 2007; Neglia, 2007]. Dessa forma, solicitando ao DHT um identificador de torrent, um par de \nBitTorrent recém­\n‑chegado pode determinar o par responsável pelo identificador (ou seja, por determinar os pares no \ntorrent). Após ter encontrado esse par, o recém-chegado pode solicitar dele uma lista de outros pares no torrent.\nCAMADA  de APLICAÇÃO  115 \n2.7  Programação de sockets: criando aplicações de rede\nAgora que já examinamos várias importantes aplicações de rede, vamos explorar como são escritos progra-\nmas de aplicação de rede. Lembre-se de que na Seção 2.1 dissemos que muitas aplicações de rede consistem em \num par de programas — um programa cliente e um programa servidor — que residem em dois sistemas finais di-\nferentes. Quando são executados, criam-se um processo cliente e um processo servidor, que se comunicam entre si \nlendo de seus sockets e escrevendo através deles. Ao criar uma aplicação de rede, a tarefa principal do programador \né escrever o código tanto para o programa cliente como para o programa servidor.\nHá dois tipos de aplicações de rede. Um deles é uma execução cuja operação é especificada em um padrão \nde protocolo, por exemplo, em um RFC ou algum outro documento de padrões; essa aplicação às vezes é deno-\nminada “aberta”\n, pois as regras especificando sua operação são conhecidas de todos. Para essa implementação, \nos programas, cliente e servidor, devem obedecer às regras ditadas pelo RFC. Por exemplo, o programa cliente \npoderia ser uma execução do lado do cliente do protocolo FTP descrito na Seção 2.3 e definido explicitamente no \nRFC 959 e o programa servidor, uma implementação do protocolo de servidor FTP também descrito de modo \nexplícito no RFC 959. Se um programador escrever um código para o programa cliente e outro programador \nindependente escrever um código para o programa servidor e ambos seguirem com atenção as regras do RFC, \nentão os dois programas poderão interagir. De fato, muitas das aplicações de rede de hoje envolvem comunicação \nentre programas cliente e servidor que foram criados por programadores diferentes — por exemplo, um navega-\ndor Firefox que se comunica com um servidor Web Apache, ou um cliente BitTorrent que se comunica com um \nrastreador BitTorrent.\nO outro tipo de aplicação de rede é uma aplicação de rede proprietária. Nesse caso, os programas cliente \ne servidor empregam um protocolo de camada de aplicação que não foi publicado abertamente em um RFC ou \nem outro lugar. Um único programador (ou equipe de desenvolvimento) cria ambos os programas cliente e ser-\nvidor, e tem completo controle sobre o que entra no código. Mas, como o código não implementa um protocolo \nde domínio público, outros programadores independentes não poderão desenvolver código que interage com a \naplicação.\nNesta seção e na próxima, examinaremos as questões fundamentais do desenvolvimento de uma aplicação \ncliente-servidor, e “sujaremos nossas mãos” examinando o código que executa uma aplicação cliente-servidor \nmuito simples. Durante a fase de desenvolvimento, uma das primeiras decisões que o programador deve tomar \né se a aplicação rodará em TCP ou UDP. Lembre-se de que o TCP é orientado para conexão e provê um canal \nconfiável de cadeia de bytes, pelo qual fluem dados entre dois sistemas finais. O UDP não é orientado para co-\nnexão e envia pacotes de dados independentes de um sistema final ao outro, sem nenhuma garantia de entrega. \nLembre-se também que, quando um programa, cliente ou servidor, executa um protocolo definido em um RFC, \ndeve usar o número de porta conhecido associado com o protocolo; por outro lado, ao desenvolver uma apli-\ncação proprietária, o programador deve ter o cuidado de evitar esses números de porta conhecidos. (Números \nde portas foram discutidos brevemente na Seção 2.1. Serão examinados mais detalhadamente no Capítulo 3.)\nApresentamos a programação de sockets UDP e TCP por meio de aplicações UDP e TCP simples em Py-\nthon. Poderíamos escrevê-las em linguagem Java, C ou C++, mas optamos por Python por diversas razões, prin-\ncipalmente porque Python expõe com clareza os conceitos principais de sockets. Com Python, há menos linhas \nde codificação e cada uma delas pode ser explicada a programadores iniciantes sem muita dificuldade. Mas não \nprecisa ficar assustado se não estiver familiarizado com a linguagem. Você conseguirá acompanhar o código com \nfacilidade se tiver experiência de programação em Java, C ou C++.\nSe estiver interessado em programação cliente-servidor em linguagem Java, veja o site de apoio que acom-\npanha este livro; lá você poderá achar todos os exemplos desta seção (e laboratórios associados) em Java. Para os \ninteressados em programação cliente-servidor em C, há várias boas referências à disposição [Donahoo, 2001; Ste-\nvens, 1997; Frost, 1994; Kurose, 1996]; nossos exemplos em Python a seguir possuem um estilo semelhante a C.\n   Redes de computadores e a Internet\n116\n2.7.1  Programação de sockets com UDP\nNesta subseção, vamos escrever programas cliente-servidor simples que usam UDP; na próxima, escrevere-\nmos programas semelhantes que usam TCP.\nComentamos na Seção 2.1 que processos que rodam em máquinas diferentes se comunicam entre si en-\nviando mensagens para sockets. Dissemos que cada processo é semelhante a uma casa e que o socket do processo \né semelhante a uma porta. A aplicação reside em um lado da porta na casa; o protocolo da camada de transporte \nreside no outro lado da porta, no mundo exterior. O programador da aplicação controla tudo que está no lado da \ncamada de aplicação da porta; contudo, tem pouco controle do lado da camada de transporte.\nAgora, vejamos mais de perto a interação entre dois processos que se comunicam, que utilizam sockets UDP. \nAntes que o processo emissor consiga empurrar um pacote de dados pela porta do socket, ao usar UDP, ele deve \nprimeiro incluir um endereço de destino no pacote. Depois que o pacote passa pelo socket do emissor, a Inter-\nnet usará esse endereço de destino para rotear o pacote pela Internet até o socket no processo receptor. Quando \no pacote chega no socket receptor, o processo receptor apanha o pacote através do socket e depois inspeciona o \nconteúdo do pacote e toma a ação apropriada.\nAssim, você pode agora querer saber: o que há no endereço de destino que é acrescentado ao pacote? Como \né de se esperar, o endereço IP do hospedeiro de destino faz parte do endereço de destino. Ao incluir o endereço IP \ndo destino no pacote, os roteadores na Internet poderão rotear o pacote pela Internet até o hospedeiro de destino. \nMas, como o hospedeiro pode estar rodando muitos processos de aplicação de rede, cada um com um ou mais \nsockets, também é preciso identificar o socket em particular no hospedeiro de destino. Quando um socket é criado, \num identificador, chamado número de porta, é designado para ele. Assim, como é de se esperar, o endereço de \ndestino do pacote também inclui o número de porta do socket. Resumindo, o processo emissor inclui no pacote \num endereço de destino que consiste no endereço IP do hospedeiro de destino e o número de porta do socket de \ndestino. Além do mais, como veremos em breve, o endereço de origem do emissor — consistindo no endereço IP \ndo hospedeiro de destino e o número de porta do socket de origem — também é acrescentado ao pacote. Porém, a \ninclusão do endereço de origem ao pacote normalmente não é feita pelo código da aplicação UDP; em vez disso, \nela é feita automaticamente pelo sistema operacional.\nUsaremos a aplicação cliente-servidor simples a seguir para demonstrar a programação de socket para UDP \ne TCP:\n1.\t Um cliente lê uma linha de caracteres (dados) do teclado e a envia para o servidor.\n2.\t O servidor recebe os dados e converte os caracteres para maiúsculas.\n3.\t O servidor envia os dados modificados ao cliente.\n4.\t O cliente recebe os dados modificados e apresenta a linha em sua tela.\nA Figura 2.28 destaca a principal atividade relacionada ao socket realizada pelo cliente e pelo servidor, que \nse comunicam por meio de um serviço de transporte.\nAgora vamos pôr as mãos na massa e dar uma olhada no par de programas cliente-servidor para uma im-\nplementação UDP dessa aplicação de exemplo. Também oferecemos uma análise detalhada, linha a linha, após \ncada programa. Começamos com um cliente UDP, que enviará uma mensagem simples, em nível de aplicação, ao \nservidor. Para que o servidor possa receber e responder à mensagem do cliente, ele precisa estar pronto e rodando \n— ou seja, precisa estar rodando como um processo antes que o cliente envie sua mensagem.\nO programa cliente é denominado UDPClient.py e o programa servidor é denominado UDPServer.py. \nPara enfatizar os principais pontos, oferecemos intencionalmente um código que seja mínimo. Um “código bom” \ncertamente teria muito mais linhas auxiliares, particularmente para tratar de casos de erro. Para esta aplicação, \nescolhemos arbitrariamente 12000 para o número de porta do servidor.\nCAMADA  de APLICAÇÃO  117 \nFigura 2.28  A aplicação cliente-servidor usando UDP\nKR 02.28.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p0 Wide x 23p10 Deep\n9/6/11, 9/13/11, 11/23/11 rossi\nCriar  socket, port=x:\nServidor\nserverSocket =\nsocket(AF_INET,SOCK_DGRAM)\n(Rodando em serverIP)\nCliente\nLer segmento UDP de\nserverSocket\nEscrever resposta em\nespeciﬁcando endereço do cliente,\nnúmero de porta\nserverSocket\nCriar datagrama com IP do servidor\ne port=x;\nenviar datagrama via\nclientSocket\nCriar socket:\nclientSocket =\nsocket(AF_INET,SOCK_DGRAM)\nLer datagrama de\nclientSocket\nFechar\nclientSocket\nUDPClient.py\nAqui está o código para o lado cliente da aplicação:\nfrom socket import *\nserverName = ‘hostname’\nserverPort = 12000\nclientSocket = socket(socket.AF_INET, socket.SOCK_DGRAM)\nmessage = raw_input(’Input lowercase sentence:’)\nclientSocket.sendto(message,(serverName, serverPort))\nmodifiedMessage, serverAddress = clientSocket.recvfrom(2048)\nprint modifiedMessage\nclientSocket.close()\nAgora, vamos examinar as linhas de código em UDPClient.py.\nfrom socket import *\nO módulo socket forma a base de todas as comunicações de rede em Python. Incluindo esta linha, podere-\nmos criar sockets dentro do nosso programa.\nserverName = ‘hostname’\nserverPort = 12000\nA primeira linha define a cadeia serverName como “hostname”\n. Aqui, oferecemos uma cadeia contendo \nou o endereço IP do servidor (por exemplo, “128.138.32.126”) ou o nome de hospedeiro do servidor (por exem-\nplo, “cis.poly.edu”). Se usarmos o nome do hospedeiro, então uma pesquisa DNS será automaticamente realizada \npara obter o endereço IP. A segunda linha define a variável inteira serverPort como 12000.\nclientSocket = socket(socket.AF_INET, socket.SOCK_DGRAM)\n   Redes de computadores e a Internet\n118\nEsta linha cria o socket do cliente, denominado clientSocket. O primeiro parâmetro indica a família do \nendereço; em particular, AF_INET indica que a rede subjacente está usando IPv4. (Não se preocupe com isso agora \n— vamos discutir sobre o IPv4 no Capítulo 4.) O segundo parâmetro indica que o socket é do tipo SOCK_DGRAM, o \nque significa que é um socket UDP (em vez de um socket TCP). Observe que não estamos especificando o número \nde porta do socket cliente quando o criamos; em vez disso, deixamos que o sistema operacional o faça por nós. Agora \nque a porta do processo cliente já foi criada, queremos criar uma mensagem para enviar pela porta.\nmessage = raw_input(’Input lowercase sentence:’)\nraw_input() é uma função interna da linguagem Python. Quando esse comando é executado, o usuário no \ncliente recebe o texto “Input data:” (digite dados). Então, o usuário usa seu teclado­\n para digitar uma linha, \nque é colocada na variável message. Agora que temos um socket e uma mensagem, queremos enviar a mensa-\ngem pelo socket ao hospedeiro de destino.\nclientSocket.sendto(message,(serverName, serverPort))\nNesta linha, o método sendto() acrescenta o endereço de destino (serverName, serverPort) à men-\nsagem e envia o pacote resultante pelo socket do processo, clientSocket. (Como já dissemos, o endereço de \norigem também é conectado ao pacote, embora isso seja feito automaticamente, e não pelo código.) O envio de \numa mensagem do cliente ao servidor por meio de um socket UDP é simples assim! Depois de enviar o pacote, o \ncliente espera receber dados do servidor.\nmodifiedMessage, serverAddress = clientSocket.recvfrom(2048)\nCom esta linha, quando um pacote chega da Internet no socket do cliente, os dados são colocados na variável \nmodifiedMessage e o endereço de origem do pacote é colocado na variá­\nvel serverAddress. A variável \nserverAddress contém tanto o endereço IP do servidor quanto o número de porta do servidor. O programa \nUDPClient não precisa realmente dessa informação de endereço do servidor, pois já sabe o endereço do servidor \ndesde o início; mas esta linha de Python oferece o endereço do servidor, apesar disso. O método recvfrom tam-\nbém toma o tamanho do buffer, 2048, como entrada. (Esse tamanho de buffer funciona para quase todos os fins.)\nprint modifiedMessage\nEsta linha imprime modifiedMessage na tela do usuário. Essa deverá ser a linha original que o usuário digi-\ntou, mas agora em letras maiúsculas.\nclientSocket.close()\nEsta linha fecha o socket. O processo, então, é concluído.\nUDPServer.py\nVamos agora dar uma olhada no lado servidor da aplicação:\nfrom socket import *\nserverPort = 12000\nserverSocket = socket(AF_INET, SOCK_DGRAM)\nserverSocket.bind((’’, serverPort))\nprint ”The server is ready to receive”\nwhile 1:\n\t\nmessage, clientAddress = serverSocket.recvfrom(2048)\n\t\nmodifiedMessage = message.upper()\n\t\nserverSocket.sendto(modifiedMessage, clientAddress)\nCAMADA  de APLICAÇÃO  119 \nObserve que o início de UDPServer é semelhante a UDPClient. Ele também importa o módulo socket, tam-\nbém define a variável inteira serverPort como 12000 e também cria um socket do tipo SOCK_DGRAM (um \nsocket UDP). A primeira linha de código que é significativamente diferente de UDPClient é:\nserverSocket.bind((’’, serverPort))\nEsta linha vincula (ou seja, designa) o número de porta 12000 ao socket do servidor. Assim, em UDPServer, \no código (escrito pelo programador de aplicação) está designando um número de porta ao socket. Dessa forma, \nquando alguém enviar um pacote à porta 12000 no endereço IP do servidor, ele será direcionado a este socket. \nUDPServer, então, entra em um laço while; o laço while permitirá que UDPServer receba e processe pacotes dos \nclientes indefinidamente. No laço while, UDPServer espera um pacote chegar.\nmessage, clientAddress = serverSocket.recvfrom(2048)\nEsta linha de código é semelhante à que vimos em UDPClient. Quando um pacote chega no socket \ndo servidor, os dados são colocados na variável message e o endereço de origem é colocado na variável \nclientAddress. A variável clientAddress contém o endereço IP e o número de porta do cliente. Aqui, \nUDPServer usará essa informação de endereço, pois oferece um endereço de retorno, semelhante ao do \nremetente no serviço postal comum. Com essa informação, o servidor agora sabe para onde deve direcio-\nnar sua resposta.\nmodifiedMessage = message.upper()\nEsta linha é o núcleo da nossa aplicação simples. Ela apanha a linha enviada pelo cliente e usa o método \nupper() para passá-la para letras maiúsculas.\nserverSocket.sendto(modifiedMessage, clientAddress)\nEsta última linha anexa o endereço do cliente (endereço IP e número de porta) à mensagem em letras \nmaiús­\nculas, enviando o pacote resultante ao socket do servidor. (Como já dissemos, o endereço do servidor \ntambém é anexado ao pacote, embora isso seja feito automaticamente, e não pelo código.) A Internet, então, \nentregará o pacote a esse endereço do cliente. Depois que o servidor envia o pacote, ele permanece no laço while, \nesperando até que outro pacote UDP chegue (de qualquer cliente rodando em qualquer hospedeiro).\nPara testar o par de programas, você instala e compila UDPClient.py em um hospedeiro e UDPServer.py \nem outro. Não se esqueça de incluir o nome de hospedeiro ou endereço IP do servidor em UDPClient.py. Em \nseguida, executa UDPServer.py, o programa servidor compilado, no hospedeiro servidor. Isso cria um processo \nno servidor que fica ocioso até que seja chamado por algum cliente. Depois você executa UDPClient.py, o progra-\nma cliente compilado, no cliente. Isso cria um processo no cliente. Por fim, para usar a aplicação no cliente, você \ndigita uma sentença seguida por um Enter.\nPara desenvolver sua própria aplicação cliente-servidor UDP, você pode começar modificando um pouco os \nprogramas cliente e servidor. Por exemplo, em vez de converter todas as letras para maiúsculas, o servidor poderia \ncontar o número de vezes que a letra s aparece e retornar esse número. Ou então o cliente pode ser modificado para \nque, depois de receber uma sentença em maiúsculas, o usuário possa continuar a enviar mais sentenças ao servidor.\n2.7.2  Programação de sockets com TCP\nDiferente do UDP, o TCP é um protocolo orientado a conexão. Isso significa que, antes que cliente e ser-\nvidor possam começar a enviar dados um para o outro, eles precisam primeiro se apresentar e estabelecer uma \nconexão TCP. Uma ponta dessa conexão está ligada ao socket cliente e a outra está ligada a um socket servidor. \nAo criar a conexão TCP, associamos a ela o endereço de socket (endereço IP e número de porta) do cliente e do \nservidor. Com a conexão estabelecida, quando um lado quer enviar dados para o outro, basta deixá-los na cone-\nxão TCP por meio de seu socket. Isso é diferente do UDP, para o qual o servidor precisa anexar um endereço de \ndestino ao pacote, antes de deixá-lo no socket.\n   Redes de computadores e a Internet\n120\nAgora, vamos examinar mais de perto a interação dos programas cliente e servidor em TCP. O cliente tem a \ntarefa de iniciar contato com o servidor. Para que o servidor possa reagir ao contato inicial do cliente, ele tem de \nestar pronto, o que implica duas coisas. Primeiro, como acontece no UDP, o programa servidor TCP precisa estar \nrodando como um processo antes de o cliente tentar iniciar contato. Segundo, o programa servidor tem de ter al-\nguma porta especial — mais precisamente, um socket especial — que acolha algum contato inicial de um processo \ncliente que esteja rodando em um hospedeiro qualquer. Recorrendo à analogia casa/porta para processo/socket, \nàs vezes nos referiremos ao contato inicial do cliente como “bater à porta”\n.\nCom o processo servidor em execução, o processo cliente pode iniciar uma conexão TCP com o servidor, o que \né feito no programa cliente pela criação de um socket TCP\n. Quando cria seu socket TCP\n, o cliente especifica o endereço \ndo socket receptivo do servidor, a saber, o endereço IP do hospedeiro servidor e o número de porta do socket. Após a \ncriação de seu socket, o cliente inicia uma apresentação de três vias e estabelece uma conexão TCP com o servidor. Essa \napresentação, que ocorre dentro da camada de transporte, é toda invisível para os programas cliente e servidor.\nDurante a apresentação de três vias, o processo cliente bate na porta de entrada do processo servidor. Quan-\ndo o servidor “ouve” a batida, cria uma nova porta (mais precisamente, um novo socket) dedicada àquele cliente. \nNo exemplo a seguir, a porta de entrada é um objeto socket do TCP que denominamos serverSocket; o socket \nrecém-criado, dedicado ao cliente que faz a conexão, é denominado connectionSocket. Os estudantes que encon-\ntram sockets TCP pela primeira vez às vezes confundem o socket de entrada (que é o ponto de contato inicial para \ntodos os clientes que querem se comunicar com o servidor) com cada socket de conexões no lado do servidor, que \né criado em seguida para a comunicação com cada cliente.\nDo ponto de vista da aplicação, o socket do cliente e o de conexão do servidor estão conectados diretamente, \ncomo se houvesse  uma tubulação entre eles. Como vemos na Figura 2.29, o processo cliente pode enviar bytes \npara seu socket de modo arbitrário; o TCP garante que o processo servidor receberá (pelo socket de conexão) \ncada byte na ordem em que foram enviados. Assim, o TCP provê um serviço confiável entre os processos cliente \ne servidor. Além disso, assim como pessoas podem entrar e sair pela mesma porta, o processo cliente não somen-\nte envia bytes a seu socket, mas também os recebe dele; de modo semelhante, o processo servidor não só recebe \nbytes de seu socket de conexão, mas também os envia por ele.\nUsamos a mesma aplicação cliente-servidor simples para demonstrar programação de sockets para \nTCP: o cliente envia uma linha de dados ao servidor, este converte a linha para letras maiúsculas e a envia \nde volta ao cliente. A Figura 2.30 destaca a principal atividade relacionada a socket do cliente e servidor que \nse comunicam pelo serviço de transporte TCP.\nFigura 2.29  O processo TCPServer tem dois sockets\nProcesso cliente\nProcesso servidor\nSocket \ndo cliente\nSocket \nreceptivo\nApresentação de três vias\nSocket de \nconexão\nbytes\nbytes\nCAMADA  de APLICAÇÃO  121 \nTCPClient.py\nEis o código para o lado cliente da aplicação:\nfrom socket import *\nserverName = ’servername’\nserverPort = 12000\nclientSocket = socket(AF_INET, SOCK_STREAM)\nclientSocket.connect((serverName,serverPort))\nsentence = raw_input(‘Input lowercase sentence:’)\nclientSocket.send(sentence)\nmodifiedSentence = clientSocket.recv(1024)\nprint ‘From Server:’, modifiedSentence\nclientSocket.close()\nVamos agora examinar as várias linhas do código, que difere ligeiramente da implementação UDP. A pri-\nmeira linha é a criação do socket do cliente.\nclientSocket = socket(AF_INET, SOCK_STREAM)\nEssa linha cria o socket do cliente, denominado clientSocket. O primeiro parâmetro novamente indica \nque a rede subjacente está usando IPv4. O segundo parâmetro indica que o socket é do tipo SOCK_STREAM, ou \nseja, é um socket TCP (em vez de um UDP). Observe que de novo não estamos especificando o número de porta \ndo socket cliente quando o criamos; em vez disso, deixamos que o sistema operacional o faça por nós. Agora, a \npróxima linha de código é muito diferente do que vimos em UDPClient:\nclientSocket.connect((serverName,serverPort))\nFigura 2.30  A aplicação cliente-servidor usando TCP\nFechar\nconnectionSocket\nEscrever resposta em\nconnectionSocket\nLer requisição de\nconnectionSocket\nKR 02.30.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p0 Wide x 29p0 Deep\n9/6/11, 10/17/11, 10/28/11, \n11/23/11 rossi\nCriar  socket, port=x,\npara requisição entrante:\nServidor\nserverSocket =\nsocket()\nEsperar por requisição \nde conexão entrante:\nconnectionSocket =\nserverSocket.accept()\n(Rodando em servidor IP)\nCliente\nEstabelecimento \nde conexão TCP\nCriar socket, conectar \na servidor IP\n, port=x:\nclientSocket =\nsocket()\nLer resposta de\nclientSocket\nEnviar requisição usando\nclientSocket\nFechar\nclientSocket\n   Redes de computadores e a Internet\n122\nLembre-se de que, antes de um cliente poder enviar dados ao servidor (e vice-versa) usando um socket TCP, \nprimeiro deve ser estabelecida uma conexão TCP entre eles, o que é feito por meio dessa linha. O parâmetro do \nmétodo connect() é o endereço do lado servidor da conexão. Depois que essa linha de código é executada, é \nfeita uma apresentação de três vias e uma conexão TCP é estabelecida.\nsentence = raw_input(‘Input lowercase sentence:’)\nAssim como em UDPClient, essa linha obtém uma sentença do usuário. A cadeia sentence continua a reu-\nnir caracteres até que o usuário termine a linha digitando um Enter. A linha de código seguinte também é muito \ndiferente do UDPClient:\nclientSocket.send(sentence)\nEssa linha envia a cadeia sentence pelo socket do cliente e para a conexão TCP\n. Observe que o programa não \ncria um pacote explicitamente, anexando o endereço de destino ao pacote, como foi feito com os sockets UDP\n. Em vez \ndisso, apenas deixa os bytes da cadeia sentence na conexão TCP\n. O cliente, então, espera para receber bytes do servidor.\nmodifiedSentence = clientSocket.recv(2048)\nQuando os caracteres chegam do servidor, eles são colocados na cadeia modifiedSentence. Os caracteres \ncontinuam a ser acumulados em modifiedSentence até que a linha termine com um caractere de Enter. Depois de \nexibir a sentença em maiúsculas, fechamos o socket do cliente:\nclientSocket.close()\nEssa última linha fecha o socket e, portanto, fecha a conexão TCP entre cliente e servidor. Ela faz o TCP no \ncliente enviar uma mensagem TCP ao TCP no servidor (ver Seção 3.5).\nTCPServer.py\nAgora vamos examinar o programa servidor.\nfrom socket import *\nserverPort = 12000\nserverSocket = socket(AF_INET,SOCK_STREAM)\nserverSocket.bind((’’,serverPort))\nserverSocket.listen(1)\nprint ‘The server is ready to receive’\nwhile 1:\n\t\nconnectionSocket, addr = serverSocket.accept()\n\t\nsentence = connectionSocket.recv(1024)\n\t\ncapitalizedSentence = sentence.upper()\n\t\nconnectionSocket.send(capitalizedSentence)\n\t\nconnectionSocket.close()\nVejamos agora as linhas que diferem significativamente de UDPServer e TCPClient. Assim como em TCP-\nClient, o servidor cria um socket TCP com:\nserverSocket=socket(AF_INET,SOCK_STREAM)\nDe modo semelhante a UDPServer, associamos o número de porta do servidor, serverPort, ao socket:\nserverSocket.bind((’’,serverPort))\nPorém, com TCP, serverSocket será nosso socket de entrada. Depois de estabelecer essa porta de entra-\nda, vamos esperar e ficar escutando até que algum cliente bata:\nserverSocket.listen(1)\nCAMADA  de APLICAÇÃO  123 \nEssa linha faz com que o servidor escute as requisições de conexão TCP do cliente. O parâmetro especifica \no número máximo de conexões em fila (pelo menos 1).\nconnectionSocket, addr = serverSocket.accept()\nQuando o cliente bate a essa porta, o programa chama o método accept() para serverSocket, que cria um \nnovo socket no servidor, chamado connectionSocket, dedicado a esse cliente específico. Cliente e servidor, \nentão, completam a apresentação, criando uma conexão TCP entre o clientSocket do cliente e o connection-\nSocket do servidor. Após estabelecer a conexão TCP, cliente e servidor podem enviar bytes um para o outro por \nela. Com TCP, todos os bytes enviados de um lado têm garantias não apenas de que chegarão ao outro lado, mas \ntambém na ordem.\nconnectionSocket.close()\nNesse programa, depois de enviar a sentença modificada ao cliente, fechamos o socket da conexão. Mas, \ncomo serverSocket permanece aberto, outro cliente agora pode bater à porta e enviar uma sentença ao ser-\nvidor, para que seja modificada.\nIsso conclui nossa discussão sobre programação de sockets em TCP. Encorajamos o leitor a executar os dois \nprogramas em dois hospedeiros separados, e também a modificá-los para realizar objetivos ligeiramente diferen-\ntes. Compare o par de programas UDP com o par de programas TCP e repare suas diferenças. Você também de-\nverá realizar várias tarefas de programação de sockets descritas ao final dos Capítulos 2, 4 e 7. Por fim, esperamos \nque, um dia, depois de dominar estes e outros programas de sockets mais avançados, você escreva sua própria \naplicação popular para redes, fique rico, famoso e lembre-se dos autores deste livro!\n2.8  Resumo\nNeste capítulo, estudamos os aspectos conceituais e os aspectos de implementação de aplicações de rede. \nConhecemos a onipresente arquitetura cliente-servidor adotada por aplicações da Internet e examinamos sua \nutilização nos protocolos HTTP, FTP, SMTP, POP3 e DNS. Analisamos esses importantes protocolos de camada \nde aplicação e suas aplicações associadas (Web, transferência de arquivos, e-mail e DNS) com algum detalhe. \nConhecemos também a arquitetura P2P, cada vez mais dominante, e examinamos sua utilização em muitas apli-\ncações. Vimos como o API socket pode ser usado para construir aplicações de rede. Examinamos a utilização de \nsockets para serviços de transporte fim a fim orientados a conexão (TCP) e não orientados a conexão (UDP). A \nprimeira etapa de nossa jornada de descida pela arquitetura das camadas da rede está concluída!\nLogo no começo deste livro, na Seção 1.1, demos uma definição um tanto vaga e despojada de um protocolo. Dis-\nsemos que um protocolo é “o formato e a ordem das mensagens trocadas entre duas ou mais entidades comunicantes, \nbem como as ações realizadas na transmissão e/ou no recebimento de uma mensagem ou outro evento”\n. O material \ndeste capítulo — em particular, o estudo detalhado dos protocolos HTTP\n, FTP\n, SMTP\n, POP3 e DNS — agregou consi-\nderável substância a essa definição. Protocolos são o conceito fundamental de redes. Nosso estudo sobre protocolos de \naplicação nos deu agora a oportunidade de desenvolver uma noção mais intuitiva do que eles realmente são.\nNa Seção 2.1, descrevemos os modelos de serviço que o TCP e o UDP oferecem às aplicações que os cha-\nmam. Nós os examinamos ainda mais de perto quando desenvolvemos, na Seção 2.7, aplicações simples que \nexecutam em TCP e UDP. Contudo, pouco dissemos sobre como o TCP e o UDP fornecem esses modelos de ser-\nviços. Por exemplo, sabemos que o TCP provê um serviço de dados confiável, mas ainda não mencionamos como \nele o faz. No próximo capítulo, examinaremos cuidadosamente não apenas o que são protocolos de transporte, \nmas também o como e o porquê deles.\nAgora que conhecemos a estrutura da aplicação da Internet e os protocolos de camada de aplicação, estamos \nprontos para continuar a descer a pilha de protocolos e examinar a camada de transporte no Capítulo 3.\n   Redes de computadores e a Internet\n124\nExercícios\nde fixação e perguntas\nQuestões de revisão do Capítulo 2\nSEÇÃO 2.1\n\t\nR1.\t Relacione cinco aplicações da Internet não proprietárias e os protocolos de camada de aplicação que elas \nusam.\n\t\nR2.\t Qual é a diferença entre arquitetura de rede e arquitetura de aplicação?\n\t\nR3.\t Para uma sessão de comunicação entre um par de processos, qual processo é o cliente e qual é o servidor?\n\t\nR4.\t Em uma aplicação de compartilhamento de arquivos P2P, você concorda com a afirmação: “não existe \nnenhuma noção de lados cliente e servidor de uma sessão de comunicação”? Justifique sua resposta.\n\t\nR5.\t Que informação é usada por um processo que está rodando em um hospedeiro para identificar um processo \nque está rodando em outro hospedeiro?\n\t\nR6.\t Suponha que você queria fazer uma transação de um cliente remoto para um servidor da maneira mais rápida \npossível. Você usaria o UDP ou o TCP? Por quê?\n\t\nR7.\t Com referência à Figura 2.4, vemos que nenhuma das aplicações relacionadas nela requer “sem perda de \ndados” e “temporização”\n. Você consegue imaginar uma aplicação que requeira “sem perda de dados” e seja \ntambém altamente sensível ao atraso?\n\t\nR8.\t Relacione quatro classes de serviços que um protocolo de transporte pode prover. Para cada uma, indique se \no UDP ou o TCP (ou ambos) fornece tal serviço.\n\t\nR9.\t Lembre-se de que o TCP pode ser aprimorado com o SSL para fornecer serviços de segurança processo a \nprocesso, incluindo a decodificação. O SSL opera na camada de transporte ou na camada de aplicação? Se \no desenvolvedor da aplicação quer que o TCP seja aprimorado com o SSL, o que ele deve fazer?\nSEÇÕES 2.2-2.5\n\t\nR10.\t O que significa protocolo de apresentação (handshaking protocol)?\n\t\nR11.\t Por que HTTP, FTP, SMTP, POP3 rodam sobre TCP e não sobre UDP?\n\t\nR12.\t Considere um site de comércio eletrônico que quer manter um registro de compras para cada um de seus \nclientes. Descreva como isso pode ser feito com cookies.\n\t\nR13.\t Descreva como o cache Web pode reduzir o atraso na recepção de um objeto requisitado. O cache Web \nreduzirá o atraso para todos os objetos requisitados por um usuário ou somente para alguns objetos? Por quê?\n\t\nR14.\t Digite um comando Telnet em um servidor Web e envie uma mensagem de requisição com várias linhas. \nInclua nessa mensagem a linha de cabeçalho If-modified-since: para forçar uma mensagem de \nresposta com a codificação de estado 304 Not Modified.\n\t\nR15.\t Por que se diz que o FTP envia informações de controle “fora da banda”?\n\t\nR16.\t Suponha que Alice envie uma mensagem a Bob por meio de uma conta de e-mail da Web (como o Hotmail \nou gmail), e que Bob acesse seu e-mail por seu servidor de correio usando POP3. Descreva como a mensagem \nvai do hospedeiro de Alice até o hospedeiro de Bob. Não se esqueça de relacionar a série de protocolos de \ncamada de aplicação usados para movimentar a mensagem entre os dois hospedeiros.\n\t\nR17.\t Imprima o cabeçalho de uma mensagem de e-mail que tenha recebido recentemente. Quantas linhas de \ncabeçalho Received: há nela? Analise cada uma.\n\t\nR18.\t Do ponto de vista de um usuário, qual é a diferença entre o modo ler-e-apagar e o modo ler-e-guardar no \nPOP3?\nCAMADA  de APLICAÇÃO  125 \n\t\nR19.\t É possível que o servidor Web e o servidor de correio de uma organização tenham exatamente o mesmo \napelido para um nome de hospedeiro (por exemplo, foo.com)? Qual seria o tipo de RR que contém o nome \nde hospedeiro do servidor de correio?\n\t\nR20.\t Examine seus e-mails recebidos e veja o cabeçalho de uma mensagem enviada de um usuário com um \nendereço de correio eletrônico .edu. É possível determinar, pelo cabeçalho, o endereço IP do hospedeiro do \nqual a mensagem foi enviada? Faça o mesmo para uma mensagem enviada de uma conta do gmail.\nSEÇÃO 2.6\n\t\nR21.\t No BitTorrent, suponha que Alice forneça blocos para Bob durante um intervalo de 30 s. Bob retornará, \nnecessariamente, o favor e fornecerá blocos para Alice no mesmo intervalo? Por quê?\n\t\nR22.\t Considere um novo par, Alice, que entra no BitTorrent sem possuir nenhum bloco. Sem qualquer bloco, ela \nnão pode se tornar uma das quatro melhores exportadoras de dados para qualquer dos outros pares, visto que \nela não possui nada para enviar. Então, como Alice obterá seu primeiro bloco?\n\t\nR23.\t O que é uma rede de sobreposição? Ela inclui roteadores? O que são as arestas da rede de sobreposição?\n\t\nR24.\t Considere um DHT com uma topologia da rede de sobreposição (ou seja, cada par rastreia todos os pares no \nsistema). Quais são as vantagens e desvantagens de um DHT circular (sem atalhos)?\n\t\nR25.\t Relacione pelo menos quatro diferentes aplicações que são apropriadas naturalmente para arquiteturas P2P. \n(Dica: Distribuição de arquivo e mensagem instantânea são duas.)\nSEÇÃO 2.7\n\t\nR26.\t O servidor UDP descrito na Seção 2.7 precisava de um socket apenas, ao passo que o servidor TCP precisava \nde dois. Por quê? Se um servidor TCP tivesse de suportar n conexões simultâneas, cada uma de um hospedeiro \ncliente diferente, de quantos sockets precisaria?\n\t\nR27.\t Para a aplicação cliente-servidor por TCP descrita na Seção 2.7, por que o programa servidor deve ser \nexecutado antes do programa cliente? Para a aplicação cliente-servidor por UDP, por que o programa \ncliente pode ser executado antes do programa servidor?\nproblemas\n\t\nP1.\t Falso ou verdadeiro?\na.\t Um usuário requisita uma página Web que consiste em algum texto e três imagens. Para essa página, o \ncliente enviará uma mensagem de requisição e receberá quatro mensagens de resposta.\nb.\t Duas páginas Web distintas (por exemplo, www.mit.edu/research.html e www.mit.edu/\nstudents.html) podem ser enviadas pela mesma conexão persistente.\nc.\t Com conexões não persistentes entre navegador e servidor de origem, é possível que um único segmento \nTCP transporte duas mensagens distintas de requisição HTTP.\nd.\t O cabeçalho Date: na mensagem de resposta HTTP indica a última vez que o objeto da resposta foi \nmodificado.\ne.\t As mensagens de resposta HTTP nunca possuem um corpo de mensagem vazio.\n\t\nP2.\t Leia o RFC 959 para FTP. Relacione todos os comandos de cliente que são suportados pelo RFC.\n\t\nP3.\t Considere um cliente HTTP que queira obter um documento Web em um dado URL. Inicialmente, o \nendereço IP do servidor HTTP é desconhecido. Nesse cenário, quais protocolos de transporte e de camada de \naplicação são necessários, além do HTTP?\n\t\nP4.\t Considere a seguinte cadeia de caracteres ASCII capturada pelo Wireshark quando o navegador enviou uma \nmensagem HTTP GET (ou seja, o conteúdo real de uma mensagem HTTP GET). Os caracteres <cr><lf> são \n   Redes de computadores e a Internet\n126\nretorno de carro e avanço de linha (ou seja, a cadeia de caracteres em itálico <cr> no texto abaixo representa \no caractere único retorno de carro que estava contido, naquele momento, no cabeçalho HTTP). Responda às \nseguintes questões, indicando onde está a resposta na mensagem HTTP GET a seguir.\nGET /cs453/index.html HTTP/1.1<cr><lf>Host: gai\na.cs.umass.edu<cr><lf>User-Agent: Mozilla/5.0 (\nWindows;U; Windows NT 5.1; en-US; rv:1.7.2) Gec\nko/20040804 Netscape/7.2 (ax) <cr><lf>Accept:ex\nt/xml, application/xml, application/xhtml+xml, text\n/html;q=0.9, text/plain;q=0.8,image/png,*/*;q=0.5\n<cr><lf>Accept-Language: en-us,en;q=0.5<cr><lf>Accept-\nEncoding: zip,deflate<cr><lf>Accept-Charset: ISO\n-8859-1,utf-8;q=0.7,*;q=0.7<cr><lf>Keep-Alive: 300<cr>\n<lf>Connection:keep-alive<cr><lf><cr><lf>\na.\t Qual é a URL do documento requisitado pelo navegador?\nb.\t Qual versão do HTTP o navegador está rodando?\nc.\t O navegador requisita uma conexão não persistente ou persistente?\nd.\t Qual é o endereço IP do hospedeiro no qual o navegador está rodando?\ne.\t Que tipo de navegador inicia essa mensagem? Por que é necessário o tipo de navegador em uma mensagem \nde requisição HTTP?\n\t\nP5.\t O texto a seguir mostra a resposta enviada do servidor em reação à mensagem HTTP GET na questão \nanterior. Responda às seguintes questões, indicando onde está a resposta na mensagem.\nHTTP/1.1 200 OK<cr><lf>Date: Tue, 07 Mar 2008\n12:39:45GMT<cr><lf>Server: Apache/2.0.52 (Fedora)\n<cr><lf>Last-Modified: Sat, 10 Dec2005 18:27:46\nGMT<cr><lf>ETag: “526c3-f22-a88a4c80”<cr><lf>Accept-\nRanges: bytes<cr><lf>Content-Length: 3874<cr><lf>\nKeep-Alive: timeout=max=100<cr><lf>Connection:\nKeep-Alive<cr><lf>Content-Type: text/html; charset=\nISO-8859-1<cr><lf><cr><lf><!doctype html public “-\n//w3c//dtd html 4.0 transitional//en”><lf><html><lf>\n<head><lf> <meta http-equiv=”Content-Type”\ncontent=”text/html; charset=iso-8859-1”><lf> <meta\nname=”GENERATOR” content=”Mozilla/4.79 [en] (Windows NT\n5.0; U) Netscape]”><lf> <title>CMPSCI 453 / 591 /\nNTU-ST550A Spring 2005 homepage</title><lf></head><lf>\n<muito mais texto do documento em seguida (não mostrado)>\na.\t O servidor foi capaz de encontrar o documento com sucesso ou não? A que horas foi apresentada a \nresposta do documento?\nb.\t Quando o documento foi modificado pela última vez?\nc.\t Quantos bytes existem no documento que está retornando?\nd.\t Quais são os 5 primeiros bytes do documento que está retornando? O servidor aceitou uma conexão \npersistente?\n\t\nP6.\t Obtenha a especificação HTTP/1.1 (RFC 2616). Responda às seguintes perguntas:\na.\t Explique o mecanismo de sinalização que cliente e servidor utilizam para indicar que uma conexão \npersistente está sendo fechada. O cliente, o servidor, ou ambos, podem sinalizar o encerramento de uma \nconexão?\nb.\t Que serviços de criptografia são providos pelo HTTP?\nc.\t O cliente é capaz de abrir três ou mais conexões simultâneas com um determinado servidor?\nd.\t Um servidor ou um cliente pode abrir uma conexão de transporte entre eles se um dos dois descobrir que \na conexão ficou lenta por um tempo. É possível que um lado comece a encerrar a conexão enquanto o \noutro está transmitindo dados por meio dessa conexão? Explique.\nCAMADA  de APLICAÇÃO  127 \n\t\nP7.\t Suponha que você clique com seu navegador Web sobre um ponteiro para obter uma página e que o endereço \nIP para o URL associado não esteja no cache de seu hospedeiro local. Portanto, será necessária uma consulta \nao DNS para obter o endereço IP. Considere que n servidores DNS sejam visitados antes que seu hospedeiro \nreceba o endereço IP do DNS; as visitas sucessivas incorrem em um RTT igual a RTT1, . . ., RTTn. Suponha \nainda que a página associada ao ponteiro contenha exatamente um objeto que consiste em uma pequena \nquantidade de texto HTML. Seja RTT0 o RTT entre o hospedeiro local e o servidor que contém o objeto. \nAdmitindo que o tempo de transmissão seja zero, quanto tempo passará desde que o cliente clica o ponteiro \naté que receba o objeto?\n\t\nP8.\t Com referência ao Problema 7, suponha que o arquivo HTML referencie oito objetos muito pequenos no \nmesmo servidor. Desprezando tempos de transmissão, quanto tempo passa, usando-se:\na.\t HTTP não persistente sem conexões TCP paralelas?\nb.\t HTTP não persistente com o navegador configurado para 5 conexões paralelas?\nc.\t HTTP persistente?\n\t\nP9.\t Considere a Figura 2.12, que mostra uma rede institucional conectada à Internet. Suponha que o tamanho \nmédio do objeto seja 850 mil bits e que a taxa média de requisição dos navegadores da instituição aos \nservidores de origem seja 16 requisições por segundo. Suponha também que a quantidade de tempo que \nleva desde o instante em que o roteador do lado da Internet do enlace de acesso transmite uma requisição \nHTTP até que receba a resposta seja 3 segundos em média (veja Seção 2.2.5). Modele o tempo total médio \nde resposta como a soma do atraso de acesso médio (isto é, o atraso entre o roteador da Internet e o \nroteador da instituição) e o tempo médio de atraso da Internet. Para a média de atraso de acesso, use Δ(1 – \nΔβ), sendo Δ o tempo médio requerido para enviar um objeto pelo enlace de acesso e β a taxa de chegada de \nobjetos ao enlace de acesso.\na.\t Determine o tempo total médio de resposta.\nb.\t Agora, considere que um cache é instalado na LAN institucional e que a taxa de resposta local seja 0,4. \nDetermine o tempo total de resposta.\n\t\nP10.\t Considere um enlace curto de 10 m através do qual um remetente pode transmitir a uma taxa de 150 bits/s \nem ambas as direções. Suponha que os pacotes com dados tenham 100 mil bits de comprimento, e os pacotes \nque contêm controle (por exemplo, ACK ou apresentação) tenham 200 bits de comprimento. Admita que N \nconexões paralelas recebam cada 1/N da largura de banda do enlace. Agora, considere o protocolo HTTP e \nsuponha que cada objeto baixado tenha 100 Kbits de comprimento e que o objeto inicial baixado contenha \n10 objetos referenciados do mesmo remetente. Os downloads paralelos por meio de instâncias paralelas de \nHTTP não persistente fazem sentido nesse caso? Agora considere o HTTP persistente. Você espera ganhos \nsignificativos sobre o caso não persistente? Justifique sua resposta.\n\t\nP11.\t Considere o cenário apresentado na questão anterior. Agora suponha que o enlace é compartilhado por Bob \ne mais quatro usuários. Bob usa instâncias paralelas de HTTP não persistente, e os outros quatro usam HTTP \nnão persistente sem downloads paralelos.\na.\t As conexões paralelas de Bob o ajudam a acessar páginas Web mais rapidamente? Por quê? Por que não?\nb.\t Se cinco usuários abrirem cinco instâncias paralelas de HTTP não persistente, então as conexões paralelas \nde Bob ainda seriam úteis? Por quê? Por que não?\n\t\nP12.\t Escreva um programa TCP simples para um servidor que aceite linhas de entrada de um cliente e envie \nas linhas para a saída-padrão do servidor. (Você pode fazer isso modificando o programa TCPServer.py \nno texto.) Compile e execute seu programa. Em qualquer outra máquina que contenha um navegador \nWeb, defina o servidor proxy no navegador para a máquina que está executando seu programa servidor e \ntambém configure o número de porta adequadamente. Seu navegador deverá agora enviar suas mensagens \nde requisição GET a seu servidor, e este deverá apresentar as mensagens em sua saída-padrão. Use essa \nplataforma para determinar se seu navegador gera mensagens GET condicionais para objetos que estão em \ncaches locais.\n   Redes de computadores e a Internet\n128\n\t\nP13.\t Qual é a diferença entre MAIL FROM: em SMTP e FROM: na própria mensagem de correio?\n\t\nP14.\t Como o SMTP marca o final de um corpo de mensagem? E o HTTP? O HTTP pode usar o mesmo método \nque o SMTP para marcar o fim de um corpo de mensagem? Explique.\n\t\nP15.\t Leia o RFC 5321 para SMTP. O que significa MTA? Considere a seguinte mensagem spam recebida \n(modificada de um spam verdadeiro). Admitindo que o criador desse spam seja malicioso e que todos os \noutros hospedeiros sejam honestos, identifique o hospedeiro malicioso que criou essa mensagem spam.\nFrom - Fri Nov 07 13:41:30 2008\nReturn-Path: <tennis5@pp33head.com>\nReceived: from barmail.cs.umass.edu\n(barmail.cs.umass.edu [128.119.240.3]) by cs.umass.edu\n(8.13.1/8.12.6) for <hg@cs.umass.edu>; Fri, 7 Nov 2008\n13:27:10 -0500\nReceived: from asusus-4b96 (localhost [127.0.0.1]) by\nbarmail.cs.umass.edu (Spam Firewall) for\n<hg@cs.umass.edu>; Fri, 7 Nov 2008 13:27:07 -0500\n(EST)\nReceived: from asusus-4b96 ([58.88.21.177]) by\nbarmail.cs.umass.edu for <hg@cs.umass.edu>; Fri,\n07 Nov 2008 13:27:07 -0500 (EST)\nReceived: from [58.88.21.177] by\ninbnd55.exchangeddd.com; Sat, 8 Nov 2008 01:27:07 +0700\nFrom: “Jonny” <tennis5@pp33head.com>\nTo: <hg@cs.umass.edu>\nSubject: How to secure your savings\n\t\nP16.\t Leia o RFC do POP3 [RFC 1939]. Qual é a finalidade do comando UIDL do POP3?\n\t\nP17.\t Imagine que você acesse seu e-mail com POP3.\na.\t Suponha que você configure seu cliente de correio POP para funcionar no modo ler­\n‑e­\n‑apagar. Conclua a \nseguinte transação:\nC:  list\nS:  1 498\nS:  2 912\nS:  .\nC:  retr 1\nS:  blah blah ...\nS:  ..........blah\nS:  .\n?\n?\nb.\t Suponha que você configure seu cliente de correio POP para funcionar no modo ler­\n‑e­\n‑guardar. Conclua a \nseguinte transação:\nC:  list\nS:  1 498\nS:  2 912\nS:  .\nC:  retr 1\nS:  blah blah ...\nS:  ..........blah\nS:  .\n?\n?\nc.\t Suponha que você configure seu cliente de correio POP para funcionar no modo ler­\n‑e­\n‑guardar. Usando \nsua solução no item (b), considere que você recupere as mensagens 1 e 2, saia do POP e então, 5 minutos \nmais tarde, acesse outra vez o POP para obter um novo e-mail. Imagine que nenhuma outra mensagem \nfoi enviada nesse intervalo de 5 minutos. Elabore uma transcrição dessa segunda sessão POP.\nCAMADA  de APLICAÇÃO  129 \n\t\nP18.\t a.   O que é um banco de dados whois?\nb.\t Use vários bancos de dados whois da Internet para obter os nomes de dois servidores DNS. Cite quais \nbancos de dados whois você utilizou.\nc.\t Use nslookup em seu hospedeiro local para enviar consultas DNS a três servidores DNS: seu servidor DNS \nlocal e os dois servidores DNS que encontrou na parte (b). Tente consultar registros dos tipos A, NS e MX. \nFaça um resumo do que encontrou.\nd.\t Use nslookup para encontrar um servidor Web que tenha vários endereços IP. O servidor de sua instituição \n(escola ou empresa) tem vários endereços IP?\ne.\t Use o banco de dados whois ARIN para determinar a faixa de endereços IP usados por sua universidade.\nf.\t Descreva como um invasor pode usar bancos de dados whois e a ferramenta nslookup para fazer o \nreconhecimento de uma instituição antes de lançar um ataque.\ng.\t Discuta por que bancos de dados whois devem estar disponíveis publicamente.\n\t\nP19.\t Neste problema, utilizamos a ferramenta funcional dig disponível em hospedeiros Unix e Linux para explorar \na hierarquia dos servidores DNS. Lembre de que, na Figura 2.21, um servidor DNS de nível superior na \nhierarquia do DNS delega uma consulta DNS para um servidor DNS de nível inferior na hierarquia, enviando \nde volta ao cliente DNS o nome daquele servidor DNS de nível inferior. Primeiro, leia a man page sobre a \nferramenta dig e responda às seguintes questões:\na.\t Iniciando com o servidor DNS raiz (de um dos servidores raiz [a-m].root-servers.net), construa uma \nsequência de consultas para o endereço IP para seu servidor de departamento utilizando o dig. Mostre a \nrelação de nomes de servidores DNS na cadeia de delegação ao responder à sua consulta.\nb.\t Repita o item (a) com vários sites da Internet populares, como google.com, yahoo.com ou amazon.com.\n\t\nP20.\t Suponha que você consiga acessar os caches nos servidores DNS locais do seu departamento. Você é capaz de \npropor uma maneira de determinar, em linhas gerais, os servidores (fora de seu departamento) que são mais \npopulares entre os usuários do seu departamento? Explique.\n\t\nP21.\t Suponha que seu departamento possua um servidor DNS local para todos os computadores do departamento. \nVocê é um usuário comum (ou seja, não é um administrador de rede/sistema). Você consegue encontrar um \nmodo de determinar se um site da Internet externo foi muito provavelmente acessado de um computador do \nseu departamento alguns segundos atrás? Explique.\n\t\nP22.\t Considere um arquivo de distribuição de F = 15 Gbits para N pares. O servidor possui uma taxa de upload \nde us = 30 Mbits/s e cada par possui uma taxa de download de di = 2 Mbits/s e uma taxa de upload de u. \nPara N = 10, 100 e 1.000 e u = 300 Kbits/s, 700 Kbits/s e 2 Mbits/s, prepare um gráfico apresentando o \ntempo mínimo de distribuição para cada uma das combinações de N e u para o modo cliente-servidor e \npara o modo distribuição P2P.\n\t\nP23.\t Considere distribuir um arquivo de F bits para N pares utilizando uma arquitetura cliente-servidor. Admita \num modelo fluido no qual o servidor pode transmitir de modo simultâneo para diversos pares, a diferentes \ntaxas, desde que a taxa combinada não ultrapasse us.\na.\t Suponha que us/N ≤ dmin. Especifique um esquema de distribuição que possua o tempo de distribuição de \nNF/us.\nb.\t Suponha que us/N ≥ dmin. Especifique um esquema de distribuição que possua o tempo de distribuição de \nF/dmin.\nc.\t Conclua que o tempo mínimo de distribuição é, geralmente, dado por max{NF/us, F/dmin}.\n\t\nP24.\t Considere distribuir um arquivo de F bits para N pares utilizando uma arquitetura P2P. Admita um modelo \nfluido e que dmin é muito grande, de modo que a largura de banda do download do par nunca é um gargalo.\na.\t Suponha que us ≤ (us + u1 + ... + uN)/N. Especifique um esquema de distribuição que possua o tempo de \ndistribuição de F/us.\nb.\t Suponha que us ≥ (us + u1 + ... + uN)/N. Especifique um esquema de distribuição que possua o tempo de \ndistribuição de NF/(us + u1 + ... + uN).\nc.\t Conclua que o tempo mínimo de distribuição é, em geral, dado por max {F/us, NF/(us + u1 + ... + uN)}.\n   Redes de computadores e a Internet\n130\n\t\nP25.\t Considere uma rede de sobreposição com N pares ativos, em que cada dupla de pares possua uma conexão \nTCP. Além disso, suponha que as conexões TCP passem por um total de M roteadores. Quantos nós e \narestas há na rede de sobreposição correspondente?\n\t\nP26.\t Suponha que Bob tenha entrado no BitTorrent, mas ele não quer fazer o upload de nenhum dado para \nqualquer outro par (denominado carona).\na.\t Bob alega que consegue receber uma cópia completa do arquivo compartilhado pelo grupo. A alegação de \nBob é possível? Por quê?\nb.\t Bob alega ainda que ele pode “pegar carona” de um modo mais eficiente usando um conjunto de diversos \ncomputadores (com endereços IP distintos) no laboratório de informática de seu departamento. Como ele \npode fazer isso?\n\t\nP27.\t \u0007\nNo exemplo de DHT circular na Seção 2.6.2, suponha que o par 3 descobriu que o par 5 saiu. Como o par 3 \natualiza informações sobre o estado de seu sucessor? Qual par é agora seu primeiro sucessor? E seu segundo \nsucessor?\n\t\nP28.\t No exemplo de DHT circular na Seção 2.6.2, suponha que um novo par 6 queira se juntar ao DHT e \ninicialmente só conheça o endereço IP do par 15. Que etapas são tomadas?\n\t\nP29.\t Como um número inteiro em [0,2n – 1] pode ser expresso como um número binário de n bit em um DHT, \ncada chave pode ser expressa como k = (k0, k1, ..., kn-1), e cada identificador de par pode ser expresso como p \n= (p0, p1, ..., pn-1). Vamos, agora, definir a distância XOR entre a chave k e o par p como\n178\nCHAPTER 2\n•\nAPPLICATION LAYER\nP28. In the circular DHT example in Section 2.6.2, suppose that a new peer 6\nwants to join the DHT and peer 6 initially only knows peer 15’s IP address.\nWhat steps are taken?\nP29. Because an integer in [0, 2n\u001f 1] can be expressed as an n-bit binary number in\na DHT, each key can be expressed as k = (k0, k1, . . . , kn–1), and each peer iden-\ntifier can be expressed p = (p0, p1, . . . , pn–1). Let’s now define the XOR dis-\ntance between a key k and peer p as\nDescribe how this metric can be used to assign (key, value) pairs to peers. \n(To learn about how to build an efficient DHT using this natural metric, see\n[Maymounkov 2002] in which the Kademlia DHT is described.)\nP30. As DHTs are overlay networks, they may not necessarily match the under-\nlay physical network well in the sense that two neighboring peers might be\nphysically very far away; for example, one peer could be in Asia and its\nneighbor could be in North America. If we randomly and uniformly assign\nidentifiers to newly joined peers, would this assignment scheme cause such\na mismatch? Explain. And how would such a mismatch affect the DHT’s\nperformance? \nP31. Install and compile the Python programs TCPClient and UDPClient on one\nhost and TCPServer and UDPServer on another host.\na. Suppose you run TCPClient before you run TCPServer. What happens?\nWhy?\nb. Suppose you run UDPClient before you run UDPServer. What happens?\nWhy?\nc. What happens if you use different port numbers for the client and server\nsides?\nP32. Suppose that in UDPClient.py, after we create the socket, we add the line: \nclientSocket.bind(('', 5432))\nWill it become necessary to change UDPServer.py? What are the port num-\nbers for the sockets in UDPClient and UDPServer? What were they before\nmaking this change?\nP33. Can you configure your browser to open multiple simultaneous connections\nto a Web site? What are the advantages and disadvantages of having a large\nnumber of simultaneous TCP connections?\nP34 We have seen that Internet TCP sockets treat the data being sent as a byte\nstream but UDP sockets recognize message boundaries. What are one\nd1k, p2 = a\nn-1\nj=0\n\u001f kj - pj\u001f2j\n02_KURO6201_06_SE_C02_PP2.qxd  1/10/12  9:45 AM  Page 178\n\t\n\t Descreva como essa métrica pode ser usada para designar duplas (chave, valor) a pares. (Para aprender mais \nsobre como construir um DHT eficiente usando essa métrica natural, consulte Maymounkov [2002], no qual \no DHT Kademlia é descrito.)\n\t\nP30.\t Como os DHTs são redes de sobreposição, eles nem sempre se adequam bem à rede física de sobreposição \nconsiderando que dois pares vizinhos podem estar fisicamente muito distantes; por exemplo, um par poderia \nestar na Ásia e seu vizinho, na América do Norte. Se atribuirmos identificadores de maneira aleatória e \nuniforme para pares recém­\n‑unidos, esse esquema de atribuição causaria essa incompatibilidade? Explique. E \ncomo tal incompatibilidade afetaria o desempenho do DHT?\n\t\nP31.\t Instale e compile os programas Python TCPClient e UDPClient em um hospedeiro e TCPServer e UDPServer \nem outro.\na.\t Suponha que você execute TCPClient antes de executar TCPServer. O que acontece? Por quê?\nb.\t Imagine que você execute UDPClient antes de UDPServer. O que acontece? Por quê?\nc.\t O que acontece se você usar números de porta diferentes para os lados cliente e servidor?\n\t\nP32.\t Suponha que, em UDPClient.py, depois de criarmos o socket, acrescentemos a linha:\nclientSocket.bind((‘’, 5432))\n\t\n\t Será necessário mudar UDPServer.py? Quais são os números de porta para os sockets em UDPClient e \nUDPServer? Quais eram esses números antes dessa mudança?\n\t\nP33.\t Você consegue configurar seu navegador para abrir várias conexões simultâneas com um site? Quais são as \nvantagens e desvantagens de ter um grande número de conexões TCP simultâneas?\n\t\nP34\t Vimos que os sockets TCP da Internet tratam os dados sendo enviados como um fluxo de bytes, mas sockets \nUDP reconhecem limites de mensagem. Quais são uma vantagem e uma desvantagem da API orientada a \nbyte em relação a fazer com que a API reconheça e preserve explicitamente os limites de mensagem definidos \npela aplicação?\nCAMADA  de APLICAÇÃO  131 \n\t\nP35.\t O que é o servidor Web Apache? Quanto ele custa? Que funcionalidade ele possui atualmente? Você pode \nquerer ver na Wikipedia para responder a essa pergunta.\n\t\nP36.\t Muitos clientes BitTorrent utilizam DHTs para criar um rastreador distribuído. Para esses DHTs, qual é a \n“chave” e qual é o “valor”?\nTarefas de programação de sockets\nO site de apoio deste livro inclui seis tarefas de programação de sockets. As quatro primeiras são resumidas \na seguir. A quinta utiliza o protocolo ICMP e está resumida ao final do Capítulo 4. A sexta tarefa emprega pro-\ntocolos de multimídia e está resumida no final do Capítulo 7. Recomenda-se bastante que os alunos completem \nvárias, se não todas essas tarefas. Os alunos podem achar detalhes completos dessas tarefas, bem como trechos \nimportantes do código Python, no site http://www.awl.com/kurose-ross.\nTarefa 1: Servidor Web\nNesta tarefa, você desenvolverá um servidor Web simples em Python, capaz de processar apenas uma requisi-\nção. Seu servidor Web (i) criará um socket de conexão quando contatado por um cliente (navegador); (ii) receberá a \nrequisição HTTP dessa conexão; (iii) analisará a requisição para determinar o arquivo específico sendo requisitado; \n(iv) obterá o arquivo requisitado do sistema de arquivo do servidor; (v) criará uma mensagem de resposta HTTP \nconsistindo no arquivo requisitado precedido por linhas de cabeçalho; e (vi) enviará a resposta pela conexão TCP ao \nnavegador requisitante. Se um navegador requisitar um arquivo que não está presente no seu servidor, seu servidor \ndeverá retornar uma mensagem de erro “404 Not Found”\n.\nNo site de apoio, oferecemos o código estrutural para o seu servidor. Sua tarefa é concluir o código, rodar \nseu servidor e depois testá-lo enviando requisições de navegadores rodando em hospedeiros diferentes. Se você \nrodar seu servidor em um hospedeiro que já tem um servidor Web rodando nele, então deverá usar uma porta \ndiferente da porta 80 para o seu servidor.\nTarefa 2: UDP Pinger\nNesta tarefa de programação, você escreverá um programa ping do cliente em Python. Seu cliente enviará \numa mensagem ping simples a um servidor, receberá uma mensagem pong correspondente de volta do servidor \ne determinará o atraso entre o momento em que o cliente enviou a mensagem ping e recebeu a mensagem pong. \nEsse atraso é denominado tempo de viagem de ida e volta (round-trip time — RTT). A funcionalidade oferecida \npelo cliente e servidor é semelhante à fornecida pelo programa ping padrão, disponível nos sistemas operacionais \nmodernos. Porém, os programas ping padrão usam o Internet Control Message Protocol (ICMP) (que veremos \nno Capítulo 4). Aqui, criaremos um programa ping baseado em UDP, fora do padrão (porém simples!).\nSeu programa ping deverá enviar 10 mensagens ping ao servidor de destino por meio de UDP. Para cada \nmensagem, seu cliente deverá determinar e imprimir o RTT quando a mensagem pong correspondente for re-\ntornada. Como o UDP é um protocolo não confiável, um pacote enviado pelo cliente ou servidor poderá ser \nperdido. Por esse motivo, o cliente não poderá esperar indefinidamente por uma resposta a uma mensagem ping. \nVocê deverá fazer que o cliente espere até 1 s por uma resposta do servidor; se nenhuma resposta for recebida, o \ncliente deverá considerar que o pacote foi perdido e imprimir uma mensagem de acordo.\nNesta tarefa, você receberá o código completo para o servidor (disponível no site de apoio). Sua tarefa é \nescrever o código cliente, que será semelhante ao código do servidor. Recomendamos que, primeiro, você estude \ncuidadosamente o código do servidor. Depois, poderá escrever seu código cliente, cortando e colando à vontade \nas linhas do código do servidor.\n   Redes de computadores e a Internet\n132\nTarefa 3: Cliente de correio\nO objetivo desta tarefa de programação é criar um cliente de correio simples, que envia e-mail a qualquer \ndestinatário. Seu cliente precisará estabelecer uma conexão TCP com um servidor de correio (por exemplo, um \nservidor de correio do Google), dialogar com esse servidor usando o protocolo SMTP, enviar uma mensagem de \ncorreio a um destinatário (por exemplo, seu amigo) pelo servidor de correio e, por fim, fechar a conexão TCP \ncom o servidor de correio.\nPara esta tarefa, o site de apoio oferece o código estrutural para o seu cliente. Sua tarefa é completar o código e \ntestar seu cliente, enviando e-mail para contas de usuário diferentes. Você também pode tentar enviar por diferentes \nservidores (por exemplo, por um servidor de correio do Google e pelo servidor de correio da sua universidade).\nTarefa 4: Servidor proxy Web multithreaded\nNesta tarefa, você desenvolverá um proxy da Web. Quando seu proxy receber de um navegador uma requisição \nHTTP para um objeto, ele gerará uma nova requisição HTTP para o mesmo objeto e a enviará para o servidor de \norigem. Quando o proxy receber a resposta HTTP correspondente com o objeto do servidor de origem, ele criará \numa nova resposta HTTP, incluindo o objeto, e a enviará ao cliente. Esse proxy será multithreaded, de modo que \npoderá lidar com várias requisições ao mesmo tempo.\nPara esta tarefa, o site de apoio oferece o código estrutural para o servidor proxy. Seu trabalho é completar o \ncódigo e depois testá-lo fazendo diferentes navegadores requisitarem objetos Web por meio do seu proxy.\nWireshark Lab: HTTP\nDepois de ter molhado nossos pés com o analisador de pacotes Wireshark no Laboratório 1, agora estamos \nprontos para usar o Wireshark para investigar protocolos em operação. Neste laboratório, vamos explorar diver-\nsos aspectos do protocolo HTTP: a interação básica GET/resposta, formatos de mensagem HTTP, recuperação \nde grandes arquivos HTML, recuperação de arquivos HTML com URLs embutidas, conexões persistentes e não \npersistentes, e autenticação e segurança HTTP.\nComo acontece com todos os laboratórios Wireshark, o desenvolvimento completo está disponível no site \nde apoio do livro.\nWireshark Lab: DNS\nNeste laboratório, examinamos mais de perto o lado cliente do DNS, o protocolo que traduz nomes de \nhospedeiro da Internet em endereços IP. Lembre-se de que, na Seção 2.5, vimos que o papel do cliente no DNS \né bastante simples — um cliente envia uma consulta ao seu servidor DNS local e recebe uma resposta de volta. \nMuita coisa pode acontecer debaixo dos panos, invisível aos clientes DNS, à medida que servidores DNS hierár-\nquicos se comunicam entre si para resolver, de forma recursiva ou iterativa, a consulta DNS do cliente. Porém, \ndo ponto de vista do cliente DNS, o protocolo é bem simples — uma consulta é formulada ao servidor DNS local \ne uma resposta é recebida desse servidor. Observamos o DNS em ação neste laboratório.\nComo acontece com todos os Wireshark labs, a descrição completa está disponível no site de apoio do livro.\nCAMADA  de APLICAÇÃO  133 \nMarc Andreessen\nMarc Andreessen é um dos criadores do Mosaic, o navegador que tornou a \nWorld Wide Web popular em 1993. O Mosaic tinha uma interface limpa, facilmente \nentendida, e foi o primeiro navegador a exibir imagens em linha com texto. Em \n1994, Marc Andreessen e Jim Clark fundaram a Netscape, cujo navegador foi, de \nlonge, o mais popular durante meados da década de 1990. A Netscape também \ndesenvolveu o protocolo Secure Sockets Layer (SSL) e muitos produtos de servidor \nda Internet, incluindo os servidores de correio e servidores Web baseados em SSL. \nAgora, ele é um dos fundadores e sócio­\n‑geral da empresa de empreendimento de \nrisco Andreessen Horowitz, supervisionando o desenvolvimento de portfólio com \nholdings como Facebook, Foursquare, Groupon, Jawbone, Twitter e Zynga. Ele \natua em diversos comitês, incluindo Bump, eBay, Glam Media, Facebook e Hewlett-Packard. Possui bacha-\nrelado em Ciência da Computação pela University of Illinois em Urbana-Champaign.\nENTREVISTA\nComo você se interessou por computação? Você \nsempre soube que queria trabalhar em tecnologia \nda informação?\nAs revoluções do videogame e da computação pes-\nsoal chegaram exatamente quando eu estava crescen-\ndo — a computação pessoal era a nova fronteira da \ntecnologia no final dos anos 1970 e início dos 1980. E \nnão foi apenas Apple e o IBM PC, mas também cen-\ntenas de novas empresas como Commodore e Atari. \nEu aprendi a programar por um livro denominado \n“Instant Freeze-Dried BASIC” (BASIC ins­\ntantâneo \ncongelado a vácuo), com dez anos, e tive meu primeiro \ncomputador (um TRS-80 Color Computer — pode pro-\ncurar!) com doze anos.\nPor favor, descreva um ou dois dos projetos \nmais interessantes em que você já trabalhou \ndurante sua carreira. Quais foram os maiores \ndesafios?\nSem dúvida alguma, o projeto mais interessante foi \no navegador Web Mosaic original, em 1992-1993; e o \nmaior desafio foi fazer que alguém o levasse a sério na \népoca, quando todos pensavam que o futuro intera-\ntivo seria entregue na forma de “televisão interativa” \npor empresas imensas, não como a Internet por ini-\nciantes.\nO que o estimula a respeito do futuro das redes e da \nInternet? Quais são suas maiores preocupações?\nA coisa mais interessante é a imensa fronteira inex-\nplorada de aplicações e serviços que programadores e \nempreendedores são capazes de explorar — a Inter-\nnet deslanchou a criatividade a um nível que, acredito \neu, nunca vimos antes. Minha maior preocupação é o \nprincípio das consequências não intencionadas — nem \nsempre sabemos as implicações do que fazemos, como \na Internet sendo usada por governos para realizar um \nnovo nível de vigilâncias sobre os cidadãos.\nHá algo em particular que os estudantes deverão \nsaber à medida que a tecnologia Web avança?\nA velocidade da mudança — a coisa mais importan-\nte a aprender é como aprender — como adaptar-se de \nmodo flexível às mudanças nas tecnologias específicas \ne como manter uma mente aberta sobre as novas opor-\ntunidades e possibilidades enquanto você prossegue \nem sua carreira.\nQue pessoas o inspiraram profissionalmente?\nVannevar Bush, Ted Nelson, Doug Engelbart, Nolan \n \nBushnell, Bill Hewlett e Dave Packard, Ken Olsen, \n \nSteve Jobs, Steve Wozniak, Andy Grove, Grace Hopper, \nHedy Lamarr, Alan Turing, Richard Stallman.\n   Redes de computadores e a Internet\n134\nQuais são suas recomendações para estudantes \nque desejam seguir carreira em computação e \ntecnologia da informação?\nAprofunde-se ao máximo possível para entender \ncomo a tecnologia é criada, e depois, complemente com \no aprendizado de como o negócio funciona.\nA tecnologia pode resolver os problemas  \ndo mundo?\nNão, mas avançamos o padrão de vida das pes­\nsoas \ncom o crescimento econômico, e quase todo cres-\ncimento econômico no decorrer da história veio da \ntecnologia — portanto, melhor que isso é impossível.\nPosicionada entre a de aplicação e a de rede, a camada de transporte é uma peça central da arquitetura de \nrede em camadas. Ela desempenha o papel fundamental de fornecer serviços de comunicação diretamente aos \nprocessos de aplicação que rodam em hospedeiros diferentes. A abordagem pedagógica que adotamos neste \ncapítulo é alternar entre discussões de princípios de camada de transporte e o modo como tais princípios são \ncolocados em prática em protocolos existentes; como de costume, daremos particular ênfase aos protocolos da \nInternet, em especial aos de camada de transporte TCP e UDP.\nComeçaremos discutindo a relação entre as camadas de transporte e de rede, preparando o cenário para \no exame de sua primeira função importante — ampliar o serviço de entrega da camada de rede entre dois sis-\ntemas finais para um serviço de entrega entre dois processos da camada de aplicação que rodam nos sistemas \nfinais. Ilustraremos essa função quando abordarmos o UDP, o protocolo de transporte não orientado para \nconexão da Internet.\nDepois, retornaremos aos princípios e trataremos de um dos problemas mais fundamentais de redes de \ncomputadores — como duas entidades podem se comunicar de maneira confiável por um meio que pode perder \ne corromper dados. Mediante uma série de cenários cada vez mais complicados (e realistas!), construiremos um \nconjunto de técnicas que os protocolos de transporte utilizam para resolver esse problema. Então, mostraremos \ncomo esses princípios estão incorporados no TCP, o protocolo de transporte orientado para conexão da Internet.\nEm seguida, passaremos para um segundo problema fundamentalmente importante em redes — o con-\ntrole da taxa de transmissão de entidades de camada de transporte para evitar ou se recuperar de congestiona-\nmentos dentro da rede. Consideraremos as causas e consequências do congestionamento, bem como técnicas \nde controle de congestionamento comumente usadas. Após adquirir um sólido conhecimento das questões \nque estão por trás do controle de congestionamento, estudaremos seu tratamento pelo TCP.\n3.1  Introdução e serviços de camada de transporte\nNos dois capítulos anteriores, citamos o papel da camada de transporte e os serviços que ela fornece. Vamos \nrevisar rapidamente o que já aprendemos sobre a camada de transporte.\nUm protocolo da camada de transporte fornece comunicação lógica entre processos de aplicação que ro-\ndam em hospedeiros diferentes. Comunicação lógica nesse contexto significa que, do ponto de vista de uma \naplicação, tudo se passa como se os hospedeiros que rodam os processos estivessem conectados diretamente; \n1\n4 5 6\n8 9\n7\nc\n \na\n \np\n \ní\n \nt\n \nu\n \nl\n \no\ncamada de\ntransporte\n23\n   Redes de computadores e a Internet\n136\nna verdade, eles poderão estar em lados opostos do planeta, conectados por diversos roteadores e uma ampla \nvariedade de tipos de enlace. Processos de aplicação usam a comunicação lógica fornecida pela camada de trans-\nporte para enviar mensagens entre si, livres da preocupação dos detalhes da infraestrutura física utilizada para \ntransportá-las. A Figura 3.1 ilustra a noção de comunicação lógica.\nComo vemos na Figura 3.1, protocolos da camada de transporte são implementados nos sistemas finais, \nmas não em roteadores de rede. No lado remetente, a camada de transporte converte as mensagens que recebe de \num processo de aplicação remetente em pacotes de camada de transporte, denominados segmentos de camada \nde transporte na terminologia da Internet. Isso é (possivelmente) feito fragmentando-se as mensagens da aplica-\nção em pedaços menores e adicionando-se um cabeçalho de camada de transporte a cada pedaço para criar o seg-\nmento de camada de transporte. Essa camada, então, passa o segmento para a de rede no sistema final remetente, \nonde ele é encapsulado em um pacote de camada de rede (um datagrama) e enviado ao destinatário. É importante \nnotar que roteadores de rede agem somente nos campos de camada de rede do datagrama; isto é, não examinam \nos campos do segmento de camada de transporte encapsulado com o datagrama. No lado destinatário, a camada \nRede móvel\nKR 03.01.eps\nAW/Kurose and Ross\nComputer Networking, 6/e\n size:  36p0  x  41p5\n9/6/11, 10/28/11, 10/31/11\n11/21/11\ni\nISP nacional\nou global\nISP local\nou regional\nRede corporativa\nRede doméstica\nRede\nEnlace de dados\nFísica\nAplicação\nTransporte\nRede\nEnlace de dados\nFísica\nAplicação\nTransporte\nRede\nEnlace de dados\nFísica\nTransporte lógico ﬁm a ﬁm\nRede\nEnlace de dados\nFísica\nRede\nEnlace de dados\nFísica\nRede\nEnlace de dados\nFísica\nRede\nEnlace de dados\nFísica\nFigura 3.1  \u0007\nA camada de transporte fornece comunicação lógica, e não física, entre \nprocessos de aplicações\nCAMADA  de transporte  137 \nde rede extrai do datagrama o segmento de camada de transporte e passa-o para a camada de transporte que, em \nseguida, processa o segmento recebido, disponibilizando os dados para a aplicação destinatária.\nMais de um protocolo de camada de transporte poderão estar disponíveis às aplicações de rede. Por exem-\nplo, a Internet possui dois protocolos — TCP e UDP. Cada um oferece um conjunto diferente de serviços de \ncamada de transporte à aplicação chamadora.\n3.1.1  Relação entre as camadas de transporte e de rede\nLembre-se de que a camada de transporte se situa logo acima da de rede na pilha de protocolos. Enquanto \num protocolo de camada de transporte fornece comunicação lógica entre processos que rodam em hospedeiros \ndiferentes, um protocolo de camada de rede fornece comunicação lógica entre hospedeiros. A distinção é sutil, \nmas importante. Vamos examiná-la com o auxílio de uma analogia com moradias.\nConsidere duas casas, uma na Costa Leste e outra na Costa Oeste dos Estados Unidos, cada qual com uma \ndúzia de crianças. As crianças da Costa Leste são primas das crianças da Costa Oeste e todas adoram escrever \ncartas umas para as outras — cada criança escreve a cada primo uma vez por semana e cada carta é entregue pelo \nserviço de correio tradicional dentro de um envelope separado. Assim, uma casa envia 144 cartas por semana \npara a outra. (Essas crianças economizariam muito dinheiro se tivessem e-mail!) Em cada moradia há uma crian-\nça responsável pela coleta e distribuição da correspondência — Ann, na casa da Costa Oeste, e Bill, na da Costa \nLeste. Toda semana, Ann coleta a correspondência de seus irmãos e irmãs e a coloca no correio. Quando as cartas \nchegam à casa da Costa Oeste, também é Ann quem tem a tarefa de distribuir a correspondência, trazida pelo \ncarteiro, a seus irmãos e irmãs. Bill realiza o mesmo trabalho na casa da Costa Leste.\nNeste exemplo, o serviço postal oferece uma comunicação lógica entre as duas casas — ele movimenta a \ncorrespondência de uma residência para outra, e não de uma pessoa para outra. Por outro lado, Ann e Bill ofere-\ncem comunicação lógica entre os primos — eles coletam e entregam a correspondência de seus irmãos e irmãs. \nNote que, do ponto de vista dos primos, Ann e Bill são o serviço postal, embora sejam apenas uma parte (a parte \ndo sistema final) do processo de entrega fim a fim. Esse exemplo das moradias é uma analogia interessante para \nexplicar como a camada de transporte se relaciona com a camada de rede:\nmensagens de aplicação = cartas em envelopes\nprocessos = primos\nhospedeiros (também denominados sistemas finais) = casas\nprotocolo de camada de transporte = Ann e Bill\nprotocolo de camada de rede = serviço postal (incluindo os carteiros)\nContinuando com essa analogia, observe que Ann e Bill fazem todo o trabalho dentro de suas respectivas \ncasas; eles não estão envolvidos, por exemplo, com a classificação da correspondência em nenhuma central inter-\nmediária dos correios ou com o transporte de uma central a outra. De maneira semelhante, protocolos de camada \nde transporte moram nos sistemas finais, onde movimentam mensagens de processos de aplicação para a borda da \nrede (isto é, para a camada de rede) e vice-versa, mas não interferem no modo como as mensagens são movimenta-\ndas dentro do núcleo. Na verdade, como ilustrado na Figura 3.1, roteadores intermediários não atuam sobre (nem \nreconhecem) qualquer informação que a camada de transporte possa ter anexado às mensagens da aplicação.\nProsseguindo com nossa saga familiar, suponha agora que, quando Ann e Bill saem de férias, outro par de \nprimos — digamos, Susan e Harvey — substitua-os e encarregue-se da coleta interna da correspondência e de sua \nentrega. Infelizmente para as duas famílias, eles não desempenham essa tarefa do mesmo modo que Ann e Bill. \nPor serem crianças mais novas, Susan e Harvey recolhem e entregam a correspondência com menos frequência \ne, às vezes, perdem cartas (que acabam mastigadas pelo cão da família). Assim, o par de primos Susan e Harvey \nnão oferece o mesmo conjunto de serviços (isto é, o mesmo modelo de serviço) proporcionado por Ann e Bill. \nDe modo similar, uma rede de computadores pode disponibilizar vários protocolos de transporte, em que cada \num oferece um modelo de serviço diferente às aplicações.\n   Redes de computadores e a Internet\n138\nOs serviços que Ann e Bill podem fornecer são claramente limitados pelos possíveis serviços que os correios \nfornecem. Por exemplo, se o serviço postal não estipula um prazo máximo para entregar a correspondência entre \nas duas casas (digamos, três dias), então não há nenhuma possibilidade de Ann e Bill definirem um atraso máximo \npara a entrega da correspondência entre qualquer par de primos. De maneira semelhante, os serviços que um pro-\ntocolo de transporte pode fornecer são muitas vezes limitados pelo modelo de serviço do protocolo subjacente da \ncamada de rede. Se o protocolo de camada de rede não puder dar garantias contra atraso ou garantias de largura de \nbanda para segmentos de camada de transporte enviados entre hospedeiros, então o protocolo de camada de trans-\nporte não poderá dar essas mesmas garantias para mensagens de aplicação enviadas entre processos.\nNo entanto, certos serviços podem ser oferecidos por um protocolo de transporte mesmo quando o protocolo \nde rede subjacente não oferece o serviço correspondente na camada de rede. Por exemplo, como veremos neste capí-\ntulo, um protocolo de transporte pode oferecer serviço confiável de transferência de dados a uma aplicação mesmo \nquando o protocolo subjacente da rede não é confiável, isto é, mesmo quando o protocolo de rede perde, embaralha \nou duplica pacotes. Como outro exemplo (que exploraremos no Capítulo 8, quando discutirmos segurança de rede), \num protocolo de transporte pode usar criptografia para garantir que as mensagens da aplicação não sejam lidas por \nintrusos mesmo quando a camada de rede não puder garantir o sigilo de segmentos de camada de transporte.\n3.1.2  Visão geral da camada de transporte na Internet\nLembre-se de que a Internet — e, de maneira mais geral, a rede TCP/IP — disponibiliza dois protocolos de \ntransporte distintos para a camada de aplicação. Um deles é o UDP (User Datagram Protocol — Protocolo de \nDatagrama de Usuário), que oferece à aplicação solicitante um serviço não confiável, não orientado para conexão. \nO segundo é o TCP (Transmission Control Protocol — Protocolo de Controle de Transmissão), que oferece à \naplicação solicitante um serviço confiável, orientado para conexão. Ao projetar uma aplicação de rede, o criador \nda aplicação deve especificar um desses dois protocolos de transporte. Como vimos nas Seções 2.7 e 2.8, o desen-\nvolvedor da aplicação escolhe entre o UDP e o TCP ao criar sockets.\nPara simplificar a terminologia, quando no contexto da Internet, faremos alusão ao pacote de camada de \ntransporte como um segmento. Devemos mencionar, contudo, que a literatura da Internet (por exemplo, os RFCs) \ntambém se refere ao pacote de camada de transporte com/para TCP como um segmento, mas muitas vezes se \nrefere ao pacote com/para UDP como um datagrama. Porém, a mesma literatura também usa o termo datagrama \npara o pacote de camada de rede! Como este é um livro de introdução a redes de computadores, acreditamos que \nserá menos confuso se nos referirmos a ambos os pacotes TCP e UDP como segmentos; reservaremos o termo \ndatagrama para o pacote de camada de rede.\nAntes de continuarmos com nossa breve apresentação do UDP e do TCP, é útil dizer algumas palavras sobre \na camada de rede da Internet. (A camada de rede é examinada detalhadamente no Capítulo 4.) O protocolo de \ncamada de rede da Internet tem um nome — IP, que quer dizer Internet Protocol. O IP oferece comunicação lógica \nentre hospedeiros. O modelo de serviço do IP é um serviço de entrega de melhor esforço, o que significa que o \nIP faz o “melhor esforço” para levar segmentos entre hospedeiros comunicantes, mas não dá nenhuma garantia. \nEm especial, o IP não garante a entrega de segmentos, a entrega ordenada de segmentos e tampouco a integridade \ndos dados nos segmentos. Por essas razões, ele é denominado um serviço não confiável. Mencionamos também \nneste livro que cada hospedeiro tem, no mínimo, um endereço de camada de rede, denominado endereço IP. \nExaminaremos endereçamento IP em detalhes no Capítulo 4. Para este capítulo, precisamos apenas ter em mente \nque cada hospedeiro tem um endereço IP.\nAgora que abordamos de modo breve o modelo de serviço IP, vamos resumir os modelos de serviço providos \npor UDP e TCP. A responsabilidade fundamental do UDP e do TCP é ampliar o serviço de entrega IP entre dois \nsistemas finais para um serviço de entrega entre dois processos que rodam nos sistemas finais. A ampliação da \nentrega hospedeiro a hospedeiro para entrega processo a processo é denominada multiplexação/demultiplexa-\nção de camada de transporte. Discutiremos esse assunto na próxima seção. O UDP e o TCP também fornecem \nverificação de integridade ao incluir campos de detecção de erros nos cabeçalhos de seus segmentos. Esses dois \nCAMADA  de transporte  139 \nserviços mínimos de camada de transporte — entrega de dados processo a processo e verificação de erros — são \nos únicos que o UDP fornece! Em especial, como o IP, o UDP é um serviço não confiável — ele não garante que \nos dados enviados por um processo cheguem (quando chegam!) intactos ao processo destinatário. O UDP será \ndiscutido detalhadamente na Seção 3.3.\nO TCP, por outro lado, oferece vários serviços adicionais às aplicações. Primeiro, e mais importante, ele \noferece transferência confiável de dados. Usando controle de fluxo, números de sequência, reconhecimentos e \ntemporizadores (técnicas que exploraremos em pormenores neste capítulo), o protocolo assegura que os dados \nsejam entregues do processo remetente ao processo destinatário corretamente e em ordem. Assim, o TCP con-\nverte o serviço não confiável do IP entre sistemas finais em um serviço confiável de transporte de dados entre \nprocessos. Ele também oferece controle de congestionamento, que não é tanto um serviço fornecido à aplicação \nsolicitante, e sim mais um serviço dirigido à Internet como um todo — para o bem geral. Em termos genéricos, \no controle de congestionamento do TCP evita que qualquer outra conexão TCP abarrote os enlaces e roteadores \nentre hospedeiros comunicantes com uma quantidade excessiva de tráfego. Em princípio, o TCP permite que \nconexões TCP trafegando por um enlace de rede congestionado compartilhem em pé de igualdade a largura de \nbanda daquele enlace. Isso é feito pela regulagem da taxa com a qual o lado remetente do TCP pode enviar tráfego \npara a rede. O tráfego UDP, por outro lado, não é regulado. Uma aplicação que usa transporte UDP pode enviar \ntráfego à taxa que quiser, pelo tempo que quiser.\nUm protocolo que fornece transferência confiável de dados e controle de congestionamento é, necessaria-\nmente, complexo. Precisaremos de várias seções para detalhar os princípios da transferência confiável de dados \ne do controle de congestionamento, bem como de seções adicionais para explicar o protocolo TCP. Esses tópicos \nsão analisados nas Seções 3.4 a 3.8. A abordagem escolhida neste capítulo é alternar entre princípios básicos e \no protocolo TCP. Por exemplo, discutiremos primeiro a transferência confiável de dados em âmbito geral e, em \nseguida, como o TCP fornece especificamente a transferência confiável de dados. De maneira semelhante, inicia-\nremos discutindo o controle de congestionamento em âmbito geral e, em seguida, como o TCP realiza o controle \nde congestionamento. Porém, antes de chegarmos a essa parte boa, vamos examinar, primeiro, multiplexação/\ndemultiplexação na camada de transporte.\n3.2  Multiplexação e demultiplexação\nNesta seção, discutiremos multiplexação e demultiplexação na camada de transporte, isto é, a ampliação do \nserviço de entrega hospedeiro a hospedeiro provido pela camada de rede para um serviço de entrega processo a \nprocesso para aplicações que rodam nesses hospedeiros. Para manter a discussão em nível concreto, vamos exa-\nminar esse serviço básico da camada de transporte no contexto da Internet. Enfatizamos, contudo, que o serviço \nde multiplexação/demultiplexação é necessário para todas as redes de computadores.\nNo hospedeiro de destino, a camada de transporte recebe segmentos da camada de rede logo abaixo dela \ne tem a responsabilidade de entregar os dados desses segmentos ao processo de aplicação apropriado que roda \nno hospedeiro. Vamos examinar um exemplo. Suponha que você esteja sentado à frente de seu computador, \nbaixando páginas Web enquanto roda uma sessão FTP e duas sessões Telnet. Por conseguinte, você tem quatro \nprocessos de aplicação de rede em execução — dois Telnet, um FTP e um HTTP. Quando a camada de transporte \nem seu computador receber dados da camada de rede abaixo dela, precisará direcionar os dados recebidos a um \ndesses quatro processos. Vamos ver agora como isso é feito.\nEm primeiro lugar, lembre-se de que dissemos, na Seção 2.7, que um processo (como parte de uma aplica-\nção de rede) pode ter um ou mais sockets, portas pelas quais dados passam da rede para o processo e do processo \npara a rede. Assim, como mostra a Figura 3.2, a camada de transporte do hospedeiro destinatário na verdade \nnão entrega dados diretamente a um processo, mas a um socket intermediário. Como, a qualquer dado instante, \npode haver mais de um socket no hospedeiro destinatário, cada um tem um identificador exclusivo. O formato do \nidentificador depende de o socket ser UDP ou TCP, como discutiremos em breve.\n   Redes de computadores e a Internet\n140\nAgora, vamos considerar como um hospedeiro destinatário direciona, ao socket apropriado, um segmento \nde camada de transporte que chega. Cada segmento de camada de transporte tem um conjunto de campos para \ntal finalidade. Na extremidade receptora, a camada de transporte examina esses campos para identificar a porta \nreceptora e direcionar o segmento a esse socket. A tarefa de entregar os dados contidos em um segmento da ca-\nmada de transporte ao socket correto é denominada demultiplexação. O trabalho de reunir, no hospedeiro de \norigem, partes de dados provenientes de diferentes sockets, encapsular cada parte de dados com informações de \ncabeçalho (que mais tarde serão usadas na demultiplexação) para criar segmentos, e passar esses segmentos para \na camada de rede é denominada multiplexação. Note que a camada de transporte do hospedeiro que está no \nmeio da Figura 3.2 tem de demultiplexar segmentos que chegam da camada de rede abaixo para os processos P1 \nou P2 acima; isso é feito direcionando, ao socket do processo correspondente, os dados contidos no segmento que \nestá chegando. A camada de transporte desse hospedeiro do meio também tem de juntar dados de saída desses \nsockets, formar segmentos de camada de transporte e passá-los à camada de rede. Embora tenhamos apresentado \nmultiplexação e demultiplexação no contexto dos protocolos de transporte da Internet, é importante entender \nque essas operações estarão presentes sempre que um único protocolo em uma camada (na de transporte ou em \nqualquer outra) for usado por vários protocolos na camada mais alta seguinte.\nPara ilustrar o serviço de demultiplexação, lembre-se da metáfora das moradias apresentada na seção an-\nterior. Cada criança é identificada por seu nome próprio. Quando Bill recebe um lote de correspondência do \ncarteiro, realiza uma operação de demultiplexação ao examinar a quem as cartas estão endereçadas e, em seguida, \nentregar a correspondência a seus irmãos e irmãs. Ann realiza uma operação de multiplexação quando coleta as \ncartas de seus irmãos e irmãs e entrega a correspondência na agência do correio.\nAgora que entendemos os papéis da multiplexação e da demultiplexação na camada de transporte, vamos \nexaminar como isso é feito em um hospedeiro. Sabemos, pela discussão anterior, que multiplexação na camada \nde rede requer (1) que as portas tenham identificadores exclusivos e (2) que cada segmento tenha campos espe-\nciais que indiquem a porta para a qual o segmento deve ser entregue. Esses campos especiais, ilustrados na Figura \n3.3, são o campo de número de porta de origem e o campo de número de porta de destino. (Os segmentos \nUDP e TCP têm outros campos também, que serão examinados nas seções subsequentes deste capítulo.) Cada \nnúmero de porta é um número de 16 bits na faixa de 0 a 65535. Os números de porta entre 0 e 1023 são denomi-\nnados números de porta bem conhecidos; eles são restritos, o que significa que estão reservados para utilização \npor protocolos de aplicação bem conhecidos, como HTTP (que usa a porta número 80) e FTP (que usa a \nporta número 21). A lista dos números de porta bem conhecidos é apresentada no RFC 1700 e atualizada \nem <http://www.iana.org> [RFC 3232]. Quando desenvolvemos uma nova aplicação (como as desenvolvidas na \nSeção 2.7), devemos atribuir a ela um número de porta.\nRede\nLegenda:\nProcesso\nSocket\nEnlace de dados\nFísica\nTransporte\nAplicação\nRede\nAplicação\nEnlace de dados\nFísica\nTransporte\nRede\nEnlace de dados\nFísica\nTransporte\nP3\nP2\nP1\nP4\nAplicação\nFigura 3.2  Multiplexação e demultiplexação na camada de transporte\nCAMADA  de transporte  141 \nAgora já deve estar claro como a camada de transporte poderia realizar o serviço de demultiplexação: cada \nsocket do hospedeiro pode receber um número designado; quando um segmento chega ao hospedeiro, a camada \nde transporte examina seu número de porta de destino e direciona o segmento ao socket correspondente. Então, \nos dados do segmento passam pela porta e entram no processo ligado a ela. Como veremos, é assim que o UDP \nfaz demultiplexação. Todavia, veremos também que multiplexação/demultiplexação em TCP é ainda mais sutil.\nMultiplexação e demultiplexação não orientadas para conexão\nLembre-se, na Seção 2.7.1, de que um programa em Python que roda em um hospedeiro pode criar uma \nporta UDP com a linha\nclientSocket = socket(socket.AF_INET, socket.SOCK_DGRAM)\nQuando um socket UDP é criado dessa maneira, a camada de transporte automaticamente designa um \nnúmero de porta ao socket. Em especial, a camada de transporte designa um número de porta na faixa de 1024 \na 65535 que não esteja sendo usado naquele instante por qualquer outro socket UDP no hospedeiro. Como al-\nternativa, podemos incluir uma linha em nosso programa Python depois de criarmos o socket para associar um \nnúmero de porta específico (digamos, 19157) a este socket UDP por meio do método bind() do socket:\nclientSocket.bind((’’, 19157))\nSe o desenvolvedor responsável por escrever o código da aplicação estivesse executando o lado servidor de \num “protocolo bem conhecido”\n, ele teria de designar o número de porta bem conhecido correspondente. O lado \ncliente da aplicação em geral permite que a camada de transporte designe o número de porta de modo automáti-\nco (e transparente), ao passo que o lado servidor da aplicação designa um número de porta específico.\nAgora que os sockets UDP já têm seus números de porta designados, podemos descrever multiplexação/\ndemultiplexação UDP com precisão. Suponha que um processo no hospedeiro A, cujo número de porta UDP é \n19157, queira enviar uma porção de dados de aplicação a um processo cujo número de porta UDP seja 46428 no \nhospedeiro B. A camada de transporte no hospedeiro A cria um segmento de camada de transporte que inclui os \ndados de aplicação, o número da porta de origem (19157), o número da porta de destino (46428) e mais outros \ndois valores (que serão discutidos mais adiante, mas que não são importantes para a discussão em curso). Então, \na camada de transporte passa o segmento resultante para a camada de rede. Essa camada encapsula o segmento \nem um datagrama IP e faz uma tentativa de melhor esforço para entregar o segmento ao hospedeiro destinatário. \nSe o segmento chegar à máquina de destino B, a camada de destino no hospedeiro destinatário examinará o nú-\nmero da porta de destino no segmento (46428) e o entregará a seu socket identificado pelo número 46428. Note \nque a máquina B poderia estar rodando vários processos, cada um com sua própria porta UDP e número de porta \nassociado. À medida que segmentos UDP chegassem da rede, a máquina B direcionaria (demultiplexaria) cada \nsegmento à porta apropriada examinando o número de porta de destino do segmento.\nFigura 3.3  \u0007\nCampos de número de porta de origem e de destino em um segmento de camada de \ntransporte\nNúmero de porta\nde origem\n32 bits\nNúmero de porta\nde destino\nOutros campos de cabeçalho\nDados da aplicação\n(mensagem)\n   Redes de computadores e a Internet\n142\nÉ importante notar que um socket UDP é totalmente identificado por uma tupla com dois elementos, con-\nsistindo em um endereço IP de destino e um número de porta de destino. Por conseguinte, se dois segmentos \nUDP tiverem endereços IP de origem e/ou números de porta de origem diferentes, porém o mesmo endereço \nIP de destino e o mesmo número de porta de destino, eles serão direcionados ao mesmo processo de destino por \nmeio do mesmo socket de destino.\nÉ possível que agora você esteja imaginando qual é a finalidade do número da porta de origem. Como mos-\ntra a Figura 3.4, no segmento A-B, o número da porta de origem serve como parte de um “endereço de retorno” \n— quando B quer enviar um segmento de volta para A, a porta de destino no segmento B-A tomará seu valor \ndo valor da porta de origem do segmento A-B. (O endereço de retorno completo é o endereço IP e o número de \nporta de origem de A.) Como exemplo, lembre-se do programa servidor UDP que estudamos na Seção 2.7. Em \nUDPServer.py, o servidor usa um método recvfrom() para extrair o número de porta cliente-servidor (de \norigem) do segmento que recebe do cliente; então envia um novo segmento ao cliente, com o número de porta \nque extraiu servindo como o número de porta de destino desse novo segmento.\nFigura 3.4  Inversão dos números de porta de origem e destino\nHospedeiro A\nProcesso cliente\nSocket\nServidor B\nporta de\norigem: 19157\nporta de\ndestino: 46428\nporta de\ndestino: 46428\nporta de\norigem: 19157\nMultiplexação e demultiplexação orientadas para conexão\nPara entender demultiplexação TCP, temos de examinar de perto sockets TCP e estabelecimento de cone-\nxão TCP. Há uma diferença sutil entre um socket UDP e um TCP: o socket TCP é identificado por uma tupla de \nquatro elementos: (endereço IP de origem, número da porta de origem, endereço IP de destino, número da porta \nde destino). Assim, quando um segmento TCP que vem da rede chega a um hospedeiro, este usa todos os quatro \nvalores para direcionar (demultiplexar) o segmento para o socket apropriado. Em especial, e ao contrário do UDP, \ndois segmentos TCP chegando com endereços IP de origem ou números de porta de origem diferentes serão \ndirecionados para dois sockets diferentes (com exceção de um TCP que esteja carregando a requisição de estabe-\nlecimento de conexão original). Para perceber melhor, vamos considerar novamente o exemplo de programação \ncliente-servidor TCP apresentado na Seção 2.7.2:\n• A aplicação servidor TCP tem um “socket de entrada” que espera requisições de estabelecimento de cone-\nxão vindas de clientes TCP (ver Figura 2.29) na porta número 12000.\n• O cliente TCP cria um socket e envia um segmento de requisição de estabelecimento de conexão com as \nlinhas:\nCAMADA  de transporte  143 \nclientSocket = socket(AF_INET, SOCK_STREAM)\nclientSocket.connect((serverName,12000))\n• Uma requisição de estabelecimento de conexão nada mais é do que um segmento TCP com número de \nporta de destino 12000 e um bit especial de estabelecimento de conexão marcado no cabeçalho TCP (que \nserá tratado na Seção 3.5). O segmento inclui também um número de porta de origem que foi escolhido \npelo cliente.\n• Quando o sistema operacional do computador que está rodando o processo servidor recebe o segmen-\nto de requisição de conexão que está chegando e cuja porta de destino é 12000, ele localiza o processo \nservidor que está à espera para aceitar uma conexão na porta número 12000. Então, o processo servidor \ncria um novo socket:\nconnectionSocket, addr = serverSocket.accept()\n• A camada de transporte no servidor também nota os quatro valores seguintes no segmento de requisição \nde conexão: (1) o número da porta de origem no segmento, (2) o endereço IP do hospedeiro de origem, \n(3) o número da porta de destino no segmento e (4) seu próprio endereço IP. O socket de conexão recém­\n‑criado é identificado por esses quatro valores; todos os segmentos subsequentes que chegarem, cuja \nporta de origem, endereço IP de origem, porta de destino e endereço IP de destino combinar com esses \nquatro valores, serão demultiplexados para esse socket. Com a conexão TCP agora ativa, o cliente e o \nservidor podem enviar dados um para o outro.\nO hospedeiro servidor pode suportar vários sockets TCP simultâneos, sendo cada qual ligado a um processo \ne identificado por sua própria tupla de quatro elementos. Quando um segmento TCP chega ao hospedeiro, todos \nos quatro campos (endereço IP da origem, porta de origem, endereço IP de destino, porta de destino) são usados \npara direcionar (demultiplexar) o segmento para o socket apropriado.\nA situação é ilustrada na Figura 3.5, na qual o hospedeiro C inicia duas sessões HTTP com o servidor B, e o \nhospedeiro A inicia uma sessão HTTP com o servidor B. Os hospedeiros A e C e o servidor B possuem, cada um, \nFigura 3.5  \u0007\nDois clientes que usam o mesmo número de porta de destino (80) para se comunicar \ncom a mesma aplicação de servidor Web\nporta de\norigem: 7532\nporta de\ndestino: 80\nIP de origem:\nC\nIP de destino:\nB\nporta de\norigem: 26145\nporta de\ndestino: 80\nIP de origem:\nC\nIP de destino:\nB\nporta de\norigem: 26145\nporta de\ndestino: 80\nIP de origem:\nA\nIP de destino:\nB\nProcessos HTTP\npor conexão\nDemultiplexação\nna camada\nde transporte\nServidor\nWeb B\nCliente Web\nHospedeiro CC\nCliente Web\nHospedeiro A\n   Redes de computadores e a Internet\n144\nVarredura de porta\nVimos que um processo servidor espera com pa-\nciência, em uma porta aberta, o contato de um cliente \nremoto. Algumas portas são reservadas para aplica-\nções familiares (por exemplo, Web, FTP\n, DNS e os ser-\nvidores SMTP); outras são utilizadas por convenção \npor aplicações populares (por exemplo, o Microsoft \nSQL Server 2000 ouve as solicitações na porta 1434 \ndo UDP). Desta forma, se determinarmos que uma \nporta está aberta em um hospedeiro, talvez possamos \nmapeá-la para uma aplicação específica sendo exe-\ncutada no hospedeiro. Isso é muito útil para adminis-\ntradores de sistemas, que muitas vezes têm interesse \nem saber quais aplicações estão sendo executadas \nnos hospedeiros em suas redes. Porém, os atacantes, \na fim de “examinarem o local”, também querem saber \nquais portas estão abertas nos hospedeiros direcio-\nnados. Se o hospedeiro estiver sendo executado em \numa aplicação com uma falha de segurança conheci-\nda (por exemplo, um servidor SQL ouvindo em uma \nporta 1434 estava sujeito a estouro de buffer, permi-\ntindo que um usuário remoto execute um código ar-\nbitrário no hospedeiro vulnerável, uma falha explora-\nda pelo worm Slammer [CERT, 2003-04]), então esse \nhospedeiro está pronto para o ataque.\nDeterminar quais aplicações estão ouvindo em \nquais portas é uma tarefa de certo modo fácil. De \nfato, há inúmeros programas de domínio público, \ndenominados varredores de porta, que fazem exa-\ntamente isso. Talvez o mais utilizado seja o nmap, \ndisponível gratuitamente em http://nmap.org e inclu-\nído na maioria das distribuições Linux. Para o TCP\n, \no nmap varre portas sequencialmente, procurando \npor portas que aceitem conexões TCP\n. Para o UDP\n, o \nnmap de novo varre portas em sequência, procuran-\ndo por portas UDP que respondam aos segmentos \nUDP transmitidos. Em ambos os casos, o nmap re-\ntorna uma lista de portas abertas, fechadas ou inal-\ncançáveis. Um hospedeiro executando o nmap pode \ntentar varrer qualquer hospedeiro direcionado em \nqualquer lugar da Internet. Voltaremos a falar sobre o \nnmap na Seção 3.5.6, ao discutirmos gerenciamento \nda conexão TCP\n.\nSegurança em foco\nseu próprio endereço IP exclusivo: A, C e B, respectivamente. O hospedeiro C atribui dois números diferentes \n(26145 e 7532) da porta de origem às suas duas conexões HTTP. Como o hospedeiro A está escolhendo números \nde porta independentemente de C, ele poderia também atribuir um número da porta de origem 26145 à sua \nconexão HTTP. Apesar disso, o servidor B ainda será capaz de demultiplexar corretamente as duas conexões que \ntêm o mesmo número de porta de origem, já que elas têm endereços IP de origem diferentes.\nServidores Web e TCP\nAntes de encerrar esta discussão, é instrutivo falar um pouco mais sobre servidores Web e como eles usam \nnúmeros de porta. Considere um hospedeiro rodando um servidor Web, tal como um Apache, na porta 80. \nQuando clientes (por exemplo, navegadores) enviam segmentos ao servidor, todos os segmentos terão a porta \nde destino 80. Em especial, os segmentos que estabelecem a conexão inicial e os que carregam as mensagens de \nrequisição HTTP, ambos terão a porta de destino 80. Como acabamos de descrever, o servidor distingue os seg-\nmentos dos diferentes clientes pelos endereços IP e pelos números da porta de origem.\nA Figura 3.5 mostra um servidor Web que cria um novo processo para cada conexão. Como mostra a figura, \ncada um desses processos tem seu próprio socket de conexão pelo qual chegam requisições HTTP e são enviadas \nrespostas HTTP. Mencionamos, contudo, que nem sempre existe uma correspondência unívoca entre sockets de \nconexão e processos. Na verdade, os servidores Web de alto desempenho atuais muitas vezes utilizam somente \num processo, mas criam uma nova thread com um novo socket de conexão para cada nova conexão cliente. (Uma \nthread pode ser considerada um subprocesso leve.) Se você fez a primeira tarefa de programação do Capítulo \n2, construiu um servidor Web que faz exatamente isso. Para um servidor desses, a qualquer dado instante pode \nhaver muitos sockets de conexão (com identificadores diferentes) ligados ao mesmo processo.\nCAMADA  de transporte  145 \nSe o cliente e o servidor estiverem usando HTTP persistente, então, durante toda a conexão persistente, \ntrocarão mensagens HTTP pelo mesmo socket do servidor. Todavia, se usarem HTTP não persistente, então uma \nnova conexão TCP é criada e encerrada para cada requisição/resposta e, portanto, um novo socket é criado e mais \ntarde encerrado para cada requisição/resposta. Essa criação e encerramento frequentes de sockets podem causar \nsério impacto sobre o desempenho de um servidor Web movimentado (embora o sistema operacional consiga \nusar várias estratégias para atenuar o problema). Aconselhamos o leitor interessado em questões de sistema ope-\nracional referentes a HTTP persistente e não persistente a consultar [Nielsen, 1997; Nahum, 2002].\nAgora que já discutimos multiplexação e demultiplexação na camada de transporte, passemos à discussão \nde um dos protocolos da Internet, o UDP. Na próxima seção, veremos que o UDP acrescenta pouco mais ao pro-\ntocolo da camada de rede do que um serviço de multiplexação/demultiplexação.\n3.3  Transporte não orientado para conexão: UDP\nNesta seção, examinaremos o UDP mais de perto, como ele funciona e o que ele faz. Aconselhamos o leitor \na rever o material apresentado na Seção 2.1, que inclui uma visão geral do modelo de serviço UDP, e o da Seção \n2.7.1, que discute a programação de portas por UDP.\nPara motivar nossa discussão sobre UDP, suponha que você esteja interessado em projetar um protocolo \nde transporte simples, bem básico. Como faria isso? De início, você deve considerar a utilização de um pro-\ntocolo de transporte vazio. Em especial, do lado do remetente, considere pegar as mensagens do processo da \naplicação e passá-las diretamente para a camada de rede; do lado do destinatário, considere pegar as mensa-\ngens que chegam da camada de rede e passá-las diretamente ao processo da aplicação. Mas, como aprendemos \nna seção anterior, o que teremos de fazer é praticamente nada. No mínimo, a camada de transporte tem de \nfornecer um serviço de multiplexação/demultiplexação para passar os dados da camada de rede ao processo \nem nível de aplicação correto.\nO UDP, definido no [RFC 768], faz apenas quase tão pouco quanto um protocolo de transporte pode \nfazer. À parte sua função de multiplexação/demultiplexação e de alguma verificação de erros simples, ele nada \nadiciona ao IP. Na verdade, se o desenvolvedor de aplicação escolher o UDP, em vez do TCP, a aplicação estará \n“falando” quase diretamente com o IP. O UDP pega as mensagens do processo da aplicação, anexa os campos \nde número da porta de origem e de destino para o serviço de multiplexação/demultiplexação, adiciona dois \noutros pequenos campos e passa o segmento resultante à camada de rede, que encapsula o segmento dentro de \num datagrama IP e, em seguida, faz a melhor tentativa para entregar o segmento ao hospedeiro receptor. Se o \nsegmento chegar ao hospedeiro receptor, o UDP usará o número de porta de destino para entregar os dados \ndo segmento ao processo de aplicação correto. Note que, com o UDP, não há apresentação entre as entidades \nremetente e destinatária da camada de transporte antes de enviar um segmento. Por essa razão, dizemos que o \nUDP é não orientado para conexão.\nO DNS é um exemplo de protocolo de camada de aplicação que usa o UDP. Quando a aplicação DNS em \num hospedeiro quer fazer uma consulta, constrói uma mensagem de consulta DNS e passa a mensagem para o \nUDP. Sem realizar nenhuma apresentação com a entidade UDP que está funcionando no sistema final de des-\ntino, o UDP do lado do hospedeiro adiciona campos de cabeçalho à mensagem e passa o segmento resultante à \ncamada de rede, que encapsula o segmento UDP em um datagrama e o envia a um servidor de nomes. A aplica-\nção DNS no hospedeiro requisitante então espera por uma resposta à sua consulta. Se não receber uma resposta \n(possivelmente porque a rede subjacente perdeu a consulta ou a resposta), ela tentará enviar a consulta a outro \nservidor de nomes ou informará à aplicação consultante que não pode obter uma resposta.\nÉ possível que agora você esteja imaginando por que um desenvolvedor de aplicação escolheria construir \numa aplicação sobre UDP, em vez de sobre TCP. O TCP não é sempre preferível ao UDP, já que fornece serviço \nconfiável de transferência de dados e o UDP não? A resposta é “não”\n, pois muitas aplicações se adaptam melhor \nao UDP pelas seguintes razões:\n   Redes de computadores e a Internet\n146\n• Melhor controle no nível da aplicação sobre quais dados são enviados e quando. Com UDP, tão logo um \nprocesso de aplicação passe dados ao UDP, o protocolo os empacotará dentro de um segmento UDP e \nos passará imediatamente à camada de rede. O TCP, por outro lado, tem um mecanismo de controle de \ncongestionamento que limita o remetente TCP da camada de transporte quando um ou mais enlaces en-\ntre os hospedeiros da origem e do destinatário ficam congestionados demais. O TCP também continuará \na reenviar um segmento até que o hospedeiro destinatário reconheça a recepção desse segmento, pouco \nimportando o tempo que a entrega confiável levar. Visto que aplicações de tempo real requerem uma taxa \nmínima de envio, não querem atrasar demais a transmissão de segmentos e podem tolerar alguma perda \nde dados, o modelo de serviço do TCP não é particularmente compatível com as necessidades dessas \naplicações. Como discutiremos adiante, essas aplicações podem usar UDP e executar, como parte da \naplicação, qualquer funcionalidade adicional necessária além do serviço de entrega de segmentos simples \ne básico do UDP.\n• Não há estabelecimento de conexão. Como discutiremos adiante, o TCP usa uma apresentação de três vias antes \nde começar a transferir dados. O UDP simplesmente envia mensagens sem nenhuma preliminar formal e, \nassim, não introduz atraso algum para estabelecer uma conexão. Talvez seja esta a principal razão pela qual o \nDNS roda sobre UDP\n, e não sobre TCP — o DNS seria muito mais lento se rodasse em TCP\n. O HTTP usa o \nTCP\n, e não o UDP\n, porque a confiabilidade é fundamental para páginas Web com texto. Mas, como discutimos \nbrevemente na Seção 2.2, o atraso de estabelecimento de uma conexão TCP é uma contribuição importante aos \natrasos associados à recepção de documentos Web.\n• Não há estados de conexão. O TCP mantém o estado de conexão nos sistemas finais. Esse estado inclui buffers \nde envio e recebimento, parâmetros de controle de congestionamento e parâmetros numéricos de sequência \ne de reconhecimento. Veremos na Seção 3.5 que essa informação de estado é necessária para implementar \no serviço de transferência confiável de dados do TCP e para prover controle de congestionamento. O UDP\n, \npor sua vez, não mantém o estado de conexão e não monitora nenhum desses parâmetros. Por essa razão, \num servidor dedicado a uma aplicação específica pode suportar um número muito maior de clientes ativos \nquando a aplicação roda sobre UDP e não sobre TCP\n.\n• Pequeno excesso de cabeçalho de pacote. O segmento TCP tem 20 bytes de excesso (overhead) de cabeça-\nlho, além dos dados para cada segmento, enquanto o UDP tem somente 8 bytes de excesso.\nA Figura 3.6 relaciona aplicações populares da Internet e os protocolos de transporte que elas usam. Como \nera de esperar, o e-mail, o acesso a terminal remoto, a Web e a transferência de arquivos rodam sobre TCP — \ntodas essas aplicações necessitam do serviço confiável de transferência de dados do TCP. Não obstante, muitas \naplicações importantes executam sobre UDP, e não sobre TCP. O UDP é usado para atualização das tabelas de \nroteamento com o protocolo RIP (Routing Information Protocol — protocolo de informações de roteamento) \n(veja Seção 4.6.1). Como as atualizações RIP são enviadas periodicamente (em geral, a cada cinco minutos), \natualizações perdidas serão substituídas por mais recentes, tornando inútil a recuperação das perdidas. O UDP \ntambém é usado para levar dados de gerenciamento de rede (SNMP; veja o Capítulo 9). Nesse caso, o UDP é \npreferível ao TCP, já que aplicações de gerenciamento de rede com frequência devem funcionar quando a rede \nestá em estado sobrecarregado — exatamente quando é difícil conseguir transferência confiável de dados com \ncongestionamento controlado. E também, como mencionamos, o DNS roda sobre UDP, evitando, desse modo, \natrasos de estabelecimento de conexões TCP.\nComo mostra a Figura 3.6, hoje o UDP e o TCP também são comumente usados para aplicações de \nmultimídia, como telefone por Internet, videoconferência em tempo real e áudio e vídeo armazenados. Exa-\nminaremos essas aplicações mais de perto no Capítulo 7. No momento, mencionamos apenas que todas essas \naplicações podem tolerar uma pequena quantidade de perda de pacotes, de modo que a transferência confiável \nde dados não é absolutamente crítica para o sucesso da aplicação. Além disso, aplicações em tempo real, como \ntelefone por Internet e videoconferência, reagem muito mal ao controle de congestionamento do TCP. Por es-\nsas razões, os desenvolvedores de aplicações de multimídia muitas vezes optam por rodar suas aplicações sobre \nUDP em vez de sobre TCP. Entretanto, o TCP está sendo utilizado cada vez mais para transporte de mídia. \nCAMADA  de transporte  147 \nPor exemplo, Sripanidkulchai [2004] descobriu que cerca de 75% do fluxo em tempo real e gravado utilizavam \nTCP. Quando as taxas de perda de pacote são baixas, junto com algumas empresas que bloqueiam o tráfego \nUDP por razões de segurança (veja Capítulo 8), o TCP se torna um protocolo cada vez mais atrativo para o \ntransporte de mídia.\nEmbora hoje seja comum rodar aplicações de multimídia sobre UDP, isso é controvertido. Como já men-\ncionamos, o UDP não tem controle de congestionamento. Mas esse controle é necessário para evitar que a rede \nentre em um estado no qual pouquíssimo trabalho útil é realizado. Se todos começassem a enviar vídeo com \nalta taxa de bits sem usar nenhum controle de congestionamento, haveria tamanho transbordamento de pa-\ncotes nos roteadores que poucos pacotes UDP conseguiriam atravessar com sucesso o caminho da origem ao \ndestino. Além do mais, as altas taxas de perda induzidas pelos remetentes UDP sem controle fariam com que \nos remetentes TCP (que, como veremos adiante, reduzem suas taxas de envio em face de congestionamento) \nreduzissem drasticamente suas taxas. Assim, a falta de controle de congestionamento no UDP pode resultar \nem altas taxas de perda entre um remetente e um destinatário UDP e no acúmulo de sessões TCP — um pro-\nblema potencialmente sério [Floyd, 1999]. Muitos pesquisadores propuseram novos mecanismos para forçar \ntodas as origens, inclusive as origens UDP, a realizar um controle de congestionamento adaptativo [Mahdavi, \n1997; Floyd, 2000; Kohler, 2006: RFC 4340].\nAntes de discutirmos a estrutura do segmento UDP, mencionaremos que é possível uma aplicação ter trans-\nferência confiável de dados usando UDP. Isso pode ser feito se a confiabilidade for embutida na própria aplicação \n(por exemplo, adicionando mecanismos de reconhecimento e de retransmissão, tais como os que estudaremos na \npróxima seção). Mas essa é uma tarefa não trivial, que manteria o desenvolvedor ocupado com a depuração por \num longo tempo. Não obstante, embutir confiabilidade diretamente na aplicação permite que ela tire proveito de \nambas as alternativas. Em outras palavras, os processos da aplicação podem se comunicar de maneira confiável \nsem ter de se sujeitar às limitações de taxa de transmissão impostas pelo mecanismo de controle de congestiona-\nmento do TCP.\n3.3.1  Estrutura do segmento UDP\nA estrutura do segmento UDP, mostrada na Figura 3.7, é definida no RFC 768. Os dados da aplicação \nocupam o campo de dados do segmento UDP. Por exemplo, para o DNS, o campo de dados contém uma men-\nsagem de consulta ou uma mensagem de resposta. Para uma aplicação de recepção de áudio, amostras de áudio \npreenchem o campo de dados. O cabeçalho UDP tem apenas quatro campos, cada um consistindo em 2 bytes. \nComo já discutido na seção anterior, os números de porta permitem que o hospedeiro destinatário passe os da-\ndos da aplicação ao processo correto que está funcionando no sistema final destinatário (isto é, realize a função \nFigura 3.6  \u0007\nAplicações populares da Internet e seus protocolos de transporte subjacentes\nAplicação\nProtocolo da camada de aplicação\nProtocolo de transporte subjacente\nCorreio eletrônico\nSMTP\nTCP\nAcesso a terminal remoto\nTelnet\nTCP\nWeb\nHTTP\nTCP\nTransferência de arquivo\nFTP\nTCP\nServidor de arquivo remoto\nNFS\nTipicamente UDP\nRecepção de multimídia\nTipicamente proprietário\nUDP ou TCP\nTelefonia por Internet\nTipicamente proprietário\nUDP ou TCP\nGerenciamento de rede\nSNMP\nTipicamente UDP\nProtocolo de roteamento\nRIP\nTipicamente UDP\nTradução de nome\nDNS\nTipicamente UDP\n   Redes de computadores e a Internet\n148\nde demultiplexação). O campo de comprimento especifica o número de bytes no segmento UDP (cabeçalho mais \ndados). Um valor de comprimento explícito é necessário porque o tamanho do campo de dados pode ser dife-\nrente de um segmento UDP para o outro. A soma de verificação é usada pelo hospedeiro receptor para verificar \nse foram introduzidos erros no segmento. Na verdade, a soma de verificação também é calculada para alguns dos \ncampos no cabeçalho IP, além do segmento UDP. Mas ignoramos esse detalhe para podermos enxergar a floresta \npor entre as árvores. Discutiremos o cálculo da soma de verificação adiante. Os princípios básicos da detecção \nde erros estão descritos na Seção 5.2. O campo de comprimento especifica o comprimento do segmento UDP, \nincluindo o cabeçalho, em bytes.\nFigura 3.7  Estrutura do segmento UDP\nNúmero da porta\nde origem\n32 bits\nNúmero da porta\nde destino\nComprimento\nSoma de\nveriﬁcação\nDados da aplicação\n(mensagem)\n3.3.2  Soma de verificação UDP\nA soma de verificação UDP serve para detectar erros. Em outras palavras, é usada para determinar se bits \ndentro do segmento UDP foram alterados (por exemplo, por ruído nos enlaces ou enquanto armazenados em um \nroteador) durante sua movimentação da origem até o destino. O UDP no lado remetente realiza o complemento \nde 1 da soma de todas as palavras de 16 bits do segmento levando em conta o “vai um” em toda a soma. Esse \nresultado é colocado no campo de soma de verificação no segmento UDP. Damos aqui um exemplo simples do \ncálculo da soma de verificação. Se quiser saber detalhes sobre a implementação eficiente do algoritmo de cálculo, \nconsulte o RFC 1071; sobre o desempenho com dados reais, consulte Stone [1998 e 2000]. Como exemplo, supo-\nnha que tenhamos as seguintes três palavras de 16 bits:\n0110011001100000\n0101010101010101\n1000111100001100\nA soma das duas primeiras é:\n0110011001100000\n0101010101010101\n1011101110110101\nAdicionando a terceira palavra à soma anterior, temos:\n1011101110110101\n1000111100001100\n0100101011000010\nCAMADA  de transporte  149 \nNote que a última adição teve “vai um” no bit mais significativo que foi somado ao bit menos significativo. O \ncomplemento de 1 é obtido pela conversão de todos os 0 em 1 e de todos os 1 em 0. Desse modo, o complemento \nde 1 da soma 0100101011000010 é 1011010100111101, que passa a ser a soma de verificação. No destinatário, \ntodas as quatro palavras de 16 bits são somadas, inclusive a soma de verificação. Se nenhum erro for introduzido \nno pacote, a soma no destinatário será, claro, 1111111111111111. Se um dos bits for um zero, saberemos então \nque um erro foi introduzido no pacote.\nTalvez você esteja imaginando por que o UDP fornece uma soma de verificação primeiro, já que muitos \nprotocolos de camada de enlace (dentre os quais, o popular Ethernet) também fornecem verificação de erros. A \nrazão é que não há garantia de que todos os enlaces entre a origem e o destino forneçam tal verificação — um de-\nles pode usar um protocolo de camada de enlace que não a forneça. Além disso, mesmo que os segmentos sejam \ncorretamente transmitidos por um enlace, pode haver introdução de erros de bits quando um segmento é arma-\nzenado na memória de um roteador. Como não são garantidas nem a confiabilidade enlace a enlace, nem a de-\ntecção de erro na memória, o UDP deve prover detecção de erro fim a fim na camada de transporte se quisermos \nque o serviço de transferência de dados fim a fim forneça detecção de erro. É um exemplo do famoso princípio \nfim a fim do projeto de sistemas [Saltzer, 1984]. Tal princípio afirma que, visto ser dado como certo que fun-\ncionalidades (detecção de erro, neste caso) devem ser executadas fim a fim, “funções colocadas nos níveis mais \nbaixos podem ser redundantes ou de pouco valor em comparação com o custo de fornecê-las no nível mais alto”\n.\nComo se pretende que o IP rode sobre qualquer protocolo de camada 2, é útil que a camada de transporte \nforneça verificação de erros como medida de segurança. Embora o UDP forneça verificação de erros, ele nada \nfaz para recuperar-se de um erro. Algumas implementações do UDP apenas descartam o segmento danificado; \noutras passam o segmento errado à aplicação acompanhado de um aviso.\nIsso encerra nossa discussão sobre o UDP. Logo veremos que o TCP oferece transferência confiável de da-\ndos a suas aplicações, bem como outros serviços que o UDP não oferece. Naturalmente, o TCP também é mais \ncomplexo do que o UDP. Contudo, antes de discutirmos o TCP, primeiro devemos examinar os princípios subja-\ncentes da transferência confiável de dados.\n3.4  \u0007\nPrincípios da transferência confiável de dados\nNesta seção, consideramos o problema conceitual da transferência confiável de dados. Isso é apropriado, já \nque o problema de realizar transferência confiável de dados ocorre não só na camada de transporte, mas também \nna de enlace e na de aplicação. Assim, o problema geral é de importância central para o trabalho em rede. Na ver-\ndade, se tivéssemos de fazer uma lista dos dez maiores problemas mais importantes para todo o trabalho em rede, \no da transferência confiável de dados seria o candidato número um da lista. Na seção seguinte, examinaremos o \nTCP e mostraremos, em especial, que ele utiliza muitos dos princípios que descreveremos aqui.\nA Figura 3.8 ilustra a estrutura para nosso estudo de transferência confiável de dados. A abstração do ser-\nviço fornecido às entidades das camadas superiores é a de um canal confiável através do qual dados podem ser \ntransferidos. Com um canal confiável, nenhum dos dados transferidos é corrompido (trocado de 0 para 1 ou \nvice-versa) nem perdido, e todos são entregues na ordem em que foram enviados. Este é exatamente o modelo de \nserviço oferecido pelo TCP às aplicações de Internet que recorrem a ele.\nÉ responsabilidade de um protocolo de transferência confiável de dados implementar essa abstração de \nserviço. A tarefa é dificultada pelo fato de que a camada abaixo do protocolo de transferência confiável de dados \ntalvez seja não confiável. Por exemplo, o TCP é um protocolo confiável de transferência de dados que é execu-\ntado sobre uma camada de rede fim a fim não confiável (IP). De modo mais geral, a camada abaixo das duas \nextremidades que se comunicam de modo confiável pode consistir em um único enlace físico (como no caso de \num protocolo de transferência de dados na camada de enlace) ou em uma rede global interligada (como em um \nprotocolo de camada de transporte). Para nossa finalidade, contudo, podemos considerar essa camada mais baixa \napenas como um canal ponto a ponto não confiável.\n   Redes de computadores e a Internet\n150\nNesta seção, desenvolveremos de modo gradual os lados remetente e destinatário de um protocolo confiável \nde transferência de dados, considerando modelos progressivamente mais complexos do canal subjacente. Por \nexemplo, vamos considerar que os mecanismos do protocolo são necessários quando o canal subjacente puder \ncorromper bits ou perder pacotes inteiros. Uma suposição que adotaremos em toda essa discussão é que os pa-\ncotes serão entregues na ordem em que foram enviados, com alguns pacotes possivelmente sendo perdidos; ou \nseja, o canal subjacente não reordenará pacotes. A Figura 3.8(b) ilustra as interfaces de nosso protocolo de trans-\nferência de dados. O lado remetente do protocolo será invocado de cima, por uma chamada a rdt_send(). \nEle passará os dados a serem entregues à camada superior no lado destinatário. (Aqui, rdt significa protocolo \nreliable data transfer — transferência confiável de dados — e _send indica que o lado remetente do rdt está \nsendo chamado. O primeiro passo no desenvolvimento de qualquer protocolo é dar-lhe um bom nome!) Do lado \ndestinatário, rdt_rcv() será chamado quando um pacote chegar pelo lado destinatário do canal. Quando o \nprotocolo rdt quiser entregar dados à camada superior, ele o fará chamando deliver_data(). No que se \nsegue, usamos a terminologia “pacote” em vez de “segmento” de camada de transporte. Como a teoria desenvol-\nvida nesta seção se aplica a redes de computadores em geral, e não só à camada de transporte da Internet, o termo \ngenérico “pacote” talvez seja mais apropriado aqui.\nNesta seção, consideramos apenas o caso de transferência unidirecional de dados, isto é, transferência de \ndados do lado remetente ao lado destinatário. O caso de transferência bidirecional confiável de dados (isto é, \nfull-duplex) não é conceitualmente mais difícil, mas é bem mais tedioso de explicar. Embora consideremos apenas \na transferência unidirecional de dados, é importante notar que, apesar disso, os lados remetente e destinatário de \nnosso protocolo terão de transmitir pacotes em ambas as direções, como mostra a Figura 3.8. Logo veremos que, \nalém de trocar pacotes contendo os dados a transferir, os lados remetente e destinatário do rdt também precisarão \nFigura 3.8  \u0007\nTransferência confiável de dados: modelo do serviço e implementação do serviço\nCanal conﬁável\nCanal não conﬁável\nrdt_send()\nudt_send()\nProcesso\nremetente\nProcesso\ndestinatário\ndeliver_data\nCamada de\naplicação\nCamada de\ntransporte\na. Serviço fornecido\nCamada\nde rede\nLegenda:\nDados\nPacote\nb. Implementação do serviço\nProtocolo de\ntransferência\nconﬁável de dados\n(lado remetente)\nProtocolo de\ntransferência\nconﬁável de dados\n(lado destinatário)\nrdt_rcv()\nCAMADA  de transporte  151 \ntrocar pacotes de controle entre si. Ambos os lados de envio e destino do rdt enviam pacotes para o outro por meio de \numa chamada a udt_send() (em que udt significa unreliable data transfer — transferência não confiável de dados).\n3.4.1  \u0007\nConstruindo um protocolo de transferência confiável de dados\nVamos percorrer agora uma série de protocolos que vão se tornando cada vez mais complexos, até chegar a \num protocolo de transferência confiável de dados impecável.\nTransferência confiável de dados sobre um canal perfeitamente confiável: rdt1.0\nConsideremos primeiro o caso mais simples, em que o canal subjacente é completamente confiável. O proto-\ncolo em si, que denominaremos rdt1.0, é trivial. As definições de máquina de estado finito (finite-state machine \n— FSM) para o remetente e o destinatário rdt1.0 são apresentadas na Figura 3.9. A FSM da Figura 3.9(a) define a \noperação do remetente, enquanto a FSM da Figura 3.9(b) define a operação do destinatário. É importante notar que \nhá FSM separadas para o remetente e o destinatário. Ambas as FSM da Figura 3.9 têm apenas um estado. As setas na \ndescrição da FSM indicam a transição do protocolo de um estado para outro. (Como cada FSM da Figura 3.9 tem \napenas um estado, uma transição é, necessariamente, de um dado estado para ele mesmo; examinaremos diagramas \nde estados mais complicados em breve.) O evento que causou a transição é mostrado acima da linha horizontal que \na rotula, e as ações realizadas quando ocorre o evento são mostradas abaixo dessa linha. Quando nenhuma ação \né realizada em um evento, ou quando não ocorre nenhum evento e uma ação é realizada, usaremos o símbolo L, \nacima ou abaixo da linha horizontal, para indicar a falta de uma ação ou de um evento, respectivamente. O estado \ninicial da FSM é indicado pela seta tracejada. Embora as FSM da Figura 3.9 tenham apenas um estado, as outras que \nveremos em breve têm vários, portanto, será importante identificar o estado inicial de cada FSM.\nO lado remetente do rdt apenas aceita dados da camada superior pelo evento rdt_send(data), cria um \npacote que contém os dados (pela ação make_pkt(data)) e o envia para dentro do canal. Na prática, o evento \nrdt_send(data) resultaria de uma chamada de procedimento (por exemplo, para rdt_send()) pela apli-\ncação da camada superior.\nDo lado destinatário, rdt recebe um pacote do canal subjacente pelo evento rdt_rcv(packet), extrai \nos dados do pacote (pela ação extract(packet, data)) e os passa para a camada superior (pela ação \nFigura 3.9  rdt1.0 – Um protocolo para um canal completamente confiável\nEsperar\nchamada\nde cima\na.  rdt1.0: lado remetente\nrdt_send(data)\npacket=make_pkt(data)\nudt_send(packet)\nEsperar\nchamada\nde baixo\nb.  rdt1.0: lado destinatário\nrdt_rcv(packet)\nextract(packet,data)\ndeliver_data(data)\n   Redes de computadores e a Internet\n152\ndeliver_data(data)). Na prática, o evento rdt_ rcv(packet) resultaria de uma chamada de proce-\ndimento (por exemplo, para rdt_rcv( )) do protocolo da camada inferior.\nNesse protocolo simples, não há diferença entre a unidade de dados e um pacote. E, também, todo o fluxo \nde pacotes corre do remetente para o destinatário; com um canal perfeitamente confiável, não há necessidade de \no lado destinatário fornecer qualquer informação ao remetente, já que nada pode dar errado! Note que também \nadmitimos que o destinatário está capacitado a receber dados seja qual for a velocidade em que o remetente os \nenvie. Assim, não há necessidade de pedir para o remetente desacelerar!\nTransferência confiável de dados por um canal com erros de bits: rdt2.0\nUm modelo mais realista de canal subjacente é um canal em que os bits de um pacote podem ser corrompi-\ndos. Esses erros de bits ocorrem em geral nos componentes físicos de uma rede enquanto o pacote é transmitido, \npropagado ou armazenado. Continuaremos a admitir, por enquanto, que todos os pacotes transmitidos sejam \nrecebidos (embora seus bits possam estar corrompidos) na ordem em que foram enviados.\nAntes de desenvolver um protocolo para se comunicar de maneira confiável com esse canal, considere pri-\nmeiro como as pessoas enfrentariam uma situação como essa. Imagine como você ditaria uma mensagem longa \npelo telefone. Em um cenário típico, quem estivesse anotando a mensagem diria “OK” após cada sentença que \nouvisse, entendesse e anotasse. Se a pessoa ouvisse uma mensagem truncada, pediria que você a repetisse. Esse \nprotocolo de ditado de mensagem usa reconhecimentos positivos (“OK”) e reconhecimentos negativos (“Repi-\nta, por favor”). Tais mensagens de controle permitem que o destinatário faça o remetente saber o que foi recebido \ncorretamente e o que foi recebido com erro e, portanto, exige repetição. Em um arranjo de rede de computadores, \nprotocolos de transferência confiável de dados baseados nesse tipo de retransmissão são conhecidos como proto-\ncolos ARQ (Automatic Repeat reQuest — solicitação automática de repetição).\nBasicamente, são exigidas três capacitações adicionais dos protocolos ARQ para manipular a presença de \nerros de bits:\n• Detecção de erros. Primeiro, é preciso um mecanismo que permita ao destinatário detectar quando \nocorrem erros. Lembre-se de que dissemos na seção anterior que o UDP usa o campo de soma de veri-\nficação da Internet exatamente para essa finalidade. No Capítulo 5, examinaremos, com mais detalhes, \ntécnicas de detecção e de correção de erros. Elas permitem que o destinatário detecte e talvez corrija \nerros de bits de pacotes. Por enquanto, basta saber que essas técnicas exigem que bits extras (além dos \nbits dos dados originais a serem transferidos) sejam enviados do remetente ao destinatário. Esses bits \nsão colocados no campo de soma de verificação do pacote de dados do protocolo rdt2.0.\n• Realimentação do destinatário. Como remetente e destinatário em geral estejam rodando em sistemas \nfinais diferentes, possivelmente separados por milhares de quilômetros, o único modo de o remetente sa-\nber qual é a visão de mundo do destinatário (neste caso, se um pacote foi recebido corretamente ou não) \né o destinatário fornecer realimentação explícita ao remetente. As respostas de reconhecimento positivo \n(ACK) ou negativo (NAK) no cenário do ditado da mensagem são exemplos dessa realimentação. Nosso \nprotocolo rdt2.0 devolverá, dessa mesma maneira, pacotes ACK e NAK do destinatário ao remetente. \nEm princípio, esses pacotes precisam apenas ter o comprimento de um bit; por exemplo, um valor 0 po-\nderia indicar um NAK e um valor 1 poderia indicar um ACK.\n• Retransmissão. Um pacote que é recebido com erro no destinatário será retransmitido pelo remetente.\nA Figura 3.10 mostra a representação por FSM do rdt2.0, um protocolo de transferência de dados que \nemprega detecção de erros, reconhecimentos positivos e reconhecimentos negativos.\nO lado remetente do rdt2.0 tem dois estados. No estado mais à esquerda, o protocolo do lado remeten-\nte está esperando que os dados sejam passados pela camada superior. Quando o evento rdt_send(data) \nocorrer, o remetente criará um pacote (sndpkt) contendo os dados a serem enviados, junto com uma soma \nde verificação do pacote (por exemplo, como discutimos na Seção 3.3.2 para o caso de um segmento UDP) e, \nCAMADA  de transporte  153 \nentão, enviará o pacote pela operação udt_send(sndpkt). No estado mais à direita, o protocolo remetente \nestá esperando por um pacote ACK ou NAK da parte do destinatário. Se um pacote ACK for recebido (a notação \nrdt_rcv(rcvpkt) && isACK (rcvpkt) na Figura 3.10 corresponde a esse evento), o remetente saberá \nque o pacote transmitido mais recentemente foi recebido corretamente. Assim, o protocolo volta ao estado de \nespera por dados vindos da camada superior. Se for recebido um NAK, o protocolo retransmitirá o último pacote \ne esperará por um ACK ou NAK a ser devolvido pelo destinatário em resposta ao pacote de dados retransmitido. \nÉ importante notar que, quando o destinatário está no estado de espera por ACK ou NAK, não pode receber \nmais dados da camada superior; isto é, o evento rdt_send() não pode ocorrer; isso somente acontecerá após \no remetente receber um ACK e sair desse estado. Assim, o remetente não enviará novos dados até ter certeza de \nque o destinatário recebeu corretamente o pacote em questão. Devido a esse comportamento, protocolos como o \nrdt2.0 são conhecidos como protocolos pare e espere (stop-and-wait).\nA FSM do lado destinatário para o rdt2.0 tem um único estado. Quando o pacote chega, o destinatário \nresponde com um ACK ou um NAK, dependendo de o pacote recebido estar ou não corrompido. Na Figura 3.10, \na notação rdt_rcv(rcvpkt) && corrupt(rcvpkt) corresponde ao evento em que um pacote é recebido \ne existe um erro.\nPode parecer que o protocolo rdt2.0 funciona, mas infelizmente ele tem um defeito fatal. Em especial, \nainda não tratamos da possibilidade de o pacote ACK ou NAK estar corrompido! (Antes de continuar, é bom \nvocê começar a pensar em como esse problema pode ser resolvido.) Lamentavelmente, nossa pequena omissão \nnão é tão inofensiva quanto possa parecer. No mínimo, precisaremos adicionar aos pacotes ACK/NAK bits de \nFigura 3.10  rdt2.0 – Um protocolo para um canal com erros de bits\nEsperar\nchamada\nde cima\na.  rdt2.0: lado remetente\nb.  rdt2.0: lado destinatário\nrdt_rcv(rcvpkt) && corrupt(rcvpkt)\nsndpkt=make_pkt(NAK)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && isNAK(rcvpkt)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && isACK(rcvpkt)\nΛ\nrdt_send(data)\nsndpkt=make_pkt(data,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK)\nudt_send(sndpkt)\nEsperar\nchamada\nde baixo\nEsperar\nACK ou\nNAK\n   Redes de computadores e a Internet\n154\nsoma de verificação para detectar esses erros. A questão mais difícil é como o protocolo deve se recuperar de \nerros em pacotes ACK ou NAK. Nesse caso, a dificuldade é que, se um ACK ou um NAK estiver corrompido, o re-\nmetente não terá como saber se o destinatário recebeu ou não corretamente a última parte de dados transmitidos.\nConsidere três possibilidades para manipular ACKs ou NAKs corrompidos:\n• Para a primeira possibilidade, imagine o que um ser humano faria no cenário do ditado da mensagem. \nSe quem estiver ditando não entender o “OK” ou o “Repita, por favor” do destinatário, provavelmente \nperguntará: “O que foi que você disse?” (introduzindo assim um novo tipo de pacote remetente-destina-\ntário em nosso protocolo). O destinatário então repetiria a resposta. Mas e se a frase “O que foi que você \ndisse?” estivesse corrompida? O destinatário, sem ter nenhuma noção se a sentença corrompida era parte \ndo ditado ou um pedido para repetir a última resposta, provavelmente responderia: “O que foi que você \ndisse?”\n. E então, é claro, essa resposta também poderia estar truncada. É óbvio que estamos entrando em \num caminho difícil.\n• Uma segunda alternativa é adicionar um número suficiente de bits de soma de verificação para permitir \nque o remetente não só detecte, mas também se recupere de erros de bits. Isso resolve o problema ime-\ndiato para um canal que pode corromper pacotes, mas não perdê-los.\n• Uma terceira técnica é o remetente reenviar o pacote de dados corrente quando receber um pacote \nACK ou NAK truncado. Esse método, no entanto, introduz pacotes duplicados no canal remetente-\ndestinatário. A dificuldade fundamental com pacotes duplicados é que o destinatário não sabe se o \núltimo ACK ou NAK que enviou foi bem recebido no remetente. Assim, ele não pode saber a priori se \num pacote que chega contém novos dados ou se é uma retransmissão!\nUma solução simples para esse novo problema (e que é adotada em quase todos os protocolos de transfe-\nrência de dados existentes, inclusive o TCP) é adicionar um novo campo ao pacote de dados e fazer o remetente \nnumerar seus pacotes de dados colocando um número de sequência nesse campo. O destinatário então teria \napenas de verificar esse número de sequência para determinar se o pacote recebido é ou não uma retransmissão. \nPara esse caso simples de protocolo pare e espere, um número de sequência de um bit é suficiente, já que per-\nmitirá que o destinatário saiba se o remetente está reenviando o pacote previamente transmitido (o número de \nsequência do pacote recebido é o mesmo do pacote recebido mais recentemente) ou um novo pacote (o número \nde sequência muda, indo “para a frente” em progressão aritmética de módulo 2). Como estamos admitindo que \neste é um canal que não perde pacotes, os pacotes ACK e NAK em si não precisam indicar o número de sequência \ndo pacote que estão reconhecendo. O remetente sabe que um pacote ACK ou NAK recebido (truncado ou não) \nfoi gerado em resposta ao seu pacote de dados transmitidos mais recentemente.\nAs figuras 3.11 e 3.12 mostram a descrição da FSM para o rdt2.1, nossa versão corrigida do rdt2.0. \nCada rdt2.1 remetente e destinatário da FSM agora tem um número duas vezes maior de estados do que antes. \nIsso acontece porque o estado do protocolo deve agora refletir se o pacote que está sendo correntemente enviado \n(pelo remetente) ou aguardado (no destinatário) deveria ter um número de sequência 0 ou 1. Note que as ações \nnos estados em que um pacote numerado com 0 está sendo enviado ou aguardado são imagens especulares da-\nquelas que devem funcionar quando estiver sendo enviado ou aguardado um pacote numerado com 1; as únicas \ndiferenças têm a ver com a manipulação do número de sequência.\nO protocolo rdt2.1 usa tanto o reconhecimento positivo como o negativo do remetente ao destinatário. \nQuando um pacote fora de ordem é recebido, o destinatário envia um reconhecimento positivo para o pacote que \nrecebeu; quando um pacote corrompido é recebido, ele envia um reconhecimento negativo. Podemos conseguir \no mesmo efeito de um pacote NAK se, em vez de enviarmos um NAK, enviarmos um ACK em seu lugar para o \núltimo pacote corretamente recebido. Um remetente que recebe dois ACKs para o mesmo pacote (isto é, ACKs \nduplicados) sabe que o destinatário não recebeu corretamente o pacote seguinte àquele para o qual estão sendo \ndados dois ACKs. Nosso protocolo de transferência confiável de dados sem NAK para um canal com erros de bits \né o rdt2.2, mostrado nas figuras 3.13 e 3.14. Uma modificação sutil entre rdt2.1 e rdt2.2 é que o destinatá-\nrio agora deve incluir o número de sequência do pacote que está sendo reconhecido por uma mensagem ACK \nCAMADA  de transporte  155 \n(o que é feito incluindo o argumento ACK, 0 ou ACK, 1 em make_pkt() na FSM destinatária) e o remetente \nagora deve verificar o número de sequência do pacote que está sendo reconhecido por uma mensagem ACK re-\ncebida (o que é feito incluindo o argumento 0 ou 1 em isACK() na FSM remetente).\nFigura 3.11  rdt2.1 remetente\nEsperar\nchamada 0\nde cima\nrdt_rcv(rcvpkt)&&\n(corrupt(rcvpkt)||\nisNAK(rcvpkt))\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)&&\n(corrupt(rcvpkt)||\nisNAK(rcvpkt))\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt)\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt)\nΛ\nΛ\nrdt_send(data)\nsndpkt=make_pkt(0,data,checksum)\nudt_send(sndpkt)\nrdt_send(data)\nsndpkt=make_pkt(1,data,checksum)\nudt_send(sndpkt)\nEsperar\nACK ou\nNAK 0\nEsperar\nACK ou\nNAK 1\nEsperar\nchamada 1\nde cima\nFigura 3.12  rdt2.1 destinatário\nrdt_rcv(rcvpkt)&& notcorrupt\n(rcvpkt)&&has_seq0(rcvpkt)\nsndpkt=make_pkt(ACK,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && corrupt(rcvpkt)\nsndpkt=make_pkt(NAK,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)\n && corrupt(rcvpkt)\nsndpkt=make_pkt(NAK,checksum)\nudt_send(sndpkt)\nsndpkt=make_pkt(ACK,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq1(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)&& notcorrupt(rcvpkt)\n && has_seq0(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK,checksum)\nudt_send(sndpkt)\nEsperar 0\nde baixo\nEsperar 1\nde baixo\nrdt_rcv(rcvpkt)&& notcorrupt\n(rcvpkt)&&has_seq1(rcvpkt)\n   Redes de computadores e a Internet\n156\nTransferência confiável de dados por um canal com perda e com erros de bits: rdt3.0\nSuponha agora que, além de corromper bits, o canal subjacente possa perder pacotes, um acontecimento \nque não é incomum nas redes de computadores de hoje (incluindo a Internet). Duas preocupações adicionais \ndevem agora ser tratadas pelo protocolo: como detectar perda de pacote e o que fazer quando isso ocorre. A \nutilização de soma de verificação, números de sequência, pacotes ACK e retransmissões — as técnicas já desen-\nvolvidas em rdt2.2 — nos permitirá atender a última preocupação. Lidar com a primeira preocupação, por sua \nvez, exigirá a adição de um novo mecanismo de protocolo.\nFigura 3.13  rdt2.2 remetente\nFigura 3.14  rdt2.2 destinatário\nEsperar\nchamada 0\nde cima\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nisACK(rcvpkt,1))\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nisACK(rcvpkt,0))\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt,0)\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt,1)\nrdt_send(data)\nsndpkt=make_pkt(0,data,checksum)\nudt_send(sndpkt)\nrdt_send(data)\nsndpkt=make_pkt(1,data,checksum)\nudt_send(sndpkt)\nEsperar\nACK 0\nEsperar\nACK 1\nΛ\nΛ\nEsperar\nchamada 1\nde cima\nEsperar 0\nde baixo\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nhas_seq0(rcvpkt))\nsndpkt=make_pkt(ACK,0,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nhas_seq1(rcvpkt))\nsndpkt=make_pkt(ACK,1,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq1(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK,1,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq0(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK,0,checksum)\nudt_send(sndpkt)\nEsperar 1\nde baixo\nCAMADA  de transporte  157 \nHá muitas abordagens possíveis para lidar com a perda de pacote (e diversas delas serão estudadas nos \nexercícios ao final do capítulo). Aqui, atribuiremos ao remetente o encargo de detectar e se recuperar das perdas \nde pacote. Suponha que o remetente transmita um pacote de dados e que esse pacote, ou o ACK do seu destina-\ntário, seja perdido. Em qualquer um dos casos, nenhuma resposta chegará ao remetente vinda do destinatário. Se \no remetente estiver disposto a esperar o tempo suficiente para ter certeza de que o pacote foi perdido, ele poderá \napenas retransmitir o pacote de dados. É preciso que você se convença de que esse protocolo funciona mesmo.\nMas quanto o remetente precisa esperar para ter certeza de que algo foi perdido? É claro que deve aguardar \nno mínimo o tempo de um atraso de ida e volta entre ele e o destinatário (o que pode incluir buffers em roteadores \nou equipamentos intermediários) e mais o tempo que for necessário para processar um pacote no destinatário. \nEm muitas redes, o atraso máximo para esses piores casos é muito difícil até de estimar, quanto mais saber com \ncerteza. Além disso, o ideal seria que o protocolo se recuperasse da perda de pacotes logo que possível; esperar \npelo atraso do pior dos casos pode significar um longo tempo até que a recuperação do erro seja iniciada. Assim, \na técnica adotada na prática é a seguinte: o remetente faz uma escolha ponderada de um valor de tempo dentro \ndo qual seria provável, mas não garantido, que a perda tivesse acontecido. Se não for recebido um ACK nesse \nperíodo, o pacote é retransmitido. Note que, se um pacote sofrer um atraso particularmente longo, o remetente \npoderá retransmiti-lo mesmo que nem o pacote de dados, nem o seu ACK tenham sido perdidos. Isso introduz a \npossibilidade de pacotes de dados duplicados no canal remetente-destinatário. Felizmente, o protocolo rdt2.2 \njá dispõe de funcionalidade suficiente (isto é, números de sequência) para tratar dos casos de pacotes duplicados.\nDo ponto de vista do remetente, a retransmissão é uma panaceia. O remetente não sabe se um pacote de \ndados foi perdido, se um ACK foi perdido ou se o pacote ou o ACK apenas estavam muito atrasados. Em todos \nos casos, a ação é a mesma: retransmitir. Para implementar um mecanismo de retransmissão com base no tempo, \né necessário um temporizador de contagem regressiva que interrompa o processo remetente após ter decorrido \num dado tempo. Assim, será preciso que o remetente possa (1) acionar o temporizador todas as vezes que um \npacote for enviado (quer seja a primeira vez, quer seja uma retransmissão), (2) responder a uma interrupção feita \npelo temporizador (realizando as ações necessárias) e (3) parar o temporizador.\nA Figura 3.15 mostra a FSM remetente para o rdt3.0, um protocolo que transfere dados de modo confiável \npor um canal que pode corromper ou perder pacotes; nos “Exercícios de fixação” pediremos a você que projete a \nEsperar\nchamada 0\nde cima\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nisACK(rcvpkt,1))\ntimeout\nudt_send(sndpkt)\nstart_timer\nrdt_rcv(rcvpkt)\nΛ\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nisACK(rcvpkt,0))\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt,0)\nstop_timer\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt,1)\nstop_timer\ntimeout\nudt_send(sndpkt)\nstart_timer\nrdt_send(data)\nsndpkt=make_pkt(0,data,checksum)\nudt_send(sndpkt)\nstart_timer\nrdt_send(data)\nsndpkt=make_pkt(1,data,checksum)\nudt_send(sndpkt)\nstart_timer\nEsperar\nACK 0\nEsperar\nACK 1\nΛ\nΛ\nEsperar\nchamada 1\nde cima\nrdt_rcv(rcvpkt)\nΛ\nFigura 3.15  rdt3.0 remetente\n   Redes de computadores e a Internet\n158\nFSM destinatária para rdt3.0. A Figura 3.16 mostra como o protocolo funciona sem pacotes perdidos ou atrasa-\ndos e como manipula pacotes de dados perdidos. Nessa figura, a passagem do tempo ocorre do topo do diagrama \npara baixo. Note que o instante de recebimento de um pacote tem de ser posterior ao instante de envio de um pacote, \ncomo resultado de atrasos de transmissão e de propagação. Nas figuras 3.16(b-d), os colchetes do lado remetente \nindicam os instantes em que o temporizador foi acionado e, mais tarde, os instantes em que ele parou. Vários dos \naspectos mais sutis desse protocolo são examinados nos exercícios ao final deste capítulo. Como os números de \nsequência se alternam entre 0 e 1, o protocolo rdt3.0 às vezes é conhecido como protocolo bit alternante.\nAgora já reunimos os elementos fundamentais de um protocolo de transferência de dados. Somas de ve-\nrificação, números de sequência, temporizadores e pacotes de reconhecimento negativo e positivo — cada um \ndesempenha um papel crucial e necessário na operação do protocolo. Temos agora em funcionamento um pro-\ntocolo de transferência confiável de dados!\nrecebe pkt0\nenvia ACK0\nrecebe pkt1\nenvia ACK1\nrecebe pkt0\nenvia ACK0\nRemetente\nDestinatário\na. Operação sem perda\npkt0\nACK0\npkt1\npkt0\nACK1\nACK0\n(perda) X\nb. Pacote perdido\nrecebe pkt0\nenvia ACK0\nrecebe pkt1\nenvia ACK1\nc. ACK perdido\nenvia pkt0\nrecebe ACK0\nenvia pkt1\nrecebe ACK1\nenvia pkt0\nenvia pkt0\nrecebe ACK0\nenvia pkt1\ntemporização\nreenvia pkt1\nrecebe ACK1\nenvia pkt0\nrecebe pkt0\nenvia ACK0\nrecebe pkt1\n(detecta\nduplicação)\nenvia ACK1\nenvia pkt0\nrecebe ACK0\nenvia pkt1\nrecebe pkt0\nenvia ACK0\ntemporização\nreenvia pkt1\nrecebe pkt1\nenvia ACK1\nd. Temporização prematura\nrecebe ACK1\nenvia pkt0\nrecebe ACK1\nnão faz nada\nrecebe pkt0\nenvia ACK0\nrecebe pkt 1\n(detecta duplicação)\nenvia ACK1\nRemetente\nDestinatário\nDestinatário\nRemetente\npkt0\nACK0\npkt1\nACK1\nACK1\nACK0\nACK1\nACK0\npkt1\npkt0\npkt0\npkt1\npkt1\npkt0\nACK1\nACK0\nX (perda)\npkt1\nrecebe pkt0\nenvia ACK0\nenvia pkt0\nrecebe ACK0\nenvia pkt1\ntemporização\nreenvia pkt1\nrecebe ACK1\nenvia pkt0\nrecebe pkt0\nenvia ACK0\nrecebe pkt1\nenvia ACK1\nRemetente\nDestinatário\npkt0\nACK0\npkt1\npkt0\nACK1\nACK0\nFigura 3.16  \u0007\nOperação do rdt3.0, o protocolo bit alternante\nCAMADA  de transporte  159 \n3.4.2  \u0007\nProtocolos de transferência confiável de dados com paralelismo\nO protocolo rdt3.0 é correto em termos funcionais, mas é pouco provável que alguém fique contente com \no desempenho dele, ainda mais nas redes de alta velocidade de hoje. No coração do problema do desempenho do \nrdt3.0 está o fato de ele ser um protocolo do tipo pare e espere.\nPara avaliar o impacto sobre o desempenho causado pelo comportamento “pare e espere”\n, considere um \ncaso ideal de dois hospedeiros, um localizado na Costa Oeste dos Estados Unidos e outro na Costa Leste, como \nmostra a Figura 3.17. O atraso de propagação de ida e volta à velocidade da luz, RTT, entre esses dois sistemas finais \né de cerca de 30 ms. Suponha que eles estejam conectados por um canal com capacidade de transmissão, R, de 1 Gbit/s \n(109 bits por segundo). Para um tamanho de pacote, L, de mil bytes (8 mil bits), incluindo o campo de cabeçalho e \ntambém o de dados, o tempo necessário para realmente transmitir o pacote para o enlace de 1 Gbit/s é:\nttrans = L\nR =\n8.000 bits/pacote\n109 bits/s\n= 8 µs\nA Figura 3.18(a) mostra que, com nosso protocolo pare e espere, se o remetente começar a enviar o pacote \nem t = 0, então em t = L/R = 8 µs, o último bit entrará no canal do lado remetente. O pacote então faz sua jornada \nde 15 ms atravessando o país, com o último bit do pacote emergindo no destinatário em t = RTT/2 + L/R = \n15,008 ms. Supondo, para simplificar, que pacotes ACK sejam extremamente pequenos (para podermos ignorar \nseu tempo de transmissão) e que o destinatário pode enviar um ACK logo que receber o último bit de um pacote \nde dados, o ACK emergirá de volta no remetente em t = RTT + L/R = 30,008 ms. Nesse ponto, o remetente agora \npoderá transmitir a próxima mensagem. Assim, em 30,008 ms, o remetente esteve enviando por apenas 0,008 ms. \nSe definirmos a utilização do remetente (ou do canal) como a fração de tempo em que o remetente está realmen-\nte ocupado enviando bits para dentro do canal, a análise da Figura 3.18(a) mostra que o protocolo pare e espere \ntem uma utilização do remetente Uremet bastante desanimadora, de:\nUremet =\nL/R\nRTT + L/R = 0,008\n30,008 = 0,00027\nPortanto, o remetente ficou ocupado apenas 2,7 centésimos de 1% do tempo! Visto de outra maneira, ele \nsó foi capaz de enviar 1.000 bytes em 30,008 ms, uma vazão efetiva de apenas 267 kbits/s — mesmo estando \ndisponível um enlace de 1 gigabit por segundo! Imagine o infeliz administrador de redes que acabou de pagar \numa fortuna para ter capacidade de enlace da ordem de gigabits, mas consegue uma vazão de apenas 267 Kb por \nsegundo! Este é um exemplo gráfico de como protocolos de rede podem limitar as capacidades oferecidas pelo \nhardware subjacente de rede. Além disso, desprezamos também os tempos de processamento de protocolo das \ncamadas inferiores no remetente e no destinatário, bem como os atrasos de processamento e de fila que ocorre-\nFigura 3.17  Protocolo pare e espere versus protocolo com paralelismo\nPacote de dados\nPacote de dados\nPacotes ACK\na. Um protocolo pare e espere em operação\nb. Um protocolo com paralelismo em operação\n   Redes de computadores e a Internet\n160\nFigura 3.18  Envio com pare e espere e com paralelismo\nPrimeiro bit do primeiro pacote\ntransmitido, t = 0\nÚltimo bit do primeiro\npacote transmitido, t = L/R\nPrimeiro bit do primeiro pacote\ntransmitido, t = 0\nÚltimo bit do primeiro\npacote transmitido, t = L/R\nACK chega, envia próximo\npacote, t = RTT + L/R\na. Operação pare e espere\nRemetente\nDestinatário\nRTT\nPrimeiro bit do primeiro pacote chega\nÚltimo bit do primeiro pacote chega, envia ACK\nPrimeiro bit do primeiro pacote chega\nÚltimo bit do primeiro pacote chega, envia ACK\nACK chega, envia próximo\npacote, T = RTT + L/R\nb. Operação com paralelismo\nRemetente\nDestinatário\nRTT\nÚltimo bit do segundo pacote chega, envia ACK\nÚltimo bit do terceiro pacote chega, envia ACK\nriam em quaisquer roteadores intermediários existentes entre o remetente e o destinatário. Incluir esses efeitos \nserviria apenas para aumentar ainda mais o atraso e piorar ainda mais o fraco desempenho.\nA solução para esse problema de desempenho em especial é simples: em vez de operar em modo pare e espere, \no remetente é autorizado a enviar vários pacotes sem esperar por reconhecimentos, como mostra a Figura 3.17(b). A \nFigura 3.18(b) mostra que, se um remetente for autorizado a transmitir três pacotes antes de ter de esperar por reco-\nnhecimentos, sua utilização será triplicada. Uma vez que os muitos pacotes em trânsito entre remetente e destinatário \npodem ser visualizados como se estivessem enchendo uma tubulação, essa técnica é conhecida, em inglês, como pipelining1\n* \n(tubulação). O paralelismo gera as seguintes consequências para protocolos de transferência confiável de dados:\n*\t Porém, como essa expressão é difícil de traduzir para o português, preferimos usar “paralelismo”\n, embora a transmissão de dados seja de fato \nsequencial. (N. do T.)\nCAMADA  de transporte  161 \n• A faixa de números de sequência tem de ser ampliada, pois cada pacote em trânsito (sem contar as re-\ntransmissões) precisa ter um número de sequência exclusivo e pode haver vários pacotes não reconhe-\ncidos em trânsito.\n• Os lados remetente e destinatário dos protocolos podem ter de reservar buffers para mais de um pacote. No \nmínimo, o remetente terá de providenciar buffers para pacotes que foram transmitidos, mas que ainda não \nforam reconhecidos. O buffer de pacotes bem recebidos pode também ser necessário no destinatário, como \ndiscutiremos a seguir.\n• A faixa de números de sequência necessária e as necessidades de buffer dependerão da maneira como um \nprotocolo de transferência de dados responde a pacotes perdidos, corrompidos e demasiadamente atrasados. \nDuas abordagens básicas em relação à recuperação de erros com paralelismo podem ser identificadas: \nGo-Back-N e repetição seletiva.\n3.4.3  Go-Back-N (GBN)\nEm um protocolo Go-Back-N (GBN), o remetente é autorizado a transmitir múltiplos pacotes (se disponí-\nveis) sem esperar por um reconhecimento, mas fica limitado a ter não mais do que algum número máximo permiti-\ndo, N, de pacotes não reconhecidos na “tubulação”\n. Nesta seção, descreveremos o protocolo GBN com detalhes. Mas \nantes de continuar a leitura, convidamos você para se divertir com o applet GBN (incrível!) no site de apoio do livro.\nA Figura 3.19 mostra a visão que o remetente tem da faixa de números de sequência em um protocolo GBN. \nSe definirmos base como o número de sequência do mais antigo pacote não reconhecido e nextseqnum como \no menor número de sequência não utilizado (isto é, o número de sequência do próximo pacote a ser enviado), \nentão quatro intervalos na faixa de números de sequência poderão ser identificados. Os números de sequência no \nintervalo [0,base-1] correspondem aos pacotes que já foram transmitidos e reconhecidos. O intervalo [ba-\nse,nextseqnum-1] corresponde aos pacotes enviados, mas ainda não foram reconhecidos. Os números de \nsequência no intervalo [nextseqnum,base+N-1] podem ser usados para pacotes que podem ser enviados \nimediatamente, caso cheguem dados vindos da camada superior. Por fim, números de sequência maiores ou \niguais a base+N não podem ser usados até que um pacote não reconhecido que esteja pendente seja reconhecido \n(especificamente, o pacote cujo número de sequência é base).\nComo sugere a Figura 3.19, a faixa de números de sequência permitidos para pacotes transmitidos, porém \nainda não reconhecidos pode ser vista como uma janela de tamanho N sobre a faixa de números de sequência. À \nmedida que o protocolo opera, a janela se desloca para a frente sobre o espaço de números de sequência. Por essa \nrazão, N é muitas vezes denominado tamanho de janela e o protocolo GBN em si, protocolo de janela deslizan-\nte (sliding-window protocol). É possível que você esteja pensando que razão teríamos, primeiro, para limitar o \nnúmero de pacotes pendentes não reconhecidos a um valor N. Por que não permitir um número ilimitado deles? \nVeremos na Seção 3.5 que o controle de fluxo é uma das razões para impor um limite ao remetente. Examinare-\nmos outra razão para isso na Seção 3.7, quando estudarmos o controle de congestionamento do TCP.\nNa prática, o número de sequência de um pacote é carregado em um campo de comprimento fixo no cabe-\nçalho do pacote. Se k for o número de bits no campo de número de sequência do pacote, a faixa de números de \nsequência será então [0,2k – 1]. Com uma faixa finita de números de sequência, toda a aritmética que envolver \nnúmeros de sequência deverá ser feita usando aritmética de módulo 2k. (Em outras palavras, o espaço do número \nFigura 3.19  Visão do remetente para os números de sequência no protocolo Go-Back-N\nbase\nnextseqnum\nTamanho da janela\nLegenda:\nJá reconhecido\nEnviado, mas ainda\nnão reconhecido\nAutorizado, mas\nainda não enviado\nNão autorizado\n   Redes de computadores e a Internet\n162\nde sequência pode ser imaginado como um anel de tamanho 2k, em que o número de sequência 2k – 1 é seguido \nde imediato pelo número de sequência 0.) Lembre-se de que rdt3.0 tem um número de sequência de 1 bit e \numa faixa de números de sequência de [0,1]. Vários problemas ao final deste capítulo tratam das consequências \nde uma faixa finita de números de sequência. Veremos na Seção 3.5 que o TCP tem um campo de número de se-\nquência de 32 bits, em que os números de sequência do TCP contam bytes na cadeia de bytes, em vez de pacotes.\nAs figuras 3.20 e 3.21 descrevem uma FSM estendida dos lados remetente e destinatário de um protocolo \nGBN baseado em ACK, mas sem NAK. Referimo-nos a essa descrição de FSM como FSM estendida porque adi-\ncionamos variáveis (semelhantes às variáveis de linguagem de programação) para base e nextseqnum; também \nadicionamos operações sobre essas variáveis e ações condicionais que as envolvem. Note que a especificação da \nFSM estendida agora está começando a parecer um pouco com uma especificação de linguagem de programação. \nBochman [1984] fornece um excelente levantamento sobre extensões adicionais às técnicas FSM, bem como so-\nbre outras técnicas para especificação de protocolos baseadas em linguagens.\nFigura 3.21  Descrição da FSM estendida do destinatário GBN\nrdt_rcv(rcvpkt)\n  && notcorrupt(rcvpkt)\n  && hasseqnum(rcvpkt,expectedseqnum)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(expectedseqnum,ACK,checksum)\nudt_send(sndpkt)\nexpectedseqnum++\nΛ\nexpectedseqnum=1\nsndpkt=make_pkt(0,ACK,checksum)\ndefault\nudt_send(sndpkt)\nEsperar\nFigura 3.20  Descrição da FSM estendida do remetente GBN\nrdt_send(data)\nif(nextseqnum<base+N){\n   sndpkt[nextseqnum]=make_pkt(nextseqnum,data,checksum)\n   udt_send(sndpkt[nextseqnum])\n   if(base==nextseqnum)\n      start_timer\n   nextseqnum++\n   }\nelse\n   refuse_data(data)\nΛ\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\nbase=getacknum(rcvpkt)+1\nIf(base==nextseqnum)\n   stop_timer\nelse\n   start_timer\nrdt_rcv(rcvpkt) && corrupt(rcvpkt)\nΛ\nbase=1\nnextseqnum=1\ntimeout\nstart_timer\nudt_send(sndpkt[base])\nudt_send(sndpkt[base+1])\n...\nudt_send(sndpkt[nextseqnum-1])\nEsperar\nCAMADA  de transporte  163 \nO remetente GBN deve responder a três tipos de eventos:\n• Chamada vinda de cima. Quando rdt_send() é chamado de cima, o remetente primeiro verifica se a \njanela está cheia, isto é, se há N pacotes pendentes não reconhecidos. Se a janela não estiver cheia, um pa-\ncote é criado e enviado e as variáveis são adequadamente atualizadas. Se estiver cheia, o remetente apenas \ndevolve os dados à camada superior — uma indicação implícita de que a janela está cheia. Presume-se \nque a camada superior então teria de tentar outra vez mais tarde. Em uma execução real, o remetente \nmuito provavelmente teria colocado esses dados em um buffer (mas não os teria enviado imediatamente) \nou teria um mecanismo de sincronização (por exemplo, um semáforo ou uma flag) que permitiria que a \ncamada superior chamasse rdt_send() apenas quando as janelas não estivessem cheias.\n• Recebimento de um ACK. Em nosso protocolo GBN, um reconhecimento de pacote com número de \nsequência n seria tomado como um reconhecimento cumulativo, indicando que todos os pacotes com \nnúmero de sequência até e inclusive n tinham sido corretamente recebidos no destinatário. Voltaremos a \nesse assunto em breve, quando examinarmos o lado destinatário do GBN.\n• Um evento de esgotamento de temporização (timeout). O nome “Go-Back-N” deriva do comportamento \ndo remetente em relação a pacotes perdidos ou demasiadamente atrasados. Como no protocolo pare e \nespere, um temporizador é usado para recuperar a perda de dados ou reconhecer pacotes. Se ocorrer o \nesgotamento da temporização, o remetente reenvia todos os pacotes que tinham sido previamente envia-\ndos, mas que ainda não tinham sido reconhecidos. Nosso remetente da Figura 3.20 usa apenas um único \ntemporizador, que pode ser imaginado como um temporizador para o mais antigo pacote já transmitido \nporém ainda não reconhecido. Se for recebido um ACK e ainda houver pacotes adicionais transmitidos \nmas ainda não reconhecidos, o temporizador será reiniciado. Se não houver nenhum pacote pendente \nnão reconhecido, o temporizador será desligado.\nAs ações do destinatário no GBN também são simples. Se um pacote com número de sequência n for \nrecebido corretamente e estiver na ordem (isto é, os últimos dados entregues à camada superior vierem de um \npacote com número de sequência n – 1), o destinatário enviará um ACK para o pacote n e entregará a parte de \ndados do pacote à camada superior. Em todos os outros casos, o destinatário descarta o pacote e reenvia um ACK \npara o mais recente que foi recebido na ordem correta. Dado que são entregues à camada superior um por vez, \nse o pacote k tiver sido recebido e entregue, então todos os pacotes com número de sequência menores do que k \ntambém terão sido entregues. Assim, o uso de reconhecimentos cumulativos é uma escolha natural para o GBN.\nEm nosso protocolo GBN, o destinatário descarta os pacotes que chegam fora de ordem. Embora pareça \nbobagem e perda de tempo descartar um pacote corretamente recebido (mas fora de ordem), existem justificati-\nvas para isso. Lembre-se de que o destinatário deve entregar dados na ordem certa à camada superior. Suponha \nagora que o pacote n esteja sendo esperado, mas quem chega é o pacote n + 1. Como os dados devem ser entre-\ngues na ordem certa, o destinatário poderia conservar o pacote n + 1 no buffer (salvá-lo) e entregá-lo à camada \nsuperior mais tarde, após ter recebido o pacote n. Contudo, se o pacote n for perdido, n e n + 1 serão ambos por \nfim retransmitidos como resultado da regra de retransmissão do GBN no remetente. Assim, o destinatário pode \napenas descartar o pacote n + 1. A vantagem dessa abordagem é a simplicidade da manipulação de buffers no des-\ntinatário — ele não precisa colocar no buffer nenhum pacote que esteja fora de ordem. Desse modo, enquanto o \nremetente deve manter os limites superior e inferior de sua janela e a posição de nextseqnum dentro dela, a única \ninformação que o destinatário precisa manter é o número de sequên­\ncia do próximo pacote esperado conforme \na ordem. Esse valor é retido na variável expectedseqnum mostrada na FSM destinatária da Figura 3.21. Claro, \na desvantagem de jogar fora um pacote recebido corretamente é que a retransmissão subsequente desse pacote \npode ser perdida ou ficar truncada, caso em que ainda mais retransmissões seriam necessárias.\nA Figura 3.22 mostra a operação do protocolo GBN para o caso de um tamanho de janela de quatro pacotes. \nPor causa da limitação do tamanho dessa janela, o remetente envia os pacotes de 0 a 3, mas, em seguida, tem de \nesperar que um ou mais desses pacotes sejam reconhecidos antes de prosseguir. E, à medida que cada ACK suces-\nsivo (por exemplo, ACK0 e ACK1) é recebido, a janela se desloca para a frente e o remetente pode transmitir um \nnovo pacote (pkt4 e pkt5, respectivamente). Do lado destinatário, o pacote 2 é perdido. Desse modo, verifica-se \nque os pacotes 3, 4 e 5 estão fora de ordem e, portanto, são descartados.\n   Redes de computadores e a Internet\n164\nAntes de encerrarmos nossa discussão sobre o GBN, devemos ressaltar que uma implementação desse pro-\ntocolo em uma pilha de protocolo provavelmente seria estruturada de modo semelhante à da FSM estendida da \nFigura 3.20. A implementação também seria estruturada sob a forma de vários procedimentos que implementam \nas ações a serem executadas em resposta aos vários eventos que podem ocorrer. Nessa programação baseada \nem eventos, os vários procedimentos são chamados (invocados) por outros procedimentos presentes na pilha \nde protocolo ou como resultado de uma interrupção. No remetente, seriam: (1) uma chamada pela entidade da \ncamada superior invocando rdt_send(), (2) uma interrupção pelo temporizador e (3) uma chamada pela \ncamada inferior invocando rdt_rcv() quando chega um pacote. Os exercícios de programação ao final deste \ncapítulo lhe darão a chance de executar de verdade essas rotinas em um ambiente de rede simulado, mas realista.\nSalientamos que o protocolo GBN incorpora quase todas as técnicas que encontraremos quando estudar-\nmos, na Seção 3.5, os componentes de transferência confiável de dados do TCP. Essas técnicas incluem a utili-\nzação de números de sequência, reconhecimentos cumulativos, somas de verificação e uma operação de esgota-\nmento de temporização/retransmissão.\n3.4.4  Repetição seletiva (SR)\nO protocolo GBN permite que o remetente potencialmente “encha a rede” com pacotes na Figura 3.17, evi-\ntando, assim, os problemas de utilização de canal observados em protocolos do tipo pare e espere. Há, contudo, \nFigura 3.22  Go-Back-N em operação\nRemetente\nDestinatário\n envia pkt0\n envia pkt1\n envia pkt2\nenvia pkt3\n  \n(espera)\n recebe ACK0\nenvia pkt4\n recebe ACK1\nenvia pkt5\nenvia pkt2\nenvia pkt3\nenvia pkt4\nenvia pkt5\npkt2\ntemporização\nrecebe pkt0\nenvia ACK0\nrecebe pkt1\nenvia ACK1\nrecebe pkt3, descarta\nenvia ACK1\nrecebe pkt4, descarta\nenvia ACK1\nrecebe pkt5, descarta\nenvia ACK1\nrecebe pkt2, descarta\nenvia ACK2\nrecebe pkt3, descarta\nenvia ACK3\nX\n(perda)\nCAMADA  de transporte  165 \ncasos em que o próprio GBN sofre com problemas de desempenho. Em especial, quando o tamanho da janela e \no produto entre o atraso e a largura de banda são grandes, pode haver muitos pacotes pendentes na rede. Assim, \num único erro de pacote pode fazer que o GBN retransmita um grande número de pacotes — muitos deles sem \nnecessidade. À medida que aumenta a probabilidade de erros no canal, a rede pode ficar lotada com essas retrans-\nmissões desnecessárias. Imagine se, em nosso cenário de conversa, toda vez que uma palavra fosse pronunciada \nde maneira truncada as outras mil que a circundam (por exemplo, um tamanho de janela de mil palavras) tives-\nsem de ser repetidas. A conversa sofreria atrasos por causa de todas essas palavras reiteradas.\nComo o próprio nome sugere, protocolos de repetição seletiva (selective repeat — SR) evitam retransmis-\nsões desnecessárias porque fazem o remetente retransmitir apenas os pacotes suspeitos de terem sido recebidos \ncom erro (isto é, que foram perdidos ou corrompidos) no destinatário. Essa retransmissão individual, só quando \nnecessária, exige que o destinatário reconheça individualmente os pacotes recebidos de modo correto. Uma jane-\nla de tamanho N será usada novamente para limitar o número de pacotes pendentes não reconhecidos dentro da \nrede. Contudo, ao contrário do GBN, o remetente já terá recebido ACKs para alguns dos pacotes na janela. A \nFigura 3.23 mostra a visão que o protocolo de SR remetente tem do espaço do número de sequência; a Figura 3.24 \ndetalha as várias ações executadas pelo protocolo SR remetente.\nFigura 3.24  Eventos e ações do protocolo SR remetente\n1.\t Dados recebidos de cima. Quando são recebidos dados de cima, o protocolo SR remetente verifica o próximo número de sequência \ndisponível para o pacote. Se o número de sequência está dentro da janela do remetente, os dados são empacotados e enviados; do \ncontrário, eles são armazenados ou devolvidos à camada superior para transmissão posterior, como acontece no GBN.\n1.\t Esgotamento de temporização. Novamente são usados temporizadores para proteção contra perda de pacotes. Contudo, cada pacote agora \ndeve ter seu próprio temporizador lógico, já que apenas um pacote será transmitido quando a temporização se esgotar. Um único hardware \nde temporizador pode ser usado para emular a operação de múltiplos temporizadores lógicos [Varghese, 1997].\n2.\t ACK recebido. Se for recebido um ACK, o SR remetente marcará aquele pacote como recebido, contanto que esteja na janela. Se o número \nde sequência do pacote for igual a send_base, a base da janela se deslocará para a frente até o pacote não reconhecido que tiver o menor \nnúmero de sequência. Se a janela se deslocar e houver pacotes não transmitidos com números de sequência que agora caem dentro da \njanela, esses pacotes serão transmitidos.\nFigura 3.23  \u0007\nVisões que os protocolos SR remetente e destinatário tÊm do espaço de número \nde sequência\nsend_base\nnextseqnum\nTamanho da janela\nN\nLegenda:\nLegenda:\nJá reconhecido\nEnviado, mas\nnão autorizado\nAutorizado, mas\nainda não enviado\nNão autorizado\nFora de ordem\n(no buffer), mas já\nreconhecido (ACK)\nAguardado, mas\nainda não recebido\nAceitável\n(dentro da janela)\nNão autorizado\na. Visão que o remetente tem dos números de sequência\nb. Visão que o destinatário tem dos números de sequência\nrcv_base\nTamanho da janela\nN\n   Redes de computadores e a Internet\n166\nO protocolo SR destinatário reconhecerá um pacote corretamente recebido esteja ele ou não na ordem certa. \nPacotes fora de ordem ficam no buffer até que todos os faltantes (isto é, os que têm números de sequência menores) \nsejam recebidos, quando então um conjunto de pacotes poderá ser entregue à camada superior na ordem correta. A \nFigura 3.25 apresenta as várias ações realizadas pelo protocolo SR destinatário. A Figura 3.26 mostra um exemplo \nde operação do protocolo SR quando ocorre perda de pacotes. Note que, nessa figura, o destinatário de início arma-\nzena os pacotes 3, 4 e 5 e os entrega junto com o pacote 2 à camada superior, quando o pacote 2 é enfim recebido.\npkt0 recebido, entregue, ACK0 enviado\n0 1 2 3 4 5 6 7 8 9\npkt1 recebido, entregue, ACK1 enviado\n0 1 2 3 4 5 6 7 8 9\npkt3 recebido, armazenado, ACK3 enviado\n0 1 2 3 4 5 6 7 8 9\npkt4 recebido, armazenado, ACK4 enviado\n0 1 2 3 4 5 6 7 8 9\npkt5 recebido, armazenado, ACK5 enviado\n0 1 2 3 4 5 6 7 8 9\npkt2 recebido, pkt2, pkt3, pkt4, pkt5\nentregues, ACK2 enviado\n0 1 2 3 4 5 6 7 8 9\npkt0 enviado\n0 1 2 3 4 5 6 7 8 9\npkt1 enviado\n0 1 2 3 4 5 6 7 8 9\npkt2 enviado\n0 1 2 3 4 5 6 7 8 9\npkt3 enviado, janela cheia\n0 1 2 3 4 5 6 7 8 9\nACK0 recebido, pkt4 enviado\n0 1 2 3 4 5 6 7 8 9\nACK1 recebido, pkt5 enviado\n0 1 2 3 4 5 6 7 8 9\nEsgotamento de temporização\n(TIMEOUT)pkt2, pkt2 reenviado\n0 1 2 3 4 5 6 7 8 9\nACK3 recebido, nada enviado\n0 1 2 3 4 5 6 7 8 9\nX\n(perda)\nRemetente\nDestinatário\nFigura 3.26  Operação SR\nFigura 3.25  Eventos e ações do protocolo SR destinatário\n1.\t Pacote com número de sequência no intervalo [rcv_base, rcv_base+N–1] foi corretamente recebido. Nesse caso, o \npacote recebido cai dentro da janela do destinatário e um pacote ACK seletivo é devolvido ao remetente. Se o pacote não tiver sido recebido \nanteriormente, irá para o buffer. Se esse pacote tiver um número de sequência igual à base da janela destinatária (rcv_base na Figura \n3.22), então ele e quaisquer outros pacotes armazenados no buffer e numerados consecutivamente (começando com rcv_base) \nserão entregues à camada superior. A janela destinatária é então deslocada para a frente de acordo com o número de pacotes entregues à \ncamada superior. Como exemplo, considere a Figura 3.26. Quando um pacote com número de sequência rcv_base=2 é recebido, ele \ne os pacotes 3, 4 e 5 podem ser entregues à camada superior.\n1.\t Pacote com número de sequência no intervalo [rcv_base-N, rcv_base-1] foi corretamente recebido. Nesse caso, um ACK \ndeve ser gerado mesmo que esse pacote já tenha sido reconhecido pelo destinatário.\n2.\t Qualquer outro. Ignore o pacote.\nCAMADA  de transporte  167 \nÉ importante notar que na etapa 2 da Figura 3.25 o destinatário reconhece novamente (em vez de ignorar) \npacotes já recebidos com certos números de sequência que estão abaixo da atual base da janela. É bom que você \nse convença de que esse reconhecimento duplo é de fato necessário. Dados os espaços dos números de sequên-\ncia do remetente e do destinatário na Figura 3.23, por exemplo, se não houver ACK para pacote com número \nsend_base propagando-se do destinatário ao remetente, este acabará retransmitindo o pacote send_base, \nembora esteja claro (para nós, e não para o remetente!) que o destinatário já o recebeu. Caso o destinatário não \no reconhecesse, a janela do remetente jamais se deslocaria para a frente! Esse exemplo ilustra um importante as-\npecto dos protocolos SR (e também de muitos outros). O remetente e o destinatário nem sempre têm uma visão \nidêntica do que foi recebido corretamente e do que não foi. Para protocolos SR, isso significa que as janelas do \nremetente e do destinatário nem sempre coincidirão.\nA falta de sincronização entre as janelas do remetente e do destinatário tem importantes consequências \nquando nos defrontamos com a realidade de uma faixa finita de números de sequência. Considere o que poderia \nacontecer, por exemplo, com uma faixa finita de quatro números de sequência de pacotes (0, 1, 2, 3) e um tama-\nnho de janela de três. Suponha que os pacotes de 0 a 2 sejam transmitidos, recebidos e reconhecidos corretamente \nno destinatário. Nesse ponto, a janela do destinatário está sobre o quarto, o quinto e o sexto pacotes, que têm os \nnúmeros de sequência 3, 0 e 1. Agora, considere dois cenários. No primeiro, mostrado na Figura 3.27(a), os ACKs \npara os três primeiros pacotes foram perdidos e o remetente os retransmite. Assim, o que o destinatário recebe \nem seguida é um pacote com o número de sequência 0 — uma cópia do primeiro pacote enviado.\nNo segundo cenário, mostrado na Figura 3.27(b), os ACKs para os três primeiros pacotes foram entregues \nde modo correto. Assim, o remetente desloca sua janela para a frente e envia o quarto, o quinto e o sexto pacotes \ncom os números de sequência 3, 0 e 1, respectivamente. O pacote com o número de sequência 3 é perdido, mas o \npacote com o número de sequência 0 chega — um pacote que contém dados novos.\nAgora, na Figura 3.27, considere o ponto de vista do destinatário, que tem uma cortina imaginária entre o \nremetente e ele, já que o destinatário não pode “ver” as ações executadas pelo remetente. Tudo o que o destina-\ntário observa é a sequência de mensagens que ele recebe do canal e envia para o canal. No que lhe concerne, os \ndois cenários da Figura 3.27 são idênticos. Não há um modo de distinguir a retransmissão do primeiro pacote da \ntransmissão original do quinto pacote. Fica claro que um tamanho de janela que seja igual ao tamanho do espaço \nde numeração sequencial menos 1 não vai funcionar. Mas qual deve ser o tamanho da janela? Um problema ao \nfinal deste capítulo pede que você demonstre que o tamanho pode ser menor ou igual à metade do tamanho do \nespaço de numeração sequencial para os protocolos SR.\nNo site de apoio do livro, você encontrará um applet que anima a operação do protocolo SR. Tente realizar \nos mesmos experimentos feitos com o applet GBN. Os resultados combinam com o que você espera?\nIsso encerra nossa discussão sobre protocolos de transferência confiável de dados. Percorremos um longo \ncaminho e apresentamos numerosos mecanismos que, juntos, proveem transferência confiável de dados. A Tabe-\nla 3.1 resume esses mecanismos. Agora que já vimos todos eles em operação e podemos enxergar “o quadro geral”\n, \naconselhamos que você leia novamente esta seção para perceber como esses mecanismos foram adicionados pou-\nco a pouco, de modo a abordar modelos (realistas) de complexidade crescente do canal que conecta o remetente \nao destinatário ou para melhorar o desempenho dos protocolos.\nEncerraremos nossa explanação considerando uma premissa remanescente em nosso modelo de canal subja-\ncente. Lembre-se de que admitimos que pacotes não podem ser reordenados dentro do canal entre o remetente e o \ndestinatário. Esta é uma premissa em geral razoável quando o remetente e o destinatário estão conectados por um \núnico fio físico. Contudo, quando o “canal” que conecta os dois é uma rede, pode ocorrer reordenação de pacotes. \nUma manifestação da reordenação de pacotes é que podem aparecer cópias antigas de um pacote com número de \nsequência ou de reconhecimento x, mesmo que nem a janela do remetente nem a do destinatário contenham x. \nCom a reordenação de pacotes, podemos considerar que o canal usa armazenamento de pacotes e emite-os espon-\ntaneamente em algum momento qualquer do futuro. Como números de sequência podem ser reutilizados, deve-\nmos tomar algum cuidado para nos prevenir contra esses pacotes duplicados. A abordagem adotada na prática é \ngarantir que um número de sequência não seja reutilizado até que o remetente esteja “certo” de que nenhum pacote \nenviado antes com número de sequência x está na rede. Isso é feito admitindo que um pacote não pode “viver” na \n   Redes de computadores e a Internet\n168\nrede mais do que um tempo máximo fixado. As extensões do TCP para redes de alta velocidade [RFC 1323] usam \num tempo de vida máximo de pacote de cerca de três minutos. Sunshine [1978] descreve um método para usar \nnúmeros de sequência tais que os problemas de reordenação podem ser completamente evitados.\n3.5  Transporte orientado para conexão: TCP\nAgora que já vimos os princípios subjacentes à transferência confiável de dados, vamos voltar para o TCP — o \nprotocolo de transporte confiável da camada de transporte, orientado para conexão, da Internet. Nesta seção, vere-\nmos que, para poder fornecer transferência confiável de dados, o TCP conta com muitos dos princípios subjacentes \nFigura 3.27  \u0007\nDilema do remetente SR com janelas muito grandes: um novo pacote ou uma \nretransmissão?\npkt0\ntimeout\nretransmite pkt0\n0 1 2 3 0 1 2\npkt0\npkt1\npkt2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\nACK0\nACK1\nACK2\nx\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\nJanela do remetente\n(após recepção)\na.\nb.\nJanela do destinatário\n(após recepção)\nrecebe pacote\ncom número de sequência 0\n0 1 2 3 0 1 2\npkt0\npkt1\npkt2\npkt3\npkt0\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\nACK0\nACK1\nACK2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\nJanela do remetente\n(após recepção)\nJanela do destinatário\n(após recepção)\nrecebe pacote\ncom número de sequência 0\n0 1 2 3 0 1 2\nx\nx\nx\nCAMADA  de transporte  169 \ndiscutidos na seção anterior, incluindo detecção de erro, retransmissões, reconhecimentos cumulativos, temporiza-\ndores e campos de cabeçalho para números de sequência e de reconhecimento. O TCP está definido nos RFCs 793, \n1122, 1323, 2018 e 2581.\n3.5.1  A conexão TCP\nDizemos que o TCP é orientado para conexão porque, antes que um processo de aplicação possa come-\nçar a enviar dados a outro, os dois processos precisam primeiro se “apresentar” — isto é, devem enviar alguns \nsegmentos preliminares um ao outro para estabelecer os parâmetros da transferência de dados. Como parte do \nestabelecimento da conexão TCP, ambos os lados da conexão iniciarão muitas variáveis de estado (muitas das \nquais serão discutidas nesta seção e na Seção 3.7) associadas com a conexão TCP.\nA “conexão” TCP não é um circuito TDM ou FDM fim a fim, como acontece em uma rede de comutação de \ncircuitos. Tampouco é um circuito virtual (veja o Capítulo 1), pois o estado de conexão reside inteiramente nos \ndois sistemas finais. Como o protocolo TCP roda apenas nesses sistemas, e não nos elementos intermediários da \nrede (roteadores e comutadores — switches — de camada de enlace), os elementos intermediários não mantêm \nestado de conexão TCP. Na verdade, os roteadores intermediários são de todo alheios às conexões TCP; eles en-\nxergam datagramas, e não conexões.\nUma conexão TCP provê um serviço full-duplex: se houver uma conexão TCP entre o processo A em um \nhospedeiro e o processo B em outro hospedeiro, os dados da camada de aplicação poderão fluir de A para B ao \nmesmo tempo em que os dados da camada de aplicação fluem de B para A. A conexão TCP é sempre ponto a \nponto, isto é, entre um único remetente e um único destinatário. O chamado “multicast” (veja a Seção 4.7) — a \ntransferência de dados de um remetente para vários destinatários em uma única operação de envio — não é pos-\nsível com o TCP. Com o TCP, dois hospedeiros é bom; três é demais!\nVamos agora examinar como uma conexão TCP é estabelecida. Suponha que um processo que roda em um \nhospedeiro queira iniciar a conexão com outro processo em outro hospedeiro. Lembre-se de que o processo que \nestá iniciando a conexão é denominado processo cliente, e o outro é denominado processo servidor. O processo de \naplicação cliente primeiro informa à camada de transporte no cliente que ele quer estabelecer uma conexão com um \nprocesso no servidor. Lembre-se (Seção 2.7.2) de que um programa cliente em Python faz isso emitindo o comando\nTabela 3.1  Resumo de mecanismos de transferência confiável de dados e sua utilização\nMecanismo\nUso, comentários\nSoma de verificação\nUsada para detectar erros de bits em um pacote transmitido.\nTemporizador\nUsado para controlar a temporização/retransmissão de um pacote, possivelmente porque o pacote (ou seu ACK) \nfoi perdido dentro do canal. Como pode ocorrer esgotamento de temporização quando um pacote está atrasado, \nmas não perdido (esgotamento de temporização prematuro), ou quando um pacote foi recebido pelo destinatário \nmas o ACK remetente-destinatário foi perdido, um destinatário pode receber cópias duplicadas de um pacote.\nNúmero de sequência\nUsado para numeração sequencial de pacotes de dados que transitam do remetente ao destinatário. Lacunas nos \nnúmeros de sequência de pacotes recebidos permitem que o destinatário detecte um pacote perdido. Pacotes \ncom números de sequência duplicados permitem que o destinatário detecte cópias duplicadas de um pacote.\nReconhecimento\nUsado pelo destinatário para avisar o remetente que um pacote ou conjunto de pacotes foi recebido \ncorretamente. Reconhecimentos normalmente portam o número de sequência do pacote, ou pacotes, que \nestão sendo reconhecidos. Reconhecimentos podem ser individuais ou cumulativos, dependendo do protocolo.\nReconhecimento negativo\nUsado pelo destinatário para avisar o remetente que um pacote não foi recebido corretamente. Reconhecimentos \nnegativos normalmente portam o número de sequência do pacote que não foi recebido corretamente.\nJanela, paralelismo\nO remetente pode ficar restrito a enviar somente pacotes com números de sequência que caiam dentro de uma \ndeterminada faixa. Permitindo que vários pacotes sejam transmitidos, ainda que não reconhecidos, a utilização \ndo remetente pode ser aumentada em relação ao modo de operação pare e espere. Em breve veremos que o \ntamanho da janela pode ser estabelecido com base na capacidade de o destinatário receber e fazer buffer de \nmensagens ou no nível de congestionamento na rede, ou em ambos.\n   Redes de computadores e a Internet\n170\nclientSocket.connect((serverName,serverPort))\nem que servername é o nome do servidor e serverPort identifica o processo no servidor. O TCP no cliente \nentão passa a estabelecer uma conexão TCP com o TCP no servidor. Discutiremos com algum detalhe o procedi-\nmento de estabelecimento de conexão ao final desta seção. Por enquanto, basta saber que o cliente primeiro envia \num segmento TCP especial; o servidor responde com um segundo segmento TCP especial e, por fim, o cliente \nresponde novamente com um terceiro segmento especial. Os primeiros dois segmentos não contêm nenhuma \n“carga útil”\n, isto é, nenhum dado da camada de aplicação; o terceiro pode carregar uma carga útil. Como três \nsegmentos são enviados entre dois hospedeiros, esse procedimento de estabelecimento de conexão é muitas vezes \ndenominado apresentação de três vias (3-way handshake).\nUma vez estabelecida uma conexão TCP, os dois processos de aplicação podem enviar dados um para o outro. \nVamos considerar o envio de dados do processo cliente para o processo servidor. O processo cliente passa uma ca-\ndeia de dados pelo socket (a porta do processo), como descrito na Seção 2.7. Tão logo passem pelo socket, os dados \nestão nas mãos do TCP que está rodando no cliente. Como mostra a Figura 3.28, o TCP direciona seus dados para o \nbuffer de envio da conexão, que é um dos buffers reservados durante a apresentação de três vias inicial. De quando \nem quando, o TCP arranca pedaços de dados do buffer de envio e passa os dados à camada de rede. O interessante \né que a especificação do TCP [RFC 793] é muito vaga ao indicar quando o TCP deve de fato enviar dados que estão \nnos buffers, determinando apenas que o TCP “deve enviar aqueles dados em segmentos segundo sua própria conve-\nniência”\n. A quantidade máxima de dados que pode ser retirada e colocada em um segmento é limitada pelo tama-\nnho máximo do segmento (maximum segment size — MSS). O MSS normalmente é estabelecido determinando \nprimeiro o tamanho do maior quadro de camada de enlace que pode ser enviado pelo hospedeiro remetente local \n(denominado unidade máxima de transmissão — maximum transmission unit — MTU) e, em seguida, estabele-\ncendo um MSS que garanta que um segmento TCP (quando encapsulado em um datagrama IP) mais o compri-\nmento do cabeçalho TCP/IP (em geral, 40 bytes) caberão em um único quadro de camada de enlace. Os protocolos \nda camada de enlace Ethernet e PPP possuem um MSS de 1.500 bytes. Também foram propostas técnicas para \ndescobrir a MTU do caminho — o maior quadro de camada de enlace que pode ser enviado por todos os enlaces \ndesde a origem até o destino [RFC 1191] — e definir o MSS com base no valor da MTU do caminho. Note que o \nVinton Cerf, Robert Kahn e TCP/IP\nNo início da década de 1970, as redes de comu-\ntação de pacotes começaram a proliferar. A ARPAnet \n— precursora da Internet — era apenas mais uma \ndentre tantas que tinham, cada uma, seu próprio \nprotocolo. Dois pesquisadores, Vinton Cerf e Robert \nKahn, reconheceram a importância de interconectar \nessas redes e inventaram um protocolo inter-redes \ndenominado TCP/IP\n, que quer dizer Transmission \nControl Protocol/Internet Protocol (protocolo de con-\ntrole de transmissão/protocolo da Internet). Embora \nno começo Cerf e Kahn considerassem o protocolo \numa entidade única, mais tarde ele foi dividido em \nduas partes, TCP e IP\n, que operavam em separado. \nCerf e Kahn publicaram um artigo sobre o TCP/IP em \nmaio de 1974 em IEEE Transactions on Communica-\ntion Technology [Cerf, 1974].\nO protocolo TCP/IP\n, que é o “feijão com arroz” da \nInternet de hoje, foi elaborado antes dos PCs, estações \nde trabalho, smartphones e tablets, antes da prolifera-\nção da Ethernet, cabo, DSL, Wi-Fi e outras tecnologias \nde redes locais, antes da Web, redes sociais e recep-\nção de vídeo. Cerf e Kahn perceberam a necessidade \nde um protocolo de rede que, de um lado, fornecesse \namplo suporte para aplicações ainda a serem definidas \ne que, de outro, permitisse a interoperação de hospe-\ndeiros arbitrários e protocolos de camada de enlace.\nEm 2004, Cerf e Kahn receberam o prêmio ACM \nTuring Award, considerado o Prêmio Nobel da Com-\nputação pelo “trabalho pioneiro sobre interligação \nem rede, incluindo o projeto e a implementação dos \nprotocolos de comunicação da Internet, TCP/IP e por \ninspirarem liderança na área de redes”.\nHistória\nCAMADA  de transporte  171 \nMSS é a quantidade máxima de dados de camada de aplicação no segmento, e não o tamanho máximo do segmento \nTCP incluindo cabeçalhos. (Essa terminologia é confusa, mas temos de conviver com ela, pois já está arraigada.)\nO TCP combina cada porção de dados do cliente com um cabeçalho TCP, formando, assim, segmentos \nTCP. Os segmentos são passados para baixo, para a camada de rede, onde são encapsulados separadamente den-\ntro dos datagramas IP da camada de rede. Os datagramas IP são então enviados para dentro da rede. Quando o \nTCP recebe um segmento na outra extremidade, os dados do segmento são colocados no buffer de recepção da \nconexão, como ilustra a Figura 3.28. A aplicação lê a cadeia de dados desse buffer. Cada lado da conexão tem seus \npróprios buffers de envio e seu próprio buffer de recepção. (Você pode ver o applet de controle de fluxo on-line \nem <http://www.awl.com/kurose-ross>, que oferece uma animação dos buffers de envio e de recepção.)\nEntendemos, dessa discussão, que uma conexão TCP consiste em buffers, variáveis e um socket de conexão \nde um processo em um hospedeiro e outro conjunto de buffers, variáveis e um socket de conexão de um processo \nem outro hospedeiro. Como mencionamos, nenhum buffer nem variáveis são alocados à conexão nos elementos \nda rede (roteadores, comutadores e repetidores) existentes entre os hospedeiros.\n3.5.2  Estrutura do segmento TCP\nAgora que examinamos de modo breve a conexão TCP, vamos verificar a estrutura do segmento TCP, que \nconsiste em campos de cabeçalho e um campo de dados. O campo de dados contém uma quantidade de dados de \naplicação. Como já dissemos, o MSS limita o tamanho máximo do campo de dados de um segmento. Quando o \nTCP envia um arquivo grande, tal como uma imagem de uma página Web, ele costuma fragmentar o segmento \nem pedaços de tamanho MSS (exceto o último, que muitas vezes é menor do que o MSS). Aplicações interativas, \ncontudo, muitas vezes transmitem quantidades de dados menores do que o MSS. Por exemplo, com aplicações \nde login remoto como Telnet, o campo de dados do segmento TCP é, muitas vezes, de apenas 1 byte. Como o \ncabeçalho TCP tem tipicamente 20 bytes (12 bytes mais do que o cabeçalho UDP), o comprimento dos segmentos \nenviados por Telnet pode ser de apenas 21 bytes.\nA Figura 3.29 mostra a estrutura do segmento TCP. Como acontece com o UDP, o cabeçalho inclui núme-\nros de porta de origem e de destino, que são usados para multiplexação e demultiplexação de dados de/para \naplicações de camadas superiores e, assim como no UDP, inclui um campo de soma de verificação. Um cabeça-\nlho de segmento TCP também contém os seguintes campos:\n• O campo de número de sequência de 32 bits e o campo de número de reconhecimento de 32 bits são \nusados pelos TCPs remetente e destinatário na execução de um serviço confiável de transferência de \ndados, como discutido a seguir.\n• O campo de janela de recepção de 16 bits é usado para controle de fluxo. Veremos em breve que esse \ncampo é usado para indicar o número de bytes que um destinatário está disposto a aceitar.\nFigura 3.28  Buffers TCP de envio e de recepção\nProcesso\nescreve dados\nProcesso\nlê dados\nBuffer TCP\nde envio\nSocket\nBuffer TCP\nde recepção\nSocket\nSegmento\nSegmento\n   Redes de computadores e a Internet\n172\n• O campo de comprimento de cabeçalho de 4 bits especifica o comprimento do cabeçalho TCP em pa-\nlavras de 32 bits. O cabeçalho TCP pode ter comprimento variável por causa do campo de opções TCP. \n(O campo de opções TCP em geral está vazio, de modo que o comprimento do cabeçalho TCP típico é \n20 bytes.)\n• O campo de opções, opcional e de comprimento variável, é usado quando um remetente e um destina-\ntário negociam o MSS, ou como um fator de aumento de escala da janela para utilização em redes de alta \nvelocidade. Uma opção de marca de tempo é também definida. Consulte o RFC 854 e o RFC 1323 para \ndetalhes adicionais.\n• O campo de flag contém 6 bits. O bit ACK é usado para indicar se o valor carregado no campo de \nreconhecimento é válido, isto é, se o segmento contém um reconhecimento para um segmento que foi \nrecebido com sucesso. Os bits RST, SYN e FIN são usados para estabelecer e encerrar a conexão, como \ndiscutiremos ao final desta seção. Marcar o bit PSH indica que o destinatário deve passar os dados para a \ncamada superior imediatamente. Por fim, o bit URG é usado para mostrar que há dados nesse segmento \nque a entidade da camada superior do lado remetente marcou como “urgentes”\n. A localização do último \nbyte desses dados urgentes é indicada pelo campo de ponteiro de urgência de 16 bits. O TCP deve in-\nformar à entidade da camada superior do lado destinatário quando existem dados urgentes e passar a ela \num ponteiro para o final desses dados. (Na prática, o PSH, o URG e o ponteiro de dados urgentes não são \nusados. Contudo, mencionamos esses campos para descrever todos.)\nNúmeros de sequência e números de reconhecimento\nDois dos mais importantes campos do cabeçalho do segmento TCP são o de número de sequência e o de nú-\nmero de reconhecimento. Esses campos são parte fundamental do serviço de transferência confiável de dados do \nTCP. Mas, antes de discutirmos como são utilizados, vamos explicar exatamente o que o TCP coloca nesses campos.\nO TCP vê os dados como uma cadeia de bytes não estruturada, mas ordenada. O uso que o TCP faz dos nú-\nmeros de sequência reflete essa visão, pois esses números são aplicados sobre a cadeia de bytes transmitidos, e não \nsobre a série de segmentos transmitidos. O número de sequência para um segmento é o número do primeiro \nFigura 3.29  Estrutura do segmento TCP\nPorta de origem #\nSoma de veriﬁcação da Internet\nComprimento\ndo cabeçalho Não utilizado\nURG\nACK\nPSH\nRST\nSYN\nFIN\n32 bits\nPorta de destino #\nJanela de recepção\nPonteiro de urgência\nNúmero de sequência\nNúmero de reconhecimento\nOpções\nDados\nCAMADA  de transporte  173 \nbyte do segmento. Vamos ver um exemplo. Suponha que um processo no hospedeiro A queira enviar uma cadeia \nde dados para um processo no hospedeiro B por uma conexão TCP. O TCP do hospedeiro A vai implicitamente \nnumerar cada byte da cadeia de dados. Suponha que a cadeia de dados consista em um arquivo composto de 500 \nmil bytes, que o MSS seja de 1.000 bytes e que seja atribuído o número 0 ao primeiro byte da cadeia de dados. \nComo mostra a Figura 3.30, o TCP constrói 500 segmentos a partir da cadeia de dados. O primeiro recebe o nú-\nmero de sequência 0; o segundo, o número de sequência 1.000; o terceiro, o número de sequência 2.000, e assim \npor diante. Cada número de sequência é inserido no campo de número de sequência no cabeçalho do segmento \nTCP apropriado.\nVamos agora considerar os números de reconhecimento. Eles são um pouco mais complicados do que os \nnúmeros de sequência. Lembre-se de que o TCP é full-duplex, portanto o hospedeiro A pode estar recebendo \ndados do hospedeiro B enquanto envia dados ao hospedeiro B (como parte da mesma conexão TCP). Cada seg-\nmento que chega do hospedeiro B tem um número de sequência para os dados que estão fluindo de B para A. \nO número de reconhecimento que o hospedeiro A atribui a seu segmento é o número de sequência do próximo byte \nque ele estiver aguardando do hospedeiro B. É bom examinarmos alguns exemplos para entendermos o que está \nacontecendo aqui. Suponha que o hospedeiro A tenha recebido do hospedeiro B todos os bytes numerados de 0 \na 535 e também que esteja prestes a enviar um segmento ao hospedeiro B. O hospedeiro A está esperando pelo \nbyte 536 e por todos os bytes subsequentes da cadeia de dados do hospedeiro B. Assim, ele coloca o número 536 \nno campo de número de reconhecimento do segmento que envia para o hospedeiro B.\nComo outro exemplo, suponha que o hospedeiro A tenha recebido um segmento do hospedeiro B contendo \nos bytes de 0 a 535 e outro segmento contendo os bytes de 900 a 1.000. Por alguma razão, o hospedeiro A ainda \nnão recebeu os bytes de 536 a 899. Nesse exemplo, ele ainda está esperando pelo byte 536 (e os superiores) para \npoder recriar a cadeia de dados de B. Assim, o segmento seguinte que A envia a B conterá 536 no campo de \nnúmero de reconhecimento. Como o TCP somente reconhece bytes até o primeiro byte que estiver faltando na \ncadeia, dizemos que o TCP provê reconhecimentos cumulativos.\nEste último exemplo também revela uma questão importante, mas sutil. O hospedeiro A recebeu o ter-\nceiro segmento (bytes de 900 a 1.000) antes do segundo (bytes de 536 a 899). Portanto, o terceiro segmento \nchegou fora de ordem. E o que um hospedeiro faz quando recebe segmentos fora de ordem em uma conexão \nTCP? Eis a questão. O interessante é que os RFCs do TCP não impõem nenhuma regra para isso e deixam \na decisão para quem estiver programando a execução TCP. Há basicamente duas opções: (1) o destinatário \ndescarta imediatamente os segmentos fora de ordem (o que, como discutimos antes, pode simplificar o pro-\njeto do destinatário) ou (2) o destinatário conserva os bytes fora de ordem e espera pelos bytes faltantes para \npreencher as lacunas. Claro que a segunda alternativa é mais eficiente em termos de largura de banda de rede \ne é a abordagem adotada na prática.\nNa Figura 3.30, admitimos que o número de sequência inicial era 0. Na verdade, ambos os lados de uma \nconexão TCP escolhem ao acaso um número de sequência inicial. Isso é feito para minimizar a possibilidade de \num segmento de uma conexão já encerrada entre dois hospedeiros e ainda presente na rede ser tomado por um \nsegmento válido em uma conexão posterior entre esses dois mesmos hospedeiros (que também podem estar \nusando os mesmos números de porta da conexão antiga) [Sunshine, 1978].\nFigura 3.30  Dividindo os dados do arquivo em segmentos TCP\n0\n1\n1.000\n1.999\n499.999\nArquivo\nDados para o primeiro segmento\nDados para o segundo segmento\n   Redes de computadores e a Internet\n174\nTelnet: um estudo de caso para números de sequência e números de reconhecimento\nO Telnet, definido no RFC 854, é um protocolo popular de camada de aplicação utilizado para fazer login \nremoto. Ele roda sobre TCP e é projetado para trabalhar entre qualquer par de hospedeiros. Diferentemente das \naplicações de transferência de dados em grandes blocos, que foram discutidas no Capítulo 2, o Telnet é uma \naplicação interativa. Discutiremos, agora, um exemplo de Telnet, pois ilustra muito bem números de sequência \ne de reconhecimento do TCP. Observamos que muitos usuários agora preferem usar o protocolo SSH, visto que \ndados enviados por uma conexão Telnet (incluindo senhas!) não são criptografados, o que torna essa aplicação \nvulnerável a ataques de bisbilhoteiros (como discutiremos na Seção 8.7).\nSuponha que o hospedeiro A inicie uma sessão Telnet com o hospedeiro B. Como o hospedeiro A inicia \na sessão, ele é rotulado de cliente, enquanto B é rotulado de servidor. Cada caractere digitado pelo usuário (no \ncliente) será enviado ao hospedeiro remoto; este devolverá uma cópia (“eco”) de cada caractere, que será apresen-\ntada na tela Telnet do usuário. Esse eco é usado para garantir que os caracteres vistos pelo usuário do Telnet já fo-\nram recebidos e processados no local remoto. Assim, cada caractere atravessa a rede duas vezes entre o momento \nem que o usuário aperta o teclado e o momento em que o caractere é apresentado em seu monitor.\nSuponha agora que o usuário digite a letra “C” e saia para tomar um café. Vamos examinar os segmentos \nTCP que são enviados entre o cliente e o servidor. Como mostra a Figura 3.31, admitamos que os números \nde sequência iniciais sejam 42 e 79 para cliente e servidor, respectivamente. Lembre-se de que o número de \nsequência de um segmento será o número de sequência do primeiro byte do seu campo de dados. Assim, o \nprimeiro segmento enviado do cliente terá número de sequência 42; o primeiro segmento enviado do servidor \nterá número de sequência 79. Note que o número de reconhecimento será o número de sequência do próximo \nbyte de dados que o hospedeiro estará aguardando. Após o estabelecimento da conexão TCP, mas antes de \nquaisquer dados serem enviados, o cliente ficará esperando pelo byte 79 e o servidor, pelo byte 42.\nComo ilustra a Figura 3.31, são enviados três segmentos. O primeiro é enviado do cliente ao servidor, \ncontendo, em seu campo de dados, um byte com a representação ASCII para a letra “C”\n. O primeiro segmento \ntambém tem 42 em seu campo de número de sequência, como acabamos de descrever. E mais, como o cliente \nFigura 3.31  \u0007\nNúmeros de sequência e de reconhecimento para uma aplicação Telnet simples \nsobre TCP\nTempo\nTempo\nHospedeiro A\nHospedeiro B\nUsuário digita \n“C”\nSeq=42, ACK=79, data='C'\nSeq=79, ACK=43, data='C'\nSeq=43, ACK=80\nHospedeiro reconhece\nrecebimento do “C”,\necoa “C”\nHospedeiro reconhece\nrecebimento do\n“C” ecoado\nCAMADA  de transporte  175 \nainda não recebeu nenhum dado do servidor, esse segmento terá o número 79 em seu campo de número de \nreconhecimento.\nO segundo segmento é enviado do servidor ao cliente. Esse segmento tem dupla finalidade. A primeira é \nfornecer um reconhecimento para os dados que o servidor recebeu. Ao colocar 43 no campo de reconhecimento, \no servidor está dizendo ao cliente que recebeu com sucesso tudo até o byte 42 e agora está aguardando os bytes de \n43 em diante. A segunda finalidade desse segmento é ecoar a letra “C”\n. Assim, o segundo segmento tem a repre-\nsentação ASCII de “C” em seu campo de dados. Ele tem o número de sequência 79, que é o número de sequência \ninicial do fluxo de dados de servidor para cliente dessa conexão TCP, pois este é o primeiríssimo byte de dados \nque o servidor está enviando. Note que o reconhecimento para dados do cliente para o servidor é levado em um \nsegmento que carrega dados do servidor para o cliente. Dizemos que esse reconhecimento pegou uma carona \n(piggybacked) no segmento de dados do servidor ao cliente.\nO terceiro é enviado do cliente ao servidor. Seu único propósito é reconhecer os dados que recebeu do servi-\ndor. (Lembre-se de que o segundo segmento continha dados — a letra “C” — do servidor para o cliente.) Ele tem \num campo de dados vazio (isto é, o reconhecimento não está pegando carona com nenhum dado do cliente para \no servidor). O segmento tem o número 80 no campo do número de reconhecimento porque o cliente recebeu \na cadeia de dados até o byte com número de sequência 79 e agora está aguardando os bytes de 80 em diante. É \npossível que você esteja pensando que é estranho que esse segmento também tenha um número de sequência, já \nque não contém dados. Mas, como o TCP tem um campo de número de sequência, o segmento precisa apresentar \nalgum número para preenchê-lo.\n3.5.3  \u0007\nEstimativa do tempo de viagem de ida e volta e de esgotamento de \ntemporização\nO TCP, assim como o nosso protocolo rdt da Seção 3.4, utiliza um mecanismo de controle de temporização/\nretransmissão para recuperar segmentos perdidos. Embora conceitualmente simples, surgem muitas questões sutis \nquando executamos um mecanismo de controle de temporização/retransmissão em um protocolo real como o \nTCP. Talvez a pergunta mais óbvia seja a duração dos intervalos de controle. Claro, esse intervalo deve ser maior do \nque o tempo de viagem de ida e volta da conexão (RTT), isto é, o tempo decorrido entre o envio de um segmento e \nseu reconhecimento. Se não fosse assim, seriam enviadas retransmissões desnecessárias. Mas quão maior deve ser \no intervalo e, antes de tudo, como o RTT deve ser estimado? Deve-se associar um temporizador a cada segmento \nnão reconhecido? São tantas perguntas! Nesta seção, nossa discussão se baseia no trabalho de Jacobson [1988] sobre \nTCP e nas recomendações da IETF vigentes para o gerenciamento de temporizadores TCP [RFC 6298].\nEstimativa do tempo de viagem de ida e volta\nVamos iniciar nosso estudo do gerenciamento do temporizador TCP considerando como esse protocolo \nestima o tempo de viagem de ida e volta entre remetente e destinatário, o que apresentaremos a seguir. O RTT \npara um segmento, denominado SampleRTT no exemplo, é o tempo transcorrido entre o momento em que o \nsegmento é enviado (isto é, passado ao IP) e o momento em que é recebido um reconhecimento para ele. Em \nvez de medir um SampleRTT para cada segmento transmitido, a maioria das implementações de TCP executa \napenas uma medição de SampleRTT por vez. Isto é, em qualquer instante, o SampleRTT estará sendo estimado \npara apenas um dos segmentos transmitidos mas ainda não reconhecidos, o que resulta em um novo valor de \nSampleRTT para mais ou menos cada RTT. E mais, o TCP nunca computa um SampleRTT para um segmento \nque foi retransmitido; apenas mede-o para segmentos que foram transmitidos uma vez [Karn, 1987]. (Um dos \nproblemas ao final do capítulo perguntará por quê.)\nClaro, os valores de SampleRTT sofrerão variação de segmento para segmento em decorrência do conges-\ntionamento nos roteadores e das variações de carga nos sistemas finais. Por causa dessa variação, qualquer dado \nvalor de SampleRTT pode ser atípico. Portanto, para estimar um RTT comum, é natural tomar alguma espécie \n   Redes de computadores e a Internet\n176\nO TCP fornece transferência confiável de dados \nusando reconhecimentos positivos e temporizado-\nres, de modo muito parecido com o que estudamos \nna Seção 3.4. O protocolo reconhece dados que fo-\nram recebidos corretamente e retransmite segmentos \nquando entende que eles ou seus reconhecimentos \ncorrespondentes foram perdidos ou corrompidos. \nCertas versões do TCP também têm um mecanismo \nNAK implícito — com o mecanismo de retransmissão \nrápida do TCP\n. O recebimento de três ACKs duplica-\ndos para um dado segmento serve como um NAK \nimplícito para o seguinte, acionando a retransmissão \ndaquele segmento antes que o tempo se esgote. O \nTCP usa sequência de números para permitir que o \ndestinatário identifique segmentos perdidos ou dupli-\ncados. Exatamente como no caso de nosso protocolo \nde transferência confiável de dados rdt3.0, o TCP \nem si não pode determinar com certeza se um seg-\nmento, ou seu ACK, está perdido, corrompido ou atra-\nsado demais. No remetente, a resposta do TCP será a \nmesma: retransmitir o segmento.\nO TCP também utiliza paralelismo, permitindo \nque o remetente tenha, a qualquer tempo, múltiplos \nsegmentos transmitidos mas ainda não reconheci-\ndos. Vimos antes que o paralelismo pode melhorar \nmuito a vazão de uma sessão quando a razão entre \no tempo de transmissão do segmento e o atraso de \nviagem de ida e volta é pequena. O número espe-\ncífico de segmentos não reconhecidos que um re-\nmetente pode ter é determinado pelos mecanismos \nde controle de fluxo e controle de congestionamento \ndo TCP\n. O controle de fluxo do TCP é discutido no \nfinal desta seção; o controle de congestionamento \ndo TCP é discutido na Seção 3.7. Por enquanto, de-\nvemos apenas ficar cientes de que o TCP remetente \nusa paralelismo.\nPrincípios na prática\nde média dos valores de SampleRTT.­\n O TCP mantém uma média, denominada EstimatedRTT, dos valores de \nSampleRTT. Ao obter um novo SampleRTT, o TCP atualiza EstimatedRTT de acordo com a seguinte fórmula:\nEstimatedRTT = (1 – ) • EstimatedRTT +  • SampleRTT\nEssa fórmula está escrita na forma de um comando de linguagem de programação — o novo valor de \nEstimatedRTT é uma combinação ponderada entre o valor anterior de ­\nEstimatedRTT e o novo valor para \nSampleRTT. O valor recomendado de  é 0,125 (isto é, 1/8) [RFC 6298], caso em que essa fórmula se torna:\nEstimatedRTT = 0,875 • EstimatedRTT + 0,125 • SampleRTT\nNote que EstimatedRTT é uma média ponderada dos valores de SampleRTT. Como veremos em um \nexercício ao final deste capítulo, essa média ponderada atribui um peso maior às amostras recentes do que às an-\ntigas. Isso é natural, pois as amostras mais recentes refletem melhor o estado atual de congestionamento da rede. \nEm estatística, esse tipo de média é denominado média móvel exponencial ponderada. A palavra “exponencial” \naparece na MMEP porque o peso atribuído a um dado SampleRTT diminui exponencialmente à medida que \nas atualizações são realizadas. Os exercícios pedirão que você derive o termo exponencial em EstimatedRTT.\nA Figura 3.32 mostra os valores de SampleRTT e EstimatedRTT para um valor de  = 1/8, para uma \nconexão TCP entre gaia.cs.umass.edu (em Amherst, Massachusetts)­\n e fantasia.eurecom.fr (no sul \nda França). Fica claro que as variações em SampleRTT são atenuadas no cálculo de EstimatedRTT.\nAlém de ter uma estimativa do RTT, também é valioso ter uma medida de sua variabilidade. O [RFC 6298] \ndefine a variação do RTT, DevRTT, como uma estimativa do desvio típico entre SampleRTT e EstimatedRTT:\nDevRTT = (1 – ) • DevRTT +  • |SampleRTT – EstimatedRTT|\nNote que DevRTT é uma MMEP da diferença entre SampleRTT e EstimatedRTT. Se os valores de \nSampleRTT apresentarem pouca variação, então DevRTT será pequeno; por outro lado, se houver muita varia-\nção, DevRTT será grande. O valor recomendado para  é 0,25.\nCAMADA  de transporte  177 \nEstabelecimento e gerenciamento da temporização de retransmissão\nDados valores de EstimatedRTT e DevRTT, que valor deve ser utilizado para a temporização de re-\ntransmissão do TCP? É óbvio que o intervalo deve ser maior ou igual a ­\nEstimatedRTT, caso contrário seriam \nenviadas retransmissões desnecessárias. Mas a temporização de retransmissão não deve ser muito maior do que \nEstimatedRTT, senão, quando um segmento fosse perdido, o TCP não o retransmitiria rápido, o que resultaria \nem grandes atrasos de transferência de dados. Portanto, é desejável que o valor estabelecido para a temporização \nseja igual a EstimatedRTT mais certa margem, que deverá ser grande quando houver muita variação nos valores \nde SampleRTT e pequena quando houver pouca variação. Assim, o valor de DevRTT deverá entrar em cena. Todas \nessas considerações são levadas em conta no método do TCP para determinar a temporização de retransmissão:\nTimeoutInterval = EstimatedRTT + 4 • DevRTT\nRecomenda-se um valor inicial de 1 segundo para TimeoutInterval [RFC 6298]. Além disso, quando \nhá temporização, o valor de TimeoutInterval é dobrado para evitar que haja uma temporização para um \nsegmento subsequente, que logo será reconhecido. Porém, assim que o segmento é recebido e EstimatedRTT é \natualizado, o TimeoutInterval novamente é calculado usando a fórmula anterior.\n3.5.4  Transferência confiável de dados\nLembre-se de que o serviço da camada de rede da Internet (serviço IP) não é confiável. O IP não garante a \nentrega de datagramas na ordem correta nem a integridade de seus dados nos datagramas. Com o serviço IP, os \ndatagramas podem transbordar dos buffers dos roteadores e jamais alcançar seu destino; podem também chegar \nfora de ordem. Além disso, os bits dos datagramas podem ser corrompidos (passar de 0 para 1 e vice-versa). \nComo os segmentos da camada de transporte são carregados pela rede por datagramas IPs, também eles podem \nsofrer esses mesmos problemas.\nO TCP cria um serviço de transferência confiável de dados sobre o serviço de melhor esforço do IP. Esse \nserviço de transferência garante que a cadeia de dados que um processo lê a partir de seu buffer de recebimento \nTCP não está corrompida, não tem lacunas, não tem duplicações e está em sequência, isto é, a cadeia de bytes \nFigura 3.32  Amostras e estimativas de RTT\nRTT (milissegundos)\n150\n200\n250\n300\n350\n100\n1\n8\n15\n22\n29\n36\n43\n50\nTempo (segundos)\nAmostra de RTT\n57\n64\n71\n78\n85\n92\n99\n106\nRTT estimado\n   Redes de computadores e a Internet\n178\né idêntica à cadeia de bytes enviada pelo sistema final que está do outro lado da conexão. O modo como o TCP \noferece transferência confiável de dados envolve muitos dos princípios estudados na Seção 3.4.\nQuando desenvolvemos técnicas de transferência confiável de dados, era conceitualmente mais fácil admitir \nque existia um temporizador individual associado com cada segmento transmitido mas ainda não reconhecido. \nEmbora, em teoria, isso seja ótimo, o gerenciamento de temporizadores pode exigir considerável sobrecarga. \nAssim, os procedimentos recomendados no [RFC 6298] para gerenciamento de temporizadores TCP utilizam \napenas um único temporizador de retransmissão, mesmo que haja vários segmentos transmitidos ainda não re-\nconhecidos. O protocolo TCP apresentado nesta seção segue essa recomendação.\nDiscutiremos como o TCP provê transferência confiável de dados em duas etapas incrementais. Primeiro, \napresentamos uma descrição muito simplificada de um remetente TCP que utiliza apenas controle de tempori-\nzadores para se recuperar da perda de segmentos; em seguida, apresentaremos uma descrição mais complexa, \nque utiliza reconhecimentos duplicados além de temporizadores de retransmissão. Na discussão que se segue, \nadmitimos que os dados estão sendo enviados em uma direção somente, do hospedeiro A ao hospedeiro B, e que \no hospedeiro A está enviando um arquivo grande.\nA Figura 3.33 apresenta uma descrição muito simplificada de um remetente TCP. Vemos que há três eventos \nimportantes relacionados com a transmissão e a retransmissão de dados no TCP remetente: dados recebidos da \nFigura 3.33  Remetente TCP simplificado\n/* Suponha que o remetente não seja compelido pelo fluxo de TCP ou controle\nde congestionamento, que o tamanho dos dados vindos de cima seja menor do que\no MSS e que a transferência de dados ocorra apenas em uma direção. */\nNextSeqNum=InitialSeqNumber\nSendBase=InitialSeqNumber\nloop (forever) { switch(event)\n\t\nevent: dados recebidos da aplicação de cima\n\t\n\t\n\u0007\ncriar segmento TCP com número de sequência NextSeqNum\n\t\n\t\nif (temporizador atualmente parado)\n\t\n\t\n\t\niniciar temporizador\n\t\n\t\npassar segmento ao IP\n\t\n\t\nNextSeqNum=NextSeqNum+length(dados)\n\t\n\t\nbreak;\n\t\nevent: esgotamento do temporizador\n\t\n\t\n\u0007\nretransmitir segmento ainda não reconhecido com o menor\n\t\n\t\n\t\nnúmero de sequência\n\t\n\t\niniciar temporizador\n\t\n\t\nbreak;\n\t\nevent: ACK recebido, com valor do campo ACK de y\n\t\n\t\nif (y > SendBase) {\n\t\n\t\n\t\nSendBase=y\n\t\n\t\n\t\nif \u0007\n(não há atualmente segmentos ainda não reconhecidos)\n\t\n\t\n\t\n\t\niniciar temporizador\n\t\n\t\n\t\n}\n\t\n\t\nbreak;\n} /* fim do loop forever */\nCAMADA  de transporte  179 \naplicação; esgotamento do temporizador e recebimento de ACK. Quando ocorre o primeiro evento importante, \no TCP recebe dados da camada de aplicação, encapsula-os em um segmento e passa-o ao IP. Note que cada seg-\nmento inclui um número de sequência que é o número da corrente de bytes do primeiro byte de dados no \nsegmento, como descrito na Seção 3.5.2. Note também que, se o temporizador não estiver funcionando naquele \ninstante para algum outro segmento, o TCP aciona o temporizador quando o segmento é passado para o IP. (Fica \nmais fácil se você imaginar que o temporizador está associado com o mais antigo segmento não reconhecido.) O \nintervalo de expiração para esse temporizador é o TimeoutInterval, calculado a partir de EstimatedRTT e \nDevRTT, como descrito na Seção 3.5.3.\nO segundo é o esgotamento do temporizador. O TCP responde a esse evento retransmitindo o segmento \nque causou o esgotamento da temporização e então reinicia o temporizador.\nO terceiro evento importante que deve ser manipulado pelo TCP remetente é a chegada de um segmento \nde reconhecimento (ACK) do destinatário (mais especificamente, um segmento contendo um valor de campo \nde ACK válido). Quando da ocorrência, o TCP compara o valor do ACK, y, com sua variável SendBase. A \nvariável de estado SendBase do TCP é o número de sequência do mais antigo byte não reconhecido. (Assim, \nSendBase-1 é o número de sequência do último byte que se sabe ter sido recebido pelo destinatário de \nmodo correto e na ordem certa.) Como comentamos, o TCP usa reconhecimentos cumulativos, de maneira \nque y reconhece o recebimento de todos os bytes antes do byte número y. Se y > SendBase, então o ACK \nestá reconhecendo um ou mais bytes não reconhecidos antes. Desse modo, o remetente atualiza sua variável \nSendBase e também reinicia o temporizador se houver quaisquer segmentos ainda não reconhecidos.\nAlguns cenários interessantes\nAcabamos de descrever uma versão muito simplificada do modo como o TCP provê transferência confiável \nde dados, mas mesmo essa descrição tão simplificada tem muitas sutilezas. Para ter uma boa ideia de como esse \nprotocolo funciona, vamos agora examinar alguns cenários simples. A Figura 3.34 ilustra o primeiro cenário, em \nque um hospedeiro A envia um segmento ao hospedeiro B. Suponha que esse segmento tenha número de sequência \nFigura 3.34  Retransmissão devido a um reconhecimento perdido\nTempo\nTempo\nHospedeiro A\nHospedeiro B\nTemporização\nSeq=92, 8 bytes de dados\nSeq=92, 8 bytes de dados\nACK=100\nACK=100\nX\n(perda)\n   Redes de computadores e a Internet\n180\n92 e contenha 8 bytes de dados. Após enviá-lo, o hospedeiro A espera por um segmento de B com número de re-\nconhecimento 100. Embora o segmento de A seja recebido em B, o reconhecimento de B para A se perde. Nesse \ncaso, ocorre o evento de expiração do temporizador e o hospedeiro A retransmite o mesmo segmento. É claro que, \nquando recebe a retransmissão, o hospedeiro B observa, pelo número de sequência, que o segmento contém dados \nque já foram recebidos. Assim, o TCP no hospedeiro B descarta os bytes do segmento retransmitido.\nEm um segundo cenário, mostrado na Figura 3.35, o hospedeiro A envia dois segmentos seguidos. O pri-\nmeiro tem número de sequência 92 e 8 bytes de dados. O segundo tem número de sequência 100 e 20 bytes de \ndados. Suponha que ambos cheguem intactos em B e que B envie dois reconhecimentos separados para cada um \ndesses segmentos. O primeiro deles tem número de reconhecimento 100; o segundo, número 120. Suponha ago-\nra que nenhum dos reconhecimentos chegue ao hospedeiro A antes do esgotamento do temporizador. Quando \nocorre o evento de expiração do temporizador, o hospedeiro A reenvia o primeiro segmento com número de \nsequência 92 e reinicia o temporizador. Contanto que o ACK do segundo segmento chegue antes que o tempori-\nzador expire novamente, o segundo segmento não será retransmitido.\nEm um terceiro e último cenário, suponha que o hospedeiro A envie dois segmentos, assim como no se-\ngundo exemplo. O reconhecimento do primeiro segmento é perdido na rede, mas, um pouco antes do evento de \nexpiração, A recebe um reconhecimento com número 120. O hospedeiro A, portanto, sabe que B recebeu tudo até \no byte 119; logo, ele não reenvia nenhum dos dois segmentos. Tal cenário está ilustrado na Figura 3.36.\nDuplicação do tempo de expiração\nDiscutiremos agora algumas modificações empregadas por grande parte das execuções do TCP. A primeira \nrefere-se à duração do tempo de expiração após a expiração de um temporizador. Nessa modificação, sempre que \nocorre tal evento, o TCP retransmite o segmento ainda não reconhecido que tenha o menor número de sequên-\ncia, como descrevemos anteriormente. Mas, a cada retransmissão, o TCP ajusta o próximo tempo de expiração \npara o dobro do valor anterior em vez de derivá-lo dos últimos Estimated-RTT e DevRTT (como descrito na \nFigura 3.35  Segmento 100 não retransmitido\nTempo\nTempo\nHospedeiro A\nHospedeiro B\nseq=92 intervalo de temporização\nSeq=92, 8 bytes de dados\nSeq=100, 20 bytes de dados\nACK=100\nACK=120\nACK=120\nseq=92 intervalo de temporização\nSeq=92, 8 bytes de dados\nCAMADA  de transporte  181 \nSeção 3.5.3). Por exemplo, suponha que o TimeoutInterval associado com o mais antigo segmento ainda não \nreconhecido seja 0,75 s quando o temporizador expirar pela primeira vez. O TCP então retransmite esse segmen-\nto e ajusta o novo tempo de expiração para 1,5 s. Se o temporizador expirar novamente 1,5 s mais tarde, o TCP \nretransmitirá de novo esse segmento, agora ajustando o tempo de expiração para 3,0 s. Assim, o tempo aumenta \nexponencialmente após cada retransmissão. Todavia, sempre que o temporizador é iniciado após qualquer um \ndos outros dois eventos (isto é, dados recebidos da aplicação anterior e ACK recebido), o TimeoutInterval \nserá derivado dos valores mais recentes de EstimatedRTT e DevRTT.\nEssa modificação provê uma forma limitada de controle de congestionamento. (Maneiras mais abrangentes de \ncontrole de congestionamento no TCP serão estudadas na Seção 3.7.) A causa mais provável da expiração do tem-\nporizador é o congestionamento na rede, isto é, um número muito grande de pacotes chegando a uma (ou mais) fila \nde roteadores no caminho entre a origem e o destino, o que provoca descarte de pacotes e/ou longos atrasos de fila. \nSe as origens continuarem a retransmitir pacotes de modo persistente durante um congestionamento, ele pode pio-\nrar. Em vez disso, o TCP age com mais educação: cada remetente retransmite após intervalos cada vez mais longos. \nVeremos que uma ideia semelhante a essa é utilizada pela Ethernet, quando estudarmos CSMA/CD no Capítulo 5.\nRetransmissão rápida\nUm dos problemas de retransmissões acionadas por expiração de temporizador é que o período de expira-\nção pode ser um tanto longo. Quando um segmento é perdido, esse longo período força o remetente a atrasar o \nreenvio do pacote perdido, aumentando por conseguinte o atraso fim a fim. Felizmente, o remetente pode com \nfrequência detectar perda de pacote bem antes de ocorrer o evento de expiração, observando os denominados \nACKs duplicados. Um ACK duplicado é um ACK que reconhece novamente um segmento para o qual o reme-\ntente já recebeu um reconhecimento anterior. Para entender a resposta do remetente a um ACK duplicado, deve-\nmos examinar por que o destinatário envia um ACK duplicado em primeiro lugar. A Tabela 3.2 resume a política \nde geração de ACKs do TCP destinatário [RFC 5681]. Quando um TCP destinatário recebe um segmento com \nTempo\nTempo\nHospedeiro A\nHospedeiro B\nSeq=92 intervalo de temporização\nSeq=92, 8 bytes de dados\nSeq=100,  20 bytes de dados\nACK=100\nACK=120\nX\n(perda)\nFigura 3.36  \u0007\nUm reconhecimento cumulativo evita retransmissão do primeiro segmento\n   Redes de computadores e a Internet\n182\nnúmero de sequência maior do que o seguinte, esperado, na ordem, ele detecta uma lacuna no fluxo de dados — \n \nou seja, um segmento faltando. Essa lacuna poderia ser o resultado de segmentos perdidos ou reordenados den-\ntro da rede. Como o TCP não usa reconhecimentos negativos, o destinatário não pode enviar um reconhecimento \nnegativo explícito de volta ao destinatário. Em vez disso, ele apenas reconhece mais uma vez (ou seja, gera um \nACK duplicado para) o último byte de dados na ordem que foi recebido. (Observe que a Tabela 3.2 tem provisão \npara o caso em que o destinatário não descarta segmentos fora de ordem.)\nTabela 3.2  TCP ACK Generation Recommendation [RFC 5681]\nEvento\nAção do TCP destinatário\nChegada de segmento na ordem com número de sequência esperado. \nTodos os dados até o número de sequência esperado já reconhecidos.\nACK retardado. Espera de até 500 ms pela chegada de outro \nsegmento na ordem. Se o segmento seguinte na ordem não chegar \nnesse intervalo, envia um ACK.\nChegada de segmento na ordem com número de sequência esperado. \nOutro segmento na ordem esperando por transmissão de ACK.\nEnvio imediato de um único ACK cumulativo, reconhecendo ambos \nos segmentos.\nChegada de um segmento fora da ordem com número de sequência \nmais alto do que o esperado. Lacuna detectada.\nEnvio imediato de um ACK duplicado, indicando número de sequência \ndo byte seguinte esperado (que é a extremidade mais baixa da lacuna).\nChegada de um segmento que preenche, parcial ou completamente, a \nlacuna nos dados recebidos.\nEnvio imediato de um ACK, contanto que o segmento comece na \nextremidade mais baixa da lacuna.\nComo um remetente quase sempre envia um grande número de segmentos, um atrás do outro, se um seg-\nmento for perdido, provavelmente existirão muitos ACKs duplicados, também um após o outro. Se o TCP reme-\ntente receber três ACKs duplicados para os mesmos dados, ele tomará isso como indicação de que o segmento \nque se seguiu ao segmento reconhecido três vezes foi perdido. (Nos exercícios de fixação consideraremos por que \no remetente espera três ACKs duplicados e não apenas um.) No caso de receber três ACKs duplicados, o TCP re-\nmetente realiza uma retransmissão rápida [RFC 5681], retransmitindo o segmento que falta antes da expiração \ndo temporizador do segmento. Isso é mostrado na Figura 3.37, em que o segundo segmento é perdido, e então \nretransmitido antes da expiração do temporizador. Para o TCP com retransmissão rápida, o seguinte trecho de \ncodificação substitui o evento ACK recebido na Figura 3.33:\nevento: ACK recebido, com valor do campo ACK y\n\t\nif (y > SendBase) {\n\t\n\t\nSendBase=y\n\t\n\t\nif (atualmente ainda não há segmentos reconhecidos)\n\t\n\t\n\t\niniciar temporizador\n\t\n\t\n}\n\t\nelse { /* um ACK duplicado para segmento já reconhecido */\n\t\n\t\nincrementar número de ACKs duplicados recebidos para y\n\t\n\t\nif (número de ACKS duplicados recebidos para y==3)\n\t\n\t\n\t\n/* retransmissão rápida do TCP */\n\t\n\t\n\t\nreenviar segmento com número de sequência y\n\t\n\t\n}\n\t\nbreak;\nObservamos antes que muitas questões sutis vêm à tona quando um mecanismo de controle de temporiza-\nção/retransmissão é executado em um protocolo real como o TCP. Os procedimentos anteriores, cuja evolução é \nresultado de mais de 15 anos de experiência com temporizadores TCP, devem convencê-lo de que, na realidade, \né isso que acontece!\nCAMADA  de transporte  183 \nGo-Back-N ou repetição seletiva?\nVamos encerrar nosso estudo do mecanismo de recuperação de erros do TCP considerando a seguinte \npergunta: o TCP é um protocolo GBN ou SR? Lembre-se de que, no TCP, os reconhecimentos são cumulativos e \nsegmentos recebidos de modo correto, mas fora da ordem, não são reconhecidos (ACK) individualmente pelo \ndestinatário. Em consequência, como mostrou a Figura 3.33 (veja também a Figura 3.19), o TCP remetente \nprecisa tão somente lembrar o menor número de sequência de um byte transmitido, porém não reconhecido \n(SendBase) e o número de sequência do byte seguinte a ser enviado (NextSeqNum). Nesse sentido, o TCP se \nparece muito com um protocolo ao estilo do GBN. Porém, há algumas diferenças surpreendentes entre o TCP e o \n \nGBN. Muitas execuções do TCP armazenarão segmentos recebidos corretamente, mas fora da ordem [Stevens, \n1994]. Considere também o que acontece quando o remetente envia uma sequência de segmentos 1, 2, ..., N \ne todos os segmentos chegam ao destinatário na ordem e sem erro. Além disso, suponha que o reconhecimento \npara o pacote n < N se perca, mas que os N – 1 reconhecimentos restantes cheguem ao remetente antes do esgota-\nmento de suas respectivas temporizações. Nesse exemplo, o GBN retransmitiria não só o pacote n, mas também \ntodos os subsequentes n + 1, n + 2, ..., N. O TCP, por outro lado, retransmitiria no máximo um segmento, a saber, \nn. E mais, o TCP nem ao menos retransmitiria o segmento n se o reconhecimento para n + 1 chegasse antes do \nfinal da temporização para o segmento n.\nUma modificação proposta para o TCP, denominada reconhecimento seletivo [RFC 2018], permite que \num destinatário TCP reconheça seletivamente segmentos fora de ordem, em vez de apenas reconhecer de modo \ncumulativo o último segmento recebido corretamente e na ordem. Quando combinado com retransmissão seleti-\nva — isto é, saltar a retransmissão de segmentos que já foram reconhecidos de modo seletivo pelo destinatário —, \no TCP se parece muito com nosso protocolo SR genérico. Assim, o mecanismo de recuperação de erros do TCP \ntalvez seja mais bem caracterizado como um híbrido dos protocolos GBN e SR.\nFigura 3.37  \u0007\nRetransmissão rápida: retransmitir o segmento que falta antes da expiração do \ntemporizador do segmento\nHospedeiro A\nHospedeiro B\nseq=100, 20 bytes de dados\nTemporização\nTempo\nTempo\nX\nseq=100, 20 bytes de dados\nseq=92, 8 bytes de dados\nseq=120, 15 bytes de dados\nseq=135, 6 bytes de dados\nseq=141, 16 bytes de dados\nack=100\nack=100\nack=100\nack=100\n   Redes de computadores e a Internet\n184\n3.5.5  Controle de fluxo\nLembre-se de que os hospedeiros de cada lado de uma conexão TCP reservam um buffer de recepção para \na conexão. Quando a conexão TCP recebe bytes que estão corretos e em sequência, ele coloca os dados no buffer \nde recepção. O processo de aplicação associado lerá os dados a partir desse buffer, mas não necessariamente no \nmomento em que são recebidos. Na verdade, a aplicação receptora pode estar ocupada com alguma outra tarefa e \nnem ao menos tentar ler os dados até muito após a chegada deles. Se a aplicação for relativamente lenta na leitura \ndos dados, o remetente pode muito facilmente saturar o buffer de recepção da conexão por enviar demasiados \ndados muito rapidamente.\nO TCP provê um serviço de controle de fluxo às suas aplicações, para eliminar a possibilidade de o re-\nmetente estourar o buffer do destinatário. Assim, controle de fluxo é um serviço de compatibilização de velo-\ncidades — compatibiliza a taxa à qual o remetente está enviando com  aquela à qual a aplicação receptora está \nlendo. Como já notamos, um TCP remetente também pode ser estrangulado por causa do congestionamento \ndentro da rede IP. Esse modo de controle do remetente é denominado controle de congestionamento, um \ntópico que será examinado em detalhes nas Seções 3.6 e 3.7. Mesmo que as ações executadas pelo controle de \nfluxo e pelo controle de congestionamento sejam semelhantes (a regulagem do remetente), fica evidente que \nelas são executadas por razões muito diferentes. Infelizmente, muitos autores usam os termos de modo inter-\ncambiável, e o leitor esperto tem de tomar muito cuidado para distinguir os dois casos. Vamos agora discutir \ncomo o TCP provê seu serviço de controle de fluxo. Para podermos enxergar o quadro geral, sem nos fixarmos \nnos detalhes, nesta seção admitiremos que essa implementação do TCP é tal que o receptor TCP descarta \nsegmentos fora da ordem.\nO TCP oferece serviço de controle de fluxo fazendo que o remetente mantenha uma variável denomi-\nnada janela de recepção. De modo informal, a janela de recepção é usada para dar ao remetente uma ideia \ndo espaço de buffer livre disponível no destinatário. Como o TCP é full-duplex, o remetente de cada lado da \nconexão mantém uma janela de recepção distinta. Vamos examinar a janela de recepção no contexto de uma \ntransferência de arquivo. Suponha que o hospedeiro A esteja enviando um arquivo grande ao hospedeiro B \npor uma conexão TCP. O hospedeiro B aloca um buffer de recepção a essa conexão; denominemos seu tama-\nnho RcvBuffer. De tempos em tempos, o processo de aplicação no hospedeiro B faz a leitura do buffer. São \ndefinidas as seguintes variáveis:\n• LastByteRead: número do último byte na cadeia de dados lido do buffer pelo processo de aplicação \nem B.\n• LastByteRcvd: número do último byte na cadeia de dados que chegou da rede e foi colocado no buffer \nde recepção de B.\nComo o TCP não tem permissão para saturar o buffer alocado, devemos ter:\nLastByteRcvd – LastByteRead ≤ RcvBuffer\nA janela de recepção, denominada rwnd, é ajustada para a quantidade de espaço disponível no buffer:\nrwnd = RcvBuffer – [LastByteRcvd – LastByteRead]\nComo o espaço disponível muda com o tempo, rwnd é dinâmica. Esta variável está ilustrada na Figura 3.38.\nComo a conexão usa a variável rwnd para prover o serviço de controle de fluxo? O hospedeiro B diz ao hos-\npedeiro A quanto espaço disponível ele tem no buffer da conexão colocando o valor corrente de rwnd no campo de \njanela de recepção de cada segmento que envia a A. No começo, o hospedeiro B estabelece rwnd = RcvBuffer. \nNote que, para conseguir isso, o hospedeiro B deve monitorar diversas variáveis específicas da conexão.\nO hospedeiro A, por sua vez, monitora duas variáveis, LastByteSent e LastByteAcked, cujos sig-\nnificados são óbvios. Note que a diferença entre essas duas variáveis, LastByteSent — LastByteAcked, \né a quantidade de dados não reconhecidos que A enviou para a conexão. Mantendo a quantidade de dados não \nCAMADA  de transporte  185 \nreconhecidos menor que o valor de rwnd, o hospedeiro A tem certeza de que não está fazendo transbordar o \nbuffer de recepção no hospedeiro B. Assim, A tem de certificar-se, durante toda a duração da conexão, de que:\nLastByteSent – LastByteAcked ≤ rwnd\nHá um pequeno problema técnico com esse esquema. Para percebê-lo, suponha que o buffer de recepção \ndo hospedeiro B fique tão cheio que rwnd = 0. Após anunciar ao hospedeiro A que rwnd = 0, imagine que B não \ntenha nada para enviar ao hospedeiro A. Agora considere o que acontece. Enquanto o processo de aplicação em \nB esvazia o buffer, o TCP não envia novos segmentos com novos valores rwnd para o hospedeiro A. Na verdade, \no TCP lhe enviará um segmento somente se tiver dados ou um reconhecimento para enviar. Por conseguinte, o \nhospedeiro A nunca será informado de que foi aberto algum espaço no buffer de recepção do hospedeiro B: ele \nficará bloqueado e não poderá transmitir mais dados! Para resolver esse problema, a especificação do TCP requer \nque o hospedeiro A continue a enviar segmentos com um byte de dados quando a janela de recepção de B for \nzero. Esses segmentos serão reconhecidos pelo receptor. Por fim, o buffer começará a esvaziar, e os reconheci-\nmentos conterão um valor diferente de zero em rwnd.\nO site de apoio deste livro fornece um applet interativo em Java que ilustra a operação da janela de recepção do TCP\n.\nAgora que descrevemos o serviço de controle de fluxo do TCP, mencionaremos de maneira breve que o \nUDP não provê controle de fluxo. Para entender a questão, considere o envio de uma série de segmentos UDP de \n \num processo no hospedeiro A para um processo no hospedeiro B. Para uma execução UDP típica, o UDP ane-\nxará os segmentos a um buffer de tamanho finito que “precede” o socket correspondente (isto é, o socket para o \nprocesso). O processo lê um segmento inteiro do buffer por vez. Se o processo não ler os segmentos com rapidez \nsuficiente, o buffer transbordará e os segmentos serão descartados.\n3.5.6  Gerenciamento da conexão TCP\nNesta subseção, examinamos mais de perto como uma conexão TCP é estabelecida e encerrada. Embora esse \ntópico talvez não pareça de particular interesse, é importante, porque o estabelecimento da conexão TCP tem um peso \nsignificativo nos atrasos percebidos (por exemplo, ao navegar pela Web). Além disso, muitos dos ataques mais comuns \na redes — entre eles o incrivelmente popular ataque de inundação SYN — exploram vulnerabilidades no gerenciamen-\nto da conexão TCP\n. Em primeiro lugar, vamos ver como essa conexão é estabelecida. Suponha que um processo que \nroda em um hospedeiro (cliente) queira iniciar uma conexão com outro processo em outro hospedeiro (servidor). O \nprocesso de aplicação cliente primeiro informa ao TCP cliente que quer estabelecer uma conexão com um processo no \nservidor. O TCP no cliente então estabelece uma conexão TCP com o TCP no servidor da seguinte maneira:\n• Etapa 1. O lado cliente do TCP primeiro envia um segmento TCP especial ao lado servidor do TCP. Esse \nsegmento não contém nenhum dado de camada de aplicação, mas um dos bits de flag no seu cabeçalho \nFigura 3.38  A janela de recepção (rwnd) e o buffer de recepção (RcvBuffer)\nProcesso de\naplicação\nDados vindos\ndo IP\nDados TCP\nno buffer\nrwnd\nRcvBuffer\nEspaço indisponível\n   Redes de computadores e a Internet\n186\n(veja a Figura 3.29), o bit SYN, é ajustado para 1. Por essa razão, o segmento é denominado um segmento \nSYN. Além disso, o cliente escolhe ao acaso um número de sequência inicial (client_isn) e o coloca \nno campo de número de sequência do segmento TCP SYN inicial. Esse segmento é encapsulado em um \ndatagrama IP e enviado ao servidor. A aleatoriedade adequada da escolha de client_isn de modo a \nevitar certos ataques à segurança tem despertado considerável interesse [CERT 2001-09].\n• Etapa 2. Assim que o datagrama IP contendo o segmento TCP SYN chega ao hospedeiro servidor (ad-\nmitindo-se que ele chegue mesmo!), o servidor extrai o segmento TCP SYN do datagrama, aloca buffers \ne variáveis TCP à conexão e envia um segmento de aceitação de conexão ao TCP cliente. (Veremos, no \nCapítulo 8, que a alocação desses buffers e variáveis, antes da conclusão da terceira etapa da apresentação \nde três vias, torna o TCP vulnerável a um ataque de recusa de serviço conhecido como inundação SYN.) \nEsse segmento de aceitação de conexão também não contém nenhum dado de camada de aplicação. \nContudo, contém três informações importantes no cabeçalho do segmento: o bit SYN está com valor \n1; o campo de reconhecimento do cabeçalho do segmento TCP está ajustado para client_isn+1; e, \npor fim, o servidor escolhe seu próprio número de sequência inicial (server_isn) e coloca esse valor \nno campo de número de sequência do cabeçalho do segmento TCP. Esse segmento de aceitação de co-\nnexão está dizendo, com efeito, “Recebi seu pacote SYN para começar uma conexão com seu número de \nsequência inicial client_isn. Concordo em estabelecer essa conexão. Meu número de sequência inicial é \nserver_isn”\n. O segmento de concessão da conexão às vezes é denominado segmento SYNACK.\n• Etapa 3. Ao receber o segmento SYNACK, o cliente também reserva buffers e variáveis para a conexão. O hos-\npedeiro cliente então envia ao servidor mais um segmento. Este último reconhece o segmento de confirmação \nda conexão do servidor (o cliente o faz colocando o valor server_isn+1 no campo de reconhecimento do \ncabeçalho do segmento TCP). O bit SYN é ajustado para 0, já que a conexão está estabelecida. A terceira etapa \nda apresentação de três vias pode conduzir os dados cliente/servidor na carga útil do segmento.\nCompletadas as três etapas, os hospedeiros cliente e servidor podem enviar segmentos contendo dados um \nao outro. Em cada um desses futuros segmentos, o bit SYN estará ajustado para 0. Note que, para estabelecer a \nconexão, três pacotes são enviados entre dois hospedeiros, como ilustra a Figura 3.39. Por tal razão, esse procedi-\nmento de estabelecimento de conexão é com frequência denominado apresentação de três vias. Vários aspectos \nda apresentação de três vias do TCP são tratados nos exercícios ao final deste capítulo (Por que são necessários \nos números de sequência iniciais? Por que é preciso uma apresentação de três vias, e não apenas de duas vias?). \nÉ interessante notar que um alpinista e seu amarrador (que fica mais abaixo e cuja tarefa é passar a corda de se-\ngurança ao alpinista) usam um protocolo de comunicação de apresentação de três vias idêntico ao do TCP para \ngarantir que ambos os lados estejam prontos antes de o alpinista iniciar a escalada.\nTudo o que é bom dura pouco, e o mesmo é válido para uma conexão TCP. Qualquer um dos dois processos \nque participam de uma conexão TCP pode encerrar a conexão. Quando esta termina, os “recursos” (isto é, os \nbuffers e as variáveis) nos hospedeiros são liberados. Como exemplo, suponha que o cliente decida encerrar a \nconexão, como mostra a Figura 3.40. O processo de aplicação cliente emite um comando para fechar. Isso faz que \no TCP cliente envie um segmento TCP especial ao processo servidor, cujo bit de flag no cabeçalho do segmento, \ndenominado bit FIN (veja a Figura 3.29), tem valor ajustado em 1. Quando o servidor recebe esse segmento, en-\nvia de volta ao cliente um segmento de reconhecimento. O servidor então envia seu próprio segmento de encer-\nramento, que tem o bit FIN ajustado em 1. Por fim, o cliente reconhece o segmento de encerramento do servidor. \nNesse ponto, todos os recursos dos dois hospedeiros estão liberados.\nDurante a vida de uma conexão TCP, o protocolo TCP que roda em cada hospedeiro faz transições pelos vá-\nrios estados do TCP. A Figura 3.41 ilustra uma sequência típica de estados do TCP visitados pelo TCP cliente. O \nTCP cliente começa no estado CLOSED. A aplicação no lado cliente inicia uma nova conexão TCP (criando um \nobjeto socket como nos exemplos em Java do Capítulo 2). Isso faz o TCP no cliente enviar um segmento SYN ao \nTCP no servidor. Após o envio, o TCP cliente entra no estado SYN_SENT e, enquanto isso, o TCP cliente espera \npor um segmento do TCP servidor que inclui um reconhecimento para o segmento anterior do cliente, e tem o \nbit SYN ajustado para o valor 1. Assim que recebe esse segmento, o TCP cliente entra no estado ESTABLISHED, \nquando pode enviar e receber segmentos TCP que contêm carga útil de dados (isto é, gerados pela aplicação).\nCAMADA  de transporte  187 \nFigura 3.39  Apresentação de três vias do TCP: troca de segmentos\nTempo\nTempo\nHospedeiro cliente\nRequisição de\nconexão\nConexão\naceita\nHospedeiro servidor\nSYN=1, seq=client_isn\nSYN=1, seq=server_isn,\nack=client_isn+1\nSYN=0, seq=client_isn+1,\nack=server_isn+1\nACK\nFigura 3.40  Encerramento de uma conexão TCP\nTempo\nTempo\nCliente\nFecha\nFecha\nServidor\nFIN\nACK\nACK\nFIN\nFecha\nEspera\ntemporizada\nSuponha que a aplicação cliente decida que quer fechar a conexão. (Note que o servidor também tem a \nalternativa de fechá-la.) Isso faz o TCP cliente enviar um segmento TCP com o bit FIN ajustado em 1 e entrar \nno estado FIN_WAIT_1. No estado FIN_WAIT_1, o TCP cliente espera por um segmento TCP do servidor com \num reconhecimento. Quando recebe esse segmento, o TCP cliente entra no estado FIN_WAIT_2. No estado \nFIN_ WAIT_2, ele espera por outro segmento do servidor com o bit FIN ajustado para 1. Após recebê-lo, o TCP \n   Redes de computadores e a Internet\n188\ncliente reconhece o segmento do servidor e entra no estado TIME_WAIT. Esse estado permite que o TCP cliente \nreenvie o reconhecimento final, caso o ACK seja perdido. O tempo passado no estado TIME_WAIT depende da \nimplementação, mas os valores típicos são 30 s, 1 min e 2 min. Após a espera, a conexão se encerra formalmente \ne todos os recursos do lado cliente (inclusive os números de porta) são liberados.\nA Figura 3.42 ilustra a série de estados normalmente visitados pelo TCP do lado servidor, admitindo-se que \né o cliente quem inicia o encerramento da conexão. As transições são autoexplicativas. Nesses dois diagramas \nde transição de estados, mostramos apenas como uma conexão TCP é em geral estabelecida e fechada. Não des-\ncrevemos o que acontece em certos cenários patológicos, por exemplo, quando ambos os lados de uma conexão \nquerem iniciar ou fechar ao mesmo tempo. Se estiver interessado em aprender mais sobre esse assunto e sobre \noutros mais avançados referentes ao TCP, consulte o abrangente livro de Stevens [1994].\nFigura 3.41  Uma sequência típica de estados do TCP visitados por um TCP cliente\nCLOSED\nSYN_SENT\nESTABLISHED\nFIN_WAIT_1\nFIN_WAIT_2\nTIME_WAIT\nEnvia FIN\nEnvia SYN\nRecebe ACK,\nnão envia nada\nEspera 30 segundos\nRecebe FIN,\nenvia ACK\nRecebe SYN & ACK,\nenvia ACK\nAplicação cliente inicia\no fechamento da conexão\nAplicação cliente inicia\numa conexão TCP\nFigura 3.42  \u0007\nUma sequência típica de estados do TCP visitados por um TCP do lado do servidor\nCLOSED\nLISTEN\nSYN_RCVD\nESTABLISHED\nCLOSE_WAIT\nLAST_ACK\nRecebe FIN,\nenvia ACK\nRecebe ACK,\nnão envia nada\nEnvia FIN\nRecebe SYN, envia\nSYN & ACK\nAplicação servidor\ncria uma porta de escuta\nRecebe ACK,\nnão envia nada\nCAMADA  de transporte  189 \nNossa discussão anterior concluiu que o cliente e o servidor estão preparados para se comunicar, isto é, que \no servidor está ouvindo na porta pela qual o cliente envia seu segmento SYN. Vamos considerar o que acontece \nquando um hospedeiro recebe um segmento TCP cujos números de porta ou endereço IP não são compatíveis \ncom nenhum dos sockets existentes no hospedeiro. Por exemplo, imagine que um hospedeiro receba um pacote \nTCP SYN com porta de destino 80, mas não está aceitando conexões nessa porta (isto é, não está rodando um \nservidor Web na porta 80). Então, ele enviará à origem um segmento especial de reinicialização. Esse segmento \nTCP tem o bit de flag RST ajustado para 1 (veja Seção 3.5.2). Assim, quando um hospedeiro envia um segmento \nde reinicialização ele está dizendo à origem: “Eu não tenho um socket para esse segmento. Favor não enviá-lo \nnovamente”\n. Quando um hospedeiro recebe um pacote UDP cujo número de porta de destino não é compatível \ncom as portas de um UDP em curso, ele envia um datagrama ICMP especial, como será discutido no Capítulo 4.\nAgora que obtivemos uma boa compreensão sobre gerenciamento da conexão TCP, vamos voltar à ferramenta \nde varredura de porta nmap e analisar mais precisamente como ela funciona. Para explorar uma porta TCP, \nO ataque Syn Flood\nVimos em nossa discussão sobre a apresentação \nde três vias do TCP que um servidor aloca e inicializa \nas variáveis da conexão e os buffers em resposta ao \nSYN recebido. O servidor, então, envia um SYNACK em \nresposta e aguarda um segmento ACK do cliente. Se \no cliente não enviar um ACK para completar o terceiro \npasso da apresentação de três vias, com o tempo (em \ngeral após um minuto ou mais), o servidor finalizará a \nconexão semiaberta e recuperará os recursos alocadas.\nEsse protocolo de gerenciamento da conexão \nTCP abre caminho para um ataque DoS clássico, ou \nseja, o ataque SYN flood. Neste ataque, o vilão en-\nvia um grande número de segmentos SYN TCP\n, sem \nconcluir a terceira etapa de apresentação. Com esse \nacúmulo de segmentos SYN, os recursos de conexão \ndo servidor podem se esgotar depressa já que são \nalocados (mas nunca usados) para conexões semia-\nbertas; clientes legítimos, então, não são atendidos. \nEsses ataques SYN flood [CERT SYN, 1996] estavam \nentre os primeiros ataques DoS documentados pelo \nCERT [CERT, 2009]. Felizmente, uma defesa eficaz, \nconhecida como SYN cookies [RFC 4987], agora é \nempregada na maioria dos principais sistemas ope-\nracionais. SYS cookies funcionam da seguinte forma:\n• \nQuando o servidor recebe um segmento SYN, não \nse sabe se ele vem de um usuário verdadeiro ou se \né parte desse ataque. Então, em vez de criar uma \nconexão TCP semiaberta para esse SYN, o servi-\ndor cria um número de sequência TCP inicial, que é \numa função hash de endereços de origem e endere-\nços de destino IP e números de porta do segmento \nSYN, assim como de um número secreto conheci-\ndo apenas pelo usuário. Esse número de sequência \ninicial criado cuidadosamente é o assim chamado \n“cookie”. O servidor, então, envia ao cliente um pa-\ncote SYNACK com esse número de sequência es-\npecial. É importante mencionar que o servidor não \nse lembra do cookie ou de qualquer outra informa-\nção de estado correspondente ao SYN.\n• Se o cliente for verdadeiro, então um segmento \nACK retornará. O servidor, ao receber esse ACK, \nprecisa verificar se ele corresponde a algum SYN \nenviado antes. Como isto é feito se ele não guar-\nda nenhuma memória sobre os segmentos SYN? \nComo você deve ter imaginado, o processo é rea­\nlizado com o cookie. Para um ACK legítimo, em \nespecial, o valor no campo de reconhecimento é \nigual ao número de sequência no SYNACK (o valor \ndo cookie, neste caso) mais um (veja Figura 3.39). \nO servidor, então, executará a mesma função utili-\nzando os mesmos campos no segmento ACK (que \nsão os mesmos que no SYN original) e número \nsecreto. Se o resultado da função mais um for o \nmesmo que o número de reconhecimento (cookie) \nno SYNACK do cliente, o servidor conclui que o \nACK corresponde a um segmento SYN anterior e, \nportanto, é válido. O servidor, então, cria uma co-\nnexão totalmente aberta com um socket.\n• \nPor outro lado, se o cliente não retorna um segmen-\nto ACK, então o SYN original não causou nenhum \ndano ao servidor, uma vez que este não alocou ne-\nnhum recurso em resposta ao SYN falso original.\nSegurança em foco\n   Redes de computadores e a Internet\n190\ndigamos que a porta 6789, o nmap enviará ao computador-alvo um segmento TCP SYN com a porta de destino. \nOs três possíveis resultados são:\n• O computador de origem recebe um segmento TCP SYNACK de um computador-alvo. Como isso significa \nque uma aplicação está sendo executada com a porta TCP 6789 no computador-alvo, o nmap retorna \n“aberto”\n.\n• O computador de origem recebe um segmento TCP RST de um computador-alvo. Isto significa que o seg-\nmento SYN atingiu o computador-alvo, mas este não está executando uma aplicação com a porta TCP \n6789. Mas o atacante, pelo menos, sabe que os segmentos destinados ao computador na porta 6789 não \nestão bloqueados pelo firewall no percurso entre o computador de origem e o alvo. (Firewalls são aborda-\ndos no Capítulo 8.)\n• A origem não recebe nada. Isto, provavelmente, significa que o segmento SYN foi bloqueado pelo firewall \ne nunca atingiu o computador-alvo.\nO nmap é uma ferramenta potente, que pode “examinar o local” não só para abrir portas TCP, mas também \npara abrir portas UDP, para firewalls e suas configurações, e até mesmo para as versões de aplicações e sistemas \noperacionais. A maior parte é feita pela manipulação dos segmentos de gerenciamento da conexão TCP [Skoudis, \n2006]. É possível fazer download do nmap pelo site <www.nmap.org>.\nCom isso, concluímos nossa introdução ao controle de erro e controle de fluxo em TCP. Voltaremos ao TCP \nna Seção 3.7 e então examinaremos em mais detalhes o controle de congestionamento do TCP. Antes, contudo, \nvamos analisar a questão do controle de congestionamento em um contexto mais amplo.\n3.6  \u0007\nPrincípios de controle de congestionamento\nNas seções anteriores, examinamos os princípios gerais e também os mecanismos específicos do TCP usa-\ndos para prover um serviço de transferência confiável de dados em relação à perda de pacotes. Mencionamos an-\ntes que, na prática, essa perda resulta, de modo característico, de uma saturação de buffers de roteadores à medida \nque a rede fica congestionada. Assim, a retransmissão de pacotes trata de um sintoma de congestionamento de \nrede (a perda de um segmento específico de camada de transporte), mas não trata da causa do congestionamento \nda rede: demasiadas fontes tentando enviar dados a uma taxa muito alta. Para tratar da causa do congestionamen-\nto de rede, são necessários mecanismos para regular os remetentes quando ele ocorre.\nNesta seção, consideramos o problema do controle de congestionamento em um contexto geral, buscando \nentender por que ele é algo ruim, como o congestionamento de rede se manifesta no desempenho recebido por \naplicações da camada superior e várias medidas que podem ser adotadas para evitá-lo ou reagir a ele. Esse estudo \nmais geral do controle de congestionamento é apropriado, já que, como acontece com a transferência confiável \nde dados, o congestionamento é um dos “dez mais” da lista de problemas fundamentalmente importantes no tra-\nbalho em rede. Concluímos esta seção com uma discussão sobre o controle de congestionamento no serviço de \ntaxa de bits disponível (available bit-rate — ABR) em redes com modo de transferência assíncrono (ATM). A \nseção seguinte contém um estudo detalhado do algoritmo de controle de congestionamento do TCP.\n3.6.1  As causas e os custos do congestionamento\nVamos começar nosso estudo geral do controle de congestionamento examinando três cenários de com-\nplexidade crescente nos quais ele ocorre. Em cada caso, examinaremos, primeiro, por que ele ocorre e, depois, \nseu custo (no que se refere aos recursos não utilizados integralmente e ao baixo desempenho recebido pelos \nsistemas finais). Não focalizaremos (ainda) como reagir a ele, ou evitá-lo; preferimos estudar uma questão \nmais simples, que é entender o que acontece quando hospedeiros aumentam sua taxa de transmissão e a rede \nfica congestionada.\nCAMADA  de transporte  191 \nCenário 1: dois remetentes, um roteador com buffers infinitos\nComeçamos considerando o que talvez seja o cenário de congestionamento mais simples possível: dois \nhospedeiros (A e B), cada um conforme uma conexão que compartilha um único trecho de rede entre a origem \ne o destino, conforme mostra a Figura 3.43.\nVamos admitir que a aplicação no hospedeiro A esteja enviando dados para a conexão (por exemplo, passando \ndados para o protocolo de camada de transporte por um socket) a uma taxa média de in bytes/s. Esses dados são \noriginais no sentido de que cada unidade de dados é enviada para dentro do socket apenas uma vez. O protocolo de \ncamada de transporte subjacente é simples. Os dados são encapsulados e enviados; não há recuperação de erros (por \nexemplo, retransmissão), controle de fluxo, nem controle de congestionamento. Desprezando a sobrecarga adicional \ncausada pela adição de informações de cabeçalhos de camada de transporte e de camadas mais baixas, a taxa à qual o \nhospedeiro A oferece tráfego ao roteador nesse primeiro cenário é in bytes/s. O hospedeiro B funciona de maneira \nsemelhante, e admitimos, por simplicidade, que ele também esteja enviando dados a uma taxa de in bytes/s. Os paco-\ntes dos hospedeiros A e B passam por um roteador e por um enlace de saída compartilhado de capacidade R. O rotea­\ndor \ntem buffers que lhe permitem armazenar os pacotes que chegam quando a taxa de chegada excede a capacidade do \nenlace de saída. No primeiro cenário, admitimos que o roteador tenha capacidade de armazenamento infinita.\nA Figura 3.44 apresenta o desempenho da conexão do hospedeiro A nesse primeiro cenário. O gráfico da es-\nquerda mostra a vazão por conexão (número de bytes por segundo no destinatário) como uma função da taxa de \nenvio da conexão. Para uma taxa de transmissão entre 0 e R/2, a vazão no destinatário é igual à velocidade de en-\nvio do remetente — tudo o que este envia é recebido no destinatário com um atraso finito. Quando a velocidade \nde envio estiver acima de R/2, contudo, a vazão será somente R/2. Esse limite superior da vazão é conse­\nquência do \ncompartilhamento da capacidade do enlace entre duas conexões. O enlace simplesmente não consegue entregar \nos pacotes a um destinatário com uma taxa em estado constante que exceda R/2. Não importa quão altas sejam as \ntaxas de envio ajustadas nos hospedeiros A e B, eles jamais alcançarão uma vazão maior do que R/2.\nAlcançar uma vazão de R/2 por conexão pode até parecer uma coisa boa, pois o enlace está sendo integral-\nmente utilizado para entregar pacotes no destinatário. No entanto, o gráfico do lado direito da Figura 3.44 mostra \nas consequências de operar próximo à capacidade máxima do enlace. À medida que a taxa de envio se aproxima \nde R/2 (partindo da esquerda), o atraso médio fica cada vez maior. Quando a taxa de envio ultrapassa R/2, o nú-\nmero médio de pacotes na fila no roteador é ilimitado e o atraso médio entre a fonte e o destino se torna infinito \n(admitindo que as conexões operem a essas velocidades de transmissão durante um período de tempo infinito e \nque a capacidade de armazenamento também seja infinita). Assim, embora operar a uma vazão agregada próxi-\nma a R possa ser ideal do ponto de vista da vazão, está bem longe de ser ideal do ponto de vista do atraso. Mesmo \nnesse cenário (extremamente) idealizado, já descobrimos um custo da rede congestionada — há grandes atrasos de \nfila quando a taxa de chegada de pacotes se aproxima da capacidade do enlace.\nFigura 3.43  \u0007\nCenário de congestionamento 1: duas conexões compartilhando um único \nroteador com número infinito de buffers\nHospedeiro B\nRoteador com capacidade de\narmazenamento inﬁnita\nλin: dados originais\nHospedeiro A\nHospedeiro D\nHospedeiro C\nλout \n   Redes de computadores e a Internet\n192\nCenário 2: dois remetentes, um roteador com buffers finitos\nVamos agora modificar um pouco o cenário 1 dos dois modos seguintes (veja a Figura 3.45). Primeiro, ad-\nmitamos que a capacidade de armazenamento do roteador seja finita. Em uma situação real, essa suposição teria \ncomo consequência o descarte de pacotes que chegam a um buffer que já está cheio. Segundo, admitamos que \ncada conexão seja confiável. Se um pacote contendo um segmento de camada de transporte for descartado no ro-\nteador, o remetente por fim o retransmitirá. Como os pacotes podem ser retransmitidos, agora temos de ser mais \ncuidadosos com o uso da expressão “taxa de envio”\n. Especificamente, vamos de novo designar a taxa com que a \naplicação envia dados originais para dentro do socket como in bytes/s. A taxa com que a camada de transporte \nenvia segmentos (contendo dados originais e dados retransmitidos) para dentro da rede será denominada 'in \nbytes/s. Essa taxa ( 'in) às vezes é denominada carga oferecida à rede.\nO desempenho obtido no cenário 2 agora dependerá muito de como a retransmissão é realizada. Primeiro, \nconsidere o caso não realista em que o hospedeiro A consiga, de algum modo (fazendo mágica!), determinar se \num buffer do roteador está livre no roteador e, portanto, envia um pacote apenas quando um buffer estiver livre. \nNesse caso, não ocorreria nenhuma perda, in seria igual a 'in e a vazão da conexão seria igual a in. Esse caso é \nmostrado pela curva superior da Figura 3.46(a). Do ponto de vista da vazão, o desempenho é ideal — tudo o que \nFigura 3.44  \u0007\nCenário de congestionamento 1: vazão e atraso em função da taxa de envio do \nhospedeiro\nR/2\nR/2\nAtraso\nR/2\nλin\nλin\nλout\na.\nb.\nFigura 3.45  \u0007\nCenário 2: dois hospedeiros (com retransmissões) e um roteador com buffers \nfinitos\nBuffers de enlace de saída\nﬁnitos compartilhados\nHospedeiro B\nHospedeiro A\nHospedeiro D\nHospedeiro C\nλout \nλin: dados originais\nλ’in: dados originais mais\ndados retransmitidos\nCAMADA  de transporte  193 \né enviado é recebido. Note que, nesse cenário, a taxa média de envio do hospedeiro não pode ultrapassar R/2, já \nque admitimos que nunca ocorre perda de pacote.\nConsidere, em seguida, o caso um pouco mais realista em que o remetente retransmite apenas quando sabe, \ncom certeza, que o pacote foi perdido. (De novo, essa suposição é um pouco forçada. Contudo, é possível ao hospe-\ndeiro remetente ajustar seu temporizador de retransmissão para uma duração longa o suficiente para ter razoável \ncerteza de que um pacote que não foi reconhecido foi perdido.) Nesse caso, o desempenho pode ser parecido com \no mostrado na Figura 3.46(b). Para avaliar o que está acontecendo aqui, considere o caso em que a carga ofereci-\nda, 'in (a taxa de transmissão dos dados originais mais as retransmissões), é igual a R/2. De acordo com a Figura \n3.46(b), nesse valor de carga oferecida, a velocidade com a qual os dados são entregues à aplicação do destinatário \né R/3. Assim, de 0,5R unidade de dados transmitida, 0,333R byte/s (em média) são dados originais e 0,166R byte/s \n(em média) são dados retransmitidos. Observamos aqui outro custo de uma rede congestionada — o remetente deve \nrealizar retransmissões para compensar os pacotes descartados (perdidos) pelo estouro do buffer.\nFinalmente, vamos considerar o caso em que a temporização do remetente se esgote prematuramente e ele \nretransmita um pacote que ficou atrasado na fila, mas que ainda não está perdido. Aqui, tanto o pacote de da-\ndos original quanto a retransmissão podem alcançar o destinatário. É claro que o destinatário precisa apenas de \numa cópia desse pacote e descartará a retransmissão. Nesse caso, o trabalho realizado pelo roteador ao repassar \na cópia retransmitida do pacote original é desperdiçado, pois o destinatário já terá recebido a cópia original do \npacote. Em vez disso, seria melhor o roteador usar a capacidade de transmissão do enlace para enviar um pacote \ndiferente. Eis aqui mais um custo da rede congestionada — retransmissões desnecessárias feitas pelo remetente em \nface de grandes atrasos podem fazer que um roteador use sua largura de banda de enlace para repassar cópias des-\nnecessárias de um pacote. A Figura 3.4.6(c) mostra a vazão versus a carga oferecida admitindo-se que cada pacote \nseja enviado (em média) duas vezes pelo roteador. Visto que cada pacote é enviado duas vezes, a vazão terá um \nvalor assintótico de R/4 à medida que a carga oferecida se aproximar de R/2.\nCenário 3: quatro remetentes, roteadores com buffers finitos e trajetos com múltiplos \nroteadores\nEm nosso cenário final de congestionamento, quatro hospedeiros transmitem pacotes sobre trajetos sobre-\npostos que apresentam dois saltos, como ilustrado na Figura 3.47. Novamente admitamos que cada hospedeiro \nuse um mecanismo de temporização/retransmissão para executar um serviço de transferência confiável de dados, \nque todos os hospedeiros tenham o mesmo valor de in e que todos os enlaces dos roteadores tenham capacidade \nde R bytes/s.\nVamos considerar a conexão do hospedeiro A ao hospedeiro C que passa pelos roteadores R1 e R2. A conexão \nA–C compartilha o roteador R1 com a conexão D–B e o roteador 2 com a conexão B–D. Para valores extremamente \npequenos de \nin, esgotamentos de buffers são raros (como acontecia nos cenários de congestionamento 1 e 2) e \nFigura 3.46  Desempenho no cenário 2 com buffers finitos\nR/2\nR/2\nR/2\nλout\na.\nb.\nR/2\nλout\nR/3\nR/2\nR/2\nλout\nR/4\nc.\nλ'in\nλ'in\nλ'in\n   Redes de computadores e a Internet\n194\na vazão é quase igual à carga oferecida. Para valores de \nin um pouco maiores, a vazão correspondente é também \nmaior, pois mais dados originais estão sendo transmitidos para a rede e entregues no destino, e os esgotamentos \nainda são raros. Assim, para valores pequenos de \nin, um aumento em \nin resulta em um aumento em \nout.\nComo já analisamos o caso de tráfego extremamente baixo, vamos examinar aquele em que \nin (e, portan-\nto, 'in) é extremamente alto. Considere o roteador R2. O tráfego A–C que chega ao roteador R2 (após ter sido \nrepassado de R1) pode ter uma taxa de chegada em R2 de, no máximo, R, que é a capacidade do enlace de R1 a \nR2, não importando qual seja o valor de \nin. Se 'in for extremamente alto para todas as conexões (incluindo a \nconexão B–D), então a taxa de chegada do tráfego B–D em R2 poderá ser muito maior do que a taxa do tráfego \nA–C. Como os tráfegos A–C e B–D têm de competir no roteador R2 pelo espaço limitado de buffer, a quantidade \nde tráfego A–C que consegue passar por R2 (isto é, que não se perde pelo congestionamento de buffer) diminui \ncada vez mais à medida que a carga oferecida de B–D vai ficando maior. No limite, quando a carga oferecida se \naproxima do infinito, um buffer vazio em R2 é logo preenchido por um pacote B–D e a vazão da conexão A–C \nem R2 cai a zero. Isso, por sua vez, implica que a vazão fim a fim de A–C vai a zero no limite de tráfego pesado. \nEssas considerações dão origem ao comportamento da carga oferecida versus a vazão mostrada na Figura 3.48.\nA razão para o decréscimo final da vazão com o crescimento da carga oferecida é evidente quando consi-\nderamos a quantidade de trabalho desperdiçado realizado pela rede. No cenário de alto tráfego que acabamos de \ndescrever, sempre que um pacote é descartado em um segundo roteador, o trabalho realizado pelo primeiro para \nenviar o pacote ao segundo acaba sendo “desperdiçado”\n. A rede teria se saído igualmente bem (melhor dizendo, \nigualmente mal) se o primeiro roteador tivesse apenas descartado aquele pacote e ficado inativo. Sendo mais \nobjetivos, a capacidade de transmissão utilizada no primeiro roteador para enviar o pacote ao segundo teria sido \nmaximizada para transmitir um pacote diferente. (Por exemplo, ao selecionar um pacote para transmissão, seria \nmelhor que um roteador desse prioridade a pacotes que já atravessaram alguns roteadores anteriores.) Portanto, \nFigura 3.47  \u0007\nQuatro remetentes, roteadores com buffers finitos e trajetos com vários \nsaltos\nHospedeiro B\nHospedeiro A\nR1\nR4\nR2\nR3\nHospedeiro C\nHospedeiro D\nBuffers de enlace de saída\nﬁnitos compartilhados\nλin: dados originais\nλ’in : dados originais\nmais dados retransmitidos\nλout\nCAMADA  de transporte  195 \nvemos aqui mais um custo, o do descarte de pacotes devido ao congestionamento — quando um pacote é descartado \nao longo de um caminho, a capacidade de transmissão que foi usada em cada um dos enlaces anteriores para repas-\nsar o pacote até o ponto em que foi descartado acaba sendo desperdiçada.\nFigura 3.48  \u0007\nDesempenho obtido no cenário 3, com buffers finitos e trajetos com múltiplos \nroteadores\nR/2\nλout\nλ'in\n3.6.2  Mecanismos de controle de congestionamento\nNa Seção 3.7, examinaremos em detalhes os mecanismos específicos do TCP para o controle de congestiona-\nmento. Aqui, identificaremos os dois procedimentos mais comuns adotados, na prática, para esse controle. Além \ndisso, examinaremos arquiteturas específicas de rede e protocolos de controle que incorporam tais procedimentos.\nNo nível mais amplo, podemos distinguir mecanismos de controle de congestionamento conforme a cama-\nda de rede ofereça ou não assistência explícita à camada de transporte com a finalidade de controle de conges-\ntionamento:\n• Controle de congestionamento fim a fim. Nesse método, a camada de rede não fornece nenhum suporte \nexplícito à camada de transporte com a finalidade de controle de congestionamento. Até mesmo a pre-\nsença de congestionamento na rede deve ser intuída pelos sistemas finais com base apenas na observação \ndo comportamento da rede (por exemplo, perda de pacotes e atraso). Veremos na Seção 3.7 que o TCP \ndeve necessariamente adotar esse método fim a fim para o controle de congestionamento, uma vez que \na camada IP não fornece realimentação de informações aos sistemas finais quanto ao congestionamento \nda rede. A perda de segmentos TCP (apontada por uma temporização ou por três reconhecimentos du-\nplicados) é tomada como indicação de congestionamento de rede, e o TCP reduz o tamanho da janela de \nacordo com isso. Veremos também que as novas propostas para o TCP usam valores de atraso de viagem \nde ida e volta crescentes como indicadores de aumento do congestionamento da rede.\n• Controle de congestionamento assistido pela rede. Com esse método, os componentes da camada de \nrede (isto é, roteadores) fornecem retroalimentação específica de informações ao remetente a respeito \ndo estado de congestionamento na rede. Essa retroalimentação pode ser tão simples como um único \nbit indicando o congestionamento em um enlace. Adotada nas primeiras arquiteturas de rede IBM \nSNA [Schwartz, 1982] e DEC DECnet [Jain, 1989; Ramakrishnan, 1990], essa abordagem foi proposta \nrecentemente para redes TCP/IP [Floyd TCP, 1994; RFC 3168]; ela é usada também no controle de \ncongestionamento em ATM com serviço de transmissão ABR (Available Bit Rate), como discutido a \nseguir. A retroalimentação mais sofisticada de rede também é possível. Por exemplo, uma forma de \ncontrole de congestionamento ATM ABR que estudaremos mais adiante permite que um roteador \ninforme explicitamente ao remetente a velocidade de transmissão que ele (o roteador) pode suportar \nem um enlace de saída. O protocolo XCP [Katabi, 2002] provê um retorno (feedback) calculado pelo \n   Redes de computadores e a Internet\n196\nroteador para cada origem, transmitido no cabeçalho do pacote, referente ao modo que essa origem \ndeve aumentar ou diminuir sua taxa de transmissão.\nPara controle de congestionamento assistido pela rede, a informação sobre congestionamento é em geral \nrealimentada da rede para o remetente por um de dois modos, como ilustra a Figura 3.49. Retroalimentação \ndireta pode ser enviada de um roteador de rede ao remetente. Esse modo de notificação em geral toma a forma \nde um pacote de congestionamento (choke packet) (que, em essência, diz: “Estou congestionado!”). O segundo \nmodo de notificação ocorre quando um roteador marca/atualiza um campo em um pacote que está fluindo do \nremetente ao destinatário para indicar congestionamento. Ao receber um pacote marcado, o destinatário informa \nao remetente a indicação de congestionamento. Note que esse último modo de notificação leva, no mínimo, o \ntempo total de uma viagem de ida e volta.\n3.6.3  \u0007\nExemplo de controle de congestionamento assistido pela rede: \ncontrole de congestionamento ATM ABR\nConcluímos esta seção com um breve estudo de caso do algoritmo de controle de congestionamento ATM \nABR — um protocolo que usa uma técnica de controle assistido pela rede. Salientamos que nossa meta aqui não \né, de modo algum, descrever em detalhes aspectos da arquitetura ATM, mas mostrar um protocolo que adota \numa abordagem de controle de congestionamento notavelmente diferente da adotada pelo protocolo TCP da \nInternet. Na verdade, apresentamos a seguir apenas os poucos aspectos da arquitetura ATM necessários para \nentender o controle de congestionamento ABR.\nNa essência, o ATM adota uma abordagem orientada para circuito virtual (CV) da comutação de pacotes. \nLembre, da nossa discussão no Capítulo 1, que isso significa que cada comutador no caminho entre a origem e o \ndestino manterá estado sobre o CV entre a origem e o destino. Esse estado por CV permite que um comutador \nmonitore o comportamento de remetentes individuais (por exemplo, monitorando sua taxa média de transmis-\nsão) e realize ações específicas de controle (tais como sinalizar de modo explícito ao remetente para que ele redu-\nza sua taxa quando o comutador fica congestionado). Esse estado por CV em comutadores de rede torna o ATM \nidealmente adequado para realizar controle de congestionamento assistido pela rede.\nO ABR foi projetado como um serviço de transferência de dados elástico que faz lembrar, de certo modo, o TCP\n. \nQuando a rede está vazia, o serviço ABR deve ser capaz de aproveitar a vantagem da largura de banda disponível. \nQuando está congestionada, deve limitar sua taxa de transmissão a algum valor mínimo predeterminado. Um tutorial \ndetalhado sobre controle de congestionamento e gerenciamento de tráfego ATM ABR é fornecido por [Jain, 1996].\nFigura 3.49  \u0007\nDois caminhos de realimentação para informação sobre congestionamento \nindicado pela rede\nHospedeiro A\nRealimentação pela rede\npor meio do destinatário\nRealimentação diretamente\npela rede\nHospedeiro B\nCAMADA  de transporte  197 \nA Figura 3.50 mostra a estrutura do controle de congestionamento para ATM ABR. Nessa discussão, ado-\ntaremos a terminologia ATM (por exemplo, usaremos o termo “comutador” em vez de “roteador” e o termo \n“célula” em vez de “pacote”). Com o serviço ATM ABR, as células de dados são transmitidas de uma origem a um \ndestino por meio de uma série de comutadores intermediários. Intercaladas às células de dados estão as células \nde gerenciamento de recursos, ou células RM (resource-management); elas podem ser usadas para portar infor-\nmações relacionadas ao congestionamento entre hospedeiros e comutadores. Quando uma célula RM chega a um \ndestinatário, ela será “virada” e enviada de volta ao remetente (possivelmente após o destinatário ter modificado \no conteúdo dela). Também é possível que um comutador gere, ele mesmo, uma célula RM e a envie direto a uma \norigem. Assim, células RM podem ser usadas para fornecer realimentação diretamente da rede e também reali-\nmentação da rede por intermédio do destinatário, como mostra a Figura 3.50.\nO controle de congestionamento ATM ABR é um método baseado em taxa. O remetente estima explici-\ntamente uma taxa máxima na qual pode enviar e se autorregula de acordo com ela. O serviço ABR provê três \nmecanismos para sinalizar informações relacionadas a congestionamento dos comutadores ao destinatário:\n• Bit EFCI. Cada célula de dados contém um bit EFCI de indicação explícita de congestionamento à frente \n(explicit forward congestion indication). Um comutador de rede congestionado pode modificar o bit EFCI \ndentro de uma célula para 1, a fim de sinalizar congestionamento ao hospedeiro destinatário, que deve ve-\nrificar o bit EFCI em todas as células de dados recebidas. Quando uma célula RM chega ao destinatário, se \na célula de dados recebida mais recentemente tiver o bit EFCI com valor 1, o destinatário modifica o bit de \nindicação de congestionamento (o bit CI) da célula RM para 1 e envia a célula RM de volta ao remetente. \nUsando o bit EFCI em células de dados e o bit CI em células RM, um remetente pode ser notificado sobre \ncongestionamento em um comutador da rede.\n• Bits CI e NI. Como já foi observado, células RM remetente/destinatário estão intercaladas com células \nde dados. A taxa de intercalação de células RM é um parâmetro ajustável, sendo uma célula RM a cada \n32 células de dados o valor default. Essas células RM têm um bit de indicação de congestionamento (CI) \ne um bit NI (no increase — não aumentar) que podem ser ajustados por um comutador de rede con-\ngestionado. Especificamente, um comutador pode modificar para 1 o bit NI de uma célula RM que está \npassando quando a condição de congestionamento é leve e pode modificar o bit CI para 1 em severas \ncondições de congestionamento. Quando um hospedeiro destinatário recebe uma célula RM, ele a envia \nde volta ao remetente com seus bits CI e NI inalterados (exceto que o CI pode ser ajustado para 1 pelo \ndestinatário como resultado do mecanismo EFCI descrito antes).\n• Ajuste de ER. Cada célula RM contém também um campo ER (explicit rate — taxa explícita) de dois bytes. \nUm comutador congestionado pode reduzir o valor contido no campo ER de uma célula RM que está pas-\nsando. Desse modo, o campo ER será ajustado para a taxa mínima suportável por todos os comutadores \nno trajeto origem-destino.\nFigura 3.50  Estrutura de controle de congestionamento para o serviço ATM ABR\nOrigem\nDestino\nComutador\nComutador\nLegenda:\nCélulas RM\nCélulas de dados\n   Redes de computadores e a Internet\n198\nUma origem ATM ABR ajusta a taxa na qual pode enviar células como uma função dos valores de CI, NI e \nER de uma célula RM devolvida. As regras para fazer esse ajuste são bastante complicadas e um tanto tediosas. O \nleitor poderá consultar Jain [1996] se quiser saber mais detalhes.\n3.7  Controle de congestionamento no TCP\nNesta seção, voltamos ao estudo do TCP. Como aprendemos na Seção 3.5, o TCP provê um serviço de trans-\nferência confiável entre dois processos que rodam em hospedeiros diferentes. Outro componente de extrema \nimportância do TCP é seu mecanismo de controle de congestionamento. Como indicamos na seção anterior, o \nTCP deve usar controle de congestionamento fim a fim em vez de controle assistido pela rede, já que a camada IP \nnão fornece aos sistemas finais realimentação explícita relativa ao congestionamento da rede.\nA abordagem adotada pelo TCP é obrigar cada remetente a limitar a taxa à qual enviam tráfego para sua co-\nnexão como uma função do congestionamento de rede percebido. Se um remetente TCP perceber que há pouco \ncongestionamento no caminho entre ele e o destinatário, aumentará sua taxa de envio; se perceber que há conges-\ntionamento, reduzirá sua taxa de envio. Mas essa abordagem levanta três questões. Primeiro, como um remetente \nTCP limita a taxa pela qual envia tráfego para sua conexão? Segundo, como um remetente TCP percebe que há \ncongestionamento entre ele e o destinatário? E terceiro, que algoritmo o remetente deve utilizar para modificar \nsua taxa de envio como uma função do congestionamento fim a fim percebido?\nPara começar, vamos examinar como um remetente TCP limita a taxa de envio à qual envia tráfego para sua \nconexão. Na Seção 3.5, vimos que cada lado de uma conexão TCP consiste em um buffer de recepção, um buffer \nde envio e diversas variáveis (LastByteRead, rwnd e assim por diante). O mecanismo de controle de con-\ngestionamento que opera no remetente monitora uma variável adicional, a janela de congestionamento. Esta, \ndenominada cwnd, impõe uma limitação à taxa à qual um remetente TCP pode enviar tráfego para dentro da \nrede. Especificamente, a quantidade de dados não reconhecidos em um hospedeiro não pode exceder o mínimo \nde cwnd e rwnd, ou seja:\nLastByteSent – LastByteAcked ≤ min{cwnd, rwnd}\nPara concentrar a discussão no controle de congestionamento (ao contrário do controle de fluxo), daqui em \ndiante vamos admitir que o buffer de recepção TCP seja tão grande que a limitação da janela de recepção pode \nser desprezada; assim, a quantidade de dados não reconhecidos no remetente estará limitada apenas por cwnd. \nVamos admitir também que o remetente sempre tenha dados para enviar, isto é, que todos os segmentos dentro \nda janela de congestionamento sejam enviados.\nA restrição citada limita a quantidade de dados não reconhecidos no remetente e, por conseguinte, limita \nindiretamente a taxa de envio do remetente. Para entender melhor, considere uma conexão na qual perdas e \natrasos de transmissão de pacotes sejam desprezíveis. Então, em linhas gerais, no início de cada RTT a limitação \npermite que o remetente envie cwnd bytes de dados para a conexão; ao final do RTT, o remetente recebe reconhe-\ncimentos para os dados. Assim, a taxa de envio do remetente é aproximadamente cwnd/RTT bytes por segundo. \nPortanto, ajustando o valor de cwnd, o remetente pode ajustar a taxa à qual envia dados para sua conexão.\nEm seguida, vamos considerar como um remetente TCP percebe que há congestionamento no caminho \nentre ele e o destino. Definimos “evento de perda” em um remetente TCP como a ocorrência de um esgotamen-\nto de temporização (timeout) ou do recebimento de três ACKs duplicados do destinatário (lembre-se da nossa \ndiscussão, na Seção 3.5.4, do evento de temporização apresentado na Figura 3.33 e da subsequente modificação \npara incluir transmissão rápida quando do recebimento de três ACKs duplicados). Quando há congestionamento \nexcessivo, então um (ou mais) buffers de roteadores ao longo do caminho transborda, fazendo que um datagrama \n(contendo um segmento TCP) seja descartado. O datagrama descartado, por sua vez, resulta em um evento de \nperda no remetente — ou um esgotamento de temporização ou o recebimento de três ACKs duplicados — que é \ntomado por ele como uma indicação de congestionamento no caminho remetente-destinatário.\nCAMADA  de transporte  199 \nJá consideramos como é detectado o congestionamento; agora vamos analisar o caso mais otimista de uma \nrede livre de congestionamento, isto é, quando não ocorre um evento de perda. Nesse caso, o TCP remetente re-\nceberá reconhecimentos para segmentos não reconhecidos antes. Como veremos, o TCP considerará a chegada \ndesses reconhecimentos como uma indicação de que tudo está bem — os segmentos que estão sendo transmiti-\ndos para a rede estão sendo entregues com sucesso no destinatário — e usará reconhecimentos para aumentar o \ntamanho de sua janela de congestionamento (e, por conseguinte, sua taxa de transmissão). Note que, se os reco-\nnhecimentos chegarem ao remetente a uma taxa relativamente baixa (por exemplo, se o atraso no caminho fim \na fim for alto ou se nele houver um enlace de baixa largura de banda), então a janela de congestionamento será \naumentada a uma taxa um tanto baixa. Por outro lado, se os reconhecimentos chegarem a uma taxa alta, então a \njanela de congestionamento será aumentada mais depressa. Como o TCP utiliza reconhecimentos para acionar \n(ou regular) o aumento de tamanho de sua janela de congestionamento, diz-se que o TCP é autorregulado.\nDado o mecanismo de ajustar o valor de cwnd para controlar a taxa de envio, permanece a importante \npergunta: Como um remetente TCP deve determinar a taxa a que deve enviar? Se os remetentes TCP enviam \ncoletivamente muito rápido, eles podem congestionar a rede, levando ao tipo de congestionamento que vimos \nna Figura 3.48. De fato, a versão de TCP que vamos estudar de modo breve foi desenvolvida em resposta aos \ncongestionamentos da Internet observados em versões anteriores do TCP [Jacobson, 1988]. Entretanto, se os \nremetentes forem muito cautelosos e enviarem lentamente, eles podem subutilizar a largura de banda na rede; ou \nseja, os remetentes TCP podem enviar a uma taxa mais alta sem congestionar a rede. Então, como os remetentes \nTCP determinam suas taxas de envio de um modo que não congestionem mas, ao mesmo tempo, façam uso de \ntoda a largura de banda? Os remetentes TCP são claramente coordenados, ou existe uma abordagem distribuída \nna qual eles podem ajustar suas taxas de envio baseando-se apenas nas informações locais? O TCP responde a \nessas perguntas utilizando os seguintes princípios:\n• Um segmento perdido implica congestionamento, portanto, a taxa do remetente TCP deve diminuir quando \num segmento é perdido. Lembre-se da nossa discussão na Seção 3.5.4, de que um evento de esgotamento \ndo temporizador ou o recebimento de quatro reconhecimentos para dado segmento (um ACK original \ne, depois, três ACKs duplicados) é interpretado como uma indicação de “evento de perda” absoluto do \nsegmento subsequente ao ACK quadruplicado, acionando uma retransmissão do segmento perdido. De \num ponto de vista do controle, a pergunta é como o remetente TCP deve diminuir sua janela de conges-\ntionamento e, portanto, sua taxa de envio, em resposta ao suposto evento de perda.\n• Um segmento reconhecido indica que a rede está enviando os segmentos do remetente ao destinatário e, por \nisso, a taxa do remetente pode aumentar quando um ACK chegar para um segmento não reconhecido antes. \nA chegada de reconhecimentos é tida como uma indicação absoluta de que tudo está bem — os segmen-\ntos estão sendo enviados com sucesso do remetente ao destinatário, fazendo, assim, que a rede não fique \ncongestionada. Dessa forma, o tamanho da janela de congestionamento pode ser elevado.\n• Busca por largura de banda. Dados os ACKs que indicam um percurso de origem a destino sem conges-\ntionamento, e eventos de perda que indicam um percurso congestionado, a estratégia do TCP de ajustar \nsua taxa de transmissão é aumentar a taxa em resposta aos ACKs que chegam até que ocorra um evento \nde perda, momento em que a taxa de transmissão diminui. Desse modo, o remetente TCP aumenta sua \ntaxa de transmissão para buscar a taxa pela qual o congestionamento se inicia, recua dela e de novo faz a \nbusca para ver se a taxa de início do congestionamento foi alterada. O comportamento do remetente TCP \né análogo a uma criança que pede (e ganha) cada vez mais doces até por fim receber um “Não!”\n, recuar, \nmas começar a pedir novamente pouco tempo depois. Observe que não há nenhuma sinalização explícita \nde congestionamento pela rede — os ACKs e eventos de perda servem como sinais implícitos — e que cada \nremetente TCP atua em informações locais em momentos diferentes de outros.\nApós essa visão geral sobre controle de congestionamento no TCP, agora podemos considerar os detalhes \ndo renomado algoritmo de controle de congestionamento TCP, sendo primeiro descrito em Jacobson [1988] \ne padronizado em [RFC 5681]. O algoritmo possui três componentes principais: (1) partida lenta, (2) contenção \n   Redes de computadores e a Internet\n200\nde congestionamento e (3) recuperação rápida. A partida lenta e a contenção de congestionamento são compo-\nnentes obrigatórios do TCP, diferenciando em como eles aumentam o tamanho do cwnd em resposta a ACKs \nrecebidos. Abordaremos em poucas palavras que a partida lenta aumenta o tamanho do cwnd de forma mais \nrápida (apesar do nome!) do que a contenção de congestionamento. A recuperação rápida é recomendada, mas \nnão exigida, para remetentes TCP.\nPartida lenta\nQuando uma conexão TCP começa, o valor de cwnd costuma ser inicializado em 1 MSS [RFC 3390], \nresultando em uma taxa inicial de envio de aproximadamente MSS/RTT. Como exemplo, se MSS = 500 bytes \ne RTT = 200 ms, então a taxa de envio inicial resultante é cerca de 20 kbits/s apenas. Como a largura de banda \ndisponível para a conexão pode ser muito maior do que MSS/RTT, o remetente TCP gostaria de aumentar a \nquantidade de largura de banda rapidamente. Dessa forma, no estado de partida lenta, o valor de cwnd começa \nem 1 MSS e aumenta 1  MSS toda vez que um segmento transmitido é reconhecido. No exemplo da Figura 3.51, o \nTCP envia o primeiro segmento para a rede e aguarda um reconhecimento. Quando este chega, o remetente TCP \naumenta a janela de congestionamento em 1 MSS e envia dois segmentos de tamanho máximo. Esses segmentos \nsão reconhecidos, e o remetente aumenta a janela de congestionamento em 1 MSS para cada reconhecimento de \nsegmento, fornecendo uma janela de congestionamento de 4 MSS e assim por diante. Esse processo resulta em \numa multiplicação da taxa de envio a cada RTT. Assim, a taxa de envio TCP se inicia lenta, mas cresce exponen-\ncialmente durante a fase de partida lenta.\nMas em que momento esse crescimento exponencial termina? A partida lenta apresenta diversas res-\npostas para essa pergunta. Primeiro, se houver um evento de perda (ou seja, um congestionamento) indi-\ncado por um esgotamento de temporização, o remetente TCP estabelece o valor de cwnd em 1 e inicia o \nprocesso de partida lenta novamente. Ele também estabelece o valor de uma segunda variável de estado, \nssthresh (abreviação de “slow start threshold” [limiar de partida lenta]), em cwnd/2 — metade do valor \nFigura 3.51  Partida lenta TCP\nHospedeiro A\nHospedeiro B\num segmento\ndois segmentos\nquatro segmentos\nRTT\nTempo\nTempo\nCAMADA  de transporte  201 \nda janela de congestionamento quando este foi detectado. O segundo modo pelo qual a partida lenta pode \nterminar é ligado diretamente ao valor de ssthresh. Visto que ssthresh é metade do valor de cwnd \nquando o congestionamento foi detectado pela última vez, pode ser uma atitude precipitada continuar du-\nplicando cwnd ao atingir ou ultrapassar o valor de ssthresh. Assim, quando o valor de cwnd se igualar \nao de ssthresh, a partida lenta termina e o TCP é alterado para o modo de prevenção de congestiona-\nmento. Como veremos, o TCP aumenta cwnd com mais cautela quando está no modo de prevenção de \ncongestionamento. O último modo pelo qual a partida lenta pode terminar é se três ACKs duplicados forem \ndetectados, caso no qual o TCP apresenta uma retransmissão rápida (veja Seção 3.5.4) e entra no estado de \nrecuperação rápida, como discutido a seguir. O comportamento do TCP na partida lenta está resumido na \ndescrição FSM do controle de congestionamento no TCP na Figura 3.52. O algoritmo de partida lenta foi \nde início proposto por Jacobson [1998]; uma abordagem semelhante à partida lenta também foi proposta de \nmaneira independente em Jain [1986].\nPrevenção de congestionamento\nAo entrar no estado de prevenção de congestionamento, o valor de cwnd é cerca de metade de seu valor \nquando o congestionamento foi encontrado pela última vez — o congestionamento poderia estar por perto! \nDesta forma, em vez de duplicar o valor de cwnd a cada RTT, o TCP adota um método mais conservador e au-\nmenta o valor de cwnd por meio de um único MSS a cada RTT [RFC 5681]. Isso pode ser realizado de diversas \nformas. Uma abordagem comum é o remetente aumentar cwnd por MSS bytes (MSS/cwnd)no momento em \nque um novo reconhecimento chegar. Por exemplo, se o MSS possui 1.460 bytes e cwnd, 14.600 bytes, então \nFigura 3.52  Descrição FSM do controle de congestionamento no TCP\nPartida\nlenta\nACK duplicado\ndupACKcount++\nACK duplicado\ndupACKcount++\nEsgotamento\nde temporização\nssthresh=cwnd/2\ncwnd=1 MSS\ndupACKcount=0\ncwnd=1 MSS\nssthresh=64 KB\ndupACKcount=0\nEsgotamento\nde temporização\nssthresh=cwnd/2\ncwnd=1\ndupACKcount=0\nEsgotamento\nde temporização\nssthresh=cwnd/2\ncwnd=1 MSS\ndupACKcount=0\ncwnd ≥ssthresh\nPrevenção de\ncongestiona-\nmento\nRecuperação\nrápida\nNovo ACK\ncwnd=cwnd+MSS •(MSS/cwnd)\ndupACKcount=0\ntransmite novos segmentos como permitido\nNovo ACK\ncwnd=cwnd+MSS\ndupACKcount=0\ntransmite novos segmentos como permitido\nretransmite o segmento que falta\nretransmite o segmento que falta\ndupACKcount==3\nssthresh=cwnd/2\ncwnd=ssthresh+3•MSS\nretransmite o segmento que falta\nACK duplicado\ncwnd=cwnd+MSS\ntransmite novos segmentos como permitido\ndupACKcount==3\nssthresh=cwnd/2\ncwnd=ssthresh+3•MSS\nretransmite o segmento que falta\nretransmite o segmento que falta\nNovo ACK\ncwnd=ssthresh\ndupACKcount=0\nΛ\nΛ\n   Redes de computadores e a Internet\n202\n10 segmentos estão sendo enviados dentro de um RTT. Cada ACK que chega (considerando um ACK por \nsegmento) aumenta o tamanho da janela de congestionamento em 1/10 MSS e, assim, o valor da janela de con-\ngestionamento terá aumentado em 1 MSS após os ACKs quando todos os segmentos tiverem sido recebidos.\nMas em que momento o aumento linear da prevenção de congestionamento (de 1 MSS por RTT) deve \nterminar? O algoritmo de prevenção de congestionamento TCP se comporta da mesma forma quando ocor-\nre um esgotamento de temporização. Como no caso da partida lenta: o valor de cwnd é ajustado para 1 MSS, \ne o valor de ssthresh é atualizado para metade do valor de cwnd quando ocorreu o evento de perda. Lem-\nbre-se, entretanto, de que um evento de perda também pode ser acionado por um evento ACK duplicado \ntriplo. Neste caso, a rede continua a enviar segmentos do remetente ao destinatário (como indicado pelo \nrecebimento de ACKs duplicados). Portanto, o comportamento do TCP para esse tipo de evento de perda \ndeve ser menos drástico do que com uma perda de esgotamento de temporização: O TCP reduz o valor de \ncwnd para metade (adicionando em 3 MSS a mais para contabilizar os ACKs duplicados triplos recebidos) e \nregistra o valor de ssthresh como metade do de cwnd quando os ACKs duplicados triplos foram recebidos. \nEntão, entra-se no estado de recuperação rápida.\nDivisão do TCP: otimizando o desempenho de serviços da nuvem\nPara serviços da nuvem como busca, e-mail e re-\ndes sociais, deseja-se prover um alto nível de respon-\nsividade, de preferência dando aos usuários a ilusão de \nque os serviços estão rodando dentro de seus próprios \nsistemas finais (inclusive seus smartphones). Isso pode \nser um grande desafio, pois os usuários em geral es-\ntão localizados distantes dos centros de dados que são \nresponsáveis por servir o conteúdo dinâmico associado \naos serviços da nuvem. Na verdade, se o sistema final \nestiver longe de um centro de dados, então o RTT será \ngrande, potencialmente levando a um tempo de respos-\nta maior, devido à partida lenta do TCP\n.\nComo um estudo de caso, considere o atraso no \nrecebimento de uma resposta para uma consulta. Em \ngeral, o servidor requer três janelas TCP durante a par-\ntida lenta para entregar a resposta [Pathak, 2010]. As-\nsim, o tempo desde que um sistema final inicia uma \nconexão TCP até o momento em que ele recebe o últi-\nmo pacote da resposta é cerca de 4 ∙ RTT (um RTT para \nestabelecer a conexão TCP mais três RTTs para as três \njanelas de dados) mais o tempo de processamento no \ncentro de dados. Esses atrasos de RTT podem levar a \num atraso observável no retorno de resultados de bus-\nca para uma fração significativa de consultas. Além do \nmais, pode haver uma significativa perda de pacotes \nnas redes de acesso, ocasionando retransmissões do \nTCP e até mesmo atrasos maiores.\nUm modo de aliviar esse problema e melhorar o \ndesempenho percebido pelo usuário é (1) instalar ser-\nvidores de front-end mais perto dos usuários e (2) uti-\nlizar a divisão do TCP, quebrando a conexão TCP no \nservidor de front-end. Com a divisão do TCP\n, o cliente \nestabelece uma conexão TCP com o front-end nas \nproximidades, e o front-end mantém uma conexão \nTCP persistente com o centro de dados, com uma \njanela de congestionamento TCP muito grande [Tariq, \n2008; Pathak, 2010; Chen 2011]. Com essa técnica, \no tempo de resposta torna-se cerca de 4 ∙ RTTFE + \nRTTBE + tempo de processamento, onde RTTFE é o \ntempo de viagem de ida e volta entre cliente e ser-\nvidor de front-end, e RTTBE é o tempo de viagem de \nida e volta entre o servidor de front-end e o centro de \ndados (servidor de back-end). Se o servidor de fron-\nt-end estiver perto do cliente, o tempo de resposta \ntorna-se mais ou menos RTT mais tempo de proces-\nsamento, pois RTTFE é insignificativamente pequeno \ne RTTBE é mais ou menos RTT. Resumindo, a divisão \ndo TCP pode reduzir o atraso da rede de cerca de 4 \n∙ RTT para RTT, melhorando significativamente o de-\nsempenho percebido pelo usuário, em particular para \nusuários que estão longe do centro de dados mais \npróximo. A divisão do TCP também ajuda a reduzir \nos atrasos de retransmissão do TCP causados por \nperdas nas redes de acesso. Hoje, Google e Akamai \nutilizam bastante seus servidores CDN em redes de \nacesso (ver Seção 7.2) para realizar a divisão do TCP \npara os serviços de nuvem que eles suportam [Chen, \n2011].\nPrincípios na prática\nCAMADA  de transporte  203 \nRecuperação rápida\nNa recuperação rápida, o valor de cwnd é aumentado em 1 MSS para cada ACK duplicado recebido no seg-\nmento perdido que fez o TCP entrar no modo de recuperação rápida. Mais cedo ou mais tarde, quando um ACK \nchega ao segmento perdido, o TCP entra no modo de prevenção de congestionamento após reduzir cwnd. Se \nhouver um evento de esgotamento de temporização, a recuperação rápida é alterada para o modo de partida lenta \napós desempenhar as mesmas ações que a partida lenta e a prevenção de congestionamento: o valor de cwnd é \najustado para 1 MSS, e o valor de ssthresh, para metade do valor de cwnd no momento em que o evento de \nperda ocorreu.\nA recuperação rápida é recomendada, mas não exigida, para o protocolo TCP [RFC 5681]. É interessante \no fato de que uma antiga versão do TCP, conhecida como TCP Tahoe, reduzia incondicionalmente sua janela \nde congestionamento para 1 MSS e entrava na fase de partida lenta após um evento de perda de esgotamento do \ntemporizador ou de ACK duplicado triplo. A versão atual do TCP, a TCP Reno, incluiu a recuperação rápida.\nA Figura 3.53 ilustra a evolução da janela de congestionamento do TCP para as versões Reno e Tahoe. Nessa \nfigura, o limiar é, no início, igual a 8 MSS. Nas primeiras oito sessões de transmissão, as duas versões possuem \nações idênticas. A janela de congestionamento se eleva exponencialmente rápido durante a partida lenta e atin-\nge o limiar na quarta sessão de transmissão. A janela de congestionamento, então, se eleva de modo linear até \nque ocorra um evento ACK duplicado triplo, logo após a oitava sessão de transmissão. Observe que a janela de \ncongestionamento é 12 • MSS quando ocorre o evento de perda. O valor de ssthresh é, então, ajustado para \n0,5 • cwnd = 6 • MSS. No TCP Reno, a janela é ajustada para cwnd = 6 • MSS, e depois cresce linearmente. No \nTCP Tahoe, é ajustada para 1 MSS e cresce de modo exponencial até que alcance o valor de ssthresh, quando \ncomeça a crescer linearmente.\nA Figura 3.52 apresentou a descrição FSM completa dos algoritmos de controle de congestionamento — \npartida lenta, prevenção de congestionamento e recuperação rápida. A figura também indica onde pode ocorrer \ntransmissão de novos segmentos ou segmentos retransmitidos. Embora seja importante diferenciar controle/ \nretransmissão de erro TCP de controle de congestionamento no TCP, também é importante avaliar como esses \ndois aspectos do TCP estão inseparavelmente ligados.\nControle de congestionamento no TCP: retrospectiva\nApós nos aprofundarmos em detalhes sobre partida lenta, prevenção de congestionamento e recuperação \nrápida, vale a pena agora voltar e ver a floresta por entre as árvores. Desconsiderando o período inicial de partida \nlenta, quando uma conexão se inicia, e supondo que as perdas são indicadas por ACKs duplicados triplos e não \npor esgotamentos de temporização, o controle de congestionamento no TCP consiste em um aumento linear \nFigura 3.53  Evolução da janela de congestionamento do TCP (Tahoe e Reno)\n0\n1\n0\n2\n3\n4\n5\n6\n7\n8\nRodada de transmissão\nTCP Tahoe\nssthresh\nssthresh\nJanela de congestionamento\n(em segmentos)\n9\n10 11 12 13 14 15\n2\n4\n6\n8\n10\n12\n14\n16\nTCP Reno\nKR 03 53 eps\n   Redes de computadores e a Internet\n204\n(aditivo) em cwnd de 1 MMS por RTT e, então, uma redução à metade (diminuição multiplicativa) de cwnd em \num evento ACK duplicado triplo. Por esta razão, o controle de congestionamento no TCP é quase sempre deno-\nminado aumento aditivo, diminuição multiplicativa (AIMD — Additive-Increase, Multiplicative-Decrease). O \ncontrole de congestionamento AIMD faz surgir o comportamento semelhante a “dentes de serra”\n, mostrado na \nFigura 3.54, a qual também ilustra de forma interessante nossa intuição anterior sobre a “sondagem” do TCP por \nlargura de banda — o TCP aumenta linearmente o tamanho de sua janela de congestionamento (e, portanto, sua \ntaxa de transmissão) até que ocorra um evento ACK duplicado triplo. Então, ele reduz o tamanho de sua janela \npor um fator de dois, mas começa de novo a aumentá-la linearmente, buscando saber se há uma largura de banda \nadicional disponível.\nComo já foi observado, a maioria das implementações TCP, hoje, utiliza o algoritmo Reno [Padhye, 2001]. \nForam propostas muitas variações desse algoritmo [RFC 3782; RFC 2018]. O algoritmo Vegas [Brakmo, 1995; \nAhn, 1995] tenta evitar o congestionamento enquanto mantém uma boa vazão. A ideia básica de tal algoritmo \né (1) detectar congestionamento nos roteadores entre a origem e o destino antes que ocorra a perda de pacote e \n(2) diminuir linearmente a taxa ao ser detectada essa perda de pacote iminente, que pode ser prevista por meio \nda observação do RTT. Quanto maior for o RTT dos pacotes, maior será o congestionamento nos roteadores. O \nLinux suporta diversos algoritmos de controle de congestionamento (incluindo o TCP Reno e o TCP Vegas) e \npermite que um administrador de sistema configure qual versão do TCP será utilizada. A versão padrão do TCP \nno Linux versão 2.6.18 foi definida para CUBIC [Ha, 2008], uma versão do TCP desenvolvida para aplicações \nde alta largura de banda. Para obter um estudo recente das muitas versões do TCP, consulte Afanasyev [2010].\nO algoritmo AIMD do TCP foi desenvolvido com base em um grande trabalho de engenharia e experiên-\ncias com controle de congestionamento em redes operacionais. Dez anos após o desenvolvimento do TCP, a \nanálise teórica mostrou que o algoritmo de controle de congestionamento do TCP serve como um algoritmo de \notimização assíncrona distribuída, que resulta em vários aspectos importantes do desempenho do usuário e da \nrede sendo otimizados em simultâneo [Kelly, 1998]. Uma rica teoria de controle de congestionamento foi desen-\nvolvida desde então [Srikant, 2004].\nDescrição macroscópica da vazão do TCP\nDado o comportamento de dentes de serra do TCP, é natural considerar qual seria a vazão média (isto é, a taxa \nmédia) de uma conexão TCP de longa duração. Nessa análise, vamos ignorar as fases de partida lenta que ocorrem \napós eventos de esgotamento de temporização. (Elas em geral são muito curtas, visto que o remetente sai com rapi-\ndez exponencial.) Durante determinado intervalo de viagem de ida e volta, a taxa à qual o TCP envia dados é uma \nfunção da janela de congestionamento e do RTT corrente. Quando o tamanho da janela for w bytes, e o tempo de \nviagem de ida e volta for RTT segundos, a taxa de transmissão do TCP será mais ou menos w/RTT. Então, o TCP faz \nFigura 3.54  \u0007\nControle de congestionamento por aumento auditivo, diminuição multiplicativa\n24 K\n16 K\n8 K\nTempo\nJanela de congestionamento\nCAMADA  de transporte  205 \numa sondagem em busca de alguma largura de banda adicional aumentando w em 1 MSS a cada RTT até ocorrer \num evento de perda. Seja W o valor de w quando ocorre um evento de perda. Admitindo que RTT e W são mais ou \nmenos constantes no período da conexão, a taxa de transmissão fica na faixa de W/(2 • RTT) a W/RTT.\nEssas suposições levam a um modelo macroscópico muito simplificado para o comportamento do TCP em \nestado constante. A rede descarta um pacote da conexão quando a taxa aumenta para W/RTT; então a taxa é re-\nduzida à metade e, em seguida, aumenta em MSS/RTT a cada RTT até alcançar W/RTT novamente. Esse processo \nse repete de modo contínuo. Como a vazão do TCP (isto é, sua taxa) aumenta linearmente entre os dois valores \nextremos, temos:\nvazão média de uma conexão = 0,75\nW\nRTT\nUsando esse modelo muito idealizado para a dinâmica de regime permanente do TCP, podemos também \nderivar uma interessante expressão que relaciona a taxa de perda de uma conexão com sua largura de banda \ndisponível [Mahdavi, 1997]. Essa derivação está delineada nos exercícios de fixação. Um modelo mais sofisticado \nque demonstrou empiricamente estar de acordo com dados medidos é apresentado em Padhye [2000].\nTCP por caminhos com alta largura de banda\nÉ importante perceber que o controle de congestionamento no TCP evoluiu ao longo dos anos e, na ver-\ndade, continua a evoluir. Um resumo das variantes atuais do TCP e uma discussão sobre a evolução do TCP \npodem ser vistos em Floyd (2001), RFC 5681 e Afanasyev (2010). O que era bom para a Internet quando a maior \nparte das conexões TCP carregava tráfego SMTP, FTP e Telnet nem sempre é bom para a de hoje, dominada pelo \nHTTP, ou para uma Internet futura, com serviços que ainda nem sonhamos.\nA necessidade da evolução contínua do TCP pode ser ilustrada considerando as conexões TCP de alta \nvelocidade que são necessárias para aplicações de computação em grade e nuvem. Por exemplo, suponha uma \nconexão TCP com segmentos de 1.500 bytes e RTT de 100 ms, e suponha que queremos enviar dados por essa \nconexão a 10 Gbits/s. Seguindo o [RFC 3649] e utilizando a fórmula de vazão do TCP apresentada anteriormente, \nnotamos que, para alcançar uma vazão de 10 Gbits/s, o tamanho médio da janela de congestionamento precisaria \nser 83.333 segmentos. São muitos segmentos, e pensar que um deles poderia ser perdido em trânsito nos deixa \nbastante preocupados. O que aconteceria no caso de uma perda? Ou, em outras palavras, que fração dos segmen-\ntos transmitidos poderia ser perdida e ainda assim permitir que o algoritmo de controle de congestionamento no \nTCP delineado na Tabela 3.52 alcançasse a taxa desejada de 10 Gbits/s? Nos exercícios de fixação deste capítulo \nvocê acompanhará a dedução de uma fórmula que relaciona a vazão de uma conexão TCP em função da taxa de \nperda (L), do tempo de viagem de ida e volta (RTT) e do tamanho máximo de segmento (MSS):\nvazão média de uma conexão = 1,22\nMSS\nRTT √L\nUsando essa fórmula, podemos ver que, para alcançar uma vazão de 10 Gbits/s, o algoritmo de controle de \ncongestionamento no TCP de hoje pode tolerar uma probabilidade de perda de segmentos de apenas 2 • 10–10 \n(equivalente a um evento de perda a cada 5 bilhões de segmentos) — uma taxa muito baixa. Essa observação le-\nvou muitos pesquisadores a investigar novas versões do TCP projetadas para esses ambientes de alta velocidade; \nJin (2004); RFC 3649; Kelly (2003); Ha (2008) apresentam discussões sobre esses esforços.\n3.7.1  Equidade\nConsidere K conexões TCP, cada uma com um caminho fim a fim diferente, mas todas passando pelo \ngargalo em um enlace com taxa de transmissão de R bits/s (aqui, gargalo em um enlace quer dizer que nenhum \n   Redes de computadores e a Internet\n206\ndos outros enlaces ao longo do caminho de cada conexão está congestionado e que todos dispõem de abundante \ncapacidade de transmissão em comparação à capacidade de transmissão do enlace com gargalo). Suponha que \ncada conexão está transferindo um arquivo grande e que não há tráfego UDP passando pelo enlace com gargalo. \nDizemos que um mecanismo de controle de congestionamento é justo se a taxa média de transmissão de cada \nconexão for mais ou menos R/K; isto é, cada uma obtém uma parcela igual da largura de banda do enlace.\nO algoritmo AIMD do TCP é justo, considerando, em especial, que diferentes conexões TCP podem co-\nmeçar em momentos diferentes e, assim, ter tamanhos de janela diferentes em um dado instante? Chiu [1989] \nexplica, de um modo elegante e intuitivo, por que o controle de congestionamento converge para fornecer um \ncompartilhamento justo da largura de banda do enlace entre conexões TCP concorrentes.\nVamos considerar o caso simples de duas conexões TCP compartilhando um único enlace com taxa de \ntransmissão R, conforme mostra a Figura 3.55. Admitamos que as duas conexões tenham os mesmos MSS e RTT \n(de modo que, se o tamanho de suas janelas de congestionamento for o mesmo, eles terão a mesma vazão) e uma \ngrande quantidade de dados para enviar e que nenhuma outra conexão TCP ou datagramas UDP atravesse esse \nenlace compartilhado. Vamos ignorar também a fase de partida lenta do TCP e admitir que as conexões TCP \nestão operando em modo prevenção de congestionamento (AIMD) todo o tempo.\nA Figura 3.56 mostra a vazão alcançada pelas duas conexões TCP. Se for para o TCP compartilhar equitati-\nvamente a largura de banda do enlace entre as duas conexões, a vazão alcançada deverá cair ao longo da linha a \n45 graus (igual compartilhamento da largura de banda) que parte da origem. Idealmente, a soma das duas vazões \nseria igual a R. (Com certeza, não é uma situação desejável cada conexão receber um compartilhamento igual, \nmas igual a zero, da capacidade do enlace!) Portanto, o objetivo é que as vazões alcançadas fiquem em algum \nlugar perto da intersecção da linha de “igual compartilhamento da largura de banda” com a linha de “utilização \ntotal da largura de banda” da Figura 3.56.\nSuponha que os tamanhos de janela TCP sejam tais que, em um determinado instante, as conexões 1 e 2 \nalcancem as vazões indicadas pelo ponto A na Figura 3.56. Como a quantidade de largura de banda do enlace \nconsumida em conjunto pelas duas conexões é menor do que R, não ocorrerá nenhuma perda e ambas as cone-\nxões aumentarão suas janelas em 1 MSS por RTT como resultado do algoritmo de prevenção de congestiona-\nmento do TCP. Assim, a vazão conjunta das duas conexões continua ao longo da linha a 45 graus (aumento igual \npara as duas), começando no ponto A. Por fim, a largura de banda do enlace consumida em conjunto pelas duas \nconexões será maior do que R e, assim, por fim ocorrerá perda de pacote. Suponha que as conexões 1 e 2 experi-\nmentem perda de pacote quando alcançarem as vazões indicadas pelo ponto B. As conexões 1 e 2 então reduzirão \nsuas janelas por um fator de 2. Assim, as vazões resultantes são as do ponto C, a meio caminho do vetor que co-\nmeça em B e termina na origem. Como a utilização conjunta da largura de banda é menor do que R no ponto C, \nas duas conexões novamente aumentam suas vazões ao longo da linha a 45 graus que começa no ponto C. Mais \ncedo ou mais tarde ocorrerá perda, por exemplo, no ponto D, e as duas conexões reduzirão de novo o tamanho \nde suas janelas por um fator de 2 — e assim por diante. Você pode ter certeza de que a largura de banda alcançada \npelas duas conexões flutuará ao longo da linha de igual compartilhamento da largura de banda. E também estar \ncerto de que as duas conexões convergirão para esse comportamento, não importando onde elas comecem no \nFigura 3.55  Duas conexões TCP compartilhando um único enlace congestionado\nConexão TCP 2\nConexão TCP 1\nRoteador com\ngargalo, de capacidade R\nCAMADA  de transporte  207 \nespaço bidimensional! Embora haja uma série de suposições idealizadas por trás desse cenário, ainda assim ele \ndá uma ideia intuitiva de por que o TCP resulta em igual compartilhamento da largura de banda entre conexões.\nEm nosso cenário idealizado, admitimos que apenas conexões TCP atravessem o enlace com gargalo, que \nelas tenham o mesmo valor de RTT e que uma única conexão TCP esteja associada com um par hospedeiro/ \ndestinatário. Na prática, essas condições não são muito encontradas e, assim, é possível que as aplicações cliente-\nservidor obtenham porções muito desiguais da largura de banda do enlace. Em especial, foi demonstrado que, \nquando várias conexões compartilham um único enlace com gargalo, as sessões cujos RTTs são menores conse-\nguem obter a largura de banda disponível naquele enlace mais rapidamente (isto é, abre suas janelas de conges-\ntionamento mais depressa) à medida que o enlace fica livre. Assim, conseguem vazões mais altas do que conexões \ncom RTTs maiores [Lakshman, 1997].\nEquidade e UDP\nAcabamos de ver como o controle de congestionamento no TCP regula a taxa de transmissão de uma \naplicação por meio do mecanismo de janela de congestionamento. Diversas aplicações de multimídia, como tele-\nfone por Internet e videoconferência, muitas vezes não rodam sobre TCP exatamente por essa razão — elas não \nquerem que sua taxa de transmissão seja limitada, mesmo que a rede esteja muito congestionada. Ao contrário, \npreferem rodar sobre UDP, que não tem controle de congestionamento. Quando rodam sobre esse protocolo, \nas aplicações podem passar seus áudios e vídeos para a rede a uma taxa constante e, de modo ocasional, perder \npacotes, em vez de reduzir suas taxas a níveis “justos” em horários de congestionamento e não perder nenhum \ndeles. Do ponto de vista do TCP, as aplicações de multimídia que rodam sobre UDP não são justas — elas não \ncooperam com as outras conexões nem ajustam suas taxas de transmissão de maneira adequada. Como o contro-\nle de congestionamento no TCP reduzirá sua taxa de transmissão quando houver aumento de congestionamento \n(perda), enquanto origens UDP não precisam fazer o mesmo, é possível que essas origens desalojem o tráfego \nTCP. Uma área importante da pesquisa hoje é o desenvolvimento de mecanismos de controle que impeçam que o \ntráfego de UDP leve a vazão da Internet a uma parada repentina [Floyd, 1999; Floyd, 2000; Kohler, 2006].\nFigura 3.56  Vazão alcançada pelas conexões TCP 1 e TCP 2\nR\nR\nIgual compartilhamento\nda largura da banda\nVazão da conexão 1\nVazão da conexão 2\nD\nB\nC\nA\nUtilização total da\nlargura de banda\n   Redes de computadores e a Internet\n208\nEquidade e conexões TCP paralelas\nMas, mesmo que pudéssemos obrigar o tráfego UDP a se comportar com equidade, o problema ainda não \nestaria resolvido por completo. Isso porque não há nada que impeça uma aplicação de rodar sobre TCP usan-\ndo múltiplas conexões paralelas. Por exemplo, navegadores Web com frequência usam múltiplas conexões TCP \nparalelas para transferir os vários objetos de uma página. (O número exato de conexões múltiplas pode ser con-\nfigurado na maioria dos navegadores.) Quando usa múltiplas conexões paralelas, uma aplicação consegue uma \nfração maior da largura de banda de um enlace congestionado. Como exemplo, considere um enlace de taxa R \nque está suportando nove aplicações cliente-servidor em curso, e cada uma das aplicações está usando uma co-\nnexão TCP. Se surgir uma nova aplicação que também utilize uma conexão TCP, então cada aplicação conseguirá \naproximadamente a mesma taxa de transmissão igual a R/10. Porém, se, em vez disso, essa nova aplicação usar 11 \nconexões TCP paralelas, então ela conseguirá uma alocação injusta de mais do que R/2. Como a penetração do \ntráfego Web na Internet é grande, as múltiplas conexões paralelas não são incomuns.\n3.8  Resumo\nComeçamos este capítulo estudando os serviços que um protocolo de camada de transporte pode prover às \naplicações de rede. Por um lado, esse protocolo pode ser muito simples e oferecer serviços básicos às aplicações, \nprovendo apenas uma função de multiplexação/demultiplexação para processos em comunicação. O protocolo \nUDP da Internet é um exemplo desse serviço básico. Por outro lado, pode fornecer uma série de garantias às apli-\ncações, como entrega confiável de dados, garantias contra atrasos e garantias de largura de banda. Não obstante, \nos serviços que um protocolo de transporte pode prover são muitas vezes limitados pelo modelo de serviço do \nprotocolo subjacente de camada de rede. Se o protocolo de camada de rede não puder proporcionar garantias \ncontra atraso ou garantias de largura de banda para segmentos da camada de transporte, então o protocolo de \ncamada de transporte não poderá fornecer tais garantias para as mensagens enviadas entre processos.\nAprendemos na Seção 3.4 que um protocolo de camada de transporte pode prover transferência confiável \nde dados mesmo que a camada de rede subjacente seja não confiável. Vimos que há muitos pontos sutis na trans-\nferência confiável de dados, mas que a tarefa pode ser realizada pela combinação cuidadosa de reconhecimentos, \ntemporizadores, retransmissões e números de sequência.\nEmbora tenhamos examinado a transferência confiável de dados neste capítulo, devemos ter em mente que \nela pode ser fornecida por protocolos de camada de enlace, de rede, de transporte ou de aplicação. Qualquer uma \ndas camadas superiores da pilha de protocolos pode executar reconhecimentos, temporizadores, retransmissões \ne números de sequência e prover transferência confiável de dados para a camada situada acima dela. Na verdade, \ncom o passar dos anos, engenheiros e cientistas da computação projetaram e realizaram, independentemente, \nprotocolos de camada de enlace, de rede, de transporte e de aplicação que fornecem transferência confiável de \ndados (embora muitos tenham desaparecido silenciosamente).\nNa Seção 3.5, examinamos em detalhes o TCP, o protocolo de camada de transporte confiável orientado \npara conexão da Internet. Aprendemos que o TCP é complexo e envolve conexão, controle de fluxo, estimativa de \ntempo de viagem de ida e volta, bem como transferência confiável de dados. Na verdade, o TCP é mais complexo \ndo que nossa descrição — de propósito, não discutimos uma série de ajustes, acertos e melhorias que estão im-\nplementados em várias versões do TCP. Toda essa complexidade, no entanto, fica escondida da aplicação de rede. \nSe um cliente em um hospedeiro quiser enviar dados de maneira confiável para outro hospedeiro, ele apenas abre \num socket TCP para o servidor e passa dados para dentro desse socket. A aplicação cliente-servidor felizmente fica \nalheia a toda a complexidade do TCP.\nNa Seção 3.6, examinamos o controle de congestionamento de um ponto de vista mais amplo e, na Seção \n3.7, demonstramos como o TCP realiza controle de congestionamento. Aprendemos que esse controle é impera-\ntivo para o bem-estar da rede. Sem ele, uma rede pode facilmente ficar travada, com pouco ou nenhum dado sen-\ndo transportado fim a fim. Na Seção 3.7, aprendemos também que o TCP executa um mecanismo de controle de \nCAMADA  de transporte  209 \ncongestionamento fim a fim que aumenta aditivamente sua taxa de transmissão quando julga que o caminho da \nconexão TCP está livre de congestionamento e reduz multiplicativamente sua taxa de transmissão quando ocorre \nperda. Esse mecanismo também luta para dar a cada conexão TCP que passa por um enlace congestionado uma \nparcela igual da largura de banda da conexão. Examinamos ainda com algum detalhe o impacto do estabeleci-\nmento da conexão TCP e da partida lenta sobre a latência. Observamos que, em muitos cenários importantes, \no estabelecimento da conexão e a partida lenta contribuem de modo significativo para o atraso fim a fim. Enfa-\ntizamos mais uma vez que, embora tenha evoluído com o passar dos anos, o controle de congestionamento no \nTCP permanece como uma área de pesquisa intensa e, provavelmente, continuará a evoluir nos anos vindouros.\nNossa discussão sobre os protocolos específicos de transporte da Internet neste capítulo concentrou-se no \nUDP e no TCP — os dois “burros de carga” da camada de transporte. Entretanto, duas décadas de experiência \ncom os dois identificaram circunstâncias nas quais nenhum deles é apropriado de maneira ideal. Desse modo, \npesquisadores se ocuparam em desenvolver protocolos da camada de transporte adicionais, muitos dos quais são, \nagora, padrões propostos pelo IETF.\nO Protocolo de Controle de Congestionamento de Datagrama (DCCP) [RFC 4340] oferece um serviço \nde baixo consumo, orientado a mensagem e não confiável de forma semelhante ao UDP, mas com uma forma \nselecionada de aplicação de controle de congestionamento que é compatível com o TCP. Caso uma aplicação \nnecessitar de uma transferência de dados confiável ou semiconfiável, então isso seria realizado dentro da própria \naplicação, talvez utilizando os mecanismos que estudamos na Seção 3.4. O DCCP é utilizado em aplicações, como \no fluxo de mídia (veja Capítulo 7), que podem se aproveitar da escolha entre conveniência e confiança do forne-\ncimento de dados, mas que querem ser sensíveis ao congestionamento da rede.\nO Protocolo de Controle de Fluxo de Transmissão (SCTP) [RFC 4960, RFC 3286] é um protocolo confiável \ne orientado a mensagens que permite que diferentes “fluxos” de aplicação sejam multiplexados por meio de uma \núnica conexão SCTP (método conhecido como “múltiplos fluxos”). De um ponto de vista confiável, os diferentes \nfluxos dentro da conexão são controlados em separado, de modo que uma perda de pacote em um fluxo não afete \no fornecimento de dados em outros fluxos. O SCTP também permite que os dados sejam transferidos por meio \nde dois percursos de saída quando um hospedeiro está conectado a duas ou mais redes, fornecimento opcional \nde dados fora de ordem, e muitos outros recursos. Os algoritmos de controle de congestionamento e de fluxo do \nSCTP são basicamente os mesmos do TCP.\nO protocolo TFRC (TCP — Friendly Rate Control) [RFC 5348] é mais um protocolo de controle de conges-\ntionamento do que um protocolo da camada de transporte completo. Ele especifica um mecanismo de controle \nque poderia ser utilizado em outro protocolo de transporte como o DCCP (de fato, um dos dois protocolos de \naplicação, disponível no DCCP, é o TFRC). O objetivo do TFRC é estabilizar o comportamento do tipo “dente \nde serra” (veja Figura 3.54) no controle de congestionamento no TCP, enquanto mantém uma taxa de envio de \nlongo prazo “razoavelmente” próxima à do TCP. Com uma taxa de envio mais estável do que a do TCP, o TFRC é \nadequado às aplicações multimídia, como telefonia IP ou fluxo de mídia, em que uma taxa estável é importante. \nO TFRC é um protocolo baseado em equação que utiliza a taxa de perda de pacote calculada como suporte a uma \nequação que estima qual seria a vazão do TCP se uma sessão TCP sofrer taxa de perda [Padhye, 2000]. Essa taxa \né então adotada como a taxa-alvo de envio do TFRC.\nSomente o futuro dirá se o DCCP, o SCTP ou o TFRC possuirá uma implementação difundida. Enquanto \nesses protocolos fornecem claramente recursos avançados sobre o TCP e o UDP, estes já provaram a si mesmos \nser “bons o bastante” com o passar dos anos. A vitória do “melhor” sobre o “bom o bastante” dependerá de uma \nmistura complexa de considerações técnicas, sociais e comerciais.\nNo Capítulo 1, dissemos que uma rede de computadores pode ser dividida em “periferia da rede” e “núcleo \nda rede”\n. A periferia abrange tudo o que acontece nos sistemas finais. Com o exame da camada de aplicação e da \ncamada de transporte, nossa discussão sobre a periferia da rede está completa. É hora de explorar o núcleo! Essa \njornada começa no próximo capítulo, em que estudaremos a camada de rede, e continua no Capítulo 5, em que \nestudaremos a camada de enlace.\n   Redes de computadores e a Internet\n210\nExercícios  \nde fixação e perguntas\nQuestões de revisão do Capítulo 3\nSEÇÕES 3.1–3.3\n\t\nR1.\t Suponha que uma camada de rede forneça o seguinte serviço. A camada de rede no computador-fonte aceita \num segmento de tamanho máximo de 1.200 bytes e um endereço de computador-alvo da camada de transporte. \nA camada de rede, então, garante encaminhar o segmento para a camada de transporte no computador-alvo. \nSuponha que muitos processos de aplicação de rede possam estar sendo executados no hospedeiro de destino.\na.\t Crie, da forma mais simples, o protocolo da camada de transporte possível que levará os dados da aplicação \npara o processo desejado no hospedeiro de destino. Suponha que o sistema operacional do hospedeiro de \ndestino determinou um número de porta de 4 bytes para cada processo de aplicação em execução.\nb.\t Modifique esse protocolo para que ele forneça um “endereço de retorno” ao processo-alvo.\nc.\t Em seus protocolos, a camada de transporte “tem de fazer algo” no núcleo da rede de computadores?\n\t\nR2.\t Considere um planeta onde todos possuam uma família com seis membros, cada família viva em sua própria casa, \ncada casa possua um endereço único e cada pessoa em certa casa possua um único nome. Suponha que esse planeta \npossua um serviço postal que entregue cartas da casa-fonte à casa-alvo. O serviço exige que (1) a carta esteja em um \nenvelope e que (2) o endereço da casa-alvo (e nada mais) esteja escrito claramente no envelope. Imagine que cada \nfamília possua um membro representante que recebe e distribui cartas para as demais. As cartas não apresentam \nnecessariamente qualquer indicação dos destinatários das cartas.\na.\t Utilizando a solução do Problema R1 como inspiração, descreva um protocolo que os representantes \npossam utilizar para entregar cartas de um membro remetente de uma família para um membro \ndestinatário de outra família.\nb.\t Em seu protocolo, o serviço postal precisa abrir o envelope e verificar a carta para fornecer o serviço?\n\t\nR3.\t Considere uma conexão TCP entre o hospedeiro A e o hospedeiro B. Suponha que os segmentos TCP que \ntrafegam do hospedeiro A para o hospedeiro B tenham número de porta da origem x e número de porta do \ndestino y. Quais são os números de porta da origem e do destino para os segmentos que trafegam do hospedeiro \nB para o hospedeiro A?\n\t\nR4.\t Descreva por que um desenvolvedor de aplicação pode escolher rodar uma aplicação sobre UDP em vez de \nsobre TCP.\n\t\nR5.\t Por que o tráfego de voz e de vídeo é frequentemente enviado por meio do UDP e não do TCP na Internet \nde hoje? (Dica: A resposta que procuramos não tem nenhuma relação com o mecanismo de controle de \ncongestionamento no TCP.)\n\t\nR6.\t É possível que uma aplicação desfrute de transferência confiável de dados mesmo quando roda sobre UDP? \nCaso a resposta seja afirmativa, como isso acontece?\n\t\nR7.\t Suponha que um processo no hospedeiro C possua um socket UDP com número de porta 6789 e que o \nhospedeiro A e o hospedeiro B, individualmente, enviem um segmento UDP ao hospedeiro C com número \nde porta de destino 6789. Os dois segmentos serão encaminhados para o mesmo socket no hospedeiro C? Se \nsim, como o processo no hospedeiro C saberá que os dois segmentos vieram de dois hospedeiros diferentes?\n\t\nR8.\t Suponha que um servidor da Web seja executado no computador C na porta 80. Esse servidor utiliza \nconexões contínuas e, no momento, está recebendo solicitações de dois computadores diferentes, A e B. Todas \nas solicitações estão sendo enviadas por meio do mesmo socket no computador C? Se estão passando por \ndiferentes sockets, dois deles possuem porta 80? Discuta e explique.\nCAMADA  de transporte  211 \nSEÇÃO 3.4\n\t\nR9.\t Em nossos protocolos rdt, por que precisamos introduzir números de sequência?\n\t\nR10.\t Em nossos protocolos rdt, por que precisamos introduzir temporizadores?\n\t\nR11.\t Suponha que o atraso de viagem de ida e volta entre o emissor e o receptor seja constante e conhecido para o \nemissor. Ainda seria necessário um temporizador no protocolo rdt 3.0, supondo que os pacotes podem \nser perdidos? Explique.\n\t\nR12.\t Visite o applet Go-Back-N Java no site de apoio do livro.\na.\t A origem enviou cinco pacotes e depois interrompeu a animação antes que qualquer um dos cinco pacotes \nchegasse ao destino. Então, elimine o primeiro pacote e reinicie a animação. Descreva o que acontece.\nb.\t Repita o experimento, mas agora deixe o primeiro pacote chegar ao destino e elimine o primeiro \nreconhecimento. Descreva novamente o que acontece.\nc.\t Por fim, tente enviar seis pacotes. O que acontece?\n\t\nR13.\t Repita a Questão 12, mas agora com o applet Java Selective Repeat. O que difere o Selective Repeat do Go-Back-N?\nSEÇÃO 3.5\n\t\nR14.\t Falso ou verdadeiro?\na.\t O hospedeiro A está enviando ao hospedeiro B um arquivo grande por uma conexão TCP. Suponha \nque o hospedeiro B não tenha dados para enviar para o hospedeiro A. O hospedeiro B não enviará \nreconhecimentos para A porque ele não pode dar carona aos reconhecimentos dos dados.\nb.\t O tamanho de rwnd do TCP nunca muda enquanto dura a conexão.\nc.\t Suponha que o hospedeiro A esteja enviando ao hospedeiro B um arquivo grande por uma conexão TCP. \nO número de bytes não reconhecidos que o hospedeiro A envia não pode exceder o tamanho do buffer de \nrecepção.\nd.\t Imagine que o hospedeiro A esteja enviando ao hospedeiro B um arquivo grande por uma conexão TCP. Se o \nnúmero de sequência para um segmento dessa conexão for m, então o número de sequência para o segmento \nsubsequente será necessariamente m + 1.\ne.\t O segmento TCP tem um campo em seu cabeçalho para rwnd.\nf.\t Suponha que o último SampleRTT de uma conexão TCP seja igual a 1 s. Então, o valor corrente de \nTimeoutInterval para a conexão será necessariamente ajustado para um valor  1 s.\ng.\t Imagine que o hospedeiro A envie ao hospedeiro B, por uma conexão TCP, um segmento com o \nnúmero de sequência 38 e 4 bytes de dados. Nesse mesmo segmento, o número de reconhecimento será \nnecessariamente 42.\n\t\nR15.\t Suponha que o hospedeiro A envie dois segmentos TCP um atrás do outro ao hospedeiro B sobre uma \nconexão TCP. O primeiro segmento tem número de sequência 90 e o segundo, número de sequência 110.\na.\t Quantos dados tem o primeiro segmento?\nb.\t Suponha que o primeiro segmento seja perdido, mas o segundo chegue a B. No reconhecimento que B \nenvia a A, qual será o número de reconhecimento?\n\t\nR16.\t Considere o exemplo do Telnet discutido na Seção 3.5. Alguns segundos após o usuário digitar a letra “C”\n, \nele digitará a letra “R”\n. Depois disso, quantos segmentos serão enviados e o que será colocado nos campos de \nnúmero de sequência e de reconhecimento dos segmentos?\nSEÇÃO 3.7\n\t\nR17.\t Suponha que duas conexões TCP estejam presentes em algum enlace congestionado de velocidade R bits/s. \nAs duas conexões têm um arquivo imenso para enviar (na mesma direção, pelo enlace congestionado). \n   Redes de computadores e a Internet\n212\nAs transmissões dos arquivos começam exatamente ao mesmo tempo. Qual é a velocidade de transmissão \nque o TCP gostaria de dar a cada uma das conexões?\n\t\nR18.\t Verdadeiro ou falso: considere o controle de congestionamento no TCP. Quando um temporizador expira no \nremetente, o valor de ssthresh é ajustado para a metade de seu valor anterior.\n\t\nR19.\t Na discussão sobre divisão do TCP, na nota em destaque da Seção 3.7, afirmamos que o tempo de resposta com \na divisão do TCP é aproximadamente 4 ∙ RTTFE + RTTBE + tempo de processamento. Justifique essa afirmação.\nproblemas\n\t\nP1.\t Suponha que o cliente A inicie uma sessão Telnet com o servidor S. Quase ao mesmo tempo, o cliente B \ntambém inicia uma sessão Telnet com o servidor S. Forneça possíveis números de porta da fonte e do destino \npara:\na.\t Os segmentos enviados de A para S.\nb.\t Os segmentos enviados de B para S.\nc.\t Os segmentos enviados de S para A.\nd.\t Os segmentos enviados de A para B.\ne.\t Se A e B são hospedeiros diferentes, é possível que o número de porta da fonte nos segmentos de A para \nS seja o mesmo que nos de B para S?\nf.\t E se forem o mesmo hospedeiro?\n\t\nP2.\t Considere a Figura 3.5. Quais são os valores da porta de fonte e da porta de destino nos segmentos que fluem \ndo servidor de volta aos processos clientes? Quais são os endereços IP nos datagramas de camada de rede que \ncarregam os segmentos de camada de transporte?\n\t\nP3.\t O UDP e o TCP usam complementos de 1 para suas somas de verificação. Suponha que você tenha as seguintes \ntrês palavras de 8 bits: 01010011, 01100110 e 01110100. Qual é o complemento de 1 para as somas dessas \npalavras? (Note que, embora o UDP e o TCP usem palavras de 16 bits no cálculo da soma de verificação, nesse \nproblema solicitamos que você considere somas de 8 bits.) Mostre todo o trabalho. Por que o UDP toma o \ncomplemento de 1 da soma, isto é, por que não toma apenas a soma? Com o esquema de complemento de 1, \ncomo o destinatário detecta erros? É possível que um erro de 1 bit passe despercebido? E um erro de 2 bits?\n\t\nP4\t a.  \u0007\nSuponha que você tenha os seguintes bytes: 01011100 e 01100101. Qual é o complemento de 1 da soma \ndesses 2 bytes?\nb.\t Suponha que você tenha os seguintes bytes: 11011010 e 01100101. Qual é o complemento de 1 da soma \ndesses 2 bytes?\nc.\t Para os bytes do item (a), dê um exemplo em que um bit é invertido em cada um dos 2 bytes e, mesmo \nassim, o complemento de um não muda.\n\t\nP5.\t Suponha que o receptor UDP calcule a soma de verificação da Internet para o segmento UDP recebido e \nencontre que essa soma coincide com o valor transportado no campo da soma de verificação. O receptor pode \nestar absolutamente certo de que não ocorreu nenhum erro de bit? Explique.\n\t\nP6.\t Considere nosso motivo para corrigir o protocolo rtd2.1. Demonstre que o destinatário apresentado \nna Figura 3.57, quando em operação com o remetente mostrado na Figura 3.11, pode levar remetente e \ndestinatário a entrar em estado de travamento, em que cada um espera por um evento que nunca vai ocorrer.\n\t\nP7.\t No protocolo rdt3.0, os pacotes ACK que fluem do destinatário ao remetente não têm números de \nsequência (embora tenham um campo de ACK que contém o número de sequência do pacote que estão \nreconhecendo). Por que nossos pacotes ACK não requerem números de sequência?\n\t\nP8.\t Elabore a FSM para o lado destinatário do protocolo rdt3.0.\nCAMADA  de transporte  213 \n\t\nP9.\t Elabore um diagrama de mensagens para a operação do protocolo rdt3.0 quando pacotes de dados e de \nreconhecimento estão truncados. Seu diagrama deve ser semelhante ao usado na Figura 3.16.\n\t\nP10.\t Considere um canal que pode perder pacotes, mas cujo atraso máximo é conhecido. Modifique o protocolo \nrdt2.1 para incluir esgotamento de temporização do remetente e retransmissão. Informalmente, argumente \npor que seu protocolo pode se comunicar de modo correto por esse canal.\n\t\nP11.\t Considere o rdt2.2 destinatário da Figura 3.14 e a criação de um novo pacote na autotransição (isto \né, a transição do estado de volta para si mesmo) nos estados “Esperar 0 de baixo” e “Esperar 1 de baixo”: \nsndpkt=make_pkt(ACK,0,checksum) e sndpkt=make_pkt(ACK,0, checksum). O protocolo \nfuncionaria corretamente se essa ação fosse removida da autotransição no estado “Esperar 1 de baixo”? Justifique \nsua resposta. E se esse evento fosse removido da autotransição no estado “Esperar 0 de baixo”? [Dica: Neste último \ncaso, considere o que aconteceria se o primeiro pacote do remetente ao destinatário fosse corrompido.]\n\t\nP12.\t O lado remetente do rdt3.0 simplesmente ignora (isto é, não realiza nenhuma ação) todos os pacotes \nrecebidos que estão errados ou com o valor errado no campo acknum de um pacote de reconhecimento. \nSuponha que em tais circunstâncias o rdt3.0 devesse apenas retransmitir o pacote de dados corrente. Nesse \ncaso, o protocolo ainda funcionaria? (Dica: Considere o que aconteceria se houvesse apenas erros de bits; não \nhá perdas de pacotes, mas podem ocorrer esgotamentos de temporização prematuros. Imagine quantas vezes \no enésimo pacote é enviado, no limite em que n se aproximasse do infinito.)\n\t\nP13.\t Considere o protocolo rdt3.0. Elabore um diagrama mostrando que, se a conexão de rede entre o remetente e o \ndestinatário puder alterar a ordem de mensagens (isto é, se for possível reordenar duas mensagens que se propagam \nno meio entre o remetente e o destinatário), então o protocolo bit alternante não funcionará corretamente (lembre­\n‑se de identificar com clareza o sentido no qual o protocolo não funcionará de modo correto). Seu diagrama deve \nmostrar o remetente à esquerda e o destinatário à direita; o eixo do tempo deverá estar orientado de cima para \nbaixo na página e mostrar a troca de mensagem de dados (D) e de reconhecimento (A). Não se esqueça de indicar \no número de sequência associado com qualquer segmento de dados ou de reconhecimento.\n\t\nP14.\t Considere um protocolo de transferência confiável de dados que use somente reconhecimentos negativos. \nSuponha que o remetente envie dados com pouca frequência. Um protocolo que utiliza somente NAKs seria \npreferível a um protocolo que utiliza ACKs? Por quê? Agora suponha que o remetente tenha uma grande \nFigura 3.57  Um receptor incorreto para o protocolo rdt 2.1.\nEsperar 0\nde baixo\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nhas_seq0(rcvpkt)))\ncompute chksum\nmake_pkt(sndpkt,NAK,chksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nhas_seq1(rcvpkt)))\ncompute chksum\nmake_pkt(sndpkt,NAK,chksum)\nudt_send(sndpkt)\nrdt_rvc(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq1(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\ncompute chksum\nmake_pkt(sendpkt,ACK,chksum)\nudt_send(sndpkt)\nrdt_rvc(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq0(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\ncompute chksum\nmake_pkt(sendpkt,ACK,chksum)\nudt_send(sndpkt)\nEsperar 1\nde baixo\n \n   Redes de computadores e a Internet\n214\nquantidade de dados para enviar e que a conexão fim a fim sofra poucas perdas. Nesse segundo caso, um \nprotocolo que utilize somente NAKs seria preferível a um protocolo que utilize ACKs? Por quê?\n\t\nP15.\t Considere o exemplo em que se atravessa os Estados Unidos mostrado na Figura 3.17. Que tamanho deveria \nter a janela para que a utilização do canal fosse maior do que 98%? Suponha que o tamanho de um pacote seja \n1.500 bytes, incluindo os campos do cabeçalho e os dados.\n\t\nP16.\t Suponha que uma aplicação utilize rdt3.0 como seu protocolo da camada de transporte. Como o protocolo \npare e espere possui uma utilização do canal muito baixa (mostrada no exemplo de travessia dos Estados Unidos), \nos criadores dessa aplicação permitem que o receptor continue enviando de volta um número (mais do que dois) \nde ACK 0 alternado e ACK 1, mesmo que os dados correspondentes não cheguem ao receptor. O projeto dessa \naplicação aumentaria a utilização do canal? Por quê? Há possíveis problemas com esse método? Explique.\n\t\nP17.\t Considere duas entidades de rede, A e B, que estão conectadas por um canal bidirecional perfeito (isto é, \nqualquer mensagem enviada será recebida corretamente; o canal não corromperá, perderá nem reordenará \npacotes). A e B devem entregar mensagens de dados entre si de forma alternada: primeiro, A deve entregar \numa mensagem a B, depois B deve entregar uma mensagem a A, e assim por diante. Se uma entidade estiver em \num estado onde não deve tentar entregar uma mensagem ao outro lado e houver um evento como a chamada \nrdt_send(data) de cima que tente transmitir dados para baixo, para o outro lado, essa chamada de cima \npode apenas ser ignorada com uma chamada a rdt_unable_to_send(data), que informa à camada \nde cima que atualmente não é possível enviar dados. [Nota: Essa suposição simplificada é para que você não \ntenha de se preocupar com o armazenamento de dados em buffer.]\n\t\n\t Elabore uma especificação FSM para este protocolo (uma FSM para A e uma para B!). Observe que você \nnão precisa se preocupar com um mecanismo de confiabilidade aqui; o ponto importante da questão é criar \numa especificação FSM que reflita o comportamento sincronizado das duas entidades. Você deverá usar os \nseguintes eventos e ações que possuem o mesmo significado do protocolo rdt1.0 da Figura 3.9: rdt_\nsend(data), packet \n= \nmake_pkt(data), udt_send(packet), rdt_rcv(packet), extract \n(packet,data), deliver_data(data). Cuide para que o protocolo reflita a alternância estrita de envio \nentre A e B. Além disso, não se esqueça de indicar os estados iniciais de A e B em suas especificações FSM.\n\t\nP18.\t No protocolo genérico SR que estudamos na Seção 3.4.4, o remetente transmite uma mensagem assim que ela \nestá disponível (se ela estiver na janela), sem esperar por um reconhecimento. Suponha, agora, que queiramos \num protocolo SR que envie duas mensagens de cada vez. Isto é, o remetente enviará um par de mensagens, e \no par de mensagens subsequente somente deverá ser enviado quando o remetente souber que ambas as men-\nsagens do primeiro par foram recebidas corretamente.\n\t\n\t Suponha que o canal possa perder mensagens, mas que não as corromperá nem as reordenará. Elabore um \nprotocolo de controle de erro para a transferência confiável unidirecional de mensagens. Dê uma descrição \nFSM do remetente e do destinatário. Descreva o formato dos pacotes enviados entre o remetente e o destinatário \ne vice-versa. Se você usar quaisquer procedimentos de chamada que não sejam os da Seção 3.4 (por exemplo, \nudt_send(), start_timer(), rdt_rcv() etc.), esclareça as ações desses procedimentos. Dê um \nexemplo (um diagrama de mensagens para o remetente e para o destinatário) mostrando como seu protocolo \nse recupera de uma perda de pacote.\n\t\nP19.\t Considere um cenário em que o hospedeiro A queira enviar pacotes para os hospedeiros B e C simultaneamente. \nO hospedeiro A está conectado a B e a C por um canal broadcast — um pacote enviado por A é levado pelo \ncanal a B e a C. Suponha que o canal broadcast que conecta A, B e C possa, de modo independente, perder e \ncorromper mensagens (e assim, por exemplo, uma mensagem enviada de A poderia ser recebida corretamente \npor B, mas não por C). Projete um protocolo de controle de erro do tipo pare e espere para a transferência \nconfiável de um pacote de A para B e para C, tal que A não receba novos dados da camada superior até que \nsaiba que B e C receberam corretamente o pacote em questão. Dê descrições FSM de A e C. (Dica: a FSM para \nB deve ser a mesma que para C.) Também dê uma descrição do(s) formato(s) de pacote usado(s).\n\t\nP20.\t Considere um cenário em que os hospedeiros A e B queiram enviar mensagens ao hospedeiro C. Os hospedeiros \nA e C estão conectados por um canal que pode perder e corromper (e não reordenar) mensagens. B e C \nestão conectados por outro canal (independente do canal que conecta A e C) com as mesmas propriedades. \nCAMADA  de transporte  215 \nA camada de transporte no hospedeiro C deve alternar o envio de mensagens de A e B para a camada acima \n(ou seja, ela deve primeiro transmitir os dados de um pacote de A e depois os dados de um pacote de B, e assim \npor diante). Elabore um protocolo de controle de erro pare e espere para transferência confiável de pacotes de \nA e B para C, com envio alternado em C, como descrito acima. Dê descrições FSM de A e C. (Dica: a FSM para \nB deve ser basicamente a mesma de A.) Dê, também, uma descrição do(s) formato(s) de pacote utilizado(s).\n\t\nP21.\t Suponha que haja duas entidades de rede A e B e que B tenha um suprimento de mensagens de dados que será \nenviado a A de acordo com as seguintes convenções: quando A recebe uma solicitação da camada superior \npara obter a mensagem de dados (D) seguinte de B, A deve enviar uma mensagem de requisição (R) para B \nno canal A-a-B; somente quando B receber uma mensagem R, ele poderá enviar uma mensagem de dados (D) \nde volta a A pelo canal B a A; A deve entregar uma cópia de cada mensagem D à camada superior; mensagens \nR podem ser perdidas (mas não corrompidas) no canal A-a-B; mensagens (D), uma vez enviadas, são sempre \nentregues corretamente; o atraso entre ambos os canais é desconhecido e variável.\n\t\n\t Elabore um protocolo (dê uma descrição FSM) que incorpore os mecanismos apropriados para compensar a \npropensão à perda do canal A a B e implemente passagem de mensagem para a camada superior na entidade \nA, como discutido antes. Utilize apenas os mecanismos absolutamente necessários.\n\t\nP22.\t Considere o protocolo GBN com um tamanho de janela remetente de 4 e uma faixa de números de sequência \nde 1.024. Suponha que, no tempo t, o pacote seguinte na ordem, pelo qual o destinatário está esperando, \ntenha um número de sequência k. Admita que o meio não reordene as mensagens. Responda às seguintes \nperguntas:\na.\t Quais são os possíveis conjuntos de números de sequência dentro da janela do remetente no tempo t? \nJustifique sua resposta.\nb.\t Quais são todos os possíveis valores do campo ACK em todas as mensagens que estão atualmente se \npropagando de volta ao remetente no tempo t? Justifique sua resposta.\n\t\nP23.\t Considere os protocolos GBN e SR. Suponha que o espaço de números de sequência seja de tamanho k. Qual \nserá o maior tamanho de janela permissível que evitará que ocorram problemas como os da Figura 3.27 para \ncada um desses protocolos?\n\t\nP24.\t Responda verdadeiro ou falso às seguintes perguntas e justifique resumidamente sua resposta:\na.\t Com o protocolo SR, é possível o remetente receber um ACK para um pacote que caia fora de sua janela \ncorrente.\nb.\t Com o GBN, é possível o remetente receber um ACK para um pacote que caia fora de sua janela corrente.\nc.\t O protocolo bit alternante é o mesmo que o protocolo SR com janela do remetente e do destinatário de \ntamanho 1.\nd.\t O protocolo bit alternante é o mesmo que o protocolo GBN com janela do remetente e do destinatário de \ntamanho 1.\n\t\nP25.\t Dissemos que um aplicação pode escolher o UDP para um protocolo de transporte, pois oferece um controle \nde aplicações melhor (do que o TCP) de quais dados são enviados em um segmento e quando isso ocorre.\na.\t Por que uma aplicação possui mais controle de quais dados são enviados em um segmento?\nb.\t Por que uma aplicação possui mais controle de quando o segmento é enviado?\n\t\nP26.\t Considere a transferência de um arquivo enorme de L bytes do hospedeiro A para o hospedeiro B. Suponha \num MSS de 536 bytes.\na.\t Qual é o máximo valor de L tal que não sejam esgotados os números de sequência TCP? Lembre-se de que \no campo de número de sequência TCP tem 4 bytes.\nb.\t Para o L que obtiver em (a), descubra quanto tempo demora para transmitir o arquivo. Admita que um \ntotal de 66 bytes de cabeçalho de transporte, de rede e de enlace de dados seja adicionado a cada segmento \nantes que o pacote resultante seja enviado por um enlace de 155 Mbits/s. Ignore controle de fluxo e controle \nde congestionamento de modo que A possa enviar os segmentos um atrás do outro e continuamente.\n   Redes de computadores e a Internet\n216\n\t\nP27.\t Os hospedeiros A e B estão se comunicando por meio de uma conexão TCP, e o hospedeiro B já recebeu de A \ntodos os bytes até o byte 126. Suponha que A envie, então, dois segmentos para B sucessivamente. O primeiro \ne o segundo segmentos contêm 80 e 40 bytes de dados. No primeiro segmento, o número de sequência é \n127, o número de porta de partida é 302, e o número de porta de destino é 80. O hospedeiro B envia um \nreconhecimento ao receber um segmento do hospedeiro A.\na.\t No segundo segmento enviado do hospedeiro A para B, quais são o número de sequência, da porta de \norigem e da porta de destino?\nb.\t Se o primeiro segmento chegar antes do segundo, no reconhecimento do primeiro segmento que chegar, \nqual é o número do reconhecimento, da porta de origem e da porta de destino?\nc.\t Se o segundo segmento chegar antes do primeiro, no reconhecimento do primeiro segmento que chegar, \nqual é o número do reconhecimento?\nd.\t Suponha que dois segmentos enviados por A cheguem em ordem a B. O primeiro reconhecimento é \nperdido e o segundo chega após o primeiro intervalo do esgotamento de temporização. Elabore um \ndiagrama de temporização, mostrando esses segmentos, e todos os outros, e os reconhecimentos enviados. \n(Suponha que não haja qualquer perda de pacote adicional.) Para cada segmento de seu desenho, apresente \no número de sequência e o número de bytes de dados; para cada reconhecimento adicionado por você, \ninforme o número do reconhecimento.\n\t\nP28.\t Os hospedeiros A e B estão diretamente conectados com um enlace de 100 Mbits/s. Existe uma conexão TCP \nentre os dois hospedeiros, e A está enviando a B um arquivo enorme por meio dessa conexão. O hospedeiro A \npode enviar seus dados da aplicação para o socket TCP a uma taxa que chega a 120 Mbits/s, mas o hospedeiro B \npode ler o buffer de recebimento TCP a uma taxa de 50 Mbits/s. Descreva o efeito do controle de fluxo do TCP.\n\t\nP29.\t Os cookies SYN foram discutidos na Seção 3.5.6.\na.\t Por que é necessário que o servidor use um número de sequência especial no SYNACK?\nb.\t Suponha que um atacante saiba que um hospedeiro-alvo utilize SYN cookies. O atacante consegue criar \nconexões semiabertas ou completamente abertas apenas enviando um pacote ACK para o alvo? Por quê?\nc.\t Suponha que um atacante receba uma grande quantidade de números de sequência enviados pelo servidor. \nO atacante consegue fazer que o servidor crie muitas conexões totalmente abertas enviando ACKs com \nesses números de sequência? Por quê?\n\t\nP30.\t Considere a rede mostrada no Cenário 2 na Seção 3.6.1. Suponha que os hospedeiros emissores A e B possuam \nvalores de esgotamento de temporização fixos.\na.\t Analise o fato de que aumentar o tamanho do buffer finito do roteador pode possivelmente reduzir a \nvazão (\nout).\nb.\t Agora suponha que os hospedeiros ajustem dinamicamente seus valores de esgotamento de temporização \n(como o que o TCP faz) baseado no atraso no buffer no roteador. Aumentar o tamanho do buffer ajudaria a \naumentar a vazão? Por quê?\n\t\nP31.\t Suponha que os cinco valores de SampleRTT medidos (ver Seção 3.5.3) sejam 106 ms, 120 ms, 140 ms, 90 ms \ne 115 ms. Calcule o EstimatedRTT depois que forem obtidos cada um desses valores de SampleRTT, \nusando um valor de  = 0,125 e supondo que o valor de EstimatedRTT seja 100 ms imediatamente antes \nque a primeira dessas cinco amostras seja obtida. Calcule também o DevRTT após a obtenção de cada \namostra, considerando um valor de  = 0,25 e que o valor de DevRTT seja 5 ms imediatamente antes que a \nprimeira dessas cinco amostras seja obtida. Por fim, calcule o TimeoutInterval do TCP após a obtenção \nde cada uma dessas amostras.\n\t\nP32.\t Considere o procedimento do TCP para estimar o RTT. Suponha que  = 0,1. Considere que SampleRTT1 \nseja a amostra de RTT mais recente, SampleRTT2 seja a próxima amostra mais recente, e assim por diante.\nCAMADA  de transporte  217 \na.\t Para uma dada conexão TCP, suponha que quatro reconhecimentos foram devolvidos com as amostras \nRTT correspondentes SampleRTT4, SampleRTT3, SampleRTT2 e SampleRTT1. Expresse \nEstimatedRTT em termos das quatro amostras RTT.\nb.\t Generalize sua fórmula para n amostras de RTTs.\nc.\t Para a fórmula em (b), considere n tendendo ao infinito. Comente por que esse procedimento de média é \ndenominado média móvel exponencial.\n\t\nP33.\t Na Seção 3.5.3 discutimos estimativa de RTT para o TCP. Em sua opinião, por que o TCP evita medir o \nSampleRTT para segmentos retransmitidos?\n\t\nP34.\t Qual é a relação entre a variável SendBase na Seção 3.5.4 e a variável LastByteRcvd na Seção 3.5.5?\n\t\nP35.\t Qual é a relação entre a variável LastByteRcvd na Seção 3.5.5 e a variável y na Seção 3.5.4?\n\t\nP36.\t Na Seção 3.5.4 vimos que o TCP espera até receber três ACKs duplicados antes de realizar uma retransmissão \nrápida. Em sua opinião, por que os projetistas do TCP preferiram não realizar uma retransmissão rápida \napós ser recebido o primeiro ACK duplicado para um segmento?\n\t\nP37.\t Compare o GBN, SR e o TCP (sem ACK retardado). Admita que os valores do esgotamento de temporização \npara os três protocolos sejam longos o suficiente de tal modo que cinco segmentos de dados consecutivos e \nseus ACKs correspondentes possam ser recebidos (se não perdidos no canal) por um hospedeiro receptor \n(hospedeiro B) e por um hospedeiro emissor (hospedeiro A), respectivamente. Suponha que A envie cinco \nsegmentos de dados para B, e que o segundo segmento (enviado de A) esteja perdido. No fim, todos os cinco \nsegmentos de dados foram corretamente recebidos pelo hospedeiro B.\na.\t Quantos segmentos A enviou no total e quantos ACKs o hospedeiro B enviou no total? Quais são seus \nnúmeros de sequência? Responda essa questão para todos os três protocolos.\nb.\t Se os valores do esgotamento de temporização para os três protocolos forem muito maiores do que 5 RTT, \nentão qual protocolo envia com sucesso todos os cinco segmentos de dados em um menor intervalo de \ntempo?\n\t\nP38.\t Em nossa descrição sobre o TCP na Figura 3.53, o valor do limiar, ssthresh, é definido como ssthresh \n= cwnd/2 em diversos lugares e o valor ssthresh é referido como sendo definido para metade do \ntamanho da janela quando ocorreu um evento de perda. A taxa à qual o emissor está enviando quando \nocorreu o evento de perda deve ser mais ou menos igual a segmentos cwnd por RTT? Explique sua resposta. \nSe for negativa, você pode sugerir uma maneira diferente pela qual ssthresh deva ser definido?\n\t\nP39.\t Considere a Figura 3.46(b). Se 'in aumentar para mais do que R/2, \nout poderá aumentar para mais do que \nR/3? Explique. Agora considere a Figura 3.46(c). Se 'in aumentar para mais do que R/2, \nout poderá aumentar \npara mais de R/4 admitindo-se que um pacote será transmitido duas vezes, em média, do roteador para o \ndestinatário? Explique.\n\t\nP40.\t Considere a Figura 3.58. Admitindo-se que TCP Reno é o protocolo que experimenta o comportamento \nmostrado no gráfico, responda às seguintes perguntas. Em todos os casos você deverá apresentar uma \njustificativa resumida para sua resposta.\na.\t Quais os intervalos de tempo em que a partida lenta do TCP está em execução?\nb.\t Quais os intervalos de tempo em que a prevenção de congestionamento do TCP está em execução?\nc.\t Após a 16a rodada de transmissão, a perda de segmento será detectada por três ACKs duplicados ou por \num esgotamento de temporização?\nd.\t Após a 22a rodada de transmissão, a perda de segmento será detectada por três ACKs duplicados ou por \num esgotamento de temporização?\ne.\t Qual é o valor inicial de ssthresh na primeira rodada de transmissão?\n   Redes de computadores e a Internet\n218\nf.\t Qual é o valor inicial de ssthresh na 18a rodada de transmissão?\ng.\t Qual é o valor de ssthresh na 24a rodada de transmissão?\nh.\t Durante qual rodada de transmissão é enviado o 70o segmento?\ni.\t Admitindo-se que uma perda de pacote será detectada após a 26a rodada pelo recebimento de três ACKs \nduplicados, quais serão os valores do tamanho da janela de congestionamento e de ssthresh?\nj.\t Suponha que o TCP Tahoe seja usado (em vez do TCP Reno) e que ACKs duplicados triplos sejam \nrecebidos na 16a rodada. Quais são o ssthresh e o tamanho da janela de congestionamento na 19ª \nrodada?\nk.\t Suponha novamente que o TCP Tahoe seja usado, e que exista um evento de esgotamento de temporização \nna 22a sessão. Quantos pacotes foram enviados da 17a sessão até a 22a, inclusive?\n\t\nP41.\t Consulte a Figura 3.56, que ilustra a convergência do algoritmo AIMD do TCP. Suponha que, em vez de \numa diminuição multiplicativa, o TCP reduza o tamanho da janela de uma quantidade constante. O AIMD \nresultante convergiria a um algoritmo de igual compartilhamento? Justifique sua resposta usando um \ndiagrama semelhante ao da Figura 3.56.\n\t\nP42.\t Na Seção 3.5.4 discutimos a duplicação do intervalo de temporização após um evento de esgotamento de \ntemporização. Esse mecanismo é uma forma de controle de congestionamento. Por que o TCP precisa de um \nmecanismo de controle de congestionamento que utiliza janelas (como estudado na Seção 3.7) além desse \nmecanismo de duplicação do intervalo de esgotamento de temporização?\n\t\nP43.\t O hospedeiro A está enviando um arquivo enorme ao hospedeiro B por uma conexão TCP. Nessa conexão \nnunca há perda de pacotes e os temporizadores nunca se esgotam. Seja R bits/s a taxa de transmissão do \nenlace que liga o hospedeiro A à Internet. Suponha que o processo em A consiga enviar dados para seu socket \nTCP a uma taxa de S bits/s, em que S = 10 ∙ R. Suponha ainda que o buffer de recepção do TCP seja grande \no suficiente para conter o arquivo inteiro e que o buffer de envio possa conter apenas 1% do arquivo. O que \nimpediria o hospedeiro A de passar dados continuamente para seu socket TCP à taxa de S bits/s: o controle de \nfluxo do TCP; o controle de congestionamento do TCP; ou alguma outra coisa? Elabore sua resposta.\n\t\nP44.\t Considere enviar um arquivo grande de um computador a outro por meio de uma conexão TCP em que não \nhaja perda.\na.\t Suponha que o TCP utilize AIMD para seu controle de congestionamento sem partida lenta. Admitindo \nque cwnd aumenta 1 MSS sempre que um lote de ACK é recebido e os tempos da viagem de ida e volta \nFigura 3.58  Tamanho da janela TCP em relação ao tempo\n0\n0\n2\n4\n6\n8\n10 12\nRodada de transmissão\n14 16 18 20 22 24 26\n5\n10\n15\n20\n25\nTamanho da janela de congestionamento\n(segmentos)\n30\n35\n40\n45\nCAMADA  de transporte  219 \nconstantes, quanto tempo leva para cwnd aumentar de 6 MSS para 12 MSS? (admitindo nenhum evento \nde perda)?\nb.\t Qual é a vazão média (em termos de MSS e RTT) para essa conexão sendo o tempo = 6 RTT?\n\t\nP45.\t Relembre a descrição macroscópica da vazão do TCP. No período de tempo transcorrido para a taxa da \nconexão variar de W/(2 ∙ RTT) a W/RTT, apenas um pacote é perdido (bem ao final do período).\na.\t Mostre que a taxa de perda (fração de pacotes perdidos) é igual a\nL = taxa de perda =\n1\n3\n8 W2 + 3\n4 W\nb.\t Use o resultado anterior para mostrar que, se uma conexão tiver taxa de perda L, sua largura de banda \nmédia é dada aproximadamente por:\n1,22\nMSS\nRTT √L\n\t\nP46.\t Considere que somente uma única conexão TCP (Reno) utiliza um enlace de 10 Mbits/s que não armazena \nnenhum dado. Suponha que esse enlace seja o único congestionado entre os hospedeiros emissor e receptor. \nAdmita que o emissor TCP tenha um arquivo enorme para enviar ao receptor e o buffer de recebimento do \nreceptor é muito maior do que a janela de congestionamento. Também fazemos as seguintes suposições: o \ntamanho de cada segmento TCP é 1.500 bytes; o atraso de propagação bidirecional dessa conexão é 150 ms; e \nessa conexão TCP está sempre na fase de prevenção de congestionamento, ou seja, ignore a partida lenta.\na.\t Qual é o tamanho máximo da janela (em segmentos) que a conexão TCP pode atingir?\nb.\t Qual é o tamanho médio da janela (em segmentos) e a vazão média (em bits/s) dessa conexão TCP?\nc.\t Quanto tempo essa conexão TCP leva para alcançar sua janela máxima novamente após se recuperar da \nperda de um pacote?\n\t\nP47.\t Considere o cenário descrito na questão anterior. Suponha que o enlace de 10 Mbits/s possa armazenar \num número finito de segmentos. Demonstre que para o enlace sempre enviar dados, teríamos que escolher \num tamanho de buffer que é, pelo menos, o produto da velocidade do enlace C e o atraso de propagação \nbidirecional entre o emissor e o receptor.\n\t\nP48.\t Repita a Questão 46, mas substituindo o enlace de 10 Mbits/s por um de 10 Gbits/s. Observe que em sua \nresposta ao item (c), verá que o tamanho da janela de congestionamento leva muito tempo para atingir seu \ntamanho máximo após se recuperar de uma perda de pacote. Elabore uma solução para resolver o problema.\n\t\nP49.\t Suponha que T (medido por RTT) seja o intervalo de tempo que uma conexão TCP leva para aumentar \nseu tamanho de janela de congestionamento de W/2 para W, sendo W o tamanho máximo da janela de \ncongestionamento. Demonstre que T é uma função da vazão média do TCP.\n\t\nP50.\t Considere um algoritmo AIMD do TCP simplificado, sendo o tamanho da janela de congestionamento \nmedido em número de segmentos e não em bytes. No aumento aditivo, o tamanho da janela de \ncongestionamento aumenta por um segmento em cada RTT. Na diminuição multiplicativa, o tamanho da \njanela de congestionamento diminui para metade (se o resultado não for um número inteiro, arredondar \npara o número inteiro mais próximo). Suponha que duas conexões TCP, C1 e C2 compartilhem um único \nenlace congestionado com 30 segmentos por segundo de velocidade. Admita que C1 e C2 estejam na fase de \nprevenção de congestionamento. O RTT da conexão C1 é de 100 ms e o da conexão C2 é de 200 ms. Suponha \nque quando a taxa de dados no enlace excede a velocidade do enlace, todas as conexões TCP sofrem perda de \nsegmento de dados.\na.\t Se C1 e C2 no tempo t0 possui uma janela de congestionamento de 10 segmentos, quais são seus tamanhos \nde janela de congestionamento após 1.000 ms?\n   Redes de computadores e a Internet\n220\nb.\t No final das contas, essas duas conexões obterão a mesma porção da largura de banda do enlace \ncongestionado? Explique.\n\t\nP51.\t Considere a rede descrita na questão anterior. Agora suponha que as duas conexões TCP, C1 e C2, possuam o \nmesmo RTT de 100 ms e que, no tempo t0, o tamanho da janela de congestionamento de C1 seja 15 segmentos, \ne que o de C2 seja 10 segmentos.\na.\t Quais são os tamanhos de suas janelas de congestionamento após 2.200 ms?\nb.\t No final das contas, essas duas conexões obterão a mesma porção da largura de banda do enlace \ncongestionado?\nc.\t Dizemos que duas conexões são sincronizadas se ambas atingirem o tamanho máximo e mínimo de janela \nao mesmo tempo. No final das contas, essas duas conexões serão sincronizadas? Se sim, quais os tamanhos \nmáximos de janela?\nd.\t Essa sincronização ajudará a melhorar a utilização do enlace compartilhado? Por quê? Elabore alguma \nideia para romper essa sincronização.\n\t\nP52.\t Considere uma modificação ao algoritmo de controle de congestionamento do TCP. Em vez do aumento \naditivo, podemos utilizar o aumento multiplicativo. Um emissor TCP aumenta seu tamanho de janela por \numa constante positiva pequena a (0 < a < 1) ao receber um ACK válido. Encontre a relação funcional \nentre a taxa de perda L e a janela máxima de congestionamento W. Para esse TCP modificado, demonstre, \nindependentemente da vazão média TCP, que uma conexão TCP sempre gasta a mesma quantidade de tempo \npara aumentar seu tamanho da janela de congestionamento de W/2 para W.\n\t\nP53.\t Quando discutimos TCPs futuros na Seção 3.7, observamos que, para conseguir uma vazão de 10 Gbits/s, \no TCP apenas poderia tolerar uma probabilidade de perda de segmentos de 2 ∙ 10-10 (ou, equivalentemente, \numa perda para cada 5 milhões de segmentos). Mostre a derivação dos valores para 2 ∙ 10-10 (ou 1: 5.000.000) \na partir dos valores de RTT e do MSS dados na Seção 3.7. Se o TCP precisasse suportar uma conexão de \n100 Gbits/s, qual seria a perda tolerável?\n\t\nP54.\t Quando discutimos controle de congestionamento em TCP na Seção 3.7, admitimos implicitamente que o \nremetente TCP sempre tinha dados para enviar. Agora considere o caso em que o remetente TCP envia uma \ngrande quantidade de dados e então fica ocioso em t1 (já que não há mais dados a enviar). O TCP permanecerá \nocioso por um período de tempo relativamente longo e então irá querer enviar mais dados em t2. Quais são \nas vantagens e desvantagens de o TCP utilizar os valores cwnd e ssthresh de t1 quando começar a enviar \ndados em t2? Que alternativa você recomendaria? Por quê?\n\t\nP55.\t Neste problema, verificamos se o UDP ou o TCP apresentam um grau de autenticação do ponto de chegada.\na.\t Considere um servidor que receba uma solicitação dentro de um pacote UDP e responda a essa solicitação \ndentro de um pacote UDP (por exemplo, como feito por um servidor DNS). Se um cliente com endereço \nIP X o engana com o endereço Y, para onde o servidor enviará sua resposta?\nb.\t Suponha que um servidor receba um SYN de endereço IP de origem Y, e depois de responder com um SYNACK, \nrecebe um ACK com o endereço IP de origem Y com o número de reconhecimento correto. Admitindo que o \nservidor escolha um número de sequência aleatório e que não haja um “man-in-the-middle”\n, o servidor pode \nter certeza de que o cliente realmente está em Y (e não em outro endereço X que está se passando por Y)?\n\t\nP56.\t Neste problema, consideramos o atraso apresentado pela fase de partida lenta do TCP. Considere um cliente \ne um servidor da Web diretamente conectados por um enlace de taxa R. Suponha que o cliente queira \nrestaurar um objeto cujo tamanho seja exatamente igual a 15 S, sendo S o tamanho máximo do segmento \n(MSS). Considere RTT o tempo de viagem de ida e volta entre o cliente e o servidor (admitindo que seja \nconstante). Ignorando os cabeçalhos do protocolo, determine o tempo para restaurar o objeto (incluindo o \nestabelecimento da conexão TCP) quando\na.\t 4 S/R > S/R + RTT > 2S/R \nb.\t S/R + RTT > 4 S/R \nc.\t S/R > RTT. \nCAMADA  de transporte  221 \nTarefa de programação\nImplementando um protocolo de transporte confiável\nNesta tarefa de programação de laboratório, você escreverá o código para a camada de transporte do reme-\ntente e do destinatário no caso da implementação de um protocolo simples de transferência confiável de dados. \nHá duas versões deste laboratório: a do protocolo de bit alternante e a do GBN. Essa tarefa será muito divertida, \njá que a sua realização não será muito diferente da que seria exigida em uma situação real.\nComo você provavelmente não tem máquinas autônomas (com um sistema operacional que possa modi-\nficar), seu código terá de rodar em um ambiente de hardware/software simulado. Contudo, a interface de pro-\ngramação fornecida a suas rotinas — isto é, o código que chamaria suas entidades de cima e de baixo — é muito \npróxima ao que é feito em um ambiente UNIX real. (Na verdade, as interfaces do software descritas nesta tarefa \nde programação são muito mais realistas do que os remetentes e destinatários de laço infinito descritos em muitos \nlivros.) A parada e o acionamento dos temporizadores também são simulados, e as interrupções do temporizador \nfarão que sua rotina de tratamento de temporização seja ativada.\nA tarefa completa de laboratório, assim como o código necessário para compilar seu próprio código, estão \ndisponíveis no site: http://sv.pearson.com.br.\nWireshark Lab: explorando o TCP\nNeste laboratório, você usará seu navegador para acessar um arquivo de um servidor Web. Como nos ante-\nriores, você usará Wireshark para capturar os pacotes que estão chegando ao seu computador. Mas, diferentemente \ndaqueles laboratórios, também poderá baixar, do mesmo servidor Web do qual baixou o arquivo, um relatório \n(trace) de pacotes que pode ser lido pelo Wireshark. Nesse relatório do servidor, você encontrará os pacotes que fo-\nram gerados pelo seu próprio acesso ao servidor Web. Você analisará os diagramas dos lados do cliente e do servidor \nde modo a explorar aspectos do TCP. Particularmente, fará uma avaliação do desempenho da conexão TCP entre \nseu computador e o servidor Web. Você analisará o comportamento da janela TCP e deduzirá comportamentos \nde perda de pacotes, de retransmissão, de controle de fluxo e de controle de congestionamento e do tempo de ida e \nvolta estimado.\nComo acontece com todos os Wireshark Labs, a descrição completa deste pode ser encontrada, em inglês, \nno site http://sv.pearson.com.br.\nWireshark Lab: explorando o UDP\nNeste pequeno laboratório, você realizará uma captura de pacote e uma análise de sua aplicação favorita que \nutiliza o UDP (por exemplo, o DNS ou uma aplicação multimídia, como o Skype). Como aprendemos na Seção \n3.3, o UDP é um protocolo de transporte simples. Neste laboratório, você examinará os campos do cabeçalho no \nsegmento UDP, assim como o cálculo da soma de verificação.\nComo acontece com todos os Wireshark Labs, a descrição completa deste pode ser encontrada, em inglês, \nno site http://sv.pearson.com.br.\n   Redes de computadores e a Internet\n222\nPor favor, descreva um ou dois dos projetos mais \ninteressantes em que você já trabalhou durante a \nsua carreira. Quais foram os maiores desafios?\nA escola nos ensina muitas maneiras de achar res-\npostas. Em cada problema interessante em que traba-\nlhei, o desafio tem sido achar a pergunta certa. Quando \nMike Karels e eu começamos a examinar o congestio-\nnamento do TCP, gastamos meses encarando o proto-\ncolo e os traces de pacotes, perguntando “por que ele \nestá falhando”? Um dia, no escritório de Mike, um de \nnós disse: “O motivo pelo qual não consigo descobrir \npor que ele falha é porque não entendo como ele che-\ngou a funcionar, para começar”\n. Aquela foi a pergunta \ncerta e nos forçou a compreender a “temporização dos \nreconhecimentos (ACK)” que faz o TCP funcionar. \nDepois disso, o resto foi fácil.\nDe modo geral, qual é o futuro que você imagina \npara as redes e a Internet?\nPara a maioria das pessoas, a Web é a Internet. O pes-\nsoal que trabalha com redes sorri educadamente, pois \nsabemos que a Web é uma aplicação rodando sobre a \nInternet, mas, e se eles estiverem certos? A Internet tra-\nta de permitir conversações entre pares de hospedeiros. \nA Web trata da produção e consumo de informações \ndistribuídas. “Propagação de informações” é uma visão \nmuito geral da comunicação, da qual a “conversa em \npares” é um minúsculo subconjunto. Precisamos pensar \nalém do que vemos. As redes de hoje lidam com a mídia \nde broadcast (rádios, PONs etc.) fingindo que ela é um \nfio de ponto a ponto. Isso é tremendamente ineficaz. Te-\nrabits de dados por segundo estão sendo trocados pelo \nmundo inteiro por meio de pendrives ou smartphones, \nmas não sabemos como tratar isso como “rede”\n. Os ISPs \nestão ocupados montando caches e CDNs para distri-\nbuir vídeo e áudio de modo a facilitar a expansão. O \ncaching é uma parte necessária da solução, mas não há \numa parte das redes de hoje — desde Informações, Enfi-\nleiramento ou Teoria de Tráfego até as especificações de \nprotocolos da Internet — que nos diga como projetá-lo \ne distribuí-lo. Acho e espero que, nos próximos anos, \nas redes evoluam para abranger a visão muito maior de \ncomunicação, que é a base da Web.\nQue pessoas o inspiraram profissionalmente?\nQuando eu cursava a faculdade, Richard Feynman \nnos visitou e deu um seminário acadêmico. Ele falou \nsobre uma parte da teoria quântica que eu estava lu-\ntando para entender durante todo o semestre, e sua \nexplicação foi tão simples e lúcida que aquilo que pa-\nrecia um lixo sem sentido para mim tornou-se óbvio e \ninevitável. Essa capacidade de ver e transmitir a sim-\nplicidade que está por trás do nosso mundo complexo \nme parece uma dádiva rara e maravilhosa.\nQuais são suas recomendações para estudantes \nque desejam seguir carreira em computação e \ntecnologia da informação?\nEste é um campo maravilhoso — computadores e \nredes provavelmente tiveram mais impacto sobre a so-\nciedade do que qualquer invenção desde a imprensa. \nRedes conectam coisas, e seu estudo o ajuda a fazer co-\nnexões intelectuais: a busca de alimento das formigas e \nas danças das abelhas demonstram o projeto de proto-\nVan Jacobson\nVan Jacobson é Research Fellow no PARC. Antes disso, foi cofundador e cientis-\nta chefe da Packet Design. Antes ainda, foi cientista chefe na Cisco. Antes de entrar \npara a Cisco, chefiou o Network Research Group no Lawrence Berkeley National \nLaboratory e lecionou na Universidade da Califórnia em Berkeley e Stanford. Van \nrecebeu o prêmio ACM SIGCOMM em 2001 pela destacada contribuição de toda \numa vida para o campo de redes de comunicação e o prêmio IEEE Kobayashi em \n2002 por “contribuir para o conhecimento do congestionamento de redes e por de-\nsenvolver mecanismos de controle de congestionamento de rede que permitiram a \nescalada bem-sucedida da Internet”. Em 2004, foi eleito para a Academia Nacional \nde Engenharia dos Estados Unidos.\nENTREVISTA\nCAMADA  de transporte  223 \ncolo melhor do que RFCs, engarrafamentos de trânsito \nou pessoas saindo de um estádio lotado são a essência \ndo congestionamento, e motoristas procurando o me-\nlhor caminho de volta para casa após uma tempestade \nque alagou a cidade constituem o núcleo do roteamen-\nto dinâmico. Se você estiver interessado em obter muito \nmaterial e quiser causar impacto, é difícil imaginar um \ncampo melhor do que este.\nVimos no capítulo anterior que a camada de transporte oferece várias formas de comunicação processo \na processo com base no serviço de comunicação entre hospedeiros da camada de rede. Vimos também que \na camada de transporte faz isso sem saber como a camada de rede implementa esse serviço. Portanto, é bem \npossível que agora você esteja imaginando o que está por baixo do serviço de comunicação hospedeiro a hos-\npedeiro; o que o faz funcionar.\nNeste capítulo estudaremos exatamente como a camada de rede executa o serviço de comunicação hospe-\ndeiro a hospedeiro. Veremos que há um pedaço da camada de rede em cada hospedeiro e roteador na rede, o que \nnão acontece com as camadas de transporte e de aplicação. Por causa disso, os protocolos de camada de rede estão \nentre os mais desafiadores (e, portanto, os mais interessantes!) da pilha de protocolos.\nA camada de rede é, também, uma das mais complexas da pilha de protocolos e, assim, temos um longo \ncaminho a percorrer. Iniciaremos nosso estudo com uma visão geral da camada de rede e dos serviços que ela \npode prover. Em seguida, examinaremos novamente as duas abordagens da estruturação da entrega de pacotes \nde camada de rede — o modelo de datagramas e o modelo de circuitos virtuais — que vimos pela primeira vez \nno Capítulo 1 e veremos o papel fundamental que o endereçamento desempenha na entrega de um pacote a seu \nhospedeiro de destino.\nFaremos, neste capítulo, uma distinção importante entre as funções de repasse e roteamento da ca-\nmada de rede. Repasse envolve a transferência de um pacote de um enlace de entrada para um enlace de \nsaída dentro de um único roteador. Roteamento envolve todos os roteadores de uma rede, cujas interações \ncoletivas por meio do protocolo de roteamento determinam os caminhos que os pacotes percorrem em suas \nviagens do nó de origem ao de destino. Essa será uma distinção importante para manter em mente ao ler \neste capítulo.\nPara aprofundar nosso conhecimento do repasse de pacotes, examinaremos o “interior” de um roteador — \n \na organização e a arquitetura de seu hardware. Então, estudaremos o repasse de pacotes na Internet, junto \ncom o famoso Protocolo da Internet (IP). Investigaremos o endereçamento na camada de rede e o formato de \ndatagrama IPv4. Em seguida, estudaremos a tradução de endereço de rede (Network Address Translation — \n \nNAT), a fragmentação de datagrama, o Protocolo de Mensagem de Controle da Internet (Internet Control \nMessage Protocol — ICMP) e IPv6.\nEntão voltaremos nossa atenção à função de roteamento da camada de rede. Veremos que o trabalho de um \nalgoritmo de roteamento é determinar bons caminhos (ou rotas) entre remetentes e destinatários. Em primeiro \nlugar, estudaremos a teoria de algoritmos de roteamento concentrando nossa atenção nas duas classes mais pre-\nA camada de\nREDE\n1\n3\n5 6\n8 9\n7\nc\n \na\n \np\n \ní\n \nt\n \nu\n \nl\n \no\n2 4\nA CAMADA  de REDE  225 \ndominantes: algoritmos de estado de enlace e de vetor de distâncias. Visto que a complexidade dos algoritmos de \nroteamento cresce consideravelmente com o aumento do número de roteadores na rede, também será interes-\nsante abordar o roteamento hierárquico. Em seguida veremos como a teoria é posta em prática quando falarmos \nsobre os protocolos de roteamento de intrassistemas autônomos da Internet (RIP, OSPF e IS-IS) e seu protocolo \nde roteamento de intersistemas autônomos, o BGP. Encerraremos este capítulo com uma discussão sobre rotea-\nmento por difusão (broadcast) e para um grupo (multicast).*1\nEm resumo, este capítulo tem três partes importantes. A primeira, seções 4.1 e 4.2, aborda funções e servi-\nços de camada de rede. A segunda, seções 4.3 e 4.4, examina o repasse. Por fim, a terceira parte, seções 4.5 a 4.7, \nestuda o roteamento.\n4.1  Introdução\nA Figura 4.1 mostra uma rede simples com dois hospedeiros, H1 e H2, e diversos roteadores no caminho \nentre H1 e H2. Suponha que H1 esteja enviando informações a H2 e considere o papel da camada de rede nesses \nhospedeiros e nos roteadores intervenientes. A camada de rede em H1 pegará segmentos da camada de transpor-\nte em H1, encapsulará cada segmento em um datagrama (isto é, em um pacote de camada de rede) e então dará \ninício à jornada dos datagramas até seu destino, isto é, ela os enviará para seu roteador vizinho, R1. No hospedei-\nro receptor (H2), a camada de rede receberá os datagramas de seu roteador vizinho R2, extrairá os segmentos de \ncamada de transporte e os entregará à camada de transporte em H2. O papel primordial dos roteadores é repassar \ndatagramas de enlaces de entrada para enlaces de saída. Note que os da Figura 4.1 são mostrados com a pilha de \nprotocolos truncada, isto é, sem as camadas superiores acima da camada de rede, porque (exceto para finalidades \nde controle) roteadores não rodam protocolos de camada de transporte e de aplicação como os que examinamos \nnos Capítulos 2 e 3.\n4.1.1  Repasse e roteamento\nAssim, o papel da camada de rede é aparentemente simples — transportar pacotes de um hospedeiro re-\nmetente a um hospedeiro destinatário. Para fazê-lo, duas importantes funções da camada de rede podem ser \nidentificadas:\n• Repasse. Quando um pacote chega ao enlace de entrada de um roteador, este deve conduzi-lo até o enlace \nde saída apropriado. Por exemplo, um pacote proveniente do hospedeiro H1 que chega ao roteador R1 \ndeve ser repassado ao roteador seguinte por um caminho até H2. Na Seção 4.3 examinaremos o interior \nde um roteador e investigaremos como um pacote é realmente repassado de um enlace de entrada de um \nroteador até um enlace de saída.\n• Roteamento. A camada de rede deve determinar a rota ou o caminho tomado pelos pacotes ao fluírem de \num remetente a um destinatário. Os algoritmos que calculam esses caminhos são denominados algorit-\nmos de roteamento. Um algoritmo de roteamento determinaria, por exemplo, o caminho pelo qual os \npacotes fluiriam de H1 para H2.\nOs termos repasse e roteamento são usados indistintamente por autores que estudam a ­\ncamada de rede. \nNeste livro, usaremos tais termos com maior exatidão. Repasse refere-se à ação local realizada por um roteador \npara transferir um pacote da interface de um enlace de ­\nentrada para a interface de enlace de saída apropriada. \nRoteamento refere-se ao processo de âmbito ­\ngeral da rede que determina os caminhos fim a fim que os pacotes \npercorrem desde a origem até o destino. Para usar uma viagem como analogia, voltemos àquele nosso viajante da \n*\t Não há uma tradução única e conclusiva sobre estes termos. Muitos textos não os traduzem. Aqui, procuramos usar a tradução que mais trans-\nmita seu sentido. Assim, usamos endereço/enlace/roteamento/transmissão/rede “por difusão” (broadcast), “para um grupo” (multicast), “indivi-\ndual” (unicast) e “para um membro do grupo” (anycast). (N. do T.)\n   Redes de computadores e a Internet\n226\nSeção 1.3.1, que vai da Pensilvânia à Flórida. Durante a viagem, nosso motorista passa por muitos cruzamentos de \nrodovias em sua rota. Podemos imaginar o repasse como o processo de passar por um único cruzamento: um car-\nro chega ao cruzamento vindo de uma rodovia e determina qual ­\nrodovia ele deve pegar para sair do cruzamento. \nPodemos imaginar o roteamento como o processo de planejamento da viagem da Pensilvânia até a Flórida: antes \nde partir, o motorista consultou um mapa e escolheu um dos muitos caminhos possíveis. Cada um deles consiste \nem uma série de trechos de rodovias conectados por cruzamentos.\nCada roteador tem uma tabela de repasse. Um roteador repassa um pacote examinando o valor de um campo \nno cabeçalho do pacote que está chegando e então utiliza esse valor para indexar sua tabela de repasse. O resultado \nda tabela de repasse indica para qual das interfaces de enlace do roteador o pacote deve ser repassado. Dependendo \ndo protocolo de camada de rede, o valor no cabeçalho do pacote pode ser o endereço de destino do pacote ou uma \nFigura 4.1  A camada de rede\nRede móvel\nRoteador R1\nRoteador R2\nISP nacional\nou global\nISP local\nou regional\nRede corporativa\nRede doméstica\nSistema ﬁnal H1\nEnlace \nFísica\nAplicação\nTransporte\nRede\nSistema ﬁnal H2\nEnlace \nFísica\nAplicação\nTransporte\nRede\nEnlace\nFísica\nRede\nEnlace\nFísica\nRede\nEnlace\nFísica\nRede\nEnlace\nFísica\nRede\nKR 04.01.eps\nAW/Kurose and Ross\nComputer Networking, 6/e\nsize:  35p9  x  38p5\n10/13/11, 10/31/11\n11/21/11 rossi \nEnlace \nFísica\nRede\nA CAMADA  de REDE  227 \nindicação da conexão à qual ele pertence. A Figura 4.2 dá um exemplo. Nessa figura, um pacote cujo valor no campo \nde cabeçalho é 0111 chega a um roteador. Este o indexa em sua tabela de repasse, determina que a interface de enlace \nde saída para o pacote é a interface 2 e, então, o repassa internamente à interface 2. Na Seção 4.3 examinaremos o \ninterior de um roteador e estudaremos a função de repasse muito mais detalhadamente.\nAgora você deve estar imaginando como são configuradas as tabelas de repasse nos roteadores. Essa é uma \nquestão crucial, que expõe a importante interação entre roteamento e repasse. Como ilustrado na Figura 4.2, \no algoritmo de roteamento determina os valores que são inseridos nas tabelas de repasse dos roteadores. Esse \nalgoritmo pode ser centralizado (por exemplo, com um algoritmo que roda em um local central e descarrega \ninformações de ­\nroteamento a cada um dos roteadores) ou descentralizado (isto é, com um pedaço do algoritmo \nde roteamento distribuído funcionando em cada roteador). Em qualquer dos casos, um ­\nroteador recebe mensa-\ngens de protocolo de roteamento que são utilizadas para configurar sua tabela de repasse. As finalidades distintas \ne diferentes das funções de repasse e roteamento podem ser mais bem esclarecidas considerando o caso hipoté-\ntico (e não realista, mas tecnicamente viável) de uma rede na qual todas as tabelas de repasse são configuradas \ndiretamente por operadores de rede humanos, fisicamente presentes nos roteadores. Nesse caso, não seria preciso \nnenhum protocolo de roteamento! É claro que os operadores humanos precisariam interagir uns com os outros \npara garantir que as tabelas fossem configuradas de tal modo que os pacotes chegassem a seus destinos pretendi-\ndos. Também é provável que uma configuração humana seria mais propensa a erro e muito mais lenta do que um \nprotocolo de roteamento para reagir a mudanças na topologia da rede. Portanto, sorte nossa que todas as redes \ntêm uma função de repasse e também uma função de roteamento!\nEnquanto estamos no tópico da terminologia, é interessante mencionar dois outros termos que tam-\nbém são utilizados indistintamente, mas que usaremos com maior cuidado. Reservaremos o termo comutador \nde pacotes para designar um dispositivo geral de comutação de pacotes que transfere um pacote de interface \nde enlace de entrada para interface de enlace de saída conforme o valor que está em um campo no cabeça-\nlho do pacote. Alguns comutadores de pacotes, denominados comutadores de camada de enlace (que vere-\nFigura 4.2  Algoritmos de roteamento determinam valores em tabelas de repasse\nValor no cabeçalho do\npacote que está chegando\n1\n2\n3\nAlgoritmo de roteamento\nTabela de repasse local\nvalor do\ncabeçalho\n0100\n0101\n0111\n1001\n0111\n3\n2\n2\n1\nenlace\nde saída\n   Redes de computadores e a Internet\n228\nmos no Capítulo 5), baseiam a decisão de repasse no valor que está no campo da camada de enlace. Outros, \n \ndenominados roteadores, baseiam sua decisão de repasse no valor que está no campo de camada de rede. Os \nroteadores, portanto, são dispositivos da camada de rede (camada 3), mas também devem utilizar  protocolos da \ncamada 2, pois os dispositivos da camada 3 exigem os serviços da camada 2 para implementar sua funcionalidade \n(camada 3). (Para dar real valor a essa importante distinção, seria interessante você ler novamente a Seção 1.5.2, \nem que discutimos datagramas de camada de rede e quadros de camada de enlace e as relações entre eles.) Visto \nque o foco deste capítulo é a camada de rede, usaremos o termo roteador no lugar de ­\ncomutador de pacotes. Usa-\nremos o termo roteador até mesmo quando falarmos sobre comutadores de pacotes em redes de circuitos virtuais \n(que discutiremos em breve).\nEstabelecimento de conexão\nAcabamos de dizer que a camada de rede tem duas funções importantes, repasse e roteamento. Mas logo \nveremos que em algumas redes de computadores há uma terceira função importante, a saber, o estabelecimento \nde conexão. Lembre-se de que, quando estudamos o TCP, verificamos que é necessária uma apresentação de três \nvias antes de os dados realmente poderem fluir do remetente ao destinatário. Isso permite que o remetente e o des-\ntinatário estabeleçam a informação de estado necessária (por exemplo, número de sequência e tamanho inicial da \njanela de controle de fluxo). De modo semelhante, algumas arquiteturas de camada de rede — por exemplo, ATM, \nframe-relay e MPLS (que estudaremos na Seção 5.8) — exigem que roteadores ao longo do caminho escolhido des-\nde a origem até o destino troquem mensagens entre si com a finalidade de estabelecer estado antes que pacotes de \ndados de camada de rede dentro de uma dada conexão origem-destino possam começar a fluir. Na camada de rede, \nesse processo é denominado estabelecimento de conexão. Examinaremos estabelecimento de conexão na Seção 4.2.\n4.1.2  Modelos de serviço de rede\nAntes de examinar a camada de rede, vamos tomar uma perspectiva mais ampla e considerar os diferentes \ntipos de serviço que poderiam ser oferecidos por ela. Quando a camada de transporte em um hospedeiro remetente \ntransmite um pacote para dentro da rede (isto é, passa o pacote para a camada de rede do hospedeiro remetente), \nela pode contar com a camada de rede para entregar o pacote no destino? Quando são enviados vários pacotes, eles \nserão entregues à camada de transporte no hospedeiro destinatário na ordem em que foram enviados? A quantidade \nde tempo decorrido entre duas transmissões de pacotes sequenciais será a mesma quantidade de tempo decorrido \nentre suas recepções? A rede fornecerá algum tipo de informação sobre congestionamento na rede? Qual é o modelo \n(propriedades) abstrato do canal que conecta a camada de transporte nos hospedeiros remetente e destinatário? As \nrespostas a essas e a outras perguntas são determinadas pelo modelo de serviço oferecido pela camada de rede. O \nmodelo de serviço de rede define as características do transporte de dados fim a fim entre uma borda da rede e a \noutra, isto é, entre sistemas finais remetente e destinatário.\nVamos considerar agora alguns serviços possíveis que a camada de rede poderia prover. No hospedeiro \nremetente, quando a camada de transporte passa um pacote para a camada de rede, alguns serviços específicos \nque poderiam ser oferecidos são:\n• Entrega garantida. Esse serviço assegura que o pacote mais cedo ou mais tarde chegará a seu destino.\n• Entrega garantida com atraso limitado. Não somente assegura a entrega de um pacote, mas também a en-\ntrega com um atraso hospedeiro a hospedeiro limitado e especificado (por exemplo, dentro de 100 ms).\nAlém disso, há outros serviços que podem ser providos a um fluxo de pacotes entre uma origem e um des-\ntino determinados, como os seguintes:\n• Entrega de pacotes na ordem. Garante que pacotes chegarão ao destino na ordem em que foram enviados.\n• Largura de banda mínima garantida. Esse serviço de camada de rede emula o comportamento de um \nenlace de transmissão com uma taxa de bits especificada (por exemplo, 1 bit/s) entre hospedeiros reme-\nA CAMADA  de REDE  229 \ntentes e destinatários. Contanto que o hospedeiro remetente transmita bits (como parte de pacotes) a \numa taxa abaixo da taxa de bits especificada, nenhum pacote será perdido e cada um chegará dentro de \num atraso hospedeiro a hospedeiro previamente especificado (por exemplo, dentro de 40 ms).\n• Jitter máximo garantido. Assegura que a quantidade de tempo entre a transmissão de dois pacotes suces-\nsivos no remetente seja igual à quantidade de tempo entre o recebimento dos dois pacotes no destino (ou \nque esse espaçamento não mude mais do que algum valor especificado).\n• Serviços de segurança. Utilizando uma chave de sessão secreta conhecida somente por um hospedeiro \nde origem e de destino, a camada de rede no computador de origem pode codificar a carga útil de todos \nos datagramas que estão sendo enviados ao computador de destino. A camada de rede no computador \nde destino, então, seria responsável por decodificar as cargas úteis. Com esse serviço, o sigilo seria \nfornecido para todos os segmentos da camada de transporte (TCP e UDP) entre os computadores de \norigem e de destino. Além do sigilo, a camada de rede poderia prover integridade dos dados e serviços \nde autenticação na origem.\nEssa é uma lista apenas parcial de serviços que uma camada de rede poderia prover — há incontáveis va-\nriações possíveis.\nA camada de rede da Internet fornece um único modelo de serviço, conhecido como serviço de melhor \nesforço. Consultando a Tabela 4.1, pode parecer que serviço de melhor esforço seja um eufemismo para absolu-\ntamente nenhum serviço. Com o serviço de melhor esforço, não há garantia de que a temporização entre pacotes \nseja preservada, não há garantia de que os pacotes sejam recebidos na ordem em que foram enviados e não há \ngarantia da entrega final dos pacotes transmitidos. Dada essa definição, uma rede que não entregasse nenhum \npacote ao destinatário satisfaria a definição de serviço de entrega de melhor esforço. Contudo, como discutire-\nmos adiante, há razões sólidas para esse modelo minimalista de serviço de camada de rede.\nOutras arquiteturas de rede definiram e puseram em prática modelos de serviço que vão além do serviço \nde melhor esforço da Internet. Por exemplo, a arquitetura de rede ATM [MFA Forum 2012; Black, 1995] habilita \nvários modelos de serviço, o que significa que, dentro da mesma rede, podem ser oferecidas conexões diferentes \ncom classes de serviço diferentes. Discutir o modo como uma rede ATM oferece esses serviços vai muito além \ndo escopo deste livro; nossa meta aqui é apenas salientar que existem alternativas ao modelo de melhor esforço \nda Internet. Dois dos modelos mais importantes de serviço ATM são os serviços de taxa constante e de taxa dis-\nponível.\n• Serviço de rede de taxa constante de bits (Constant Bit Rate — CBR). Foi o primeiro modelo de serviço \nATM a ser padronizado, refletindo o interesse imediato das empresas de telefonia por esse serviço e a \nadequação do serviço CBR para transmitir tráfego de áudio e vídeo de taxa constante de bits. O objetivo \né conceitualmente simples: prover um fluxo de pacotes (conhecidos como células na terminologia ATM) \ncom uma tubulação virtual cujas propriedades são iguais às de um hipotético enlace de transmissão \ndedicado de largura de banda fixa entre os hospedeiros remetente e destinatário. Com serviço CBR, um \nfluxo de células ATM é carregado através da rede de modo tal que garanta que o atraso fim a fim de uma \nTabela 4.1  Modelos de serviço das redes Internet, ATM CBR e ATM ABR\nArquitetura \nda rede\nModelo de \nserviço\nGarantia de largura \nde banda\nGarantia contra \nperda\nOrdenação\nTemporização\nIndicação de \ncongestionamento\nInternet\nMelhor \nesforço\nNenhuma\nNenhuma\nQualquer ordem \npossível\nNão mantida\nNenhuma\nATM\nCBR\nTaxa constante \ngarantida\nSim\nNa ordem\nMantida\nNão haverá \ncongestionamento\nATM\nABR\nMínima garantida\nNenhuma\nNa ordem\nNão mantida\nIndicação de \ncongestionamento\n   Redes de computadores e a Internet\n230\ncélula, a variabilidade do atraso (isto é, o jitter) e a fração de células perdidas ou entregues atrasadas \nsejam menores do que valores especificados. Tais valores são acertados entre o hospedeiro remetente e a \nrede ATM quando a conexão CBR é estabelecida pela primeira vez.\n• Serviço de rede de taxa de bits disponível (Available Bit Rate — ABR). Como o serviço oferecido pela \nInternet é “de melhor esforço”\n, o ATM ABR pode ser caracterizado como um “serviço de melhor esforço \nligeiramente melhorado”\n. Como acontece com o modelo de serviço da Internet, também com o serviço \nABR pode haver perda de células. Mas, ao contrário da Internet, as células não podem ser reordenadas \n(embora possam ser perdidas) e é garantida uma taxa mínima de transmissão de células (minimum cell \ntransmission rate — MCR) para uma conexão que está usando o serviço ABR. Se, contudo, a rede dispu-\nser de suficientes recursos livres em dado momento, um remetente também poderá enviar células com \nsucesso a uma taxa mais alta do que a MCR. Além disso, como vimos na Seção 3.6, o serviço ATM ABR \npode prover realimentação ao remetente (em termos de um bit de notificação de congestionamento ou \nde uma taxa de envio explícita), que controla o modo como o remetente ajusta sua taxa entre a MCR e \numa taxa de pico admissível.\n4.2  Redes de circuitos virtuais e de datagramas\nLembre-se de que, no Capítulo 3, dissemos que a camada de transporte pode oferecer às aplicações serviço \nnão orientado para conexão ou serviço orientado para conexão. Por exemplo, a camada de transporte da Inter-\nnet oferece a cada aplicação uma alternativa entre dois serviços: UDP, um não orientado para conexão; ou TCP, \norientado para conexão. De modo semelhante, uma camada de rede também pode oferecer qualquer dos dois. \nServiços de ­\ncamada de rede orientados para conexão e não orientados para conexão são, em muitos aspectos, \nsemelhantes a esses mesmos serviços providos pela camada de transporte. Por exemplo, um serviço de camada de \nrede orientado para conexão começa com uma apresentação entre os hospedeiros de origem e de destino; e um \nserviço de camada de rede não orientado para conexão não tem nenhuma apresentação preliminar.\nEmbora os serviços de camada de rede orientados para conexão e não orientados para conexão tenham \nalgumas semelhanças com os mesmos serviços oferecidos pela camada de transporte, há diferenças cruciais:\n• Na camada de rede, são serviços de hospedeiro a hospedeiro providos pela camada de rede à camada \nde transporte. Na camada de transporte, são serviços de processo a processo fornecidos pela camada de \ntransporte à camada de aplicação.\n• Em todas as arquiteturas importantes de redes de computadores existentes até agora (Internet, ATM, \nframe relay e assim por diante), a camada de rede oferece um serviço entre hospedeiros não orientado \npara conexão, ou um serviço entre hospedeiros orientado para conexão, mas não ambos. Redes de com-\nputadores que oferecem apenas um serviço orientado para conexão na camada de rede são denominadas \nredes de circuitos virtuais (redes CV); redes de computadores que oferecem apenas um serviço não \norientado para conexão na camada de rede são denominadas redes de datagramas.\n• As execuções de serviço orientado para conexão na camada de transporte e de serviço de conexão na ca-\nmada de rede são fundamentalmente diferentes. No capítulo anterior vimos que o serviço de camada de \ntransporte orientado para conexão é executado na borda da rede nos sistemas finais; em breve veremos \nque o serviço da camada de rede orientado para conexão é realizado nos roteadores no núcleo da rede, \nbem como nos sistemas finais.\nRedes de circuitos virtuais e redes de datagramas são duas classes fundamentais de redes de computadores. \nElas utilizam informações muito diferentes para tomar suas decisões de repasse. Vamos agora examinar suas \nimplementações mais de perto.\nA CAMADA  de REDE  231 \n4.2.1  Redes de circuitos virtuais\nEmbora a Internet seja uma rede de datagramas, muitas arquiteturas de rede alternativas — entre elas as \ndas redes ATM e frame relay — são redes de circuitos virtuais e, portanto, usam conexões na camada de rede. \nEssas conexões de camada de rede são denominadas circuitos virtuais (CVs). Vamos considerar agora como um \nserviço de CVs pode ser implantado em uma rede de computadores.\nUm circuito virtual (CV) consiste em (1) um caminho (isto é, uma série de enlaces e roteadores) entre \nhospedeiros de origem e de destino, (2) números de CVs, um número para cada enlace ao longo do caminho e \n(3) registros na tabela de repasse em cada roteador ao longo do caminho. Um pacote que pertence a um circuito \nvirtual portará um número de CV em seu cabeçalho. Como um circuito virtual pode ter um número de CV di-\nferente em cada enlace, cada roteador interveniente deve substituir esse número de cada pacote em trânsito por \num novo número. Esse número novo do CV é obtido da tabela de repasse.\nPara ilustrar o conceito, considere a rede mostrada na Figura 4.3. Os números ao lado dos enlaces de R1 na \nFigura 4.3 são os das interfaces de enlaces. Suponha agora que o hospedeiro A solicite à rede que estabeleça um CV \nentre ele e o hospedeiro B. Suponha ainda que a rede escolha o caminho A–R1–R2–B e atribua números de CV 12, \n22, 32 aos três enlaces no caminho para o circuito virtual. Nesse caso, quando um pacote nesse CV sai do hospe-\ndeiro A, o valor no campo do número de CV é 12; quando sai de R1, o valor é 22, e, quando sai de R2, o valor é 32.\nComo o roteador determina o novo número de CV para um pacote que passa por ele? Para uma rede de CV, \na tabela de repasse de cada roteador inclui a tradução de número de CV; por exemplo, a tabela de repasse de R1 \npode ser algo parecido com o seguinte:\nInterface de entrada\nNo do CV de entrada\nInterface de saída\nNo do CV de saída\n1\n12\n2\n22\n2\n63\n1\n18\n3\n7\n2\n17\n1\n97\n3\n87\n...\n...\n...\n...\nSempre que um novo CV é estabelecido através de um roteador, um registro é adicionado à tabela de repas-\nse. De maneira semelhante, sempre que um CV termina, são removidos os registros apropriados em cada tabela \nao longo de seu caminho.\nÉ bem provável que você esteja pensando por que um pacote não conserva o mesmo número de CV em \ncada um dos enlaces ao longo de sua rota. Isso ocorre por dois motivos: primeiro, substituir o número de enlace \nem enlace reduz o comprimento do campo do CV no cabeçalho do pacote. Segundo, e mais importante, o esta-\nFigura 4.3  Uma rede de circuitos virtuais simples\nR1\nR2\nA\nB\n1\n2\n3\n1\n2\n3\nR3\nR4\n   Redes de computadores e a Internet\n232\nbelecimento de um CV é consideravelmente simplificado se for permitido um número diferente de CV em cada \nenlace no caminho do circuito virtual. De modo específico, com vários números de CV, cada enlace do caminho \npode escolher um número de CV independente daqueles escolhidos em outros enlaces do caminho. Se fosse \nexigido um mesmo número de CV para todos os enlaces, os roteadores teriam de trocar e processar um número \nsubstancial de mensagens para escolher o número de CV a ser usado para uma conexão (por exemplo, um núme-\nro que não está sendo usado por nenhum outro CV nesses roteadores).\nEm uma rede de circuitos virtuais, os roteadores da rede devem manter informação de estado de conexão \npara as conexões em curso. Especificamente, cada vez que uma nova conexão for estabelecida através de um rotea-\ndor, um novo registro de conexão deve ser adicionado à tabela de repasse do roteador. E, sempre que uma conexão \nfor desativada, um registro deve ser removido da tabela. Observe que, mesmo que não haja tradução de números de \nCVs, ainda assim é necessário manter informação de estado de conexão que associe números de CVs com números \ndas interfaces de saída. A questão de um roteador manter ou não informação de estado de conexão para cada cone-\nxão em curso é crucial — e retornaremos a ela várias vezes neste livro.\nHá três fases que podem ser identificadas em um circuito virtual:\n• Estabelecimento de CV. Durante a fase de estabelecimento, a camada de transporte ­\nremetente contata \na camada de rede, especifica o endereço do receptor e espera até a rede estabelecer o CV. A camada de \nrede determina o caminho entre remetente e destinatário, ou seja, a série de enlaces e roteadores pelos \nquais todos os pacotes do CV trafegarão. A camada de rede também determina o número de CV para \ncada enlace ao longo do caminho e, por fim, adiciona um registro na tabela de repasse em cada rotea-\ndor no caminho. Durante o estabelecimento do CV, a camada de rede pode também reservar recursos \n(por exemplo, largura de banda)  no caminho.\n• Transferência de dados. Como mostra a Figura 4.4, tão logo estabelecido o CV, pacotes podem começar \na fluir ao longo dele.\n• Encerramento do CV. O encerramento começa quando o remetente (ou o destinatário) informa à camada \nde rede seu desejo de desativar o CV. A camada de rede então informará o sistema final do outro lado da \nrede do término de conexão e atualizará as tabelas de repasse em cada um dos roteadores de pacotes no \ncaminho para indicar que o CV não existe mais.\nHá uma distinção sutil, mas importante, entre estabelecimento de CV na camada de rede e estabelecimento \nde conexão na camada de transporte (por exemplo, a apresentação TCP de três vias que estudamos no Capítulo \n3). Estabelecer conexão na camada de transporte envolve apenas os dois sistemas finais. Durante o estabeleci-\nmento da conexão na camada de transporte, os dois sistemas finais determinam os parâmetros (por exemplo, \nnúmero de sequência inicial e tamanho da janela de controle de fluxo) de sua conexão de camada de transporte. \nEmbora os dois sistemas finais fiquem cientes da conexão de camada de transporte, os roteadores dentro da rede \nFigura 4.4  Estabelecimento de circuito virtual\nTransporte\nEnlace\nFísica\nAplicação\nRede\nTransporte\nEnlace\nFísica\nAplicação\nRede\n1. Inicia chamada\n2. Chamada chegando\n5. Começo do ﬂuxo\n    de dados\n   \n6. Recebimento\n    de dados\n4. Chamada conectada\n3. Chamada aceita\nA CAMADA  de REDE  233 \nficam completamente alheios a ela. Por outro lado, com uma camada de rede de CV, os roteadores do caminho \nentre os dois sistemas finais estão envolvidos no estabelecimento de CV e cada roteador fica totalmente ciente de \ntodos os CVs que passam por ele.\nAs mensagens que os sistemas finais enviam à rede para iniciar ou encerrar um CV e aquelas passadas \nentre os roteadores para estabelecer o CV (isto é, modificar estado de conexão em tabelas de roteadores) são \nconhecidas como mensagens de sinalização e os protocolos usados para trocá-las costumam ser denomi-\nnados protocolos de sinalização. O estabelecimento de CV está ilustrado na Figura 4.4. Não abordaremos \nprotocolos de sinalização de CVs neste livro; Black [1997] apresenta uma discussão geral sobre sinalização \nem redes orientadas para conexão e ITU-T Q.2931 [1994] mostra a especificação do protocolo de sinalização \nQ.2931 do ATM.\n4.2.2  Redes de datagramas\nEm uma rede de datagramas, toda vez que um sistema final quer enviar um pacote, ele marca o pacote com \no endereço do sistema final de destino e então o envia para dentro da rede. Como mostra a Figura 4.5, isso é feito \nsem o estabelecimento de nenhum CV. Roteadores em uma rede de datagramas não mantêm nenhuma informa-\nção de estado sobre CVs (porque não há nenhum!).\nAo ser transmitido da origem ao destino, um pacote passa por uma série de roteadores. Cada um desses \nroteadores usa o endereço de destino do pacote para repassá-lo. Especificamente, cada roteador tem uma tabela \nde repasse que mapeia endereços de destino para interfaces de enlaces; quando um pacote chega ao roteador, este \nusa o endereço de destino do pacote para procurar a interface de enlace de saída apropriada na tabela de repasse. \nEntão, o roteador transmite o pacote para aquela interface de enlace de saída.\nPara entender melhor a operação de consulta, vamos examinar um exemplo específico. Suponha que todos \nos endereços de destino tenham 32 bits (que, por acaso, é exatamente o comprimento do endereço de destino em \num datagrama IP). Uma execução de força bruta da tabela de repasse teria um registro para cada endereço de des-\ntino possível. Como há mais de quatro bilhões de endereços possíveis, essa opção está totalmente fora de questão.\nVamos supor ainda que nosso roteador tenha quatro enlaces numerados de 0 a 3, e que os pacotes devem \nser repassados para as interfaces de enlace como mostrado a seguir:\nFaixa de endereços de destino\nInterface de enlace\n11001000 00010111 00010000 00000000\naté\n0\n11001000 00010111 00010111 11111111\n11001000 00010111 00011000 00000000\naté\n1\n11001000 00010111 00011000 11111111\n11001000 00010111 00011001 00000000\naté\n2\n11001000 00010111 00011111 11111111\nsenão\n3\n   Redes de computadores e a Internet\n234\nFica claro, por este exemplo, que não é necessário ter quatro bilhões de registros na tabela de repasse do \nroteador. Poderíamos, por exemplo, ter a seguinte tabela de repasse com apenas quatro registros:\nPrefixo do endereço\nInterface de enlace\n11001000 00010111 00010\n0\n11001000 00010111 00011000\n1\n11001000 00010111 00011\n2\nsenão\n3\nCom esse tipo de tabela de repasse, o roteador compara um prefixo do endereço de ­\ndestino do pacote com os \nregistros na tabela; se houver uma concordância de prefixos, o roteador transmite o pacote para o enlace associado \nàquele prefixo correspondente. Por exemplo, suponha que o endereço de destino do pacote seja 11001000 00010111 \n00010110 10100001; como o prefixo de 21 bits desse endereço é igual ao primeiro registro na tabela, o roteador \ntransmite o pacote para a interface de enlace 0. Se o prefixo do pacote não combinar com nenhum dos três primeiros \n­\nregistros, o roteador envia o pacote para a interface 3. Embora isso pareça ­\nbastante ­\nsimples, há aqui uma sutileza im-\nportante. Você talvez tenha notado a possibilidade de um endereço de destino combinar com mais de um registro. \nPor exemplo, os primeiros 24 bits do endereço 11001000 00010111 00011000 10101010 combinam com o segundo \nregistro na tabela e os primeiros 21 bits do endereço combinam com o terceiro registro. Quando há várias concor-\ndâncias de prefixos, o roteador usa a regra da concordância do prefixo mais longo, isto é, encontra o registro cujo \nprefixo tem mais bits correspondentes aos bits do endereço do pacote e envia o pacote à interface de enlace associada \ncom esse prefixo mais longo que tenha correspondência. Veremos exatamente por que essa regra de prefixo mais \nlongo correspondente é utilizada quando estudarmos endereçamento da Internet em mais detalhes da Seção 4.4.\nEmbora em redes de datagramas os roteadores não mantenham nenhuma informação de estado de conexão, \nainda assim mantêm informação de estado de repasse em suas tabelas de repasse. Todavia, a escala temporal da mu-\ndança dessas informações de estado é um tanto lenta. Na verdade, as tabelas de repasse em uma rede de datagramas \nsão modificadas pelos algoritmos de roteamento que em geral atualizam uma tabela de repasse em intervalos de um \na cinco minutos, mais ou menos. A tabela de repasse de um roteador em uma rede de CVs é modificada sempre \nque é estabelecida uma nova conexão através do roteador ou sempre que uma conexão existente é desativada. Em \num roteador de backbone de nível 1, isso poderia acontecer facilmente em uma escala temporal de microssegundos.\nComo em redes de datagramas as tabelas de repasse podem ser modificadas a qualquer momento, uma série \nde pacotes enviados de um sistema final para outro pode seguir caminhos diferentes pela rede e muitos podem \nchegar fora da ordem. Paxson [1997] e Jaiswal [2003] apresentam interessantes estudos de medição da reordena-\nção de pacotes e de outros fenômenos na Internet pública.\nFigura 4.5  Rede de datagramas\nTransporte\n1. Envia dados\n2. Recebe dados\nEnlace\nFísica\nAplicação\nRede\nTransporte\nEnlace\nFísica\nAplicação\nRede\nA CAMADA  de REDE  235 \n4.2.3  Origens das redes de circuitos virtuais e de datagramas\nA evolução das redes de datagramas e de circuitos virtuais reflete as origens dessas redes. A ideia de um \ncircuito virtual como princípio fundamental de organização tem suas raízes no ­\nmundo da telefonia (que usa \ncircuitos reais). Como em redes de CV os roteadores mantêm estabelecimento da chamada e estado por cha-\nmada, esse tipo de rede é consideravelmente mais ­\ncomplexo do que uma rede de datagramas. (Molinero-Fer-\nnandez [2002] faz uma comparação ­\ninteressante entre as complexidades de redes de comutação de circuitos e \nde comutação de ­\npacotes.) Isso também está de acordo com a herança da telefonia. A complexidade das redes \ntelefônicas estava, necessariamente, dentro da própria rede, já que elas conectavam sistemas ­\nfinais não in-\nteligentes tais como telefones de disco. (Para os mais jovens que não o conhecem, um telefone de disco é \n \num telefone analógico [isto é, não digital] sem teclas — tem somente um mostrador com ­\nnúmeros e um dispositivo \nmecânico denominado disco.)\nPor sua vez, a Internet como uma rede de datagramas surgiu da necessidade de conectar computadores. \nComo esses sistemas finais são mais sofisticados, os arquitetos da Internet preferiram construir um modelo de \nserviço de camada de rede o mais simples possível. Como já vimos nos capítulos 2 e 3, funcionalidades adicionais \n(por exemplo, entrega na ordem, transferência confiável de dados, controle de congestionamento e resolução \nde nomes DNS) são executadas em uma camada mais alta, nos sistemas finais. Isso inverte o modelo da rede de \ntelefonia, com algumas consequências interessantes:\n• Visto que o modelo de serviço de camada de rede resultante da Internet, que oferece garantias mínimas \n(nenhuma!) de serviço, impõe exigências mínimas sobre a camada de rede. Isso facilita a interconexão de \nredes que usam tecnologias de camada de enlace muito diferentes (por exemplo, satélite, Ethernet, fibra \nou rádio) e cujas taxas de transmissão e características de perda também são muito diferentes. Vamos \nabordar detalhadamente a interconexão de redes IP na Seção 4.4.\n• Como vimos no Capítulo 2, aplicações como e-mail, a Web e até mesmo um serviço centrado na camada \nde rede como o DNS são executadas em hospedeiros (servidores) na borda da rede. A capacidade de \nadicionar um novo serviço apenas ligando um hospedeiro à rede e definindo um novo protocolo de ca-\nmada de aplicação (como o HTTP) permitiu que novas aplicações como a Web fossem distribuídas pela \nInternet em um período notavelmente curto.\n4.3  O que há dentro de um roteador?\nAgora que já tivemos uma visão geral das funções e serviços da camada de rede, voltaremos nossa atenção para \na função de repasse — a transferência, propriamente dita, de pacotes dos enlaces de entrada até os enlaces de saída \nadequados de um roteador. Já estudamos algumas questões de repasse na Seção 4.2, a saber, endereçamento e cor-\nrespondência com o prefixo mais longo. Mencionamos, de passagem, que os pesquisadores e profissionais de redes \nde computadores usam as palavras repasse e comutação indistintamente; nós usaremos ambos os termos neste livro.\nUma visão de alto nível da arquitetura de um roteador genérico é mostrada na Figura 4.6. Quatro compo-\nnentes de um roteador podem ser identificados:\n• Portas de entrada. A porta de entrada tem diversas funções. Ela realiza as funções de camada física (a \ncaixa mais à esquerda da porta de entrada e a caixa mais à direita da porta de saída na Figura 4.6) de \nterminar um enlace físico de entrada em um roteador. Executa também as de camada de enlace (re-\npresentadas pelas caixas do meio nas portas de entrada e de saída) necessárias para interoperar com as \nfunções da camada de enlace do outro lado do enlace de entrada. Talvez mais importante, a função de \nexame também é realizada na porta de entrada; isso ocorrerá na caixa mais à direita da porta de entrada. \nÉ aqui que a tabela de repasse é consultada para determinar a porta de saída do roteador à qual um pacote \nque chega será repassado por meio do elemento de comutação. Pacotes de controle (por exemplo, paco-\ntes carregando informações de protocolo de roteamento) são repassados de uma porta de entrada até o \n   Redes de computadores e a Internet\n236\nprocessador de roteamento. Note que o termo porta aqui — referindo-se às interfaces físicas de entrada \ne saída do roteador — é distintamente diferente das portas de software associadas a aplicações de rede e \nsockets, discutidos nos Capítulos 2 e 3.\n• Elemento de comutação. O elemento de comutação conecta as portas de entrada do roteador às suas por-\ntas de saída. Ele está integralmente contido no interior do roteador — uma rede dentro de um roteador \nda rede!\n• Portas de saída. Uma porta de saída armazena os pacotes que foram repassados a ela através do elemento \nde comutação e, então, os transmite até o enlace de saída, realizando as funções necessárias da camada \nde enlace e da camada física. Quando um enlace é bidirecional (isto é, carrega um tráfego em ambas as \ndireções), uma porta de saída para o enlace será emparelhada com a porta de entrada para esse enlace na \nmesma placa de linha (uma placa de circuito impresso contendo uma ou mais portas de entrada, e que \nestá conectada ao elemento de comutação).\n• Processador de roteamento. O processador de roteamento executa os protocolos de roteamento (que estu-\ndaremos na Seção 4.6), mantém as tabelas de roteamento e as informações de estado do enlace, e calcula \na tabela de repasse para o roteador. Ele também realiza funções de gerenciamento de rede, que estudare-\nmos no Capítulo 9.\nLembre-se de que, na Seção 4.1.1, distinguimos entre as funções de repasse e roteamento de um roteador. \nAs portas de entrada, portas de saída e elemento de comutação de um roteador executam a função de repasse e \nquase sempre são implementadas no hardware, como ilustra a Figura 4.6. Essas funções às vezes são chamadas \ncoletivamente de plano de repasse do ­\nroteador. Para entender por que é necessário haver uma execução no \nhardware, considere que, com um enlace de entrada de 10 bits/s e um datagrama IP de 64 bytes, a porta de \nentrada tem apenas 51,2 ns para processar o datagrama antes que outro datagrama possa chegar. Se N portas \nforem combinadas em uma placa de linha (como em geral é feito na prática), a canalização de processamento \nde datagrama precisa operar N vezes mais rápido — muito rápido para uma realização em software. O \nhardware do plano de repasse pode ser realizado usando os próprios projetos de hardware de um fabricante de \nroteador ou pode ser construído usando chips de silício comprados no mercado (por exemplo, vendidos por \nempresas como Intel e Broadcom).\nEmbora o plano de repasse opere em uma escala de tempo de nanossegundo, as funções de controle de um \nroteador — executando os protocolos de roteamento, respondendo a enlaces conectados que são ativados ou \ndesativados, e realizando funções de gerenciamento como aquelas que estudaremos no Capítulo 9 — operam na \nescala de tempo de milissegundo ou segundo. Essas funções do plano de controle do roteador costumam ser \nrealizadas no software e executam no processador de roteamento (em geral, uma CPU tradicional).\nFigura 4.6  Arquitetura de roteador\nKR 04.06.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n32p0 Wide x 15p3 Deep\n10/13/1, 10/21/11,\n10/28/11, 11/8/11 rossi\nPorta de entrada\nPorta de saída\nPorta de entrada\nPorta de saída\nProcessador\nde roteamento\nRoteamento, gerenciamento\nplano de controle (software)\nRepasse\nplano de dados (hardware)\nElemento\nde\ncomutação\nA CAMADA  de REDE  237 \nAntes de entrarmos nos detalhes do plano de controle e dados de um roteador, vamos retornar à analogia \nda Seção 4.1.1, em que o repasse de pacotes foi comparado com carros entrando e saindo de um pedágio. Vamos \nsupor que, antes que um carro entre no pedágio, algum processamento seja necessário — o veículo para em \numa estação de entrada e indica seu destino final (não no pedágio local, mas o destino final de sua viagem). Um \natendente na cabine examina o destino final, determina a saída do pedágio que leva a esse destino final e diz ao \nmotorista qual saída ele deve tomar. O carro entra no pedágio (que pode estar cheio de outros carros entrando de \noutras estradas de entrada e seguindo para outras saídas do pedágio) e por fim segue pela pista de saída indicada, \nonde poderá encontrar outros carros saindo do pedágio nessa mesma saída.\nNessa analogia, podemos reconhecer os componentes principais do roteador na Figura 4.6 — a pista de \nentrada e a cabine de entrada correspondem à porta de entrada (com uma função de consulta para determinar \na porta de saída local); o pedágio corresponde ao elemento de comutação; e a pista de saída do pedágio corres-\nponde à porta de saída. Com essa analogia, é instrutivo considerar onde poderiam acontecer os gargalos. O que \nacontece se os carros chegarem muito depressa (por exemplo, o pedágio está na Alemanha ou na Itália!), mas o \natendente na cabine for lento? Com que velocidade o atendente deverá trabalhar para garantir que não haja en-\ngarrafamento na pista de entrada? Mesmo com um atendente incrivelmente rápido, o que acontece se os carros \natravessarem o pedágio devagar — ainda poderá haver engarrafamentos? E o que ocorre se a maioria dos carros \nque entram quiserem sair do pedágio na mesma pista de saída — pode haver engarrafamentos na pista de saída \nou em outro lugar? Como o pedágio deve ser operado se quisermos atribuir prioridades a carros diferentes, ou \nimpedir que certos veículos sequer entrem no pedágio? Todas estas são questões críticas ­\nsemelhantes às enfren-\ntadas pelos projetistas de roteador e de comutador.\nNas próximas subseções, vamos examinar as funções do roteador com mais detalhes. Iyer [2008]; Chao [2001]; \nChuang [2005]; Turner [1988]; McKeown [1997a] e Partridge [1998] oferecem uma discussão das arquiteturas es-\npecíficas de roteador. Por concretude, a discussão a seguir considera uma rede de datagramas em que as decisões \nde repasse são baseadas no endereço de destino do pacote (em vez de um número de VC em uma rede de circuitos \nvirtuais). Porém, os conceitos e as técnicas são muito semelhantes para uma rede de circuitos virtuais.\n4.3.1  Processamento de entrada\nUma visão mais detalhada da funcionalidade de porta da entrada é apresentada na Figura 4.7. Como dis-\ncutido anteriormente, as funções de terminação de linha e de processamento de enlace realizadas pela porta de \nentrada implementam as funções das camadas física e de enlace associadas a um enlace de entrada individual \ndo roteador. A pesquisa realizada na porta de entrada é fundamental para a operação do roteador — é aqui \nque o roteador usa a tabela de repasse para determinar a porta de saída para a qual o pacote que está chegando \nserá repassado pelo elemento de comutação. A tabela de repasse é calculada e atualizada pelo processador de \nroteamento, e uma cópia da tabela é comumente armazenada em cada porta de entrada. A tabela de repasse é \ncopiada do processador de roteamento para as placas de linha por um barramento separado (por exemplo, um \nbarramento PCI), indicado na Figura 4.6 pela linha tracejada do processador de roteamento às placas de linha da \nentrada. Com uma cópia de sombra, as decisões de repasse podem ser feitas no local, em cada porta de entrada, \nsem chamada ao processador de roteamento centralizado a cada pacote, evitando assim um gargalo de processa-\nmento centralizado.\nDada a existência de uma tabela de repasse, o exame é conceitualmente simples — basta procurar o registro \nmais longo correspondente ao endereço de destino, como descrito na Seção 4.2.2. Porém, com taxas de transmis-\nsão de gigabits, a procura precisa ser realizada em nanossegundos (lembre-se do exemplo anterior de um enlace \nde 10 bits/s e um datagrama IP de 64 bytes). Assim, não apenas a pesquisa deve ser realizada no hardware, mas \nsão necessárias outras técnicas além da busca linear simples por uma tabela grande; estudos sobre algoritmos \nde pesquisa rápidos podem ser encontrados em Gupta [2001] e Ruiz-Sanchez [2001]. É preciso prestar atenção \nespecial aos tempos de acesso da memória, resultando em projetos com memórias de DRAM e SRAM (usadas \ncomo cache de DRAM) mais rápidas, embutidas no chip. Ternary Content Address Memories (TCAMs) também \n   Redes de computadores e a Internet\n238\nsão usadas para pesquisa. Com uma TCAM, um endereço IP de 32 bits é apresentado à memória, que retorna \no conteúdo da entrada da tabela de repasse para esse endereço em um tempo constante. O Cisco 8500 tem uma \nCAM de 64K para cada porta de entrada.\nQuando a porta de saída de um pacote tiver sido determinada por meio da pesquisa, ele pode ser enviado para o \nelemento de comutação. Em alguns projetos, um pacote pode ser temporariamente impedido de entrar no elemento de \ncomutação se os pacotes de outras portas de entrada estiverem usando o elemento nesse instante. Um pacote impedido \nficará enfileirado na porta de entrada e depois escalonado para cruzar o elemento em outra oportunidade. Veremos \nmais de perto os processos de bloqueio, enfileiramento e escalonamento de pacotes (nas portas de entrada e de saída) \nna Seção 4.3.4. Embora a “pesquisa” seja comprovadamente a ação mais importante no processamento da porta de \nentrada, muitas outras ações devem ser tomadas: (1) o processamento da camada física e de enlace deverá ocorrer, \nconforme já vimos; (2) os campos de número de versão, soma de verificação e tempo de vida do pacote — todos eles \nestudados na Seção 4.4.1 — deverão ser verificados, e os dois últimos campos reescritos; e (3) contadores usados para \no gerenciamento de rede (como o número de datagramas IP recebidos) devem ser atualizados.\nVamos encerrar nossa discussão de processamento de porta de entrada observando que as etapas da porta \nde entrada de pesquisar um endereço IP (“combinação”) e depois enviar o pacote para o elemento de comutação \n(“ação”) é um caso específico de uma abstração “combinação mais ação”\n, mais ampla, que é realizada em muitos \nFigura 4.7  Processamento na porta de entrada\nTerminação\nde linha\nProcessamento\nde enlace\n(protocolo,\ndesencapsulamento)\nConsulta,\nrepasse, ﬁla\nElemento\nde comutação\nDominando o núcleo da rede\nQuando este livro foi escrito, em 2012, a Cisco \nempregava mais de 65 mil pessoas. Como surgiu \nessa gigantesca empresa de rede? Tudo começou \nem 1984 na sala de estar de um apartamento no Vale \ndo Silício.\nLen Bosak e sua esposa Sandy Lerner trabalha-\nvam na universidade de Stanford quando tiveram a \nideia de construir e vender roteadores de Internet a \ninstituições acadêmicas e de pesquisa, os primeiros \na adotarem a Internet naquela época. Sandy Lerner \nteve a ideia do nome “Cisco” (uma abreviação de San \nFrancisco) e também criou o logotipo da empresa, \nque é uma ponte. A sede era originalmente a sala de \nestar do casal que, no início, financiou o projeto com \ncartões de crédito e trabalhos de consultoria à noite. \nNo final de 1986, as receitas da empresa chegaram \na 250 mil dólares por mês. No fim de 1987, a Cisco \nconseguiu atrair capital de risco de dois milhões de \ndólares da Sequoia Capital em troca de um terço da \nempresa. Nos anos seguintes continuou a crescer e \na conquistar cada vez mais participação de merca-\ndo. Ao mesmo tempo, o relacionamento entre Bosak/\nLerner e a administração da empresa começou a fi-\ncar tenso. A Cisco abriu seu capital em 1990 e, nesse \nmesmo ano, Lerner e Bosak saíram da empresa.\nCom o passar dos anos, a Cisco expandiu ampla-\nmente seu mercado de roteadores, vendendo produ-\ntos voltados à segurança, redes sem fio, comutador \nEthernet, infraestrutura de centro de dados, videocon-\nferência e produtos e serviços de Voz sobre IP\n. En-\ntretanto, está enfrentando concorrência internacional \ncrescente, incluindo a empresa chinesa em cresci-\nmento Huawei, que fornece equipamentos de rede. \nOutras fontes de concorrência para a Cisco no mer-\ncado de roteadores e Ethernet comutada incluem a \nAlcatel-Lucent e a Juniper.\nHistória\nA CAMADA  de REDE  239 \ndispositivos da rede, não apenas roteadores. Em comutadores da camada de enlace (explicados no Capítulo 5), \nos endereços de destino da camada de enlace são pesquisados e várias ações podem ser tomadas além do envio \ndo quadro ao elemento de comutação pela porta de saída. Em firewalls (explicados no Capítulo 8) — dispositivos \nque filtram pacotes selecionados que chegam —, um pacote que chega, cujo cabeçalho combina com determinado \ncritério (por exemplo, uma combinação de endereços IP de origem/destino e números de porta da camada de \ntransporte), pode ter o repasse impedido (ação). Em um tradutor de endereço de rede (NAT, Network Address \nTranslator, discutido na Seção 4.4), um pacote que chega, cujo número de porta da camada de transporte combi-\nna com determinado valor, terá seu número de porta reescrito antes de ser repassado (ação). Assim, a abstração \n“combinação mais ação” é tanto poderosa quanto prevalente nos dispositivos da rede.\n4.3.2  Elemento de comutação\nO elemento de comutação está no coração de um roteador. É por meio do elemento de comutação que os \npacotes são comutados (isto é, repassados) de uma porta de entrada para uma porta de saída. A comutação pode \nser realizada de inúmeras maneiras, como mostra a Figura 4.8.\n• Comutação por memória. Os primeiros e mais simples roteadores quase sempre eram computadores \ntradicionais nos quais a comutação entre as portas de entrada e de saída era realizada sob o controle di-\nreto da CPU (processador de roteamento). Essas portas funcionavam como dispositivos tradicionais de \nentrada/saída de um sistema operacional tradicional. Uma porta de entrada na qual um pacote estivesse \nentrando primeiro sinalizaria ao processador de roteamento por meio de uma interrupção. O pacote era \nentão copiado da porta de entrada para a memória do processador. O processador de roteamento então \nextraía o endereço de destino do cabeçalho, consultava a porta de saída apropriada na tabela de repasse \ne copiava o pacote para os buffers da porta de saída. Neste cenário, se a largura de banda da memória \nfor tal que B pacotes/segundo possam ser escritos ou lidos na memória, então a vazão total de repasse (a \nvelocidade total com que os pacotes são transferidos de portas de entrada para portas de saída) deverá \nFigura 4.8  Três técnicas de comutação\nMemória\nA\nB\nC\nX\nY\nZ\nMemória\nLegenda:\nPorta de entrada\nPorta de saída\nA\nX\nY\nZ\nB\nC\nRede de interconexão\nA\nB\nC\nX\nY\nZ\nBarramento\n   Redes de computadores e a Internet\n240\nser menor do que B/2. Observe também que dois pacotes não podem ser repassados ao mesmo tempo, \nmesmo que tenham diferentes portas de destino, pois somente uma leitura/escrita de memória pelo bar-\nramento compartilhado do sistema pode ser feita de cada vez.\n\t\nMuitos roteadores modernos também comutam por memória. Contudo, uma diferença importante entre \nesses roteadores e os antigos é que a consulta do endereço de destino e o armazenamento do pacote na \nlocalização adequada da memória são realizados por processadores nas placas de linha de entrada. Em \ncertos aspectos, roteadores que comutam por memória se parecem muito com multiprocessadores de \nmemória compartilhada, nos quais os processadores de uma placa de linha comutam (escrevem) ­\npacotes \npara a memória da porta de saída adequada. Os comutadores série 8500 Catalyst da Cisco [Cisco 8500, \n2012] comutam pacotes por uma memória compartilhada.\n• Comutação por um barramento. Nessa abordagem, as portas de entrada transferem um pacote direta-\nmente para a porta de saída por um barramento compartilhado sem a intervenção do processador de \nroteamento. Para isso, a porta de entrada insere um rótulo interno ao comutador (cabeçalho) antes do \npacote, indicando a porta de saída local à qual ele está sendo transferido e o pacote é transmitido para o \nbarramento. Ele é recebido por todas as portas de saída, mas somente a porta que combina com o rótulo \nmanterá o pacote. O rótulo é então removido na porta de saída, pois só é usado dentro do comutador \npara atravessar o barramento. Se vários pacotes chegarem ao roteador ao mesmo tempo, cada um em \numa porta de entrada diferente, todos menos um deverão esperar, pois apenas um pacote pode cruzar \no barramento de cada vez. Como cada pacote precisa atravessar o único barramento, a velocidade de \ncomutação do roteador é limitada à velocidade do barramento; em nossa analogia com o sistema de pe-\ndágio, é como se o sistema de pedágio só pudesse conter um carro de cada vez. Apesar disso, a comutação \npor um barramento muitas vezes é suficiente para roteadores que operam em redes de acesso e redes de \nempresas. Os comutadores Cisco 5600 [Cisco Switches, 2012] comutam pacotes por um barramento da \nplaca-mãe (backplane bus) de 32 bits/s.\n• Comutação por uma rede de interconexão. Um modo de vencer a limitação da largura de banda de um \nbarramento único compartilhado é usar uma rede de interconexão mais sofisticada, tal como as que eram \nutilizadas no passado para interconectar processadores em uma arquitetura de computadores multipro-\ncessadores. Um comutador do tipo crossbar é uma rede de interconexão que consiste em 2n barramentos \nque conectam n portas de entrada com n portas de saída, como ilustra a Figura 4.8. Cada barramento ver-\ntical atravessa cada barramento horizontal em um cruzamento, que pode ser aberto ou fechado a qual-\nquer momento pelo controlador do elemento de comutação (cuja lógica faz parte do próprio elemento de \ncomutação). Quando um pacote chega da porta A e precisa ser repassado para a porta Y, o controlador do \n \ncomutador fecha o cruzamento na interseção dos barramentos A e Y, e a porta A, então, envia o pacote \npara seu barramento, que é apanhado (apenas) pelo barramento Y. Observe que um pacote da porta B \npode ser repassado para a porta X ao mesmo tempo, pois os pacotes A-para-Y e B-para-X usam dife-\nrentes barramentos de entrada e saída. Assim, diferentemente das duas técnicas de comutação anterio-\nres, as redes do tipo crossbar são capazes de repassar vários pacotes em paralelo. Porém, se dois pacotes \nde duas portas de entrada diferentes forem destinados à mesma porta de saída, então um terá que \nesperar na entrada, pois somente um pacote pode ser enviado por qualquer barramento de cada vez.\n\t\nRedes de comutação mais sofisticadas utilizam vários estágios de elementos de comutação para per-\nmitir que pacotes de diferentes portas de entrada prossigam para a mesma porta de saída ao mesmo \ntempo através do elemento de comutação. Consulte Tobagi [1990] para ver um levantamento de arqui-\nteturas de comutação. Os comutadores da família 12000 da Cisco [Cisco 12000, 2012] usam uma rede \nde interconexão.\nA CAMADA  de REDE  241 \n4.3.3  Processamento de saída\nO processamento de portas de saída, mostrado na Figura 4.9, toma os pacotes que foram armazenados na \nmemória da porta de saída e os transmite pelo enlace de saída. Isso inclui a seleção e a retirada dos pacotes da fila \npara transmissão, com a realização das funções de transmissão necessárias nas camadas de enlace e física.\nFigura 4.9  Processamento de porta de saída\nTerminação\nde linha\nProcessamento\nde enlace\n(protocolo,\nencapsulamento)\nFila (gerenciamento\nde buffer)\nElemento\nde\ncomutação\n4.3.4  Onde ocorre formação de fila?\nSe examinarmos a funcionalidade da porta de entrada e da porta de saída e as ­\nconfigurações mostradas na \nFigura 4.8, veremos que filas de pacotes podem se formar tanto nas portas de entrada como nas de saída, assim \ncomo identificamos casos em que os carros podem esperar nas entradas e saídas em nossa analogia de pedágio. O \nlocal e a extensão da formação de fila (seja nas da porta de entrada ou nas da porta de saída) dependerão da carga \nde tráfego, da velocidade relativa do elemento de comutação e da taxa da linha. Agora, vamos examinar essas filas \ncom um pouco mais de detalhes, já que, à medida que elas ficam maiores, a memória do roteador será finalmente \nexaurida e ocorrerá perda de pacote, quando nenhuma memória estará disponível para armazenar os pacotes que \nchegam. Lembre-se de que, em nossas discussões anteriores, dissemos que pacotes eram “perdidos dentro da rede” \nou “descartados em um roteador”\n. E é aí, nessas filas dentro de um roteador, que esses pacotes são de fato descartados \ne perdidos.\nSuponha que as taxas da linha de entrada e as taxas da linha de saída (taxas de transmissão) tenham todas uma \ntaxa de transmissão idêntica de Rlinha pacotes por segundo, e que haja N portas de entrada e N portas de saída. Para \nsimplificar ainda mais a discussão, vamos supor que todos os pacotes tenham o mesmo comprimento fixo e que eles \nchegam às portas de entrada de uma forma síncrona. Isto é, o tempo para enviar um pacote em qualquer enlace é \nigual ao tempo para receber um pacote em qualquer enlace, e durante esse intervalo, zero ou um pacote pode chegar \nem um enlace de entrada. Defina a taxa de transferência do elemento de comutação Rcomutação como a taxa na qual \nos pacotes podem ser movimentados da porta de entrada à porta de saída. Se Rcomutação for N vezes mais rápida que \nRlinha, então haverá apenas uma formação de fila insignificante nas portas de entrada. Isso porque, mesmo no pior \ncaso, em que todas as N linhas de entrada estiverem recebendo pacotes, e todos eles tiverem de ser repassados para a \nmesma porta de saída, cada lote de N pacotes (um pacote por porta de entrada) poderá ser absorvido pelo elemento \nde comutação antes que o próximo lote chegue.\nMas o que pode acontecer nas portas de saída? Vamos supor que Rcomutação ainda seja N vezes Rlinha. Mais uma \nvez, os pacotes que chegarem a cada uma das N portas de entrada serão destinados à mesma porta de saída. Nesse \ncaso, no tempo que leva para enviar um único pacote no enlace de saída, N pacotes novos chegarão a essa porta \nde saída. Uma vez que essa porta pode transmitir somente um único pacote em cada unidade de tempo (o tempo \nde transmissão do pacote), os N pacotes que chegarem terão de entrar na fila (esperar) para transmissão pelo \nenlace de saída. Então, mais N pacotes poderão chegar durante o tempo que leva para transmitir apenas um dos \nN pacotes que estavam na fila antes, e assim por diante. Por fim, o número de pacotes na fila pode ficar grande o \nbastante para exaurir o espaço de memória na porta de saída, caso em que os pacotes são descartados.\nA formação de fila na porta de saída está ilustrada na Figura 4.10. No tempo t, um pacote chegou a cada \numa das portas de entrada, cada um deles destinado à porta de saída que está mais acima na figura. Admitindo \n   Redes de computadores e a Internet\n242\ntaxas da linha idênticas e um comutador operando a uma taxa três vezes maior do que a da linha, uma unidade de \ntempo mais tarde (isto é, no tempo necessário para receber ou enviar um pacote), todos os três pacotes originais \nforam transferidos para a porta de saída e estão em fila aguardando transmissão. Na unidade de tempo seguinte, \num desses três terá sido transmitido pelo enlace de saída. Em nosso exemplo, dois novos pacotes chegaram do \nlado de entrada do comutador; um deles é destinado àquela mesma porta de saída que está mais acima na figura.\nDado que os buffers do roteador são necessários para absorver as oscilações da carga de tráfego, deve-se per-\nguntar quanto de armazenamento em buffers é necessário. Durante muitos anos, a regra prática [RFC 3439] para \ndimensionamento de buffers foi que a quantidade de armazenamento em buffers (B) deveria ser igual a um tempo \nde viagem de ida e volta (RTT, digamos, 250 ms) vezes a capacidade do enlace (C). Esse resultado é baseado em uma \nanálise da dinâmica de filas com um número relativamente pequeno dos fluxos do TCP [Villamizar, 1994]. Assim, um \nenlace de 10 bits/s com um RTT de 250 ms precisaria de uma quantidade de armazenamento em buffers B = RTT ∙ C \n= 2,5 bits/s de buffers. Esforços teóricos e experimentais recentes [Appenzeller, 2004], entretanto, sugerem que quan-\ndo há um grande número de fluxos do TCP (N) passando por um enlace, a quantidade de armazenamento em \n \nbuffers necessária é B = RTT ∙ C/√N. Com um grande número de fluxos passando normalmente por grandes enlaces dos \n \nroteadores de backbone (consulte, por exemplo, Fraleigh [2003]), o valor de N pode ser grande, e a diminuição do \ntamanho do buffers necessário se torna bastante significativa. Appenzellar [2004]; Wischik [2005] e Behesht [2008] \napresentam discussões esclarecedoras em relação ao problema do dimensionamento de buffers a partir de um ponto \nde vista teórico, de aplicação e ­\noperacional.\nUma consequência da fila na porta de saída é que um escalonador de pacotes na porta de saída deve escolher \npara transmissão um dentre os que estão na fila. Essa seleção pode ser feita com base em uma regra simples, como \nno escalonamento do primeiro a chegar, primeiro a ser atendido (FCFS), ou por uma regra mais sofisticada, tal \ncomo a fila ponderada justa (weighted fair queuing — WFQ), que compartilha o enlace de saída com equidade en-\ntre as diferentes conexões fim a fim que têm pacotes na fila para transmissão. O escalonamento de pacotes desem-\npenha um papel crucial no fornecimento de garantia de qualidade de serviço. Examinaremos o escalonamento \nde pacotes em profundidade no Capítulo 7. Uma discussão sobre disciplinas de escalonamento de pacotes na porta \nde saída pode ser encontrada em Cisco Queue [2012].\nFigura 4.10  Formação de fila na porta de saída\nElemento de\ncomutação\nDisputa pela porta de saída no tempo t\nUm tempo de pacote mais tarde\nElemento\nde\ncomutação\nA CAMADA  de REDE  243 \nDe modo semelhante, se não houver memória suficiente para armazenar um pacote que está chegando, \nserá preciso tomar a decisão de descartar esse pacote (política conhecida como descarte do final da fila) ou \nremover um ou mais já enfileirados para liberar lugar para o pacote recém-chegado. Em alguns casos pode ser \nvantajoso descartar um pacote (ou marcar o seu cabeçalho) antes de o buffers ficar cheio, para dar um sinal de \ncongestionamento ao remetente. Várias políticas de descarte e marcação de pacotes (conhecidas coletivamente \ncomo algoritmos de gerenciamento ativo de fila [active queue management — AQM]) foram propostas e ana-\nlisadas [Labrador, 1999; Hollot, 2002]. Um dos algoritmos AQM mais estudados e executados é o de detecção \naleatória rápida (random early detection — RED). Ele mantém uma média ponderada do comprimento da fila \nde saída. Se o comprimento médio da fila for menor do que um valor limite mínimo, minth, quando um pacote \nchegar, será admitido na fila. Inversamente, se a fila estiver cheia ou se o comprimento médio da fila for maior \ndo que um valor limite máximo, maxth, quando um pacote chegar, será marcado ou descartado. Finalmente, \nse o pacote chegar e encontrar uma fila de comprimento médio no intervalo [minth, maxth], o pacote será mar-\ncado ou descartado com uma probabilidade que em geral é alguma função do comprimento médio da fila, de \nminth e de maxth. Foram propostas inúmeras funções probabilísticas para marcação/descarte e várias versões \ndo RED foram modeladas, simuladas e/ou implementadas analiticamente. Christiansen [2001] e Floyd [2012] \noferecem visões gerais e indicações de leituras adicionais.\nSe o elemento de comutação não for veloz o suficiente (em relação às taxas da linha de entrada) para transmi-\ntir sem atraso todos os pacotes que chegam através dele, então poderá haver formação de fila também nas portas de \nentrada, pois os pacotes devem se juntar às filas nas portas de entrada para esperar sua vez de ser transferidos pelo \nelemento de comutação até a porta de saída. Para ilustrar uma importante consequência dessa fila, considere um \nelemento de comutação do tipo crossbar e suponha que (1) todas as velocidades de enlace sejam idênticas, (2) um \npacote possa ser transferido de qualquer uma das portas de entrada até uma dada porta de saída no mesmo tempo \nque leva para um pacote ser recebido em um enlace de entrada e (3) pacotes sejam movimentados de uma fila de \nentrada até sua fila de saída desejada no modo FCFS. Vários pacotes podem ser transferidos em paralelo, contanto \nque suas portas de saída sejam diferentes. Entretanto, se dois pacotes que estão à frente das duas filas de entrada \nforem destinados à mesma fila de saída, então um deles ficará bloqueado e terá de esperar na fila de entrada — o \nelemento comutador só pode transferir um pacote por vez até uma porta de saída.\nA parte superior da Figura 4.11 apresenta um exemplo em que dois pacotes (mais escuros) à frente de suas \nfilas de entrada são destinados à mesma porta de saída mais alta à direita. Suponha que o elemento de comutação \nescolha transferir o pacote que está à frente da fila mais alta à esquerda. Nesse caso, o pacote mais escuro na fila \nmais baixa à esquerda tem de esperar. Mas não é apenas este último que tem de aguardar; também tem de esperar \no pacote claro que está na fila atrás dele (no retângulo inferior à esquerda), mesmo que não haja nenhuma disputa \npela porta de saída do meio à direita (que é o destino do pacote claro). Esse fenômeno é conhecido como bloqueio \nde cabeça de fila (HOL — head-of-the-line blocking) em um comutador com fila de entrada — um pacote que \nestá na fila de entrada deve esperar pela transferência através do elemento de comutação (mesmo que sua porta \nde saída esteja livre) porque ele está ­\nbloqueado por outro pacote na cabeça da fila. Karol [1987] demonstra que, \npor causa do ­\nbloqueio HOL, o comprimento da fila de entrada cresce sem limites (informalmente, isso equivale a \ndizer que haverá significativas perdas de pacotes) em determinadas circunstâncias assim que a taxa de chegada de \npacotes no enlace de entrada alcançar apenas 58% de sua capacidade. Uma série de soluções para o bloqueio HOL \né discutida por McKeown [1997b].\n4.3.5  O plano de controle de roteamento\nEm nossa discussão até aqui e na Figura 4.6, consideramos de modo implícito que o plano de contro-\nle de roteamento reside totalmente e é executado em um processador de ­\nroteamento dentro do roteador. \nO plano de controle de roteamento em âmbito de rede é, portato, descentralizado — com diferentes partes \n(por exemplo, de um algoritmo de roteamento) executando em diferentes roteadores e interagindo pelo \nenvio de mensagens de controle entre si. Na verdade, os roteadores atuais na Internet e os algoritmos de \n   Redes de computadores e a Internet\n244\nroteamento que estudaremos na Seção 4.6 operam exatamente dessa maneira. Além disso, fornecedores \nde roteador e comutador agrupam seu plano de dados do hardware e plano de controle do software em \nplataformas fechadas (porém, interoperáveis) em um produto verticalmente integrado.\nHá pouco, diversos pesquisadores [Caesar, 2005a; Casado, 2009; McKeown, 2008] começaram a explorar \nnovas arquiteturas de plano de controle de roteador, nas quais parte das funções do plano é executada nos rotea-\ndores (por exemplo, medição/relatório local do estado do enlace, instalação da tabela de repasse e manutenção), \njunto com o plano de dados, e parte do plano de controle pode ser implementada externamente ao roteador (por \nexemplo, em um servidor centralizado, que poderia realizar cálculo de rota). Uma API bem definida determina \ncomo essas duas partes interagem e se comunicam. Esses pesquisadores argumentam que a separação do plano \nde controle em software do plano de dados em hardware (com um mínimo de plano de ­\ncontrole residente no \nroteador) pode simplificar o roteamento, substituindo o cálculo de roteamento distribuido pelo cálculo de rote-\namento centralizado, e permitir a inovação na rede, aceitando que diferentes planos de controle customizados \noperem por planos de dados de hardware velozes.\n4.4  \u0007\nO Protocolo da Internet (IP): repasse e endereçamento \nna Internet\nAté agora discutimos endereçamento e repasse na camada de rede, sem referências a nenhuma rede de \ncomputadores específica. Nesta seção, voltaremos nossa atenção a como são feitos o endereçamento e o repasse \nna Internet. Veremos que o endereçamento e o repasse na Internet são componentes importantes do Protocolo da \nInternet (IP). Há duas versões do protocolo IP em uso hoje. Examinaremos primeiro a mais utilizada, a versão 4, \nque em geral é denominada apenas IPv4 [RFC 791]. Examinaremos a versão 6 do IP [RFC 2460; RFC 4291], que \nfoi proposta para substituir o IPv4, no final desta seção.\nFigura 4.11  Bloqueio de cabeça de fila em um comutador com fila de entrada\nElemento de\ncomutação\nDisputa pela porta de saída no tempo t  —\num pacote escuro pode ser transferido\nPacote claro do último retângulo sofre bloqueio HOL\nElemento de\ncomutação\nLegenda:\ndestinado à porta de saída\nmais alta\ndestinado à porta de saída\ndo meio\ndestinado à porta de saída\nmais baixa\nA CAMADA  de REDE  245 \nMas, antes de iniciar nossa investida no IP\n, vamos voltar um pouco atrás e considerar os componentes que \nformam a camada de rede da Internet. Conforme mostra a Figura 4.12, essa camada tem três componentes mais \nimportantes. O primeiro é o protocolo IP\n, que é o tópico desta seção. O segundo é o componente de roteamento, que \ndetermina o caminho que um datagrama segue desde a origem até o destino. Mencionamos anteriormente que pro-\ntocolos de roteamento calculam as tabelas de repasse que são usadas para transmitir pacotes pela rede. Estudaremos \nos protocolos de roteamento da Internet na Seção 4.6. O componente final da camada de rede é um dispositivo para \ncomunicação de erros em datagramas e para atender requisições de certas informações de camada de rede. Examina-\nremos o protocolo de comunicação de erro e de informações da Internet, ICMP (Internet Control Message Protocol), \nna Seção 4.4.3.\nFigura 4.12  Contemplando o interior da camada de rede da Internet\nProtocolos de roteamento\n• seleção de caminho\n• RIP\n, OSPF\n, BGP\nProtocolo IP\n• convenções de endereçamento\n• formato de datagrama\n• convenções de manuseio\n   de pacotes\nProtocolo ICMP\n• comunicação de erro\n• “sinalização” de roteador\nTabela de\nrepasse\nCamada de transporte: TCP\n, UDP\nCamada de enlace\nCamada física\nCamada de rede\n4.4.1  Formato de datagrama\nLembre-se de que um pacote de camada de rede é denominado um datagrama. Iniciamos nosso estudo do \nIP com uma visão geral da sintaxe e da semântica do datagrama IPv4. Você talvez esteja pensando que nada po-\nderia ser mais desinteressante do que a sintaxe e a semântica dos bits de um pacote. Mesmo assim, o datagrama \ndesempenha um papel central na Internet — todos os estudantes e profissionais de rede precisam vê-lo, absorvê-lo \ne dominá-lo. O formato do datagrama IPv4 é mostrado na Figura 4.13. Seus principais campos são os seguintes:\n• Número da versão. Esses quatro bits especificam a versão do protocolo IP do datagrama. Examinando o \nnúmero da versão, o roteador pode determinar como interpretar o restante do datagrama IP. Diferentes \nversões de IP usam diferentes formatos de datagramas. O formato para a versão atual do IP (IPv4) é \nmostrado na Figura 4.13. O formato do datagrama para a nova versão do IP (IPv6) será discutido no \nfinal desta seção.\n• Comprimento do cabeçalho. Como um datagrama IPv4 pode conter um número variá­\nvel de opções (in-\ncluídas no cabeçalho do datagrama IPv4), esses quatro bits são necessários para determinar onde, no \ndatagrama IP, os dados começam de fato. A maior parte dos datagramas IP não contém opções; portanto, \no datagrama IP típico tem um cabeçalho de 20 bytes. \n• Tipo de serviço. Os bits de tipo de serviço (type of service — TOS) foram incluídos no cabeçalho do IPv4 \npara poder diferenciar os diferentes tipos de datagramas IP (por exemplo, que requerem, particularmen-\nte, baixo atraso, alta vazão ou confiabilidade) que devem ser distinguidos uns dos outros. Por exemplo, \n   Redes de computadores e a Internet\n246\npoderia ser útil distinguir datagramas de tempo real (como os usados por uma aplicação de telefonia IP) \nde tráfego que não é de tempo real (por exemplo, FTP). O nível de serviço a ser fornecido é uma questão \nde política determinada pelo administrador do roteador. Examinaremos em detalhes a questão do servi-\nço diferenciado no Capítulo 7.\n• Comprimento do datagrama. É o comprimento total do datagrama IP (cabeçalho mais dados) medido em \nbytes. Uma vez que esse campo tem 16 bits de comprimento, o tamanho máximo teórico do datagrama \nIP é 65.535 bytes. Contudo, datagramas raramente são maiores do que 1.500 bytes.\n• Identificador, flags, deslocamento de fragmentação. Esses três campos têm a ver com a fragmentação do IP, \num tópico que em breve vamos detalhar. O interessante é que a nova versão do IP, o IPv6, não permite \nfragmentação em roteadores.\n• Tempo de vida. O campo de tempo de vida (time-to-live — TTL) é incluído para garantir que datagramas \nnão fiquem circulando para sempre na rede (por causa de, por exemplo, um laço de roteamento de longa \nduração). Esse campo é decrementado de uma unidade cada vez que o datagrama é processado por um \nroteador. Se o campo TTL chegar a 0, o datagrama deve ser descartado.\n• Protocolo. Usado somente quando um datagrama IP chega a seu destino final. O valor do campo indica o \nprotocolo de camada de transporte específico ao qual a porção de dados desse datagrama IP deverá ser passa-\nda. Por exemplo, um valor 6 indica que a porção de dados será passada ao TCP\n, enquanto um valor 17 indica \nque os dados serão passados ao UDP\n. Consulte IANA Protocol Numbers [2012] para ver uma lista de todos \nos valores possíveis. Note que o número do protocolo no datagrama IP tem um papel análogo ao do campo \nde número de porta no segmento da camada de transporte. O número do protocolo é o elo entre as camadas \nde rede e de transporte, ao passo que o número de porta liga as camadas de transporte e de aplicação. Vere-\nmos no Capítulo 5 que o quadro de camada de enlace também tem um campo especial que liga a camada de \nenlace à camada de rede.\n• Soma de verificação do cabeçalho. A soma de verificação do cabeçalho auxilia um roteador na detecção de \nerros de bits em um datagrama IP recebido. É calculada tratando cada 2 bytes do cabeçalho como se fossem \num número e somando esses números usando complementos aritméticos de 1. Como discutimos na Seção \n3.3, o complemento de 1 dessa soma, conhecida como soma de verificação da Internet, é armazenado no \ncampo de soma de verificação. Um roteador calculará o valor da soma de verificação para cada datagrama \nIP recebido e detectará uma condição de erro se o valor carregado no cabeçalho do datagrama não for igual \nà soma de verificação calculada. Roteadores em geral descartam datagramas quando um erro é detectado. \nFigura 4.13  Formato do datagrama IPv4\nVersão\nTipo de serviço\nComprimento\ndo cabeçalho\nProtocolo da\ncamada superior\nIdentiﬁcador de 16 bits\nTempo de vida\nDeslocamento de\nfragmentação (13 bits)\nFlags\nComprimento do datagrama (bytes)\nSoma de veriﬁcação do cabeçalho\n32 bits\nEndereço IP da origem\nEndereço IP do destino\nOpções (se houver)\nDados\nA CAMADA  de REDE  247 \nNote que a soma de verificação deve ser recalculada e armazenada de novo em cada roteador, pois o cam-\npo TTL e, possivelmente, também os campos de opções podem mudar. Uma discussão interessante sobre \nalgoritmos rápidos para calcular a soma de verificação da Internet é encontrada em [RFC 1071]. Uma per-\ngunta que sempre é feita nesse ponto é: por que o TCP/IP faz verificação de erro nas camadas de transporte \ne de rede? Há várias razões para essa repetição. Primeiro, note que, na camada IP, a soma de verificação é \ncalculada só para o cabeçalho IP, enquanto no TCP/UDP a soma de verificação é calculada para todo o \nsegmento TCP/IP. Segundo, o TCP/UDP e o IP não precisam necessariamente pertencer à mesma pilha de \nprotocolos. O TCP pode, em princípio, rodar sobre um protocolo diferente (por exemplo, ATM) e o IP pode \ncarregar dados que não serão passados ao TCP/UDP.\n• Endereços IP de origem e de destino. Quando uma origem cria um datagrama, insere seu endereço IP no \ncampo de endereço de origem IP e insere o endereço do destino final no campo de endereço de destina-\ntário IP. Muitas vezes o hospedeiro da origem determina o endereço do destinatário por meio de uma \nconsulta ao DNS, como discutimos no Capítulo 2. Discutiremos endereçamento IP detalhadamente na \nSeção 4.4.2.\n• Opções. O campo de opções permite que um cabeçalho IP seja estendido. A intenção é que as opções de \ncabeçalho sejam usadas raramente — daí a decisão de poupar sobrecarga não incluindo a informação \nem campos de opções em todos os cabeçalhos de datagrama. Contudo, a mera existência de opções, na \nrealidade, complica as coisas — uma vez que cabeçalhos de datagramas podem ter comprimentos va-\nriáveis, não é possível determinar a priori onde começará o campo de dados. Além disso, como alguns \ndatagramas podem requerer processamento de opções e outros não, o tempo necessário para processar \num datagrama IP em um roteador pode variar bastante. Essas considerações se tornam particular-\nmente importantes para o processamento do IP em roteadores e hospedeiros de alto desempenho. Por \nessas e outras razões, as opções IP foram descartadas no cabeçalho da versão IPv6, como discutimos \nna Seção 4.4.4.\n• Dados (carga útil). Por fim, chegamos ao último e mais importante campo — a razão de ser do datagrama! \nEm muitas circunstâncias, o campo de dados do datagrama IP contém o segmento da camada de trans-\nporte (TCP ou UDP) a ser entregue ao destino. Contudo, o campo de dados pode carregar outros tipos de \ndados, como mensagens ICMP (discutidas na Seção 4.4.3).\nNote que um datagrama IP tem um total de 20 bytes de cabeçalho (admitindo-se que não haja opções). Se \no datagrama carregar um segmento TCP, então cada datagrama (não fragmentado) carrega um total de 40 bytes \nde cabeçalho (20 bytes de cabeçalho IP, mais 20 bytes de cabeçalho TCP) junto com a mensagem de camada de \naplicação.\nFragmentação do datagrama IP\nVeremos no Capítulo 5 que nem todos os protocolos de camada de enlace podem transportar pacotes do \nmesmo tamanho. Alguns protocolos podem transportar datagramas grandes, ao passo que outros apenas pacotes \npequenos. Por exemplo, quadros Ethernet não podem conter mais do que 1.500 bytes de dados, enquanto quadros \npara alguns enlaces de longa distância não podem conter mais do que 576 bytes. A quantidade máxima de dados \nque um quadro de camada de enlace pode carregar é denominada unidade máxima de transmissão (maximum \ntransmission unit — MTU). Como cada datagrama IP é encapsulado dentro do quadro de camada de enlace para \nser transportado de um roteador até o roteador seguinte, a MTU do protocolo de camada de enlace estabelece um \nlimite estrito para o comprimento de um datagrama IP. Ter uma limitação estrita para o tamanho de um datagrama \nIP não é grande problema. O problema é que cada um dos enlaces da rota entre remetente e destinatário pode usar \ndiferentes protocolos de camada de enlace, e cada um desses protocolos pode ter diferentes MTUs.\nPara entender melhor a questão do repasse, imagine que você é um roteador que está interligando diversos \nenlaces, cada um rodando diferentes protocolos de camada de enlace com diferentes MTUs. Suponha que você \nreceba um datagrama IP de um enlace, verifique sua tabela de repasse para determinar o enlace de saída e perceba \n   Redes de computadores e a Internet\n248\nque este tem uma MTU menor do que o comprimento do datagrama IP. É hora de entrar em pânico — como você \nvai comprimir esse datagrama IP de tamanho excessivo no campo de carga útil do quadro da camada de enlace? \nA solução para esse problema é fragmentar os dados do datagrama IP em dois ou mais datagramas IP menores, \nencapsular cada um em um quadro separado na camada de enlace e, então, enviá-­\nlos pelo enlace de saída. Cada um \ndesses datagramas menores é denominado um fragmento.\nFragmentos precisam ser reconstruídos antes que cheguem à camada de transporte no destino. Na verdade, \ntanto o TCP quanto o UDP esperam receber da camada de rede segmentos completos, não fragmentados. Os \nprojetistas do IPv4 perceberam que a reconstrução de datagramas nos roteadores introduziria uma complicação \nsignificativa no protocolo e colocaria um freio no desempenho do roteador. (Se você fosse um roteador, ia querer \nreconstruir fragmentos além de tudo mais que tem de fazer?) Seguindo o princípio de manter a simplicidade do \nnúcleo da rede, os projetistas do IPv4 decidiram alocar a tarefa de reconstrução de datagramas aos sistemas finais, \ne não aos roteadores da rede.\nQuando um hospedeiro destinatário recebe uma série de datagramas da mesma origem, ele precisa deter-\nminar se alguns deles são fragmentos de um datagrama original de maior tamanho. Se alguns forem fragmentos, \no hospedeiro ainda deverá determinar quando recebeu o último fragmento e como os fragmentos recebidos \ndevem ser reconstruídos para voltar à forma do datagrama original. Para permitir que o hospedeiro destinatário \nrealize essas tarefas de reconstrução, os projetistas do IP (versão 4) criaram campos de identificação, flag e deslo-\ncamento de fragmentação no cabeçalho do datagrama IP. Quando um datagrama é criado, o hospedeiro remetente \nmarca o datagrama com um número de identificação, bem como com os endereços de origem e de destino. Em \ngeral, o hospedeiro remetente incrementa o número de identificação para cada datagrama que envia. Quando um \nroteador precisa fragmentar um datagrama, cada datagrama resultante (isto é, cada fragmento) é marcado com \no endereço de origem, o endereço do destino e o número de identificação do datagrama original. Quando o des-\ntinatário recebe uma série de datagramas do mesmo hospedeiro remetente, pode examinar os números de iden-\ntificação para determinar quais deles são, na verdade, fragmentos de um mesmo datagrama de tamanho maior. \nComo o IP é um serviço não confiável, é possível que um ou mais fragmentos jamais cheguem ao destino. Por \nessa razão, para que o hospedeiro de destino fique absolutamente seguro de que recebeu o último fragmento do \ndatagrama original, o último datagrama tem um bit de flag ajustado para 0, ao passo que todos os outros têm um \nbit de flag ajustado para 1. Além disso, para que o hospedeiro destinatário determine se falta algum fragmento (e \npossa reconstruí-los na ordem correta), o campo de deslocamento é usado para especificar a localização exata do \nfragmento no datagrama IP original.\nA Figura 4.14 apresenta um exemplo. Um datagrama de 4 mil bytes (20 bytes de cabeçalho IP, mais 3.980 by-\ntes de carga útil IP) chega a um roteador e deve ser repassado a um enlace com MTU de 1.500 bytes. Isso implica \nque os 3.980 bytes de dados do datagrama original devem ser alocados a três fragmentos separados (cada qual é \ntambém um datagrama IP). Suponha que o datagrama original esteja marcado com o número de identificação \n777. As características dos três fragmentos são mostradas na Tabela 4.2. Os valores da tabela refletem a exigência \nde que a quantidade de dados da carga útil original em todos os fragmentos, exceto o último, seja um múltiplo de \n8 bytes, e que o valor de deslocamento seja especificado em unidades de porções de 8 bytes.\nNo destino, a carga útil do datagrama é passada para a camada de transporte apenas após a camada IP ter \nreconstruído totalmente o datagrama IP original. Se um ou mais fragmentos não chegarem ao destino, o datagra-\nma incompleto será descartado e não será passado à camada de transporte. Mas, como aprendemos no capítulo \nanterior, se o TCP estiver sendo usado na camada de transporte, ele recuperará essa perda fazendo a origem \nretransmitir os dados do datagrama original.\nVimos que a fragmentação do IP tem papel importante na união das diversas tecnologias distintas da cama-\nda de enlace. Mas a fragmentação também tem seu preço. Primeiro, ela dificulta roteadores e sistemas finais, que \nprecisam ser projetados para acomodar a fragmentação do datagrama e o reagrupamento. Segundo, a fragmen-\ntação pode ser usada para criar ataques DoS fatais, em que o atacante envia uma série de fragmentos estranhos e \ninesperados. Um exemplo clássico é o ataque Jolt2, no qual o atacante envia uma cadeia de pequenos fragmentos \npara o computador-alvo, nenhum dos quais com um deslocamento de zero. O alvo pode se recolher ao tentar \nA CAMADA  de REDE  249 \nreconstruir datagramas pelos pacotes degenerados. Outra classe de ­\nexploits envia sobreposição de fragmentos IP, \nou seja, fragmentos cujos valores de deslocamento são definidos para que não se alinhem corretamente. Sistemas \noperacionais vulneráveis, sem ­\nsaber o que fazer com a sobreposição de fragmentos, podem pifar [Skoudis, 2006]. \nComo veremos no final desta seção, uma nova versão do protocolo IP, o IPv6, põe fim por completo à fragmen-\ntação, o processamento do pacote IP e tornando o IP menos vulnerável a ataques.\nNo site de apoio deste livro apresentamos um applets Java que gera fragmentos. Basta ­\ninformar o tamanho do \ndatagrama que está chegando, a MTU e a identificação do datagrama, e o applets gera automaticamente os fragmen-\ntos para você. \nTabela 4.2  fragmentos IP\nFragmento\nBytes\nID\nDeslocamento\nFlag\n1o fragmento\n1.480 bytes no campo de dados \ndo datagrama IP\nidentificação = 777\n0 (o que significa que os dados \ndevem ser inseridos a partir do \nbyte 0)\n1 (o que significa que \nhá mais)\n2o fragmento\n1.480 bytes de dados\nidentificação = 777\n185 (o que significa que os dados \ndevem ser inseridos a partir do byte \n1.480. Note que 185 x 8 = 1.480)\n1 (o que significa que \nhá mais)\n3o fragmento\n1.020 bytes de dados  \n(= 3.980 –1.480 –1.480)\nidentificação = 777\n370 (o que significa que os dados \ndevem ser inseridos a partir do byte \n2.960. Note que 370 x 8 = 2.960)\n0 (o que significa \nque esse é o último \nfragmento)\n4.4.2  Endereçamento IPv4\nAgora voltaremos nossa atenção ao endereçamento IPv4. Embora você talvez esteja pensando que o endereça-\nmento seja um tópico complicado, esperamos que, no final deste capítulo, convença-se de que o endereçamento na \nInternet não é apenas um tópico rico, cheio de sutilezas e interessante, mas também de crucial importância. Tratamen-\ntos excelentes do endereçamento IPv4 podem ser encontrados em 3Com Addressing [2012] e no primeiro capítulo de \nStewart [1999].\nFigura 4.14  Fragmentação e reconstrução IP\nFragmentação:\nEntrada: um datagrama grande (4.000 bytes)\nSaída: três datagramas menores\nReconstrução:\nEntrada: três datagramas menores\nSaída: um datagrama grande (4.000 bytes)\nMTU do enlace: 1.500 bytes\n   Redes de computadores e a Internet\n250\nAntes de discutirmos o endereçamento IP, contudo, temos de falar um pouco sobre como hospedeiros e ro-\nteadores estão interconectados na rede. Um hospedeiro em geral tem apenas um único enlace com a rede; quando \no IP no hospedeiro quer enviar um datagrama, ele o faz por meio desse enlace. A fronteira entre o hospedeiro e o \nenlace físico é denominada interface. Agora considere um roteador e suas interfaces. Como a tarefa de um roteador \né receber um datagrama em um enlace e repassá-lo a algum outro enlace, ele necessariamente estará ligado a dois \nou mais enlaces. A fronteira entre o roteador e qualquer um desses enlaces também é denominada uma interface. \nAssim, um roteador tem múltiplas interfaces, uma para cada enlace. Como todos os hospedeiros e roteadores po-\ndem enviar e receber datagramas IP, o IP exige que cada interface tenha seu próprio endereço IP. Desse modo, um \nendereço IP está tecnicamente associado com uma interface, e não com um hospedeiro ou um roteador que contém \naquela interface.\nCada endereço IP tem comprimento de 32 bits (equivalente a 4 bytes). Portanto, há um total de 232\n ende-\nreços IP possíveis. Fazendo uma aproximação de 210\n por 103, é fácil ver que há cerca de 4 bilhões de endereços \nIP possíveis. Esses endereços são escritos em notação decimal separada por pontos (dotted-decimal notation), \nna qual cada byte do endereço é escrito em sua forma decimal e separado dos outros bytes do endereço por um \nponto. Por exemplo, considere o endereço IP 193.32.216.9. O 193 é o número decimal equivalente aos primeiros \n8 bits do endereço; o 32 é o decimal equivalente ao segundo conjunto de 8 bits do endereço e assim por diante. \nPor conseguinte, o endereço 193.32.216.9, em notação binária, é:\n11000001 00100000 11011000 00001001\nCada interface em cada hospedeiro e roteador da Internet global tem de ter um endereço IP globalmente \nexclusivo (exceto as interfaces por trás de NATs, como discutiremos no final desta seção). Contudo, os endereços \nnão podem ser escolhidos de qualquer maneira. Uma parte do endereço IP de uma interface será determinada pela \nsub-rede à qual ela está conectada.\nA Figura 4.15 fornece um exemplo de endereçamento IP e interfaces. Nela, um roteador (com três interfaces) é \nusado para interconectar sete hospedeiros. Examine os endereços IP atribuídos às interfaces de hospedeiros e rotea-\ndores; há diversas particularidades a notar. Todos os três hospedeiros da parte superior esquerda da Figura 4.15 — e \ntambém a interface do roteador ao qual estão ligados — têm um endereço IP na forma 223.1.1.xxx, ou seja, todos \ntêm os mesmos 24 bits mais à esquerda em seus endereços IP\n. As quatro interfaces também estão interconectadas por \numa rede que não contém nenhum roteador. Essa rede poderia ser interconectada por uma LAN Ethernet, caso em \nque as interfaces seriam conectadas por um comutador Ethernet (conforme discutiremos no Capítulo 5) ou por um \nponto de acesso sem fio (como veremos no Capítulo 6). Vamos representar essa rede sem roteador que conecta esses \nhospedeiros como uma nuvem por enquanto, mas entraremos nos detalhes internos dessas redes nos Capítulos 5 e 6.\nNa terminologia IP, essa rede que interconecta três interfaces de hospedeiros e uma interface de roteador \nforma uma sub-rede [RFC 950]. (Na literatura da Internet, uma sub-rede também é denominada uma rede IP \nou simplesmente uma rede.) O endereçamento IP designa um endereço a essa sub-rede: 223.1.1.0/24, no qual \na notação /24, às vezes conhecida como uma máscara de sub-rede, indica que os 24 bits mais à esquerda do \nconjunto de 32 bits definem o endereço da sub-rede. Assim, a sub-rede 223.1.1.0/24 consiste em três interfaces \nde hospedeiros (223.1.1.1, 223.1.1.2 e 223.1.1.3) e uma interface de roteador (223.1.1.4). Quaisquer hospedeiros \nadicionais ligados à sub-rede 223.1.1.0/24 seriam obrigados a ter um endereço na forma 223.1.1.xxx. Há duas \nsub-redes adicionais mostradas na Figura 4.15: a sub-rede 223.1.2.0/24 e a sub-rede 223.1.3.0/24. A Figura 4.16 \nilustra as três sub-redes IP presentes na Figura 4.15.\nA definição IP de uma sub-rede não está restrita a segmentos Ethernet que conectam múltiplos hospedeiros \na uma interface de roteador. Para entender melhor, considere a Figura 4.17, que mostra três roteadores interco-\nnectados por enlaces ponto a ponto. Cada roteador tem três interfaces, uma para cada enlace ponto a ponto e uma \npara o enlace para um grupo, que conecta diretamente o roteador a um par de hospedeiros. Que sub-redes IP \nestão presentes aqui? Três sub-redes, 223.1.1.0/24, 223.1.2.0/24 e 223.1.3.0/24, semelhantes às que encontramos na \nFigura 4.15. Mas note que também há três sub-redes adicionais nesse exemplo: uma sub-rede, 223.1.9.0/24, para as \ninterfaces que conectam os roteadores R1 e R2; outra, 223.1.8.0/24, para as interfaces que conectam os roteadores \nA CAMADA  de REDE  251 \nR2 e R3, e uma terceira, 223.1.7.0/24, para as interfaces que conectam os roteadores R3 e R1. Para um sistema geral \ninterconectado de roteadores e hospedeiros, podemos usar a seguinte receita para definir as sub-redes no sistema:\nPara determinar as sub-redes, destaque cada interface de seu hospedeiro ou roteador, criando ilhas de \nredes isoladas com interfaces fechando as terminações das redes isoladas. Cada uma dessas redes isoladas é \ndenominada sub-rede.\nSe aplicarmos esse procedimento ao sistema interconectado da Figura 4.17, teremos seis ilhas ou sub-redes.\nFica claro, com essa discussão, que uma organização (tal como uma empresa ou instituição acadêmica) que \ntenha vários segmentos Ethernet e enlaces ponto a ponto terá várias sub-redes, e todos os equipamentos e dispositi-\nvos em uma dada sub-rede terão o mesmo endereço de sub--rede. Em princípio, as diversas sub-redes poderiam ter \nendereços bem diferentes. Entretanto, na prática, os endereços de sub-rede com frequência têm muito em comum. \nPara entender por que, voltemos nossa atenção ao modo como o endereçamento é manipulado na Internet global.\nA estratégia de atribuição de endereços da Internet é conhecida como roteamento interdomínio sem clas-\nses (Classless Interdomain Routing — CIDR), que se pronuncia “sáider”\n, como a palavra cider (cidra) em inglês \n[RFC 4632]. O CIDR generaliza a noção de endereçamento de sub-rede. Como acontece com o endereçamento \nde sub-redes, o endereço IP de 32 bits é dividido em duas partes e, mais uma vez, tem a forma decimal com pon-\ntos de separação a.b.c.d/x, em que x indica o número de bits da primeira parte do endereço.\nOs x bits mais significativos de um endereço na forma a.b.c.d/x constituem a parcela da rede do endereço IP e \ncostumam ser denominados prefixo (ou prefixo de rede). Uma organização em geral recebe um bloco de endereços \ncontíguos, isto é, uma faixa de endereços com um prefixo comum (ver quadro “Princípios na prática”). Nesse caso, os \nendereços IP de equipamentos e dispositivos dentro da organização compartilharão o prefixo comum. Quando estu-\ndarmos, na Seção 4.6, o protocolo de roteamento BGP da Internet, veremos que somente esses x bits indicativos do \nprefixo são considerados por roteadores que estão fora da rede da organização. Isto é, quando um roteador de fora re-\npassar um datagrama cujo endereço de destino esteja dentro da organização, terá de considerar apenas os x bits indi-\ncativos do endereço. Isso reduz de modo considerável o tamanho da tabela de repasse nesses roteadores, visto que um \núnico registro da forma a.b.c.d/x será suficiente para transmitir pacotes para qualquer destino dentro da organização.\nOs restantes (32 – x) bits de um endereço podem ser considerados os bits que distinguem os equipamentos \ne dispositivos dentro da organização e todos eles têm o mesmo prefixo de rede. Eles serão os bits considerados no \nrepasse de pacotes em roteadores dentro da organização. Esses bits de ordem mais baixa podem (ou não) ter uma \nFigura 4.15  Endereços de interfaces e sub-redes\n223.1.1.1\n223.1.2.1\n223.1.2.2\n223.1.1.2\n223.1.1.4\n223.1.2.9\n223.1.3.27\n223.1.1.3\n223.1.3.1\n223.1.3.2\n   Redes de computadores e a Internet\n252\nestrutura adicional de sub-rede tal como a discutida anteriormente. Por exemplo, suponha que os primeiros 21 bits \ndo endereço a.b.c.d/21, por assim dizer, “ciderizado”\n, especificam o prefixo da rede da organização e são comuns aos \nendereços IP de todos os hospedeiros da organização. Os 11 bits restantes então identificam os hospedeiros específi-\ncos da organização. A estrutura interna da organização poderia ser tal que esses 11 bits mais à direita seriam usados \npara criar uma sub-rede dentro da organização, como discutido antes. Por exemplo, a.b.c.d/24 poderia se referir a \numa sub-rede específica dentro da organização.\nAntes da adoção do CIDR, os tamanhos das parcelas de um endereço IP estavam limitados a 8, 16 ou 24 bits, \num esquema de endereçamento conhecido como endereçamento com classes, já que sub-redes com endereços \n223.1.1.0/23\n223.1.2.0/23\n223.1.3.0/23\nFigura 4.16  Endereços de sub-redes\nFigura 4.17  Três roteadores interconectando seis sub-redes\n223.1.8.1\n223.1.8.0\n223.1.9.1\n223.1.7.1\n223.1.2.6\n223.1.2.1\n223.1.2.2\n223.1.3.1\n223.1.3.2\n223.1.1.3\n223.1.7.0\n223.1.9.2\n223.1.3.27\n223.1.1.1\n223.1.1.4\nR1\nR2\nR3\nA CAMADA  de REDE  253 \nde sub-rede de 8, 16 e 24 eram conhecidas como redes de classe A, B e C, respectivamente. A exigência de que \na parcela da sub-rede de um endereço IP tenha exatos 1, 2 ou 3 bytes há muito tempo se mostrou problemática \npara suportar o rápido crescimento do número de organizações com sub-redes de pequeno e médio portes. Uma \nsub-rede de classe C (/24) poderia acomodar apenas 28\n – 2 = 254 hospedeiros (dois dos 28 = 256 endereços são \nreservados para uso especial) — muito pequena para inúmeras organizações. Contudo, uma sub-rede de classe \nB (/16), que suporta até 65.634 hospedeiros, seria grande demais. Com o endereçamento com classes, uma orga-\nnização com, digamos, dois mil hospedeiros, recebia um endereço de sub-rede de classe B (/16), o que resultava \nno rápido esgotamento do espaço de endereços de classe B e na má utilização do espaço de endereço alocado. Por \nexemplo, uma organização que usasse um endereço de classe B para seus dois mil hospedeiros, recebia espaço de \nendereços suficiente para até 65.534 interfaces — deixando mais de 63 mil endereços sem uso e que não poderiam \nser utilizados por outras organizações.\nSeríamos omissos se não mencionássemos ainda outro tipo de endereço IP, o endereço de difusão \n255.255.255.255. Quando um hospedeiro emite um datagrama com endereço de destino 255.255.255.255, a men-\nsagem é entregue a todos os hospedeiros na mesma sub-rede. Os roteadores também têm a opção de repassar a \nmensagem para suas sub-redes vizinhas (embora em geral não o façam).\nAgora que já estudamos o endereçamento IP detalhadamente, precisamos saber, antes de qualquer coisa, \ncomo hospedeiros e sub-redes obtêm seus endereços. Vamos começar examinando como uma organização \nobtém um bloco de endereços para seus equipamentos e, então, veremos como um equipamento (tal como um \nhospedeiro) recebe um endereço do bloco de endereços da organização.\nEsse exemplo de um ISP que conecta oito organiza-\nções com a Internet também ilustra de maneira elegante \ncomo endereços “ciderizados” alocados com cuidado \nfacilitam o roteamento. Suponha, como mostra a Figura \n4.18, que o ISP (que chamaremos de ISP Fly-By-Night) \nanuncie ao mundo exterior que devem ser enviados a ele \nquaisquer datagramas cujos primeiros 20 bits de ende-\nreço sejam iguais a 200.23.16.0/20. O resto do mundo \nnão precisa saber que dentro do bloco de endereços \n200.23.16.0/20 existem, na verdade, oito outras organi-\nzações, cada qual com suas próprias sub-redes. A ca-\npacidade de usar um único prefixo de rede para anunciar \nvárias redes costuma ser denominada agregação de \nendereços (e também agregação de rotas ou resumo \nde rotas).\nA agregação de endereços funciona muito bem \nquando estes são alocados em blocos ao ISP e, então, \npelo ISP às organizações clientes. Mas o que ocorre \nquando os endereços não são alocados dessa maneira \nhierárquica? O que aconteceria, por exemplo, se o ISP \nFly-By-Night adquirisse o ISP-R-Us e então ligasse a \nOrganização 1 com a Internet por meio de sua subsidiá-\nria ISP-R-Us? Como mostra a Figura 4.18, a subsidiária \n \nISP-R-Us é dona do bloco de endereços 199.31.0.0/16, \nmas os endereços IP da Organização 1 infelizmente \nestão fora desse bloco. O que deveria ser feito nesse \ncaso? Com certeza, a Organização 1 poderia renumerar \ntodos os seus roteadores e hospedeiros para que seus \nendereços ficassem dentro do bloco de endereços do \nISP-R-Us. Mas essa é uma solução dispendiosa, e a \nOrganização 1 poderia muito bem preferir mudar para \noutra subsidiária no futuro. A solução em geral ado-\ntada é a Organização 1 manter seus endereços IP em \n200.23.18.0/23. Nesse caso, como mostra a Figura 4.19, \no ISP Fly-By-Night continua a anunciar o bloco de en-\ndereços 200.23.16.0/20 e o ISP-R-Us continua a anun-\nciar 199.31.0.0/16. Contudo, o ISP-R-Us agora anuncia \ntambém o bloco de endereços para a Organização 1, \n200.23.18.0/23. Quando outros roteadores da Internet \nvirem os blocos de endereços 200.23.16.0/20 (do ISP \nFly-By-Night) e 200.23.18.0/23 (do ISP-R-Us) e quise-\nrem rotear para um endereço no bloco 200.23.18.0/23, \nusarão a regra de correspondência com o prefixo mais \nlongo (veja Seção 4.2.2) e rotearão para o ISP-R-Us, já \nque ele anuncia o prefixo de endereço mais longo (mais \nespecífico) que combina com o endereço de destino.\nPrincípios na prática\n   Redes de computadores e a Internet\n254\nFigura 4.18  Endereçamento hierárquico e agregação de rotas\nOrganização 0\n200.23.16.0/23\nOrganização 1\nISP Fly-By-Night\n“Envie-me quaisquer\npacotes cujos\nendereços comecem\ncom 200.23.16.0/20”\nISP-R-Us\n200.23.18.0/23\nOrganização 2\n200.23.20.0/23\nOrganização 7\n200.23.30.0/23\nInternet\n“Envie-me quaisquer\npacotes cujos\nendereços comecem\ncom 199.31.0.0/16”\nFigura 4.19  ISP-R-Us tem uma rota mais específica para a Organização 1\nOrganização 0\n200.23.16.0/23\nOrganização 2\nISP Fly-By-Night\n“Envie-me quaisquer\npacotes cujos\nendereços comecem\ncom 200.23.16.0/20”\nISP-R-Us\n200.23.20.0/23\nOrganização 7\n200.23.30.0/23\nOrganização 1\n200.23.18.0/23\nInternet\n“Envie-me quaisquer\npacotes cujos\nendereços comecem\ncom 199.31.0.0/16\nou 200.23.18.0/23”\nObtenção de um bloco de endereços\nPara obter um bloco de endereços IP para utilizar dentro da sub-rede de uma organização, um administra-\ndor de rede poderia, primeiro, contatar seu ISP, que forneceria endereços a partir de um bloco maior de endereços \nque já estão alocados ao ISP. Por exemplo, o próprio ISP pode ter recebido o bloco de endereços 200.23.16.0/20. O \nISP, por sua vez, dividiria seu bloco de endereços em oito blocos de endereços contíguos, do mesmo tamanho, \n \nA CAMADA  de REDE  255 \ne daria um deles a cada uma de um conjunto de oito organizações suportadas por ele, como demonstrado a se-\nguir. (Sublinhamos a parte da sub-rede desses endereços para melhor visualização.)\nBloco do ISP\n200.23.16.0/20\n11001000\n00010111\n00010000\n00000000\nOrganização 0\n200.23.16.0/23\n11001000\n00010111\n00010000\n00000000\nOrganização 1\n200.23.18.0/23\n11001000\n00010111\n00010010\n00000000\nOrganização 2\n200.23.20.0/23\n11001000\n00010111\n00010100\n00000000\n. . .\n. . .\n. . .\nOrganização 7\n200.23.30.0/23\n11001000\n00010111\n00011110\n00000000\nEmbora a obtenção de um conjunto de endereços de um ISP seja um modo de conseguir um bloco de ende-\nreços, não é o único. Claro, também deve haver um modo de o próprio ISP obter um bloco de endereços. Há uma \nautoridade global que tenha a responsabilidade final de gerenciar o espaço de endereços IP e alocar blocos a ISPs e \noutras organizações? Sem dúvida que há! Endereços IP são administrados sob a autoridade da Internet Corporation \nfor Assigned Names and Numbers (ICANN) [ICANN, 2012], com base em diretrizes estabelecidas no RFC 2050. O \npapel da ICANN, uma organização sem fins lucrativos [NTIA, 1998], não é apenas alocar endereços IP\n, mas também \nadministrar os servidores DNS raiz. Essa organização também tem a controvertida tarefa de atribuir nomes de domí-\nnio e resolver disputas de nomes de domínio. A ICANN aloca endereços a serviços regionais de registro da Internet \n(por exemplo, ARIN, RIPE, APNIC e LACNIC), que, juntos, formam a Address Supporting Organization da ICANN \n[ASO-ICANN, 2012], e é responsável pela alocação/ administração de endereços dentro de suas regiões.\nObtenção de um endereço de hospedeiro: o Protocolo de Configuração Dinâmica de \nHospedeiros (DHCP)\nTão logo tenha obtido um bloco de endereços, uma organização pode atribuir endereços IP individuais às \ninterfaces de hospedeiros e roteadores. Em geral, um administrador de sistemas configurará de modo manual \nos endereços IP no roteador (muitas vezes remotamente, com uma ferramenta de gerenciamento de rede). Os \nendereços dos hospedeiros podem também ser configurados manualmente, mas essa tarefa costuma ser feita \nusando o Protocolo de Configuração Dinâmica de Hospedeiros (DHCP) [RFC 2131]. O DHCP permite que \num hospedeiro obtenha (seja alocado a) um endereço IP de maneira automática. Um administrador de rede pode \nconfigurar o DHCP para que determinado hospedeiro receba o mesmo endereço IP toda vez que se conectar, ou \num hospedeiro pode receber um endereço IP temporário diferente sempre que se conectar. Além de receber um \nendereço IP temporário, o DHCP também permite que o hospedeiro descubra informações adicionais, como a \nmáscara de sub-rede, o endereço do primeiro roteador (em geral chamado de roteador de borda padrão — default \ngateway) e o endereço de seu servidor DNS local.\nPor causa de sua capacidade de automatizar os aspectos relativos à rede da conexão de um hospedeiro, \no DHCP é em geral denominado um protocolo plug and play. Essa capacidade o torna muito atraente para \no administrador de rede que, caso contrário, teria de executar essas tarefas manualmente! O DHCP também \nestá conquistando ampla utilização em redes residenciais de acesso à Internet e em LANs sem fio, nas quais \nhospedeiros entram e saem da rede com frequência. Considere, por exemplo, um estudante que leva seu laptop \ndo quarto para a biblioteca, para a sala de aula. É provável que ele se conecte a uma nova sub-rede em cada um \ndesses lugares e, por conseguinte, precisará de um novo endereço IP em cada um deles. O DHCP é ideal para \n   Redes de computadores e a Internet\n256\nessa situação, pois há muitos usuários em trânsito e os endereços são utilizados apenas por um tempo limita-\ndo. O DHCP é, de maneira semelhante, útil em redes domésticas de acesso. Como exemplo, considere um ISP \nresidencial que tem dois mil clientes, porém, nunca mais de 400 estão on-line ao mesmo tempo. Nesse caso, em \nvez de precisar de um bloco de 2.048 endereços, um servidor DHCP que designa endereços dinamicamente só \nprecisa de um bloco de 512 endereços (por exemplo, um bloco da forma a.b.c.d/23). À medida que hospedeiros \nentram e saem da rede, o servidor DHCP precisa atualizar sua lista de endereços IP disponíveis. Toda vez que \num hospedeiro se conecta à rede, o servidor DHCP designa a ele um endereço arbitrário do seu reservatório \nde endereços disponíveis; toda vez que um hospedeiro sai, o endereço é devolvido ao reservatório.\nO DHCP é um protocolo cliente-servidor. Em geral o cliente é um hospedeiro recém-chegado que quer \nobter informações sobre configuração da rede, incluindo endereço IP, para si mesmo. Em um caso mais sim-\nples, cada sub-rede (no sentido de endereçamento da Figura 4.17) terá um servidor DHCP. Se não houver um \nservidor na sub-rede, é necessário um agente relé DHCP (normalmente um roteador) que sabe o endereço \nde um servidor DHCP para tal rede. A Figura 4.20 ilustra um servidor DHCP conectado à rede 223.1.2/24, \nservindo o roteador de agente de repasse para clientes que chegam conectados às sub-redes 223.1.1/24 \n \ne 223.1.3/24. Em nossa discussão a seguir, admitiremos que um servidor DHCP está disponível na sub-rede.\nPara um hospedeiro recém-chegado, o protocolo DHCP é um processo de quatro etapas, como mostrado na \nFigura 4.21 para a configuração de rede mostrada na Figura 4.20. Nesta figura, “Internet” (significando “seu ende-\nreço na Internet”) indica que o endereço está sendo alocado para um cliente recém-chegado. As quatro etapas são:\n• Descoberta do servidor DHCP. A primeira tarefa de um hospedeiro recém-chegado é encontrar um ser-\nvidor DHCP com quem interagir. Isso é feito utilizando uma mensagem de descoberta DHCP, a qual \no cliente envia dentro de um pacote UDP para a porta 67. O pacote UDP é envolvido em um datagrama \nIP. Mas para quem esse datagrama deve ser enviado? O hospedeiro não sabe o endereço IP da rede à qual \nestá se conectando, muito menos o endereço de um servidor DHCP para essa rede. Desse modo, o cliente \nDHCP cria um datagrama IP contendo sua mensagem de descoberta DHCP com o endereço IP de desti-\nno de difusão 255.255.255.255 e um endereço IP destinatário “desse hospedeiro” 0.0.0.0. O cliente DHCP \nFigura 4.20  Cenário cliente-servidor DHCP\n223.1.1.1\n223.1.1.2\n223.1.1.4\n223.1.2.9\n223.1.3.27\n223.1.1.3\n223.1.3.1\n223.1.3.2\n223.1.2.1\n223.1.2.5\n223.1.2.2\nCliente\nDHCP\nrecém-chegado\nServidor\nDHCP\nA CAMADA  de REDE  257 \ntransmite o datagrama IP por difusão à camada de enlace que, então, transmite esse quadro para todos \nos nós conectados à sub-rede (discutiremos os detalhes sobre difusão da camada de enlace na Seção 5.4).\n• Oferta(s) dos servidores DHCP. Um servidor DHCP que recebe uma mensagem de descoberta DHCP \nresponde ao cliente com uma mensagem de oferta DHCP, transmitida por difusão a todos os nós pre-\nsentes na sub-rede, novamente utilizando o endereço IP de transmissão 255.255.255.255. (Você poderia \nquestionar que essa resposta do servidor também deve ser transmitida por difusão.) Como diversos \nservidores DHCP podem estar presentes na sub-rede, o cliente pode se dar ao luxo de escolher dentre \nas muitas ofertas. Cada mensagem de oferta do servidor contém o ID de transação da mensagem de \ndescoberta recebida, o endereço IP proposto para o cliente, a máscara da rede e o tempo de concessão \ndo endereço IP — o tempo pelo qual o endereço IP será válido. É comum o servidor definir o tempo de \nconcessão para várias horas ou dias [Droms, 2002].\n• Solicitação DHCP\n. O cliente recém-chegado escolherá dentre uma ou mais ofertas do servidor e responderá à \nsua oferta selecionada com uma mensagem de solicitação DHCP, repetindo os parâmetros de configuração.\n• DHCP ACK. O servidor responde a mensagem de requisição DHCP com uma mensagem DHCP ACK, \nconfirmando os parâmetros requisitados.\nUma vez que o cliente recebe o DHCP ACK, a interação é concluída e ele pode usar o endereço IP alocado \npelo DHCP pelo tempo de concessão. Caso queira usar seu endereço após a expiração da concessão, o DHCP \ntambém fornece um mecanismo que permite ao cliente renovar sua concessão sobre um endereço IP.\nServidor DHCP:\n223.1.2.5\nCliente recém-chegado\nDescoberta DHCP\nTempo\nTempo\nOrigem: 0.0.0.0, 68\nDestino: 255.255.255.255, 67\nDHCPDISCOVER\nInternet: 0.0.0.0\nID transação: 654\nOrigem: 223.1.2.5,67\nDestino: 255.255.255.255, 68\nDHCPOFFER\nInternet: 223.1.2.4\nID transação: 654\nID servidor DHCP: 223.1.2.5\nVida útil: 3.600 s\nOferta DHCP\nOrigem: 223.1.2.5,67\nDestino: 255.255.255.255, 68\nDHCPACK\nInternet: 223.1.2.4\nID transação: 655\nID servidor DHCP: 223.1.2.5\nVida útil: 3.600 s\nACK DHCP\nOrigem: 0.0.0.0, 68\nDestino: 255.255.255.255, 67\nDHCPREQUEST\nInternet: 223.1.2.4\nID transação: 655\nID servidor DHCP: 223.1.2.5\nVida útil: 3600 s\nRequisição DHCP\nFigura 4.21  Interação cliente-servidor DHCP\n   Redes de computadores e a Internet\n258\nO valor da capacidade plug and play do DHCP é clara, considerando o fato de que a alternativa é configurar \nmanualmente um endereço IP do hospedeiro. Considere o aluno que se locomove da sala de aula para a biblioteca \naté seu dormitório com um laptop, entra em uma nova sub-rede, obtendo, assim, um novo endereço IP em cada \nlocal. É inimaginável que um administrador de sistema tivesse de reconfigurar laptops em cada local, e poucos \nalunos (exceto os que estão tendo aula de rede de computadores!) teriam condições de configurar seus laptops \nmanualmente. Entretanto, pelo aspecto da mobilidade, o DHCP possui desvantagens. Como um novo endereço \nIP é obtido de um novo DHCP toda vez que um nó se conecta a uma nova sub-rede, uma conexão TCP com uma \naplicação remota não pode ser mantida enquanto o nó móvel se locomove entre as sub-redes. No Capítulo 6, ana-\nlisaremos o IP móvel — uma extensão recente para a infraestrutura IP que permite que um nó móvel utilize um \núnico endereço permanente à medida que se locomove entre as sub-redes. Mais detalhes sobre o DHCP podem \nser encontrados em Droms [2002] e dhc [2012]. Uma implementação de referência de código-fonte aberto do \nDHCP está disponível em Internet System Consortium [ISC, 2012].\nTradução de endereços na rede (NAT)\nDada a nossa discussão sobre endereços de Internet e o formato do datagrama IPv4, agora estamos bem cien-\ntes de que todo equipamento que utiliza IP precisa de um endereço IP. Isso parece significar que, com a proliferação \nde sub-redes de pequenos escritórios e de escritórios residenciais (small office home office — SOHO), sempre que \num desses escritórios quiser instalar uma LAN para conectar várias máquinas, o ISP precisará alocar uma faixa de \nendereços para atender a todas as máquinas que usam IP ali. Se a sub-rede ficasse maior (por exemplo, as crianças \nda casa não só têm seus próprios computadores, mas também smartphones e jogos Game Boys em rede), seria pre-\nciso alocar um bloco de endereços maior. Mas, e se o ISP já tiver alocado as porções contíguas da faixa de endereços \nutilizada atualmente por esse escritório residencial? E, antes de tudo, qual é o proprietário típico de uma residência \nque quer (ou precisaria) saber como administrar endereços IP? Felizmente, há uma abordagem mais simples da \nalocação de endereços que vem conquistando uma utilização crescente nesses tipos de cenários: a tradução de en-\ndereços de rede (network address translation — NAT) [RFC 2663; RFC 3022; Zhang, 2007].\nA Figura 4.22 mostra a operação de um roteador que utiliza NAT. Esse roteador, que fica na residência, tem \numa interface que faz parte da rede residencial do lado direito da Figura 4.22. O endereçamento dentro dessa \nrede é exatamente como vimos antes — todas as quatro interfaces da rede têm o mesmo endereço de sub-rede, \n10.00.0/24. O espaço de endereço 10.0.0.0/8 é uma das três porções do espaço de endereço IP reservado pelo \n[RFC 1918] para uma rede privada, ou um domínio com endereços privados, tal como a rede residencial da Figu-\nra 4.22. Um domínio com endereços privados refere-se a uma rede cujos endereços somente têm significado para \nequipamentos pertencentes àquela rede. Para ver por que isso é importante, considere o fato de haver centenas \nde milhares de redes residenciais, muitas delas utilizando o mesmo espaço de endereço 10.0.0.0/24. Equipamen-\ntos que pertencem a determinada rede residencial podem enviar pacotes entre si utilizando o endereçamento \n10.0.0.0/24. Contudo, é claro que pacotes repassados da rede residencial para a Internet global não podem usar \nesses endereços (nem como de origem, nem como de destino) porque há centenas de milhares de redes utilizan-\ndo esse bloco de endereços. Isto é, os endereços 10.0.0.0/24 somente podem ter significado dentro daquela rede \nresidencial. Mas, se endereços privados têm significado apenas dentro de uma dada rede, como o endereçamento \né administrado quando pacotes são recebidos ou enviados para a Internet global, onde os endereços são necessa-\nriamente exclusivos? A resposta será dada pelo estudo da NAT.\nO roteador que usa NAT não parece um roteador para o mundo externo, pois se comporta como um equi-\npamento único com um único endereço IP. Na Figura 4.22, todo o tráfego que sai do roteador residencial para a \nInternet tem um endereço IP de origem 138.76.29.7, e todo o tráfego que entra nessa rede tem de ter endereço de \ndestino 138.76.29.7. Na essência, o roteador que usa NAT está ocultando do mundo exterior os detalhes da rede \nresidencial. (A propósito, você talvez esteja imaginando onde os computadores de redes residenciais obtêm seus \nendereços e onde o roteador obtém seu endereço IP exclusivo. A resposta é quase sempre a mesma — DHCP! \nO roteador obtém seu endereço do servidor DHCP do ISP e roda um servidor DHCP para fornecer endereços a \ncomputadores que estão no espaço de endereços NAT da rede residencial controlado pelo DHCP.)\nA CAMADA  de REDE  259 \nSe todos os datagramas que chegam ao roteador NAT provenientes da WAN tiverem o mesmo endereço IP \nde destino (especificamente, o endereço da interface do roteador NAT do lado da WAN), então como o roteador \nsabe para qual hospedeiro interno deve repassar um datagrama? O truque é utilizar uma tabela de tradução NAT \nno roteador NAT e incluir nos registros da tabela números de portas, bem como endereços IP.\nConsidere o exemplo da Figura 4.22. Suponha que um usuário que está utilizando o hospedeiro 10.0.0.1 da \nrede residencial requisite uma página de algum servidor Web (porta 80) cujo endereço IP é 128.119.40.186. O \nhospedeiro 10.0.0.1 escolhe o número de porta de origem (arbitrário) 3345 e envia o datagrama para dentro da \nLAN. O roteador NAT recebe o datagrama, gera um novo número de porta de origem, 5001, para o datagrama, \nsubstitui o endereço IP de origem por seu endereço IP do lado da WAN, 138.76.29.7, e substitui o número de \nporta de origem original, 3345, pelo novo número de porta de origem, 5001. Ao gerar um novo número de porta \nde origem, o roteador NAT pode selecionar qualquer número de porta de origem que não esteja correntemente \nna tabela de tradução NAT. (Note que, como o comprimento de um campo de número de porta é 16 bits, o proto-\ncolo NAT pode suportar mais de 60 mil conexões simultâneas com um único endereço IP do lado da WAN para \no roteador!) A NAT no roteador também adiciona um registro à sua tabela de tradução NAT. O servidor Web, \ntotalmente alheio ao fato de que o datagrama que está chegando com uma requisição HTTP foi manipulado pelo \nroteador NAT, responde com um datagrama cujo endereço de destino é o endereço IP do roteador NAT, e cujo \nnúmero de porta de destino é 5001. Quando esse datagrama chega ao roteador NAT, ele indexa a tabela de tradu-\nção NAT usando o endereço IP de destino e o número de porta de destino para obter o endereço IP (10.0.0.1) e o \nnúmero de porta de destino (3345) adequados para o navegador na rede residencial. O roteador então reescreve \no endereço de destino e o número de porta de destino do datagrama e o repassa para a rede residencial.\nA NAT conquistou ampla aceitação nos últimos anos. Mas devemos mencionar que muitos puristas da \ncomunidade da IETF têm grandes restrições à NAT. Primeiro, argumentam, a finalidade dos números de portas \né endereçar processos, e não hospedeiros. (De fato, a violação dessa regra pode causar problemas para servidores \nque rodam em redes residenciais, pois, como vimos no Capítulo 2, processos servidores esperam pela chegada de \nrequisições em números de portas bem conhecidos.) Segundo, alegam que roteadores devem processar pacotes \napenas até a camada 3. Terceiro, discutem, o protocolo NAT viola o argumento denominado fim a fim; isto é, hos-\npedeiros devem falar diretamente uns com os outros, sem a interferência de nós que modifiquem endereços IP e \nnúmeros de portas. Quarto, argumentam que deveríamos usar o IPv6 (veja Seção 4.4.4) para resolver a escassez \nde endereços IP, e não tentar resolver o problema imprudentemente com uma solução temporária como a NAT. \nMas, gostemos ou não, a NAT tornou-se um componente importante da Internet.\nFigura 4.22  Tradução de endereços de rede (S = Origem, D = Destino)\n3\n2\n10.0.0.1\n138.76.29.7\n10.0.0.4\n10.0.0.2\n10.0.0.3\nTabela de tradução NAT\n138.76.29.7, 5001\nLado da LAN\n10.0.0.1, 3345\n. . .\n. . .\nS = 138.76.29.7, 5001\nD = 128.119.40.186, 80 \n1\n4\nS = 128.119.40.186, 80\nD = 138.76.29.7, 5001\nS = 128.119.40.186, 80\nD = 10.0.0.1, 3345 \nS = 10.0.0.1, 3345\nD = 128.119.40.186, 80\nLado da WAN\n   Redes de computadores e a Internet\n260\nOutro problema importante da NAT é que ela interfere com aplicações P2P, incluindo as de compartilhamento \nde arquivos P2P e as de voz sobre IP. Lembre-se de que, no Capítulo 2, dissemos que, em uma aplicação P2P, qual-\nquer par A participante deve poder iniciar uma conexão TCP com qualquer outro par B participante. A essência do \nproblema é que, se o par B estiver por trás de uma NAT, não poderá agir como um servidor e aceitar conexões TCP. \nComo veremos nos problemas de fixação, essa questão da NAT pode ser contornada se o par A se conectar primei-\nro com o par B através de um par C intermediário, que não está por trás de uma NAT, e com o qual o par B tenha \nestabelecido uma conexão TCP que está em curso. Então, o par A pode solicitar ao par B, por intermédio do par C, \nque inicie uma conexão TCP diretamente com ele. Uma vez estabelecida a conexão P2P TCP direta entre os pares \nA e B, os dois podem trocar mensagens ou arquivos. Essa solução, denominada reversão de conexão, na verdade \né usada por muitas aplicações P2P para a travessia de NAT. Se ambos os pares, A e B, estiverem por trás de suas \npróprias NATs, a situação é um pouco mais complicada, mas pode ser resolvida utilizando repasses para aplicação, \ncomo vimos em repasses do Skype, no Capítulo 2.\nUPnP\nA travessia da NAT está sendo cada vez mais fornecida pelo Universal Plug and Play (UPnP), um proto-\ncolo que permite a um hospedeiro descobrir e configurar uma NAT próxima [UPnP Forum, 2012]. O UPnP \nexige que o hospedeiro e a NAT sejam compatíveis com ele. Com respeito ao UPnP, uma aplicação que roda \nem um hospedeiro pode solicitar um ­\nmapeamento da NAT entre seus (endereço IP particular, número de porta \nparticular) e (­\nendereço IP público, número de porta pública) para algum número de porta pública requisitado. \nSe a NAT aceitar a requisição e criar um mapeamento, então nós externos podem iniciar ­\nconexões TCP para \n(endereço IP público, número de porta pública). Além disso, o UPnP deixa a ­\naplicação saber o valor de (endereço \nIP público, número de porta pública), de modo que ela possa anunciá-los ao mundo exterior.\nComo exemplo, suponha que seu hospedeiro, localizado atrás de uma NAT com o UPnP habilitado, possua \no endereço IP particular 10.0.0.1 e esteja rodando o BitTorrent na porta 3345. Suponha também que o ende-\nreço IP público da NAT seja 138.76.29.7. Sua aplicação BitTorrent decerto quer poder aceitar conexões vindas \nde outros hospedeiros, para que blocos sejam adquiridos através delas. Para isso, a aplicação BitTorrent, em \nseu hospedeiro, solicita que a NAT crie uma “abertura” que mapeia (10.0.0.1, 3345) para (138.76.29.7, 5001). \n(A porta pública número 5001 é escolhida pela aplicação.) A aplicação BitTorrent em seu ­\nhospedeiro pode-\nria, também, anunciar a seu rastreador que está disponível em (138.76.29.7, 5001). Dessa maneira, um hos-\npedeiro externo que está rodando o BitTorrent consegue contatar o ­\nrastreador e descobrir que sua aplicação \nBitTorrent está rodando em (138.76.29.7, 5001). O hospedeiro externo pode enviar um pacote SYN TCP para \n(138.76.29.7, 5001). Quando a NAT receber o pacote SYN, alterará o endereço IP destinatário e o número da porta \n \nno pacote para (10.0.0.1, 3345) e encaminhará o pacote pela NAT.\nEm resumo, o UPnP permite que hospedeiros externos iniciem sessões de comunicação com hospedeiros \nque utilizam NAT, usando TCP ou UDP. As NATs foram inimigas das aplicações P2P por um longo tempo; o \nUPnP, que apresenta uma solução eficaz e potente para a travessia da NAT, pode ser a salvação. Nossa discussão \nsobre NAT e UPnP foi necessariamente breve. Para obter mais detalhes sobre a NAT, consulte Huston [2004] e \nCisco NAT [2012].\n4.4.3  Protocolo de Mensagens de Controle da Internet (ICMP)\nLembre-se de que a camada de rede da Internet tem três componentes principais: o protocolo IP, discutido \nna seção anterior; os protocolos de roteamento da Internet (entre eles RIP, OSPF e BGP), que serão estudados na \nSeção 4.6; e o ICMP, objeto desta seção.\nO ICMP, especificado no [RFC 792], é usado por hospedeiros e roteadores para comunicar informações de \ncamada de rede entre si. A utilização mais comum do ICMP é para comunicação de erros. Por exemplo, ao rodar \numa sessão Telnet, FTP ou HTTP, é possível que você já tenha encontrado uma mensagem de erro como “Rede de \ndestino inalcançável”\n. Essa mensagem teve sua origem no ICMP. Em algum ponto, um roteador IP não conseguiu \nA CAMADA  de REDE  261 \ndescobrir um caminho para o hospedeiro especificado em sua aplicação Telnet, FTP ou HTTP. O roteador criou \ne enviou uma mensagem ICMP do tipo 3 a seu hospedeiro indicando o erro.\nO ICMP é com frequência considerado parte do IP, mas, em termos de arquitetura, está logo acima, pois \nmensagens ICMP são carregadas dentro de datagramas IP. Isto é, mensagens ICMP são carregadas como carga útil \nIP, exatamente como segmentos TCP ou UDP, que também o são. De maneira semelhante, quando um hospedeiro \nrecebe um datagrama IP com ICMP especificado como protocolo de camada superior, ele demultiplexa o conteúdo \ndo datagrama para ICMP, exatamente como demultiplexaria o conteúdo de um datagrama para TCP ou UDP.\nMensagens ICMP têm um campo de tipo e um campo de código. Além disso, contêm o cabeçalho e os \nprimeiros 8 bytes do datagrama IP que causou a criação da mensagem ICMP em primeiro lugar (de modo que o \nremetente pode determinar o datagrama que causou o erro). Alguns tipos de mensagens ICMP selecionadas são \nmostrados na Figura 4.23. Note que mensagens ICMP não são usadas somente para sinalizar condições de erro.\nO conhecido programa ping envia uma mensagem ICMP do tipo 8 código 0 para o hospedeiro especificado. \nO hospedeiro de destino, ao ver a solicitação de eco, devolve uma resposta de eco ICMP do tipo 0 código 0. A \nmaior parte das execuções de TCP/IP suporta o servidor ping diretamente no sistema operacional; isto é, o ser-\nvidor não é um processo. O Capítulo 11 de Stevens [1990] fornece o código-fonte para o programa ping cliente. \nNote que o programa cliente tem de ser capaz de instruir o sistema operacional para que ele gere uma mensagem \nICMP do tipo 8 código 0.\nOutra mensagem ICMP interessante é a de redução da origem. Essa mensagem é pouco usada na prática. \nSua finalidade original era realizar controle de congestionamento — permitir que um roteador congestionado \nenviasse uma mensagem ICMP de redução da origem a um hospedeiro para obrigá-lo a reduzir sua velocidade \nde transmissão. Vimos no Capítulo 3 que o TCP tem seu próprio mecanismo de controle de congestionamento, \nque funciona na camada de transporte sem usar realimentação da camada de rede tal como a mensagem ICMP \nde redução da origem.\nNo Capítulo 1, apresentamos o programa Traceroute, que nos permite acompanhar a rota de um hos-\npedeiro a qualquer outro hospedeiro no mundo. O interessante é que o Traceroute é executado com men-\nsagens ICMP. Para determinar os nomes e endereços de roteadores entre a origem e o destino, o Traceroute \nda origem envia uma série de datagramas comuns ao destino. O primeiro deles tem um TTL de 1, o segun-\ndo tem um TTL de 2, o terceiro tem um TTL de 3 e assim por diante. A origem também aciona temporiza-\ndores para cada datagrama. Quando o ­\nenésimo datagrama chega ao enésimo roteador, este observa que o \nTTL do datagrama acabou de expirar. Segundo as regras do protocolo IP, o roteador descarta o datagrama \nFigura 4.23  Tipos de mensagens ICMP\nTipo ICMP\nCódigo\nDescrição\n0\n0\nresposta de eco (para ping)\n3\n0\nrede de destino inalcançável\n3\n1\nhospedeiro de destino inalcançável\n3\n2\nprotocolo de destino inalcançável\n3\n3\nporta de destino inalcançável\n3\n6\nrede de destino desconhecida\n3\n7\nhospedeiro de destino desconhecido\n4\n0\nrepressão da origem (controle de congestionamento)\n8\n0\nsolicitação de eco\n9\n0\nanúncio do roteador\n10\n0\ndescoberta do roteador\n11\n0\nTTL expirado\n12\n0\ncabeçalho IP inválido\n   Redes de computadores e a Internet\n262\ne envia uma mensagem ICMP de aviso à origem (tipo 11 código 0). Essa mensagem de aviso inclui o nome \ndo ­\nroteador e seu endereço IP. Quando chega à origem, a mensagem obtém, do temporizador, o tempo \nde viagem de ida e volta e, da mensagem ICMP, o nome e o endereço IP do enésimo roteador.\nComo uma origem de Traceroute sabe quando parar de enviar segmentos UDP? Lembre-se de que a ori-\ngem incrementa o campo do TTL para cada datagrama que envia. Assim, um deles conseguirá enfim chegar ao \nhospedeiro de destino. Como tal datagrama contém um segmento UDP com um número de porta improvável, \no hospedeiro de destino devolve à ­\norigem uma mensagem ICMP indicando que a porta não pôde ser alcançada \n(mensagem tipo 3, código 3). Quando recebe essa mensagem ICMP particular, o hospedeiro de origem sabe que \nnão precisa enviar mais pacotes de sondagem. (Na verdade, o programa Traceroute padrão envia conjuntos de \ntrês pacotes com o mesmo TTL; assim, a saída de Traceroute oferece três resultados para cada TTL.)\nDesse modo, o hospedeiro de origem fica a par do número e das identidades de roteadores que estão entre ele \ne o hospedeiro de destino e o tempo de viagem de ida e volta entre os dois. Note que o programa cliente Traceroute \ntem de ser capaz de instruir o sistema operacional para que este gere datagramas UDP com valores específicos de \nTTL e também tem de poder ser avisado por seu sistema operacional quando chegam mensagens ICMP. Agora que \nvocê entende como o Traceroute funciona, é provável que queira voltar e brincar um pouco com ele.\nInspecionando datagramas: firewalls e sistemas de detecção de intrusão\nSuponha que você deva administrar uma rede do-\nméstica, departamental, acadêmica ou corporativa. \nOs atacantes, sabendo a faixa de endereço IP da sua \nrede, podem enviar facilmente datagramas IP para en-\ndereços da sua faixa. Tais datagramas são capazes \nde fazer todos os tipos de coisas desonestas, inclu-\nsive mapear sua rede, fazendo seu reconhecimento \n(ping sweep) e varredura de portas, prejudicar hospe-\ndeiros vulneráveis com pacotes defeituosos, inundar \nservidores com milhares de pacotes ICMP e infectar \nhospedeiros incluindo malwares nos pacotes. Como \nadministrador de rede, o que você vai fazer com todos \nesses vilões capazes de enviar pacotes maliciosos à \nsua rede? Dois consagrados mecanismos de defesa \npara esses ataques de pacote são os firewalls e os \nsistemas de detecção de intrusão (IDSs).\nComo administrador de rede, você pode, primeiro, \ntentar instalar um firewall entre sua rede e a Internet. \n(A maioria dos roteadores de acesso, hoje, possui ca-\npacidade para firewall.) Os firewalls inspecionam o \ndatagrama e campos do cabeçalho do segmento, evi-\ntando que datagramas suspeitos entrem na rede inter-\nna. Um firewall pode estar configurado, por exemplo, \npara bloquear ­\ntodos os pacotes de requisição de eco \nICMP\n, ­\nimpedindo assim que um atacante realize um re-\nconhecimento de rede por sua faixa de endereço IP\n. É \ncapaz também de bloquear pacotes ­\nbaseando-se nos \n­\nendereços IP remetente e destinatário e em números de \nporta. Além disso, os firewalls podem ser configurados \npara rastrear ­\nconexões TCP\n, permitindo a entrada so-\nmente de datagramas que pertençam a conexões apro-\nvadas.\nUma proteção adicional pode ser fornecida com \num IDS. Um IDS, em geral localizado no limite de rede, \nrealiza “uma inspeção profunda de pacote”, examinan-\ndo não apenas campos de cabeçalho, como também \ncargas úteis no datagrama (incluindo dados da camada \nde aplicação). Um IDS possui um banco de dados de \nassinaturas de pacote à medida que novos ataques são \ndescobertos. Enquanto os pacotes percorrem o IDS, \neste tenta combinar campos de cabeçalhos e cargas \núteis às assinaturas em seu banco de dados de assi-\nnaturas. Se tal combinação é encontrada, cria-se um \nalerta. Os sistemas de prevenção de intrusão (IPS) são \nsemelhantes a um IDS, exceto pelo fato de bloquearem \npacotes além de criar alertas. No Capítulo 8, explorare-\nmos mais detalhadamente firewalls e IDSs.\nOs firewalls e os IDSs são capazes de proteger \nsua rede de todos os ataques? Claro, a resposta é \nnão, visto que os atacantes encontram novas formas \nde ataques continuamente para os quais as assinatu-\nras ainda não estão disponíveis. Mas os firewalls e os \nIDSs baseados em assinaturas tradicionais são úteis \npara proteger sua rede de ataques conhecidos.\nSegurança em foco\nA CAMADA  de REDE  263 \n4.4.4  IPv6\nNo começo da década de 1990, a IETF (Internet Engineering Task Force) iniciou um esforço para desenvol-\nver o sucessor do protocolo IPv4. Uma motivação importante para isso foi o entendimento de que o espaço de \nendereços IP de 32 bits estava começando a escassear, com novas sub-redes e nós IP sendo anexados à Internet \n(e ainda recebendo endereços IP exclusivos) a uma velocidade estonteante. Para atender a essa necessidade de \nmaior espaço para endereços IP, foi desenvolvido um novo protocolo IP, o IPv6. Os projetistas do IPv6 também \naproveitaram essa oportunidade para ajustar e ampliar outros aspectos do IPv4, com base na experiência opera-\ncional acumulada sobre esse protocolo.\nO momento em que todos os endereços IPv4 estariam alocados (e, por conseguinte, mais nenhuma sub-rede \npoderia ser ligada à Internet) foi objeto de considerável debate. Os dois ­\nlíderes do grupo de trabalho de Expectativa \nde Tempo de Vida dos Endereços (Address ­\nLifetime Expectations) da IETF estimaram que os endereços se esgota-\nriam em 2008 e 2018, respectivamente [Solensky, 1996]. Em fevereiro de 2011, a IANA alocou o último conjunto \nrestante de endereços IPv4 a um registrador regional. Embora esses registradores ainda tenham endereços IPv4 \ndisponíveis dentro de seus conjuntos, quando esses endereços se esgotarem, não haverá mais blocos de endereços \ndisponíveis para serem alocados a partir de um conjunto central [Houston, 2011a]. Embora as estimativas de esgo-\ntamento de endereço IPv4 de meados de 1990 sugerissem que poderia se passar um longo tempo até que o espaço \nde endereços do IPv4 fosse esgotado, ficou claro que seria necessário um tempo expressivo para disponibilizar uma \nnova tecnologia em escala tão gigantesca. Assim, foi dado início ao esforço denominado Próxima Geração do IP \n(Next Generation IP — IPng) [Bradner, 1996; RFC 1752]. O resultado foi a ­\nespecificação IP versão 6 (IPv6) [RFC \n2460]. (Uma pergunta recorrente é o que aconteceu com o IPv5. Foi proposto de início que o protocolo ST-2 se \ntornasse o IPv5, porém, mais tarde, esse protocolo foi descartado.) Excelentes fontes de informação sobre o IPv6 \npodem ser encontradas em Huitema [1998] e IPv6 [2012].\nFormato do datagrama IPv6\nO formato do datagrama IPv6 é mostrado na Figura 4.24. As mudanças mais importantes introduzidas no \nIPv6 ficam evidentes no formato do datagrama:\n• Capacidade de endereçamento expandida. O IPv6 aumenta o tamanho do endereço IP de 32 bits para 128 \nbits. Isso garante que o mundo não ficará sem endereços IP. Agora, cada grão de areia do planeta pode \nter um endereço IP. Além dos endereços para um grupo de individuais, o IPv6 introduziu um novo tipo \nde endereço, denominado endereço para qualquer membro do grupo (anycast), que permite que um \ndatagrama seja entregue a qualquer hospedeiro de um grupo. (Essa característica poderia ser usada, por \nexemplo, para enviar uma mensagem HTTP GET ao site mais próximo de um conjunto de sites espelha-\ndos que contenham um dado documento.)\n• Cabeçalho aprimorado de 40 bytes. Como discutiremos adiante, vários campos IPv4 foram descartados \nou tornaram-se opcionais. O cabeçalho de comprimento fixo de 40 bytes resultante permite processa-\nmento mais veloz do datagrama IP. Uma nova codificação de opções permite um processamento de \nopções mais flexível.\n• Rotulação de fluxo e prioridade. O IPv6 tem uma definição dúbia de fluxo. O RFC 1752 e o RFC 2460 \ndeclaram que isso permite “rotular pacotes que pertencem a fluxos particulares para os quais o reme-\ntente requisita tratamento especial, tal como um serviço de qualidade não padrão ou um serviço de \ntempo real”. Por exemplo, a transmissão de áudio e vídeo seria tratada como um fluxo. Por outro lado, \naplicações mais tradicionais, como transferência de arquivos e e-mail, poderiam não ser tratadas as-\nsim. É possível que o tráfego carregado por um usuário de alta prioridade (digamos, alguém que paga \npor um serviço melhor de tráfego) seja também tratado como um fluxo. O que fica claro, contudo, é \nque os projetistas do IPv6 preveem a possível necessidade de conseguir diferenciá-los, mesmo que \no exato significado de fluxo ainda não tenha sido determinado. O cabeçalho IPv6 também tem um \n   Redes de computadores e a Internet\n264\ncampo de 8 bits para classe de tráfego. Assim, como o campo TOS do IPv4, ele pode ser usado para \ndar prioridade a certos datagramas em um fluxo ou a datagramas de certas aplicações (por exemplo, \npacotes ICMP) em relação aos de outras (por exemplo, notícias pela rede).\nComo foi observado anteriormente, uma comparação entre as figuras 4.24 e 4.13 revela uma estrutura mais \nsimples e mais aprimorada para o datagrama IPv6. Os seguintes campos são definidos no IPv6:\n• Versão. Esse campo de 4 bits identifica o número da versão do IP. Não é surpresa que o IPv6 tenha o valor \n6. Note que colocar 4 nesse campo não cria um datagrama IPv4 válido. (Se criasse, a vida seria bem mais \nsimples — veja a discussão mais adiante, referente à transição do IPv4 para o IPv6.)\n• Classe de tráfego. Esse campo de 8 bits tem função semelhante à do campo TOS que vimos no IPv4.\n• Rótulo de fluxo. Como já discutimos, esse campo de 20 bits é usado para identificar um fluxo de datagramas.\n• Comprimento da carga útil. Esse valor de 16 bits é tratado como um número inteiro sem sinal que dá \no número de bytes no datagrama IPv6 que se segue ao pacote do cabeçalho, que tem tamanho fixo de \n40 bytes.\n• Próximo cabeçalho. Esse campo identifica o protocolo ao qual o conteúdo (campo de dados) desse da-\ntagrama será entregue (por exemplo, TCP ou UDP). Usa os mesmos valores do campo de protocolo no \ncabeçalho IPv4.\n• Limite de saltos. O conteúdo desse campo é decrementado em um para cada roteador que repassa o data-\ngrama. Se a contagem do limite de saltos chegar a zero, o datagrama será descartado.\n• Endereços de origem e de destino. Os vários formatos do endereço de 128 bits do IPv6 são descritos no \nRFC 4291.\n• Dados. Esta é a parte da carga útil do datagrama IPv6. Quando este alcança seu destino, a carga útil pode \nser extraída do datagrama IP e passada adiante para o protocolo especificado no campo de próximo \ncabeçalho.\nNessa discussão, apresentamos a finalidade dos campos que estão incluídos no datagrama IPv6. Quando \ncomparamos o formato do datagrama IPv6 da Figura 4.24 com o formato do datagrama IPv4 que vimos na Figu-\nra 4.13, notamos que diversos campos que aparecem no datagrama IPv4 não estão presentes no datagrama IPv6:\n• Fragmentação/remontagem. O IPv6 não permite fragmentação e remontagem em ­\nroteadores intermediá­\nrios; essas operações podem ser realizadas apenas pela origem e pelo destino. Se um datagrama IPv6 \nrecebido por um roteador for muito grande para ser repassado pelo enlace de saída, o roteador apenas \ndescartará o datagrama e devolveráao remetente uma mensagem de erro ICMP “Pacote muito grande” \n(veja a ­\nseguir). O remetente pode então reenviar os dados usando um datagrama IP de ­\ntamanho menor. \nFigura 4.24  Formato do datagrama IPv6\nVersão\nClasse de\ntráfego\nComprimento da carga útil\nPróximo\ncabeçalho (Hdr)\nLimite de saltos\nRótulo de ﬂuxo\n32 bits\nEndereço de origem\n(128 bits)\nEndereço de destino\n(128 bits)\nDados\nA CAMADA  de REDE  265 \nFragmentação e remontagem são operações que tomam muito tempo; retirar essas funcionalidades dos \nroteadores e colocá-las nos sistemas finais acelera consideravelmente o repasse IP para dentro da rede.\n• Soma de verificação do cabeçalho. Como os protocolos de camada de transporte (por exemplo, TCP e \nUDP) e de enlace de dados (por exemplo, Ethernet) nas camadas da Internet realizam soma de verifica-\nção, os projetistas do IP provavelmente acharam que essa funcionalidade era tão redundante na camada \nde rede que podia ser retirada. Mais uma vez, o processamento rápido de pacotes IP era uma preocu-\npação principal. Lembre-se de que em nossa discussão sobre o IPv4 na Seção 4.4.1 vimos que, como o \ncabeçalho IPv4 contém um campo TTL (semelhante ao campo de limite de saltos no IPv6), a soma de \nverificação do cabeçalho IPv4 precisava ser recalculada em cada roteador. Como acontece com a frag-\nmentação e a remontagem, essa também era uma operação de alto custo no IPv4.\n• Opções. O campo de opções não faz mais parte do cabeçalho-padrão do IP. Contudo, ele ainda não saiu \nde cena. Em vez disso, passou a ser um dos possíveis próximos cabeçalhos a ser apontados pelo cabeçalho \ndo IPv6. Ou seja, assim como os cabeçalhos dos protocolos TCP e UPD podem ser o próximo dentro de \num pacote IP, o campo de opções também poderá ser. A remoção do campo de opções resulta em um \ncabeçalho IP de tamanho fixo de 40 bytes.\nLembre-se de que dissemos na Seção 4.4.3 que o protocolo ICMP é usado pelos nós IP para informar con-\ndições de erro e fornecer informações limitadas (por exemplo, a resposta de eco para uma mensagem ping) a \num sistema final. Uma nova versão do ICMP foi definida para o IPv6 no RFC 4443. Além da reorganização das \ndefinições existentes de tipos e códigos ICMP, o ICMPv6 adicionou novos tipos e códigos exigidos pela nova \nfuncionalidade do IPv6. Entre eles estão incluídos o tipo “Pacote muito grande” e um código de erro “opções IPv6 \nnão reconhecidas”\n. Além disso, o ICMPv6 incorpora a funcionalidade do IGMP (Internet Group Management \nProtocol — protocolo de gerenciamento de grupos da Internet), que estudaremos na Seção 4.7. O IGMP, que é \nusado para gerenciar a adesão ou a saída de um hospedeiro de grupos multicast, era anteriormente um protocolo \nseparado do ICMP no IPv4.\nTransição do IPv4 para o IPv6\nAgora que vimos os detalhes técnicos do IPv6, vamos tratar de um assunto muito prático: como a Internet \npública, que é baseada no IPv4, fará a transição para o IPv6? O problema é que, enquanto os novos sistemas ha-\nbilitados para IPv6 podem ser compatíveis, isto é, podem enviar, rotear e receber datagramas IPv4, os sistemas \nhabilitados para IPv4 não podem manusear datagramas IPv6. Há várias opções possíveis [Huston 2011b].\nUma opção seria determinar um “dia da conversão” — uma data e um horário definidos em que todas \nas máquinas da Internet seriam desligadas e atualizadas, passando do IPv4 para o IPv6. A última transição \nimportante de tecnologia (do uso do NCP para o uso do TCP para serviço confiável de transporte) ocorreu há \nquase 25 anos. E, mesmo naquela época [RFC 801], quando a Internet era pequenina e ainda gerenciada por \num número reduzido de “sabichões”, ficou claro que esse “dia da conversão” não era possível. Um dia assim, \nenvolvendo centenas de milhões de máquinas e milhões de administradores e usuários de rede, é ainda mais \nimpensável hoje. O RFC 4213 descreve duas abordagens (que podem ser usadas independentemente ou em \nconjunto) para a integração gradual dos hospedeiros e roteadores IPv4 ao mundo IPv6 (com a meta de longo \nprazo de fazer a transição de todos os nós IPv4 para IPv6).\nProvavelmente, a maneira mais direta de introduzir nós habilitados ao IPv6 seja uma abordagem de pilha \ndupla, em que nós IPv6 também tenham uma implementação IPv4 completa. Esse nó, denominado nó IPv6/\nIPv4 no RFC 4213, estaria habilitado a enviar e receber datagramas tanto IPv4 quanto IPv6. Ao interagir com \num nó IPv4, um nó IPv6/IPv4 poderá usar datagramas IPv4; ao interagir com um nó IPv6, poderá utilizar IPv6. \nNós IPv6/IPv4 devem ter endereços IPv6 e IPv4. Além disso, devem ser capazes de determinar se outro nó é \nhabilitado para IPv6 ou apenas para IPv4. Esse problema pode ser resolvido usando o DNS (veja o Capítulo 2), \nque poderá retornar um endereço IPv6 se o nome do nó a ser resolvido for capacitado para IPv6. Caso contrário, \n   Redes de computadores e a Internet\n266\nele retornará um endereço IPv4. É claro que, se o nó que estiver emitindo a requisição DNS for habilitado apenas \npara IPv4, o DNS retornará apenas um endereço IPv4.\nNa abordagem de pilha dupla, se o remetente ou o destinatário forem habilitados apenas para IPv4, um \ndatagrama IPv4 deverá ser usado. Como resultado, é possível que dois nós habilitados para IPv6 acabem, basica-\nmente, enviando datagramas IPv4 um para o outro. Isso é ilustrado na Figura 4.25. Suponha que o nó A utiliza \nIPv6 e queira enviar um datagrama IP ao nó F, que também utiliza IPv6. Os nós A e B podem trocar um data-\ngrama IPv6. Contudo, o nó B deve criar um datagrama IPv4 para enviar a C. É claro que o campo de dados do \ndatagrama IPv6 pode ser copiado para o do datagrama IPv4 e que o mapeamento adequado de endereço também \npode ser feito. No entanto, ao realizar a conversão de IPv6 para IPv4, haverá campos IPv6 específicos no datagra-\nma IPv6 (por exemplo, o campo do identificador de fluxo) que não terão contrapartes em IPv4. As informações \ncontidas nesses campos serão perdidas. Assim, mesmo que E e F possam trocar datagramas IPv6, os datagramas \nIPv4 que chegarem a E e D não conterão todos os campos que estavam no datagrama IPv6 original enviado de A.\nUma alternativa para a abordagem de pilha dupla, também discutida no RFC 4213, é conhecida como \nimplantação de túnel. O túnel pode resolver o problema observado antes, permitindo, por exemplo, que E \nreceba o datagrama IPv6 originado por A. A ideia básica por trás da implementação do túnel é a seguinte. Su-\nponha que dois nós IPv6 (por exemplo, B e E na Figura 4.25) queiram interagir usando datagramas IPv6, mas \nestão conectados um ao outro por roteadores intervenientes IPv4. Referimo-nos ao conjunto de roteadores \nintervenientes IPv4 entre dois roteadores IPv6 como um túnel, como ilustrado na Figura 4.26. Com a imple-\nmentação do túnel, o nó IPv6 no lado remetente do túnel (por exemplo, B) pega o datagrama IPv6 inteiro e \no coloca no campo de dados (carga útil) de um datagrama IPv4. Esse datagrama IPv4 é então endereçado ao \nnó IPv6 no lado receptor (por exemplo, E) e enviado ao primeiro nó do túnel (por exemplo, C). Os roteadores \nIPv4 intermediários o direcionam entre eles, exatamente como fariam com qualquer outro, alheios ao fato \nde que o datagrama IPv4 contém um datagrama IPv6 completo. O nó IPv6 do lado receptor do túnel por fim \nrecebe o datagrama IPv4 (pois ele é o destino do datagrama IPv4!), determina que ele contém um datagrama \nIPv6, extrai este último e, então, o roteia exatamente como faria se tivesse recebido o datagrama IPv6 de um \nvizinho IPv6 diretamente ligado a ele.\nEncerramos esta seção mencionando que enquanto a adoção do IPv6 estava demorando para decolar [Law-\nton, 2001], a iniciativa foi tomada recentemente. Consulte Huston [2008b] para o emprego do IPv6 a partir de \n2008; veja em NIST IPv6 [2012] um instantâneo da implantação do IPv6 nos Estados Unidos. A proliferação de \ndispositivos como telefones e outros equipamentos portáteis preparados para IP dá um empurrão extra para a \nimplementação geral do IPv6. O Third Generation Partnership Program [3GPP 2012] na Europa especificou o \nIPv6 como o ­\nesquema de endereçamento padrão para multimídia em dispositivos móveis.\nUma lição importante que podemos aprender com a experiência do IPv6 é que há enorme dificuldade para \nmudar protocolos de camada de rede. Desde o início da década de 1990, numerosos novos protocolos foram \nanunciados como a próxima maior revolução da Internet, mas a maioria deles teve pouca aceitação até agora. \nDentre esses, estão o IPv6, os protocolos de grupo (ou multicast, veja a Seção 4.7) e os protocolos de reserva de \nFigura 4.25  Abordagem de pilha dupla\nA\nB\nC\nD\nE\nF\nIPv6\nA para B: IPv6\nB para C: IPv4\nD para E: IPv4\nE para F: IPv6\nIPv6\nIPv4\nIPv4\nIPv6\nIPv6\nFluxo: X\nOrigem: A\nDestino: F\ndados\nOrigem: A\nDestino: F\ndados\nOrigem: A\nDestino: F\ndados\nFluxo: ??\nOrigem: A\nDestino: F\ndados\nA CAMADA  de REDE  267 \nrecursos (Capítulo 7). Na verdade, introduzir novos protocolos na camada de rede é como substituir os alicerces \nde uma casa — é difícil de fazer sem demolir a casa inteira ou, no mínimo, retirar os moradores temporariamente \nda residência. Por outro lado, a Internet vem testemunhando a rápida disponibilização de novos protocolos na \ncamada de aplicação. Os exemplos clássicos são, é claro, a Web, mensagens ­\ninstantâneas e o compartilhamento de \narquivos P2P. Outros exemplos são a recepção de áudio e vídeo em tempo real e os jogos distribuídos. Introduzir \nnovos protocolos de camada de aplicação é como acrescentar uma nova camada de tinta em uma casa — é relati-\nvamente fácil de fazer e, se você escolher uma cor atraente, outras casas da vizinhança vão imitá-lo. Em resumo, \nno futuro podemos esperar mudanças na camada de rede da Internet, mas elas provavelmente ocorrerão dentro \nde uma escala de tempo bem mais lenta do que as que acontecerão na camada de aplicação.\n4.4.5  Uma breve investida em segurança IP\nNa Seção 4.4.3 discutimos o IPv4 com alguns detalhes, incluindo os serviços que oferece e como são imple-\nmentados. Ao ler esta seção, você pode ter percebido que nenhum serviço de segurança foi mencionado. De fato, \no IPv4 foi projetado em uma era (anos 1970) em que a Internet era utilizada, principalmente, entre pesquisadores \nde rede mutuamente confiáveis. Criar uma rede de computadores que integrava uma grande quantidade de tec-\nnologias da camada de enlace já era desafiador, sem ter de se preocupar com a segurança.\nContudo, com a segurança sendo a principal preocupação hoje, pesquisadores da Internet começaram a \ncriar novos protocolos da camada de rede que oferecem variados serviços de segurança. Um desses protocolos \né o IPsec, um dos protocolos da camada de rede mais conhecidos e empregados em Redes Virtuais Privadas \n(VPNs). Embora o IPsec e seus suportes sejam abordados no Capítulo 8, apresentamos uma breve introdução \nsobre seus serviços nesta seção.\nO IPsec foi desenvolvido para ser compatível com o IPv4 e o IPv6. Em particular, para obter os benefícios \ndo IPv6, não precisamos substituir as pilhas dos protocolos em todos os roteadores e hospedeiros na Internet. Por \nexemplo, usando o modo transporte (um dos “modos” do IPsec), se dois hospedeiros querem se comunicar em \nsegurança, o IPsec precisa estar disponível apenas nesses dois. Todos os outros roteadores e hospedeiros podem \ncontinuar a rodar o IPv4 simples.\nFigura 4.26  Implementação de túnel\nA\nB\nC\nD\nE\nF\nIPv6\nA para B: IPv6\nVisão física\nB para C: IPv4\n(encapsulando IPv6)\nD para E: IPv4\n(encapsulando IPv6)\nE para F: IPv6\nIPv6\nIPv4\nIPv4\nIPv6\nIPv6\nFluxo: X\nOrigem: A\nDestino: F\ndados\nOrigem: B\nDestino: E\nOrigem: B\nDestino: E\nA\nB\nE\nF\nIPv6\nVisão lógica\nIPv6\nTúnel\nIPv6\nIPv6\nFluxo: X\nOrigem: A\nDestino: F\ndados\nFluxo: X\nOrigem: A\nDestino: F\ndados\nFluxo: X\nOrigem: A\nDestino: F\ndados\n   Redes de computadores e a Internet\n268\nPara uma abordagem concreta, a partir daqui focaremos no modo de transporte do IPsec. Nesse modo, dois \nhospedeiros estabelecem, primeiro, uma sessão IPsec entre si mesmos. (Assim, o IPsec é orientado a conexão!) \nCom a sessão pronta, todos os segmentos TCP e UDP enviados entre os dois hospedeiros aproveitam os serviços \nde segurança fornecidos pelo IPsec. No lado remetente, a camada de transporte passa um segmento para o IPsec. \nEste, então, codifica o segmento, acrescenta campos de segurança adicionais a ele e envolve a carga útil resultante \nem um datagrama IP comum. (Na verdade, isso é um pouco mais complicado, como veremos no Capítulo 8.) De-\npois, o hospedeiro remetente envia o datagrama para a Internet, a qual o transporta ao destinatário. Em seguida, \no IPsec decodifica o segmento e o encaminha à camada de transporte.\nOs serviços oferecidos por uma sessão IPsec incluem:\n• Acordo criptográfico. Mecanismos que permitem que dois hospedeiros de comunicação concordem nos \nalgoritmos criptográficos e chaves.\n• Codificação das cargas úteis do datagrama IP. Quando o hospedeiro destinatário recebe um segmento \nda camada de transporte, o IPsec codifica a carga útil, que pode ser somente decodificada pelo IPsec no \nhospedeiro destinatário.\n• Integridade dos dados. O IPsec permite que o hospedeiro destinatário verifique se os campos do cabe-\nçalho do datagrama e a carga útil codificada não foram modificados enquanto o datagrama estava no \ncaminho da origem ao destino.\n• Autenticação de origem. Quando um hospedeiro recebe um datagrama IPsec de uma origem confiável (com \numa chave confiável — veja no Capítulo 8), o hospedeiro está certo de que o endereço IP remetente no \ndatagrama é a verdadeira origem do datagrama.\nQuando dois hospedeiros estabelecem uma sessão IPsec, todos os segmentos TCP e UDP enviados entre eles \nserão codificados e autenticados. O IPsec, portanto, oferece uma cobertura geral, protegendo toda a comunicação \nentre os dois hospedeiros para todas as aplicações de rede.\nUma empresa pode utilizar o IPsec para se comunicar de forma segura na Internet pública não segura. Para fins \nde ilustração, verificaremos um exemplo simples. Considere uma empresa que possua um grande número de ven-\ndedores que viajam, cada um levando um notebook da companhia. Suponha que os vendedores precisem consultar, \ncom frequência, informações confidenciais sobre a empresa (por exemplo, sobre preços e produtos) armazenadas \nem um servidor na matriz. Imagine, ainda, que os vendedores também precisem enviar documentos confidenciais \num para o outro. Como isso pode ser feito com o IPsec? Como você pode imaginar, instalamos o IPsec no servidor \ne em todos os notebooks dos vendedores. Com ele instalado nesses hospedeiros, quando um vendedor precisar se \ncomunicar com o servidor ou com outro vendedor, a sessão de comunicação estará protegida.\n4.5  Algoritmos de roteamento\nExploramos, neste capítulo, a função de repasse da camada de rede mais do que qualquer outra. Aprende-\nmos que, quando um pacote chega a um roteador, este indexa uma tabela de repasse e determina a interface de \nenlace para a qual o pacote deve ser dirigido. Aprendemos também que algoritmos de roteamento que rodam em \nroteadores de rede trocam e calculam as informações que são utilizadas para configurar essas tabelas de repasse. \nA interação entre algoritmos de roteamento e tabelas de repasse foi ilustrada na Figura 4.2. Agora que já nos apro-\nfundamos um pouco na questão do repasse, voltaremos a atenção para o outro tópico importante desse capítulo, \na saber, a função crítica da camada de rede, o roteamento. Quer ofereça um serviço de datagramas (quando pa-\ncotes diferentes entre um determinado par origem-destino podem seguir rotas diferentes) ou um serviço de VCs \n(quando todos os pacotes entre uma origem e um destino determinados pegarão o mesmo caminho), a camada \nde rede deve, mesmo assim, determinar o caminho que os pacotes percorrem entre remetentes e destinatários. \nVeremos que a tarefa do roteamento é determinar bons caminhos (ou rotas) entre remetentes e destinatários \natravés da rede de roteadores.\nA CAMADA  de REDE  269 \nEm geral um hospedeiro está ligado diretamente a um roteador, o roteador default para esse hospedeiro \n(também denominado roteador do primeiro salto). Sempre que um hospedeiro emitir um pacote, o pacote \nserá transferido para seu roteador default. Denominamos roteador de origem o roteador default do hospe-\ndeiro de origem e roteador de destino o roteador default do hospedeiro de destino. O problema de rotear um \npacote do hospedeiro de origem até o hospedeiro de destino se reduz, claramente, ao problema de direcionar \no pacote do roteador de origem ao roteador de destino, que é o foco desta seção.\nPortanto, a finalidade de um algoritmo de roteamento é simples: dado um conjunto de roteadores conec-\ntados por enlaces, um algoritmo de roteamento descobre um “bom” caminho entre o roteador de origem e o de \ndestino. Em geral, um “bom” caminho é aquele que tem o “menor custo”\n. No entanto, veremos que, na prática, \npreocupações do mundo real, como questões de política (por exemplo, uma regra que determina que “o roteador \nx, de propriedade da organização Y, não deverá repassar nenhum pacote originário da rede de propriedade da \norganização Z”), também entram em jogo para complicar algoritmos conceitualmente simples e elegantes, cuja \nteoria fundamenta a prática de roteamento nas redes de hoje.\nUm grafo é usado para formular problemas de roteamento. Lembre-se de que um grafo G = (N,E) é um \nconjunto N de nós e uma coleção E de arestas, no qual cada aresta é um par de nós do conjunto N. No contexto \ndo roteamento da camada de rede, os nós do grafo representam roteadores — os pontos nos quais são tomadas \ndecisões de repasse de pacotes — e as arestas que conectam os nós representam os enlaces físicos entre esses \nroteadores. Uma abstração gráfica de uma rede de computadores está exibida na Figura 4.27. Para ver alguns \ngrafos representando mapas de rede reais, consulte Dodge [2012]; Cheswick [2000]; para uma discussão de \ncomo os diferentes modelos baseados em grafo modelam a Internet, consulte Zegura [1997]; Faloutsos [1999]; \nLi [2004].\nComo ilustrado na Figura 4.27, uma aresta também tem um valor que representa seu ­\ncusto. Em geral, o \ncusto de uma aresta pode refletir o tamanho físico do enlace correspondente (por exemplo, um enlace transoceâ­\nnico poderia ter um custo mais alto do que um enlace terrestre de curta distância), a velocidade do enlace ou o \ncusto monetário a ele associado. Para nossos objetivos, consideraremos os custos da aresta apenas como um dado \ne não nos preocuparemos com o modo como eles são determinados. Para qualquer aresta (x, y) em E, denomi-\nnamos c(x, y) o custo da aresta entre os nós x e y. Se o par (x, y) não pertencer a E, estabelecemos c(x, y) = ∞. \nAlém disso, sempre consideraremos somente grafos não direcionados (isto é, grafos cujas arestas não têm uma \ndireção), de modo que a aresta (x,y) é a mesma que a aresta (y,x) e c(x,y) = c(y,x). Dizemos também que y é um \nvizinho do nó x se (x,y) pertencer a E.\nDado que são atribuídos custos às várias arestas na abstração do grafo, uma meta natural de um algoritmo \nde roteamento é identificar o caminho de menor custo entre origens e destinos. Para tornar esse problema mais \npreciso, lembremos que um caminho em um grafo G = (N,E) é uma sequência de nós (x1, x2,..., xp) tal que cada \num dos pares (x1, x2), (x2, x3),...,(xp–1, xp) são arestas em E. O custo de um caminho (x1, x2,..., xp) é apenas a soma \nde todos os custos das arestas ao longo do caminho, ou seja, c(x1, x2) + c(x2, x3) + ... + c(xp–1,xp). Dados quaisquer \ndois nós x e y, em geral há muitos caminhos entre os dois, e cada caminho tem um custo. Um ou mais desses \ncaminhos é um caminho de menor custo. Por conseguinte, o problema do menor custo é claro: descobrir um \ncaminho entre a origem e o destino que tenha o menor custo. Na Figura 4.27, por exemplo, o caminho de menor \ncusto entre o nó da origem u e o de destino w é (u, x, y, w), cujo custo de caminho é 3. Note que, se todas as arestas \ndo grafo tiverem o mesmo custo, o caminho de menor custo também é o caminho mais curto (isto é, o que tem \no menor número de enlaces entre a origem e o destino).\nApenas como simples exercício, tente descobrir o caminho de menor custo entre os nós u e z na Figura 4.27 \ne reflita um pouco sobre como você o calculou. Se você for como a maioria das pessoas, descobriu o caminho de \nu a z examinando a figura, traçando algumas rotas de u a z, e se convencendo, de algum modo, que o caminho \nescolhido tinha o menor custo entre todos os possíveis. (Você verificou todos os 17 possíveis caminhos entre u e z? \nProvavelmente, não!) Esse cálculo é um exemplo de um algoritmo de roteamento centralizado — o algoritmo é ro-\ndado em um local, o seu cérebro, com informações completas sobre a rede. De modo geral, uma maneira possível \nde classificar algoritmos de roteamento é como globais ou centralizados.\n   Redes de computadores e a Internet\n270\n• Um algoritmo de roteamento global calcula o caminho de menor custo entre uma origem e um destino \nusando conhecimento completo e global sobre a rede. Em outras palavras, o algoritmo considera como \nentradas a conectividade entre todos os nós e todos os custos dos enlaces. E isso exige que o algoritmo \nobtenha essas informações, de algum modo, antes de realizar de fato o cálculo. Este pode ser rodado em \num local (um algoritmo de roteamento global centralizado) ou replicado em vários locais. Contudo, a \nprincipal característica distintiva, nesse caso, é que um algoritmo global tem informação completa sobre \nconectividade e custo de enlaces. Na prática, algoritmos com informação global de estado são com fre-\nquência denominados algoritmos de estado de enlace (link-state — LS), já que devem estar a par dos \ncustos de cada enlace na rede. Estudaremos algoritmos de estado de enlace na Seção 4.5.1.\n• Em um algoritmo de roteamento descentralizado, o cálculo do caminho de menor custo é realizado de \nmodo iterativo e distribuído. Nenhum nó tem informação completa sobre os custos de todos os enlaces \nda rede. Em vez disso, cada nó começa sabendo apenas os custos dos enlaces diretamente ligados a ele. \nEntão, por meio de um processo iterativo de cálculo e de troca de informações com seus nós vizinhos \n(isto é, que estão na outra extremidade dos enlaces aos quais ele próprio está ligado), um nó gradual-\nmente calcula o caminho de menor custo até um destino ou um conjunto de destinos. O algoritmo de \nroteamento descentralizado que estudaremos logo adiante na Seção 4.5.2 é denominado algoritmo de \nvetor de distâncias (distance-vector algorithm — DV), porque cada nó mantém um vetor de estimativas \nde custos (distâncias) de um nó até todos os outros nós da rede.\nUma segunda maneira geral de classificar algoritmos de roteamento é como estáticos ou dinâmicos. Em \nalgoritmos de roteamento estáticos, as rotas mudam muito devagar ao longo do tempo, muitas vezes como \nresultado de intervenção humana (por exemplo, uma pessoa editando manualmente a tabela de repasse do \nroteador). Algoritmos de roteamento dinâmicos mudam os caminhos de roteamento à medida que mudam \nas cargas de tráfego ou a topologia da rede. Um algoritmo dinâmico pode ser rodado periodicamente ou como \nreação direta a mudanças de topologia ou de custo dos enlaces. Ao mesmo tempo em que são mais sensíveis a \nmudanças na rede, algoritmos dinâmicos também são mais suscetíveis a problemas como loops de roteamento \ne oscilação em rotas.\nUma terceira maneira de classificar algoritmos de roteamento é como sensíveis à carga ou insensíveis à carga. \nEm um algoritmo sensível à carga, custos de enlace variam dinamicamente para refletir o nível corrente de con-\ngestionamento no enlace subjacente. Se houver um alto custo associado com um enlace que está congestionado, um \nalgoritmo de roteamento tenderá a escolher rotas que evitem esse enlace. Embora antigos algoritmos de roteamento \nda ARPAnet fossem sensíveis à carga [McQuillan, 1980], foram encontradas várias dificuldades [Huitema, 1998]. Os \nalgoritmos de roteamento utilizados na Internet hoje (como RIP, OSPF e BGP) são insensíveis à carga, pois o custo \nde um enlace não reflete explicitamente seu nível de congestionamento atual (nem o mais recente).\nFigura 4.27  Modelo abstrato de grafo de uma rede de computadores\nx\ny\nv\n3\n5\n2\n5\n2\n3\n1\n1\n2\n1\nu\nz\nw\nA CAMADA  de REDE  271 \n4.5.1  O algoritmo de roteamento de estado de enlace (LS)\nLembre-se de que, em um algoritmo de estado de enlace, a topologia da rede e todos os custos de enlace \nsão conhecidos, isto é, estão disponíveis como dados para o algoritmo de estado de enlace. Na prática, isso se \nconsegue fazendo cada nó transmitir pacotes de estado de enlace a todos os outros nós da rede, uma vez que \ncada um desses pacotes contém as identidades e os custos dos enlaces ligados a ele. Na prática (por exemplo, \ncom o protocolo de roteamento OSPF da Internet, discutido na Seção 4.6.1) isso frequentemente é ­\nconseguido \ncom um algoritmo­\n de transmissão por difusão de estado de enlace [Perlman, 1999]. Algoritmos de transmis-\nsão por difusão serão estudados na Seção 4.7. O resultado da transmissão por difusão dos nós é que todos os \nnós têm uma visão idêntica e completa da rede. Cada um pode, então, rodar o algoritmo de estado de enlace e \ncalcular o mesmo conjunto de caminhos de menor custo como todos os outros nós.\nO algoritmo de roteamento de estado de enlace que apresentamos adiante é conhecido como algoritmo de \nDijkstra, o nome de seu inventor. Um algoritmo que guarda relações muito próximas com ele é o algoritmo de \nPrim; consulte Cormen [2001] para ver uma discussão geral sobre algoritmos de grafo. O algoritmo de Dijkstra \ncalcula o caminho de menor custo entre um nó (a origem, que chamaremos de u) e todos os outros nós da rede. \nÉ um algoritmo iterativo e tem a propriedade de, após a k-ésima iteração, conhecer os caminhos de menor custo \npara k nós de destino e, dentre os caminhos de menor custo até todos os nós de destino, esses k caminhos terão \nos k menores custos. Vamos definir a seguinte notação:\n• D(v): custo do caminho de menor custo entre o nó de origem e o destino v até essa iteração do algoritmo.\n• p(v): nó anterior (vizinho de v) ao longo do caminho de menor custo corrente desde a origem até v.\n• N': subconjunto de nós; v pertence a N' se o caminho de menor custo entre a origem e v for inequivoca-\nmente conhecido.\nO algoritmo de roteamento global consiste em uma etapa de inicialização seguida de um loop. O número de \nvezes que o loop é rodado é igual ao número de nós na rede. Ao terminar, o algoritmo terá calculado os caminhos \nmais curtos desde o nó de origem u até cada um dos outros nós da rede.\nAlgoritmo de estado de enlace para o nó de origem u\n1\t\nInicialização\n2\t\n\t\nN’ = {u}\n3\t\n\t\npara todos os nós v\n4\t\n\t\n\t\nse v for um vizinho de u\n5\t\n\t\n\t\n\t\nentão D(v) = c(u,v)\n6\t\n\t\n\t\nsenão D(v) = ∞\n7\n8\t\nLoop\n9\t\n\t\nencontre w não em N’ tal que D(w) é um mínimo\n10\t \t\nadicione w a N’\n11\t \t\natualize D(v) para cada vizinho v de w e não em N’:\n12\t \t\n\t\n\t\nD(v) = min( D(v), D(w) + c(w,v) )\n13\t \t\n/* o novo custo para v é o velho custo para v ou\n14\t \t\n\t\no custo do menor caminho conhecido para w mais o custo de w para v */\n15\t até N’= N\nComo exemplo, vamos considerar a rede da Figura 4.27 e calcular os ­\ncaminhos de menor custo de u \naté todos os destinos possíveis. Os cálculos do algoritmo estão resumidos na Tabela 4.3, na qual cada linha \nfornece os valores das variá­\nveis do algoritmo ao final da iteração. Vamos examinar ­\ndetalhadamente alguns \ndos primeiros estágios:\n• No estágio de inicialização, os caminhos de menor custo mais conhecidos de u até os vizinhos diretamen-\nte ligados a ele (v, w e x) são inicializados em 2, 1 e 5, respectivamente. Note, em particular, que o custo \naté w é estabelecido em 5 (embora logo veremos que, na realidade, existe um trajeto cujo custo é ainda \n   Redes de computadores e a Internet\n272\nmenor), já que este é o custo do enlace (um salto) direto u a w. Os custos até y e z são estabelecidos como \ninfinito, porque eles não estão diretamente conectados a u.\n• Na primeira iteração, examinamos os nós que ainda não foram adicionados ao conjunto N' e descobri-\nmos o nó de menor custo ao final da iteração anterior. Este é o nó x, com um custo de 1, e, assim, x é adi-\ncionado ao conjunto N'. A linha 12 do algoritmo de vetor de distâncias (LS) é então rodada para atualizar \nD(v) para todos os nós v, produzindo os resultados mostrados na segunda linha (Etapa 1) da Tabela 4.3. \nO custo do caminho até v não muda. Descobriremos que o custo do caminho até w pelo nó x (que era 5 \nao final da inicialização) é 4. Por conseguinte, esse caminho de custo mais baixo é selecionado e o prede-\ncessor de w ao longo do caminho mais curto a partir de u é definido como x. De maneira semelhante, o \ncusto até y (através de x) é calculado como 2 e a tabela é atualizada de acordo com isso.\n• Na segunda iteração, verificamos que os nós v e y são os que têm os caminhos de menor custo (2); deci-\ndimos o empate arbitrariamente e adicionamos y ao conjunto N' de modo que N' agora contém u, x e y. \nO custo dos nós remanescentes que ainda não estão em N' (isto é, nós v, w e z) são atualizados pela linha \n12 do algoritmo LS, produzindo os resultados mostrados na terceira linha da Tabela 4.3.\n• E assim por diante…\nQuando o algoritmo LS termina, temos, para cada nó, seu predecessor ao longo do caminho de menor custo \na partir do nó de origem. Temos também o predecessor para cada um deles; assim, podemos construir o caminho \ninteiro da origem até todos os destinos. Então, a tabela de repasse em um nó, por exemplo, u, pode ser construída \na partir dessas informações, armazenando, para cada destino, o nó do salto seguinte no caminho de menor custo \nde u até o destino. A Figura 4.28 mostra os caminhos de menor custo resultantes e a tabela de repasse em u para \na rede na Figura 4.27.\nQual é a complexidade do cálculo desse algoritmo? Isto é, dados n nós (sem contar a origem), quanto cálcu-\nlo é preciso efetuar no pior caso para descobrir os caminhos de menor custo entre a origem e todos os destinos? \nNa primeira iteração, precisamos pesquisar todos os n nós para determinar o nó w, que não está em N', e que tem \no custo mínimo. Na segunda, temos de verificar n – 1 nós para determinar o custo mínimo. Na terceira, n – 2 \nnós. E assim por diante. Em termos gerais, o número total de nós que precisamos pesquisar em todas as iterações \né n(n + 1)/2, e, assim, dizemos que a complexidade da implementação do algoritmo de estado de enlace para o \npior caso é de ordem n ao quadrado: O(n2). (Uma execução mais sofisticada, que utiliza uma estrutura de dados \nconhecida como pilha, pode descobrir o mínimo na linha 9 em tempo logarítmico e não linear, reduzindo assim \na complexidade.)\nAntes de concluirmos nossa discussão sobre o algoritmo LS, vamos considerar uma patologia que pode \nsurgir. A Figura 4.29 mostra uma topologia de rede simples em que os custos dos enlaces são iguais à carga trans-\nportada pelo enlace, refletindo, por exemplo, o atraso que seria experimentado. Nesse exemplo, os custos dos \nenlaces não são simétricos, isto é, c(u,v) é igual a c(v,u) apenas se a carga transportada em ambas as direções do \nenlace (u,v) for a mesma. Nesse exemplo, o nó z origina uma unidade de tráfego destinada a w, o nó x também \norigina uma unidade de tráfego destinada a w e o nó y injeta uma quantidade de tráfego igual a e, também destinada a w. \nTabela 4.3  Execução do algoritmo de estado de enlace na rede da Figura 4.27\nEtapa\nN'\nD(v),p(v)\nD(w),p(w)\nD(x),p(x)\nD(y),p(y)\nD(z),p(z)\n0\nu\n2,u\n5,u\n1,u\n∞\n∞\n1\nux\n2,u\n4,x\n2,x\n∞\n2\nuxy\n2,u\n3,y\n4,y\n3\nuxyv\n3,y\n4,y\n4\nuxyvw\n4,y\n5\nuxyvwz\nA CAMADA  de REDE  273 \nO roteamento inicial é mostrado na Figura 4.29(a) com os custos dos enlaces correspondentes à quantidade de \ntráfego transportada.\nQuando o algoritmo LS é rodado de novo, o nó y determina — baseado nos custos dos enlaces mostrados na \nFigura 4.29(a) — que o caminho em sentido horário até w tem um custo de 1, ao passo que o caminho em sentido \nanti-horário até w (que estava sendo usado) tem o custo de 1 + e. Por conseguinte, o caminho de menor custo de \ny até w é agora em sentido horário. De maneira semelhante, x determina que seu novo caminho de menor custo \naté w é também em sentido horário, resultando nos custos mostrados na Figura 4.29(b). Na próxima vez em que \no algoritmo LS é rodado, os nós x, y e z detectam um caminho de custo zero até w na direção anti-horária, e todos \ndirigem seu tráfego para as rotas anti-horárias. Na próxima vez em que o algoritmo LS é rodado, os nós x, y e z \nentão dirigem seu tráfego para as rotas em sentido horário.\nO que pode ser feito para evitar essas oscilações (que podem ocorrer com qualquer algoritmo, e não apenas \ncom um algoritmo LS, que use uma métrica de enlace baseada em congestionamento ou em atraso)? Uma solu-\nFigura 4.28  Caminhos de menor custo resultantes e tabela de repasse para o nó u\nDestino\nEnlace\nv\nw\nx\ny\nz\n(u, v)\n(u, x)\n(u, x)\n(u, x)\n(u, x)\nX\nY\nV\nU\nZ\nW\nFigura 4.29  Oscilações com roteamento sensível ao congestionamento\nw\ny\nz\nx\n1\n0\n0\n0\ne\n1 + e\n1\na.  Roteamento inicial\n1\ne\nw\ny\nz\nx\n2 + e\n1 + e\n1\n0\n0\n0\nb.  x, y detectam melhor caminho\n     até w em sentido horário\nw\ny\nz\nx\n0\n0\n0\n1\n1 + e\n2+ e\nc.  x, y, z detectam melhor caminho\n     até w em sentido anti-horário\nw\ny\nz\nx\n2 + e\n1 + e\n1\n0\n0\n0\nd.  x, y, z, detectam melhor caminho\n     até w em sentido horário\n1\n1\ne\n1\n1\ne\n1\n1\ne\n   Redes de computadores e a Internet\n274\nção seria tornar obrigatório que os custos dos enlaces não dependessem da quantidade de tráfego transportada \n— uma solução inaceitável, já que um dos objetivos do roteamento é evitar enlaces muito congestionados (por \nexemplo, enlaces com grande atraso). Outra solução seria assegurar que nem todos os roteadores rodassem o \nalgoritmo LS ao mesmo tempo. Esta parece ser uma solução mais razoável, já que é de esperar que, mesmo que \nos roteadores rodem o algoritmo LS com idêntica periodicidade, o instante de execução do algoritmo não seja \no mesmo em cada nó. O interessante é que os pesquisadores descobriram que os roteadores da Internet podem \nse autossincronizar [Floyd Synchronization, 1994]. Isto é, mesmo que inicialmente rodem o algoritmo com o \nmesmo período, mas em diferentes momentos, a instância de execução do algoritmo pode finalmente se tornar, \ne permanecer, sincronizada nos roteadores. Um modo de evitar essa autossincronização é cada roteador variar \naleatoriamente o instante em que envia um anúncio de enlace.\nAgora que examinamos o algoritmo de estado de enlace, vamos analisar outro importante algoritmo usado \nhoje na prática — o algoritmo de roteamento de vetor de distâncias.\n4.5.2  O algoritmo de roteamento de vetor de distâncias (DV)\nEnquanto o algoritmo LS usa informação global, o algoritmo de vetor de distâncias (distance-vector — \nDV) é iterativo, assíncrono e distribuído. É distribuído porque cada nó recebe alguma informação de um ou mais \nvizinhos diretamente ligados a ele, realiza cálculos e, em seguida, distribui os resultados de seus cálculos para \nseus vizinhos. É iterativo porque esse processo continua até que mais nenhuma informação seja trocada entre \nvizinhos. (O interessante é que este é um algoritmo finito — não há nenhum sinal de que o cálculo deve parar; \nele apenas para.) O algoritmo é assíncrono porque não requer que todos os nós rodem simultaneamente. Vere-\nmos que um algoritmo assíncrono, iterativo, finito e distribuído é muito mais interessante e divertido do que um \nalgoritmo centralizado!\nAntes de apresentar o algoritmo DV\n, é bom discutir uma relação importante que existe entre os custos dos cami-\nnhos de menor custo. Seja dx(y) o custo do caminho de menor custo do nó x ao nó y. Então, os menores custos estão \nrelacionados segundo a famosa equação de Bellman-Ford:\n\t\ndx(y) = minv{c(x,v) + dv(y)},\t\n(4.1)\nsendo o minv da equação calculado para todos os vizinhos de x. A equação de Bellman-Ford é bastante intuitiva. \nRealmente, se após transitarmos de x para v tomarmos o caminho de menor custo de v a y, o custo do caminho \nserá c(x,v) + dv(y). Como devemos começar viajando até algum vizinho v, o caminho de menor custo de x a y é o \nmínimo do conjunto dos c(x,v) + dv(y) calculados para todos os vizinhos v.\nMas, para aqueles que ainda se mostrem céticos quanto à validade da equação, vamos verificá-la para o nó \nde origem u e o nó de destino z na Figura 4.27. O nó de origem u tem três vizinhos: nós v, x e w. Percorrendo \nvários caminhos no grafo, é fácil ver que dv(z) = 5, dx(z) = 3 e dw(z) = 3. Passando esses valores para a Equação \n4.1, junto com os custos c(u,v) = 2, c(u,x) = 1 e c(u,w) = 5, temos du(z) = min{2 + 5, 5 + 3, 1 + 3} = 4, que é, claro, \nverdade e que é, exatamente, o resultado conseguido com o algoritmo de Dijkstra para a mesma rede. Essa veri-\nficação rápida deve ajudá-lo a vencer qualquer ceticismo que ainda possa ter.\nA equação de Bellman-Ford não é apenas uma curiosidade intelectual. Na verdade, ela tem uma importância \nprática significativa. Em particular, sua solução fornece os registros da tabela de repasse do nó x. Para verificar, seja \nv* qualquer nó vizinho que represente o mínimo na Equação 4.1. Então, se o nó x quiser enviar um pacote ao nó y \npelo caminho de menor custo, deverá, primeiro, repassá-lo para o nó v*. Assim, a tabela de repasse do nó x especifi-\ncaria o nó v* como o roteador do próximo salto para o destino final y. Outra contribuição importante dessa equação \né que ela sugere a forma da comunicação vizinho para vizinho que ocorrerá no algoritmo DV\n.\nA ideia básica é a seguinte. Cada nó começa com Dx(y), uma estimativa do custo do ­\ncaminho de menor custo \nentre ele mesmo e o nó y, para todos os nós em N. Seja Dx=[Dx(y): y em N] o vetor de distâncias do nó x, que é \nA CAMADA  de REDE  275 \no vetor de estimativas de custo de x até todos os ­\noutros nós, y, em N. Com o algoritmo DV cada nó x mantém os \nseguintes dados de roteamento:\n• Para cada vizinho v, o custo c(x,v) de x até o vizinho diretamente ligado a ele, v\n• O vetor de distâncias do nó x, isto é, Dx = [Dx(y): y em N], contendo a estimativa de x para seus custos até \ntodos os destinos, y, em N\n• Os vetores de distâncias de seus vizinhos, isto é, Dv = [Dv(y): y em N] para cada vizinho v de x\nNo algoritmo distribuído, assíncrono, cada nó envia, a intervalos regulares, uma cópia do seu vetor de \ndistâncias a cada um de seus vizinhos. Quando um nó x recebe um novo vetor de distâncias de qualquer de seus \nvizinhos v, ele armazena o vetor de distâncias de v e então usa a equação de Bellman-Ford para atualizar seu \npróprio vetor de distâncias, como a seguir:\nDx(y) = minv{c(x,v) + Dv(y)}      para cada nó y em N\nSe o vetor de distâncias do nó x tiver mudado como resultado dessa etapa de atualização, o nó x então \nenviará seu vetor de distâncias atualizado para cada um de seus vizinhos que, por sua vez, podem atualizar seus \npróprios vetores de distâncias. Parece milagre, mas, contanto que todos os nós continuem a trocar seus vetores de \ndistâncias de forma assíncrona, cada estimativa de custo Dx(y) convergirá para dx(y), que é, na verdade, o custo \ndo caminho de menor custo do nó x ao nó y [Bertsekas, 1991]!\nAlgoritmo de vetor de distâncias (DV)\nPara cada nó, x:\n1\t\nInicialização:\n2\t\n\t\npara todos os destinos y em N:\n3\t\n\t\n\t\nDx(y) = c(x,y) /* se y não é um vizinho então c(x,y) = ∞ */\n4\t\npara cada vizinho w\n5\t\n\t\nDw(y) = ? para todos os destinos y em N\n6\t\npara cada vizinho w\n7\t\n\t\nenvia vetor de distâncias Dx  = [Dx(y): y em N] para w\n8\n9\t\nloop\n10\t \t\nespere (até que ocorra uma mudança no custo do enlace ao vizinho\n11\t \t\n\t\n\t\nw ou até a recepção de um vetor de distâncias do vizinho w)\n12\n13\t \t\npara cada y em N:\n14\t \t\n\t\nDx(y) = minv{c(x,v) + Dv(y)}\n15\n16\t \t\nse Dx(y) mudou para algum destino y\n17\t \t\n\t\nenvia vetor de distâncias Dx = [Dx(y): y em N] para todos os vizinhos\n18\n19\t para sempre\nNo algoritmo DV, um nó x atualiza sua estimativa do vetor de distâncias quando percebe uma mudança de \ncusto em um dos enlaces ligados diretamente a ele ou recebe uma atualização do vetor de distâncias de algum vi-\nzinho. Mas, para atualizar sua própria tabela de repasse para um dado destino y, o que o nó x de fato precisa saber \nnão é a distância do caminho mais curto até y, mas qual nó vizinho v*(y) é o roteador do próximo salto ao longo \ndo caminho mais curto até y. Como era de se esperar, o roteador do próximo salto v*(y) é o vizinho v que represen-\ntar o mínimo na Linha 14 do algoritmo DV. (Se houver vários vizinhos v que representem o mínimo, então v*(y) \npode ser qualquer um dos vizinhos minimizadores.) Assim, nas Linhas 13-14, para cada destino y, o nó x também \ndetermina v*(y) e atualiza sua tabela de repasse para o destino y.\nLembre-se de que o algoritmo LS é um algoritmo global no sentido de que requer que cada nó obtenha, \nprimeiro, um mapa completo da rede antes de rodar o algoritmo de Dijkstra. O algoritmo DV é descentralizado \ne não usa essa informação global. De fato, a única informação que um nó terá são os custos dos enlaces até os \n   Redes de computadores e a Internet\n276\nvizinhos diretamente ligados a ele e as informações que recebe desses vizinhos. Cada nó espera uma atualização \nde qualquer vizinho (Linhas 10–11), calcula seu novo vetor de distâncias ao receber uma atualização (Linha 14) \ne distribui seu novo vetor de distâncias a seus vizinhos (Linhas 16–17). Algoritmos semelhantes ao DV são uti-\nlizados em muitos protocolos de roteamento na prática, entre eles o RIP e o BGP da Internet, o ISO IDRP, o IPX \nda Novell, e o ARPAnet original.\nA Figura 4.30 ilustra a operação do algoritmo DV para a rede simples de três nós mostrada na parte superior da \nfigura. A operação do algoritmo é ilustrada de um modo síncrono, no qual todos os nós recebem vetores de distâncias \nsimultaneamente de seus vizinhos, calculam seus novos vetores de distâncias e informam a seus vizinhos se esses ve-\ntores mudaram. Após estudar esse exemplo, você deve se convencer de que o algoritmo também opera corretamente \nem modo assíncrono, com cálculos de nós e atualizações de geração/recepção ocorrendo a qualquer instante.\nA coluna mais à esquerda na figura mostra três tabelas de roteamento iniciais para cada um dos três nós. \nPor exemplo, a tabela no canto superior à esquerda é a tabela de roteamento inicial do nó x. Dentro de uma tabela \nde roteamento específica, cada linha é um vetor de distâncias — em especial, a tabela de roteamento de cada nó \ninclui seu próprio vetor de distâncias e os vetores de cada um de seus vizinhos. Assim, a primeira linha da tabela \nde roteamento inicial do nó x é Dx = [Dx(x), Dx(y), Dx(z)] = [0, 2, 7]. A segunda linha e a terceira linha nessa tabela \nsão os vetores de distâncias recebidos mais recentemente dos nós y e z. Como na inicialização o nó x não recebeu \nnada do nó y ou z, os registros da segunda linha e da terceira linha estão definidos como infinito.\nFigura 4.30  Algoritmo de vetor de distâncias (DV)\nTabela do nó y\nTabela do nó x\n0 2 7\nx y z\n∞ ∞ ∞\n∞ ∞ ∞\nTempo\n7\n2\n1\ny\nx\nz\nTabela do nó z\nDe\nCusto até\nx\ny\nz\n0 2 3\nx y z\n2 0 1\n7 1 0\nDe\nCusto até\nx\ny\nz\n0 2 3\nx y z\n2 0 1\n3 1 0\nDe\nCusto até\nx\ny\nz\n2 0 1\nx y z\n∞ ∞ ∞\n∞ ∞ ∞\nDe\nCusto até\nx\ny\nz\n0 2 7\nx y z\n2 0 1\n7 1 0\nDe\nCusto até\nx\ny\nz\n0 2 3\nx y z\n2 0 1\n3 1 0\nDe\nCusto até\nx\ny\nz\n7 1 0\nx y z\n∞ ∞ ∞\n∞ ∞ ∞\nDe\nCusto até\nx\ny\nz\n0 2 7\nx y z\n2 0 1\n3 1 0\nDe\nCusto até\nx\ny\nz\n0 2 3\nx y z\n2 0 1\n3 1 0\nDe\nCusto até\nx\ny\nz\nA CAMADA  de REDE  277 \nApós a inicialização, cada nó envia seu vetor de distâncias a cada um de seus dois vizinhos. Isso é ilustrado \nna Figura 4.30 pelas setas que vão da primeira coluna de tabelas até a segunda. Por exemplo, o nó x envia seu vetor \nde distâncias Dx = [0, 2, 7] a ambos os nós y e z. Após receber as atualizações, cada nó recalcula seu próprio vetor \nde distâncias. Por exemplo, o nó x calcula\nDx(x) = 0\nDx(y) = min{c(x,y) + Dy(y), c(x,z) + Dz(y)} = min{2 + 0, 7 + 1} = 2\nDx(z) = min{c(x,y) + Dy(z), c(x,z) + Dz(z)} = min{2 + 1, 7 + 0} = 3\nPor conseguinte, a segunda coluna mostra, para cada nó, o novo vetor de distâncias do nó, junto com os \nvetores de distâncias que acabou de receber de seus vizinhos. Observe, por exemplo, que a estimativa do nó x para \no menor custo até o nó z, Dx(z), mudou de 7 para 3. Note também que, para par ao nó x, o nó vizinho y alcança o \nmínimo na linha 14 do algoritmo DV; assim, nesse estágio do algoritmo, temos que, no nó x, v*(y) = y e v*(z) = y.\nDepois que recalculam seus vetores de distâncias, os nós enviam novamente seus vetores de distâncias recal-\nculados a seus vizinhos (se houver uma mudança). Isso é ilustrado na Figura 4.30 pelas setas que vão da segunda \naté a terceira coluna de tabelas. Note que apenas os nós x e z enviam atualizações: o vetor de distâncias do nó y \nnão mudou, então esse nó não envia uma atualização. Após recebê-las, os nós então recalculam seus vetores de \ndistâncias e atualizam suas tabelas de roteamento, que são mostradas na terceira coluna.\nO processo de receber vetores de distâncias atualizados de vizinhos, recalcular os registros de tabelas de \nroteamento e informar aos vizinhos os custos modificados do caminho de menor custo até o destino continua \naté que mais nenhuma mensagem de atualização seja enviada. Nesse ponto, não ocorrerá mais nenhum cálculo \nde tabela de roteamento e o algoritmo entra em estado de inatividade; isto é, todos os nós estarão realizando a \nespera nas Linhas 10-11 do algoritmo DV. Este permanece no estado de inatividade até que o custo de um enlace \nmude, como veremos a seguir.\nAlgoritmo de vetor de distâncias: mudanças no custo do enlace e falha no enlace\nQuando um nó que está rodando o algoritmo DV detecta uma mudança no custo do enlace dele mesmo \naté um vizinho (Linhas 10-11), ele atualiza seu vetor de distâncias (Linhas 13-14) e, se houver uma modificação \nno custo do caminho de menor custo, informa a seus vizinhos (Linhas 16-17) seu novo vetor de distâncias. A \nFigura 4.31(a) ilustra um cenário em que o custo do enlace de y a x muda de 4 para 1. Destacamos aqui somente \nos registros na tabela de distâncias de y e z até o destino x. O algoritmo DV faz que ocorra a seguinte sequência \nde eventos:\n• No tempo t0, y detecta a mudança no custo do enlace (o custo mudou de 4 para 1), atualiza seu vetor de \ndistâncias e informa essa mudança a seus vizinhos, já que o vetor mudou.\n• No tempo t1, z recebe a atualização de y e atualiza sua própria tabela. Calcula um novo menor custo para \nx (cujo custo diminuiu de 5 para 2), e envia seu novo vetor de distâncias a seus vizinhos.\n• No tempo t2, y recebe a atualização de z e atualiza sua tabela de distâncias. Os menores custos de y não \nmudaram e, por conseguinte, y não envia nenhuma mensagem a z. O algoritmo entra em estado de ina-\ntividade.\nAssim, apenas duas iterações são necessárias para o algoritmo DV alcançar o estado de ­\ninatividade. A boa \nnotícia sobre a redução do custo entre x e y se propagou rapidamente pela rede.\nAgora vamos considerar o que pode acontecer quando o custo de um enlace aumenta. Suponha que o custo do \nenlace entre x e y aumente de 4 para 60, conforme mostra a Figura 4.31(b).\n1.\t Antes da mudança do custo do enlace, Dy(x) = 4, Dy(z) = 1, Dz(y) = 1, e Dz(x) = 5. No tempo t0, y detecta \numa mudança no custo do enlace (o custo mudou de 4 para 60). y calcula seu novo caminho de custo \nmínimo até x, de modo a ter um custo de\nDy(x) = min{c(y,x) + Dx(x), c(y,z) + Dz(x)} = min{60 + 0, 1 + 5} = 6\n   Redes de computadores e a Internet\n278\n\t\nÉ claro que, com nossa visão global da rede, podemos ver que esse novo custo via z está errado. Mas as únicas \ninformações que o nó y tem é que seu custo direto até x é 60 e que z disse a y que z pode chegar a x com um \ncusto de 5. Assim, para chegar a x, y teria de fazer a rota através de z, com a expectativa de que z será capaz de \nchegar a x com um custo de 5. A partir de t1, temos um loop de roteamento — para poder chegar a x, y faz a \nrota através de z, que por sua vez faz a rota através de y. Um loop de roteamento é como um buraco negro — \num pacote destinado a x que ao chegar a y ou a z a partir do momento t1 vai ricochetear entre esses dois nós \npara sempre (ou até que as tabelas de repasse sejam mudadas).\n2.\t Tão logo o nó y tenha calculado um novo custo mínimo até x, ele informará a z esse novo vetor de distân-\ncias no tempo t1.\n3.\t Algum tempo depois de t1, z recebe o novo vetor de distâncias de y, que indica que o custo mínimo de y \naté x é 6. z sabe que pode chegar até y com um custo de 1 e, por conseguinte, calcula um novo menor custo \naté x, D2(x) = min{50 + 0,1 + 6} = 7. Uma vez que o custo mínimo de z até x aumentou, z informa a y o seu \nnovo vetor de distâncias em t2.\n4.\t De maneira semelhante, após receber o novo vetor de distâncias de z, y determina Dy(x) = 8 e envia a z seu \nvetor de distâncias. Então z determina Dz(x)= 9 e envia a y seu vetor de distâncias e assim por diante.\nPor quanto tempo esse processo continua? Pode ter certeza de que o loop persistirá por 44 iterações (trocas \nde mensagens entre y e z) até que z possa, enfim, calcular que o custo de seu caminho via y é maior do que 50. \nNesse ponto, z (enfim!) determinará que seu caminho de menor custo até x é via sua conexão direta com x. En-\ntão y fará a rota até x via z. O resultado das más notícias sobre o aumento do custo do enlace na verdade viajou \ndevagar! O que teria acontecido se o custo do enlace c(y, x) tivesse mudado de 4 para 10.000 e o custo c(z, x) \nfosse 9.999? Por causa de cenários como esses, o problema que acabamos de examinar é, às vezes, denominado \nproblema de contagem ao infinito.\nAlgoritmo de vetor de distâncias: adição de reversão envenenada\nO cenário específico de looping que acabamos de descrever pode ser evitado usando uma técnica denomi-\nnada reversão envenenada (poisoned reverse). A ideia é simples — se a rota de z para chegar a x passa por y, então \nz deve anunciar a y que sua distância a x é infinita, isto é, z anunciará a y que Dz(x) = ∞ (mesmo que z saiba que, na \nverdade, Dz(x) = 5). z continuará contando essa mentirinha inocente a y enquanto a rota de z a x estiver passando \npor y. Enquanto y acreditar que z não tem nenhum caminho até x, y jamais tentará a rota até x por z, contanto \nque a rota de z a x continue a passar por y (e ele minta sobre isso).\nAgora vamos ver como a reversão envenenada resolve o problema específico do looping que encontramos \nantes na Figura 4.31(b). Como resultado da reversão envenenada, a tabela de distâncias de y indica que Dz(x) = ∞. \nQuando o custo do enlace (x, y) muda de 4 para 60 no tempo t0, y atualiza sua tabela e continua a estabelecer \nrotas diretamente para x, embora com um custo mais alto do que 60, e informa a z o seu novo custo até x, isto é, \nDy(x) = 60. Após receber a atualização em t1, z imediatamente desloca sua rota para x, para que passe pelo enlace \ndireto (z, x) a um custo de 50. Como este é um novo menor custo até x, e já que o caminho não passa mais por y, \nz agora informa a y que Dz(x) = 50 em t2. Após receber a atualização de z, y atualiza sua tabela de distâncias com \nFigura 4.31  Mudanças no custo do enlace\n50\n4\n1\n60\n1\ny\nx\na.\nb.\nz\n50\n4\n1\ny\nx\nz\nA CAMADA  de REDE  279 \nDz(x) = 51. E, também, como z está agora no caminho de menor custo até x, y envenena o caminho inverso de z \na x, informando a z, no tempo t3, que Dy(x) = ∞ (mesmo que y saiba que, na verdade, Dy(x) = 51).\nA reversão envenenada resolve o problema geral da contagem até o infinito? Não resolve. É bom que você se \nconvença de que loops que envolvem três ou mais nós (e não apenas dois nós imediatamente vizinhos) não serão \ndetectados pela técnica da reversão envenenada.\nUma comparação entre os algoritmos de roteamento de estado de enlace (LS) e de \nvetor de distâncias (DV)\nOs algoritmos DV e LS adotam abordagens complementares em relação ao cálculo do roteamento. No al-\ngoritmo DV, cada nó fala somente com os vizinhos diretamente conectados a ele, mas informa a esses vizinhos as \nestimativas de menor custo entre ele mesmo e todos os outros nós da rede (isto é, todos os que ele sabe que exis-\ntem). No algoritmo LS, cada nó fala com todos os outros nós (por difusão), mas informa somente os custos dos \nenlaces diretamente ligados a ele. Vamos concluir nosso estudo sobre algoritmos de estado de enlace e de vetor \nde distâncias com uma rápida comparação de alguns de seus atributos. Lembre-se de que N é o conjunto de nós \n(roteadores) e E é o conjunto de arestas (enlaces).\n• Complexidade da mensagem. Vimos que o LS requer que cada nó saiba o custo de cada enlace da rede. \nIsso exige que sejam enviadas O(|N| |E|) mensagens. E, também, sempre que o custo de um enlace muda, \no novo custo deve ser enviado a todos os nós. O algoritmo DV requer troca de mensagens entre vizinhos \ndiretamente conectados a cada iteração. Já vimos que o tempo necessário para que o algoritmo convirja \npode depender de muitos fatores. Quando o custo do enlace muda, o algoritmo DV propaga os resulta-\ndos do custo modificado do enlace apenas se o novo custo resultar em mudança no caminho de menor \ncusto para um dos nós ligado ao enlace.\n• Velocidade de convergência. Já vimos que nossa implementação de LS é um algoritmo O(|N|2) que requer \nO(N| |E|) mensagens. O algoritmo DV pode convergir lentamente e pode ter loops de roteamento en-\nquanto estiver convergindo. O algoritmo DV também tem o problema da contagem até o infinito.\n• Robustez. O que pode acontecer se um roteador falhar, se comportar mal ou for sabotado? Sob o LS, \num roteador poderia transmitir um custo incorreto para um de seus enlaces diretos (mas não para \noutros). Um nó poderia também corromper ou descartar quaisquer pacotes recebidos como parte de \numa difusão de estado de enlace. Mas um nó LS está calculando apenas suas próprias tabelas de rote-\namento; os outros nós estão realizando cálculos semelhantes para si próprios. Isso significa que, sob o \nLS, os cálculos de rota são, de certa forma, isolados, fornecendo um grau de robustez. Sob o DV, um \nnó pode anunciar incorretamente caminhos de menor custo para qualquer destino, ou para todos os \ndestinos. (Na verdade, em 1997, um roteador que estava funcionando mal em um pequeno ISP forne-\nceu aos roteadores nacionais de backbone tabelas de roteamento errôneas. Isso fez outros roteadores \ninundarem de tráfego o roteador que estava funcionando mal. Com isso, grandes porções da Internet \nficaram desconectadas durante muitas horas [Neumann, 1997].) De modo geral, notamos que, a cada \niteração, um cálculo de nó em DV é passado adiante a seu vizinho e, em seguida, indiretamente ao \nvizinho de seu vizinho na iteração seguinte. Nesse sentido, sob o DV, um cálculo incorreto do nó pode \nser difundido pela rede inteira.\nNo final, nenhum algoritmo ganha do outro; na verdade, ambos são usados na Internet.\nOutros algoritmos de roteamento\nOs algoritmos LS e DV que estudamos não somente são utilizados em grande escala na prática, mas tam-\nbém são, na essência, os únicos utilizados hoje na Internet. Não obstante, muitos algoritmos de roteamento fo-\n   Redes de computadores e a Internet\n280\nram propostos por pesquisadores nos últimos 30 anos, abrangendo desde o extremamente simples até o muito \nsofisticado e complexo. Há uma grande classe de algoritmos de roteamento cuja base é considerar o tráfego como \nfluxos entre origens e destinos em uma rede. Nessa abordagem, o problema do roteamento pode ser formulado \nem termos matemáticos como um problema de otimização restrita, conhecido como problema de fluxo da rede \n[Bertsekas, 1991]. Ainda outro conjunto de algoritmos de roteamento que mencionamos aqui são os derivados \ndo mundo da telefonia. Esses algoritmos de roteamento de comutação de circuitos são de interesse para redes \nde comutação de circuitos nos casos em que devem ser reservados recursos (por exemplo, buffers ou uma fra-\nção da largura de banda do enlace) por enlace para cada conexão roteada pelo enlace. Embora a formulação do \nproblema do roteamento talvez pareça bem diferente da do problema do roteamento de menor custo que vimos \nneste capítulo, existem muitas semelhanças, ao menos quanto ao algoritmo de busca de caminhos (algoritmo de \nroteamento). Veja em Ash [1998]; Ross [1995]; Girard [1990] uma discussão detalhada dessa área de pesquisa.\n4.5.3  Roteamento hierárquico\nQuando estudamos os algoritmos LS e DV, consideramos a rede apenas como uma coleção de roteadores \ninterconectados. Um roteador não se distinguia de outro no sentido de que todos rodavam o mesmo algoritmo \nde roteamento para calcular os caminhos de roteamento pela rede inteira. Na prática, esse modelo e sua visão de \num conjunto homogêneo de roteadores, todos rodando o mesmo algoritmo de roteamento, é um tanto simplista \npor pelo menos duas razões importantes:\n• Escala. À medida que aumenta o número de roteadores, a sobrecarga relativa ao cálculo, ao armazena-\nmento e à comunicação de informações de roteamento (por exemplo, atualizações de estado de enlace \nou mudanças no caminho de menor custo) se torna proibitiva. A Internet pública de hoje consiste em \ncentenas de milhões de hospedeiros. Armazenar informações de roteamento para cada um desses hospe-\ndeiros evidentemente exigiria quantidades enormes de memória. Com a sobrecarga exigida para trans-\nmitir atualizações do estado de enlace por difusão entre todos os roteadores da Internet, não sobraria \nnenhuma largura de banda para enviar pacotes de dados! Um algoritmo DV que fizesse iterações entre \nesse número tão grande de roteadores decerto jamais convergiria! Fica claro que algo deve ser feito para \nreduzir a complexidade do cálculo de rotas em redes tão grandes como a Internet pública.\n• Autonomia administrativa. Embora os pesquisadores tendam a ignorar questões como o desejo das em-\npresas de controlar seus roteadores como bem entendem (por exemplo, rodar qualquer algoritmo de \nroteamento que escolherem) ou ocultar do público externo aspectos da organização interna das redes, \nessas considerações são importantes. Idealmente, uma organização deveria poder executar e administrar \nsua rede como bem entendesse, mantendo a capacidade de conectar sua rede a outras redes externas.\nEsses problemas podem ser resolvidos agrupando roteadores em sistemas autônomos (autonomous sys-\ntems — ASs), com cada AS consistindo em um grupo de roteadores sob o mesmo controle administrativo (por \nexemplo, operados pelo mesmo ISP ou pertencentes a uma mesma rede corporativa). Todos os roteadores dentro \ndo mesmo AS rodam o mesmo algoritmo de roteamento (por exemplo, LS ou DV) e dispõem das informações \nsobre cada um dos outros — exatamente como foi o caso do modelo idealizado da seção anterior. O algoritmo de \nroteamento que roda dentro de um AS é denominado um protocolo de roteamento intrassistema autônomo. \nÉ claro que será necessário conectar os ASs entre si e, assim, um ou mais dos roteadores em um AS terá a tarefa \nadicional de ficar responsável por transmitir pacotes a destinos que estão fora do AS — esses roteadores são de-\nnominados roteadores de borda (gateway routers).\nA Figura 4.32 ilustra um exemplo simples com três ASs: AS1, AS2 E AS3. Na figura, as linhas escuras repre-\nsentam conexões diretas de enlaces entre pares e roteadores. As linhas mais finas e interrompidas que saem dos \nroteadores representam sub-redes conectadas diretamente a eles. O AS1 tem quatro roteadores, 1a, 1b, 1c e 1d, \ne cada qual roda o protocolo de roteamento utilizado dentro do AS1. Assim, cada um desses quatro roteadores \nA CAMADA  de REDE  281 \nsabe como transmitir pacotes ao longo do caminho ideal para qualquer destino dentro de AS1. De maneira seme-\nlhante, cada um dos sistemas autônomos AS2 e AS3 tem três roteadores. Note que os protocolos de roteamento \nintra-AS que rodam em AS1, AS2 e AS3 não precisam ser os mesmos. Note também que os roteadores 1b, 1c, 2a \ne 3a são roteadores de borda.\nAgora já deve estar claro como os roteadores em um AS determinam caminhos de roteamento para pares \norigem-destino internos ao AS. Mas ainda há uma peça faltando nesse quebra-cabeça de roteamento fim a fim. De \nque forma um roteador que está dentro de algum AS sabe como rotear um pacote até um destino que está fora do \nAS? Essa pergunta é fácil de responder se o AS tiver somente um roteador de borda que se conecta com somente \noutro AS. Nesse caso, como o algoritmo de roteamento intra-AS do sistema autônomo determinou o caminho de \nmenor custo entre cada roteador interno e o de borda, cada roteador interno sabe como deve enviar o pacote. Ao \nrecebê-lo, o roteador de borda o repassa para o único enlace que leva ao exterior do AS. Então, o AS que está na \noutra extremidade do enlace assume a responsabilidade de rotear o pacote até seu destino final. Como exemplo, \nsuponha que o roteador 2b da Figura 4.32 receba um pacote cujo destino está fora do AS2. O roteador 2b, então, o \ntransmite ao roteador 2a ou 2c, como especificado pela tabela de repasse de 2b, que foi configurada pelo protocolo \nde roteamento intra-AS de AS2. O pacote eventualmente chegará ao roteador de borda 2a, que o repassará ao 1b. \nTão logo o pacote tenha saído de 2a, termina o trabalho de AS2 com referência a esse pacote.\nPortanto, o problema é fácil quando o AS de origem tem apenas um enlace que leva para fora do AS. Mas, e \nse o AS de origem tiver dois ou mais enlaces (passando por um ou mais roteadores de borda) que levam para fora \ndo AS? Então, o problema de saber para onde repassar o pacote torna-se bem mais desafiador. Por exemplo, consi-\ndere um roteador em AS1 e suponha que ele recebe um pacote cujo destino está fora do AS. É claro que o roteador \ndeveria repassar o pacote para um de seus dois roteadores de borda, 1b ou 1c, mas para qual deles? Para resolver \nesse problema, AS1 (1) precisa saber quais destinos podem ser alcançados via AS2 e quais podem ser alcançados \nvia AS3 e (2) precisa propagar a informação a todos os roteadores dentro de AS1, de modo que cada roteador possa \nconfigurar sua tabela de repasse para manipular destinos externos ao AS. Essas duas tarefas — obter informações \nsobre as condições de alcance de ASs vizinhos e propagá-las a todos os outros roteadores internos ao AS — são \ngerenciadas pelo protocolo de roteamento inter-AS. Visto que tal protocolo envolve comunicação entre dois ASs, \nesses dois ASs comunicantes devem rodar o mesmo protocolo de roteamento inter-AS. De fato, na Internet, todos \nos ASs rodam o mesmo protocolo de roteamento inter-AS, denominado BGP4, que discutiremos na próxima se-\nção. Como ilustrado na Figura 4.32, cada roteador recebe informações de um protocolo de roteamento intra-AS \ne de um protocolo de roteamento inter-AS, e usa as informações de ambos para configurar sua tabela de repasse.\nFigura 4.32  Um exemplo de sistemas autônomos interconectados\nAS1\nAS3\n3b\n3c\n3a\n1a\n1c\n1b\n1d\nAS2\n2a\n2c\n2b\nAlgoritmo de\nroteamento intra-AS\nTabela de\nrepasse\nAlgoritmo de\nroteamento inter-AS\n   Redes de computadores e a Internet\n282\nComo exemplo, considere uma sub-rede x (identificada por seu endereço “ciderizado”) e suponha que AS1 \nsabe, por meio do protocolo de roteamento inter-AS, que a sub-rede x pode ser alcançada de AS3, mas não pode \nser alcançada de AS2. Então, AS1 propaga essa informação a todos os seus roteadores. Quando o roteador 1d \nfica sabendo que a sub-rede x pode ser alcançada de AS3 e, por conseguinte, do roteador de borda 1c, determina, \ncom base na informação fornecida pelo protocolo de roteamento intra-AS, a interface de roteador que está no \ncaminho de menor custo entre o roteador 1d e o roteador de borda 1c. Seja I essa interface. O roteador 1d então \npode colocar o registro (x, I) em sua tabela de repasse. (Esse exemplo, e outros apresentados nesta seção, passam \nas ideias gerais, mas são simplificações do que de fato acontece na Internet. Na seção seguinte daremos uma des-\ncrição mais detalhada, se bem que mais complicada, quando discutirmos o BGP.)\nContinuando com nosso exemplo anterior, suponha agora que AS2 e AS3 estão conectados com outros ASs, \nque não aparecem no diagrama. Suponha também que AS1 fica sabendo, pelo protocolo de roteamento inter-AS, \nque a sub-rede x pode ser alcançada a partir de AS2, via roteador de borda 1b, e também de AS3, via roteador de \nborda 1c. Então, AS1 propagaria essa informação a todos os seus roteadores, incluindo 1d. Para configurar sua \ntabela de repasse, o roteador 1d teria de determinar para qual roteador de borda, 1b ou 1c, deve dirigir pacotes \ndestinados à sub-rede x. Uma abordagem, que costuma ser empregada na prática, é utilizar o roteamento da \nbatata quente. Com ele, o AS se livra do pacote (a batata quente) o mais depressa possível (mais precisamente, \ncom o menor custo possível). Isso é feito obrigando um roteador a enviar o pacote ao roteador de borda que tiver \no menor custo roteador-roteador de borda entre todos os que têm um caminho para o destino. No contexto do \nexemplo que estamos examinando, o roteamento da batata quente, rodando em 1d, usaria informação recebida \ndo protocolo de roteamento intra-AS para determinar os custos de caminho até 1b e 1c, e então escolheria o de \nmenor custo. Uma vez escolhido esse caminho, o roteador 1d adiciona em sua tabela de repasse um registro para \na sub-rede x. A Figura 4.33 resume as ações executadas no roteador 1d para adicionar o novo registro para x na \ntabela de repasse.\nQuando um AS fica sabendo de um destino por meio de um AS vizinho, pode anunciar essa informação de \nroteamento a alguns outros ASs vizinhos. Por exemplo, suponha que AS1 fica sabendo, por AS2, que a sub-rede x \npode ser alcançada via AS2. Então, AS1 diria a AS3 que x pode ser atingida via AS1. Desse modo, se AS3 precisar \nrotear um pacote destinado a x, repassaria o pacote a AS1 que, por sua vez, o repassaria para AS2. Como veremos \nquando discutirmos BGP, um AS tem bastante flexibilidade para decidir quais destinos anuncia aos ASs vizinhos. \nEsta é uma decisão política, que em geral depende mais de questões econômicas do que técnicas.\nLembre-se de que dissemos na Seção 1.5 que a Internet consiste em uma hierarquia de ISPs interconectados. \nEntão, qual é a relação entre ISPs e ASs? Você talvez pense que os roteadores em um ISP e os enlaces que os interco-\nnectam constituem um único AS. Embora esse seja com frequência o caso, muitos ISPs dividem sua rede em vários \nASs. Por exemplo, alguns ISPs de nível 1 utilizam um AS para toda a sua rede; outros a subdividem em dezenas de \nASs interconectados.\nEm resumo, os problemas de escala e de autoridade administrativa são resolvidos pela definição de siste-\nmas autônomos. Dentro de um AS, todos os roteadores rodam o mesmo protocolo de roteamento intrassistema \nautônomo. Entre eles, os ASs rodam o mesmo protocolo de roteamento inter-AS. O problema de escala é re-\nsolvido porque um roteador intra-AS precisa saber apenas dos outros roteadores dentro do AS. O problema de \nautoridade administrativa é resolvido, já que uma organização pode rodar o protocolo de roteamento intra-AS \nFigura 4.33  \u0007\nEtapas da adição de um destino fora do AS à tabela de repasse de um roteador\nAprende por um \nprotocolo inter-AS que \na sub-rede x pode ser \nalcançada via vários \nroteadores de borda.\nUsa informações de \nroteamento do \nprotocolo intra-AS para \ndeterminar custos de \ncaminhos de menor \ncusto para cada um dos \nroteadores de borda.\nRoteamento da batata \nquente: escolhe o \nroteador de borda que \ntenha o menor custo.\nDetermina, pela tabela \nde repasse, a interface \nI que leva ao roteador \nde borda de menor \ncusto. Adiciona (x,I) à \ntabela de repasse.\nA CAMADA  de REDE  283 \nque quiser; todavia, cada par de ASs conectados precisa rodar o mesmo protocolo de roteamento inter-AS para \ntrocar informações de alcançabilidade.\nNa seção seguinte, examinaremos dois protocolos de roteamento intra-AS (RIP e OSPF) e o protocolo de \nroteamento inter-AS (BGP), que são usados na Internet de hoje. Esses estudos de casos dão um bom arremate ao \nnosso estudo de roteamento hierárquico.\n4.6  Roteamento na Internet\nAgora que já estudamos endereçamento na Internet e o protocolo IP\n, vamos voltar nossa atenção aos proto-\ncolos de roteamento da Internet. A tarefa deles é determinar o caminho tomado por um datagrama entre a origem \ne o destino. Veremos que esses protocolos incorporam muitos dos princípios que aprendemos antes neste capítulo. \nAs abordagens de estado de enlace e de vetor de distâncias estudadas nas Seções 4.5.1 e 4.5.2 e a ideia de um sistema \nautônomo considerada na Seção 4.5.3 são fundamentais para o modo como o roteamento é feito na Internet hoje.\nLembre-se de que, na Seção 4.5.3, vimos que um sistema autônomo (AS) é um conjunto de roteadores que estão \nsob o mesmo controle administrativo e técnico e que rodam, todos, o mesmo protocolo de roteamento entre eles. Cada \nAS, por sua vez, normalmente contém várias sub-redes (aqui, usamos o termo sub-rede no sentido preciso de endere-\nçamento, como na Seção 4.4.2).\n4.6.1  Roteamento intra-AS na Internet: RIP\nUm protocolo de roteamento intra-AS é usado para determinar como é rodado o roteamento dentro de \num sistema autônomo (AS). Esses protocolos são também conhecidos como protocolos de roteadores internos \n(IGP — interior gateway protocols). Historicamente, dois protocolos de roteamento têm sido usados para rotea-\nmento dentro de um sistema autônomo na Internet: o protocolo de informações de roteamento, RIP (Routing \nInformation Protocol) e o OSPF (Open Shortest Path First). Um protocolo muito relacionado com o OSPF é o \nIS-IS [RFC 1142; Perlman, 1999]. Primeiro, discutiremos o RIP e, em seguida, consideraremos o OSPF.\nO RIP foi um dos primeiros protocolos de roteamento intra-AS da Internet, e seu uso é ainda muito disse-\nminado. Sua origem e seu nome vêm da arquitetura XNS (Xerox Network ­\nSystems). A ampla disponibilização do \nRIP se deveu em grande parte à inclusão, em 1982, na versão do UNIX do Berkeley Software Distribution (BSD), \nque suportava TCP/IP. A versão 1 do RIP está definida no [RFC 1058] e a versão 2, compatível com a versão 1, \nno [RFC 2453].\nO RIP é um protocolo de vetor de distâncias que funciona de um modo muito parecido com o protocolo DV \nidealizado que examinamos na Seção 4.5.2. A versão do RIP especificada no RFC 1058 usa contagem de saltos como \nmétrica de custo, isto é, cada enlace tem um custo 1. Por simplicidade, no algoritmo DV da Seção 4.5.2 os custos \nforam definidos entre pares de ­\nroteadores. No RIP (e também no OSPF), na realidade, os custos são definidos desde \num roteador de origem até uma sub-rede de destino. O RIP usa o termo salto (hop), que é o número de sub-redes \npercorridas no caminho mais curto entre o roteador de origem e uma sub-rede de destino, inclusive. A Figura 4.34 \nilustra um AS com seis sub-redes de folha. A tabela da figura indica o número de saltos desde o roteador de origem \nA até todas as sub-redes folha.\nO custo máximo de um caminho é limitado a 15, restringindo assim o uso do RIP a ­\nsistemas autônomos \nque têm menos de 15 saltos de diâmetro. Lembre-se de que, em protocolos DV, roteadores vizinhos trocam \nvetores de distância entre si. O vetor de distâncias para qualquer roteador é a estimativa atual das distâncias \ndos caminhos de menor custo entre aquele roteador e as sub-redes no AS. No RIP, atualizações de roteamento \nsão trocadas entre vizinhos a cada 30 s mais ou menos, usando uma mensagem de resposta RIP. A mensa-\ngem de resposta enviada por um roteador ou um hospedeiro contém uma lista de até 25 sub-redes de destino \ndentro do AS, bem como as distâncias entre o remetente e cada uma delas. Mensagens de resposta também são \nconhecidas como anúncios RIP.\n   Redes de computadores e a Internet\n284\nVamos examinar um exemplo simples de como funcionam os anúncios RIP. Considere a parte de um AS \nmostrada na Figura 4.35. Nessa figura, as linhas que conectam os roteadores representam sub-redes. Apenas os \nroteadores (A, B, C e D) e as sub-redes (w, x, y, z) selecionados são rotulados. As linhas tracejadas indicam que \no AS continua; portanto, esse sistema autônomo tem muito mais roteadores e enlaces do que os mostrados na \nfigura.\nCada roteador mantém uma tabela RIP denominada tabela de roteamento. A tabela de roteamento de um \nroteador inclui o vetor de distâncias e a tabela de repasse desse roteador. A Figura 4.36 mostra a tabela do rotea-\ndor D. Note que essa tabela tem três colunas. A primeira é para a sub-rede de destino, a segunda indica a identi-\ndade do roteador seguinte no caminho mais curto até a sub-rede de destino e a terceira indica o número de saltos \n(isto é, o número de sub-redes que têm de ser atravessadas, incluindo a rede de destino) para chegar à sub-rede \nde destino no caminho mais curto. Para esse exemplo, a tabela mostra que, para enviar um datagrama do roteador \nD até a sub-rede de destino w, o datagrama deve primeiro ser repassado ao roteador vizinho A; a tabela também \nmostra que a sub-rede de destino w está a dois saltos de distância no caminho mais curto. De modo semelhante, \nindica que a sub-rede z está a sete saltos de distância via roteador B. Em princípio, uma tabela de roteamento \nterá apenas uma linha para cada sub-rede no AS, embora a versão 2 do RIP permita a agregação de registros de \nsub-redes usando técnicas de agregação de rotas semelhantes àquelas que examinamos na Seção 4.4. A tabela na \nFigura 4.36 e as subsequentes estão apenas parcialmente completas.\nSuponha agora que 30 s mais tarde o roteador D receba do roteador A o anúncio mostrado na Figura 4.37. \nNote que esse anúncio nada mais é do que informações da tabela de roteamento do roteador A! A informação \nindica, em particular, que a sub-rede z está a apenas quatro saltos do roteador A. Ao receber o anúncio, o roteador \nD o reúne (Figura 4.37) à tabela de roteamento antiga (Figura 4.36). Em particular, o roteador D fica sabendo que \n­\nagora há um novo caminho pelo roteador A até a sub-rede z que é mais curto do que o ­\ncaminho pelo roteador B. \nAssim, o roteador D atualiza sua tabela para levar em conta o mais curto dos caminhos mais curtos, conforme \nmostra a Figura 4.38. Você poderia perguntar como o caminho mais curto até a sub-rede z se tornou mais curto \nainda? Possivelmente, o algoritmo de vetor de distâncias descentralizado ainda estava em processo de convergên-\nFigura 4.34  Número de saltos do roteador de origem A até várias sub-redes\nC\nD\nA\nu\nDestino\nSaltos\nu\nv\nw\nx\ny\nz\n1\n2\n2\n3\n3\n2\n \nv\n \nw\n \nx\n \ny\nz\nB\nFigura 4.35  Uma parte de um sistema autônomo\nA\nC\nD\nB\nz\nw\nx\ny\nA CAMADA  de REDE  285 \ncia (veja a Seção 4.5.2) ou, talvez, novos enlaces e/ou roteadores tenham sido adicionados ao AS, mudando assim \nos caminhos mais curtos dentro dele.\nVamos agora considerar alguns dos aspectos da implementação do RIP. Lembre-se de que os roteadores RIP \ntrocam anúncios a cada 30 s aproximadamente. Se um roteador não ouvir nada de seu vizinho ao menos uma \nvez a cada 180 s, esse vizinho será considerado impossível de ser alcançado dali em diante, isto é, está inoperante \nou o enlace de conexão caiu. Quando isso acontece, o RIP modifica a tabela de roteamento local e, em seguida, \npropaga essa informação enviando anúncios a seus roteadores vizinhos (os que ainda podem ser alcançados). \nUm roteador pode também requisitar informação sobre o custo de seu vizinho até um dado destino usando uma \nmensagem de requisição RIP. Roteadores enviam mensagens de requisição e de resposta RIP uns aos outros usan-\ndo o número de porta 520. O segmento UDP é carregado entre roteadores dentro de um datagrama IP padrão. O \nfato de o RIP usar um protocolo de camada de transporte (UDP) sobre um protocolo de camada de rede (IP) para \nimplementar funcionalidade de camada de rede (um algoritmo de roteamento) pode parecer bastante confuso (e \né!). Um exame mais profundo sobre como o RIP é implementado esclarecerá esse assunto.\nA Figura 4.39 ilustra esquematicamente como o RIP costuma ser executado em um sistema UNIX, por \nexemplo, uma estação de trabalho UNIX que está servindo como um roteador. Um processo denominado routed \n(pronunciado como “route dee”) roda o RIP, isto é, mantém informações de roteamento e troca mensagens com \nprocessos routed que rodam em ­\nroteadores vizinhos. Como o RIP é realizado como um processo da camada de \naplicação (se bem que um processo muito especial, capaz de manipular as tabelas de roteamento dentro do núcleo \ndo UNIX), ele pode enviar e receber mensagens por uma porta padrão e usar um protocolo de transporte padrão. \nAssim, o RIP é um protocolo de camada de aplicação (veja o Capítulo 2) que roda sobre UDP. Se estiver interes-\nsado em examinar uma implementação do RIP (ou dos protocolos OSPF e BGP, que estudaremos em seguida), \nconsulte Quagga [2012].\nFigura 4.36  \u0007\nTabela de roteamento no roteador D antes de receber anúncio do roteador A\nSub-rede de destino\nRoteador seguinte\nNúmero de saltos até o destino\nw\nA\n2\ny \nB\n2\nz\nB\n7\nx\n—\n1\n. . . .\n. . . .\n. . . .\nFigura 4.37  Anúncio vindo do roteador A\nSub-rede de destino\nRoteador seguinte\nNúmero de saltos até o destino\nz\nC\n4\nw\n—\n1\nx\n—\n1\n. . . .\n. . . .\n. . . .\nFigura 4.38  \u0007\nTabela de roteamento no roteador D após ter recebido anúncio do roteador A\nSub-rede de destino\nRoteador seguinte\nNúmero de saltos até o destino\nw\nA\n2\ny\nB\n2\nz\nA\n5\n. . . .\n. . . .\n. . . .\n   Redes de computadores e a Internet\n286\n4.6.2  Roteamento intra-AS na Internet: OSPF\nComo o RIP, o roteamento OSPF é bastante usado para roteamento intra-AS na Internet. O OSPF e seu \nprimo, IS-IS, muito parecido com ele, são em geral disponibilizados em ISPs de níveis mais altos, ao passo que o \nRIP está disponível em ISPs de níveis mais baixos e redes corporativas. O “open” do OSPF significa que as espe-\ncificações do protocolo de roteamento estão abertas ao público (ao contrário do protocolo EIGRP da Cisco, por \nexemplo). A versão mais recente do OSPF, versão 2, está definida no RFC 2328, um documento público.\nO OSPF foi concebido como sucessor do RIP e como tal tem uma série de características avançadas. Em \nseu âmago, contudo, é um protocolo de estado de enlace que usa inundação de informação de estado de enlace e \num algoritmo de caminho de menor custo de Dijkstra. Com o OSPF, um roteador constrói um mapa topológico \ncompleto (isto é, um grafo) de todo o sistema autônomo. O roteador então roda localmente o algoritmo do cami-\nnho mais curto de Dijkstra para determinar uma árvore de caminho mais curto para todas as sub-redes, sendo ele \npróprio o nó raiz. Os custos de enlaces individuais são configurados pelo administrador da rede (veja “Princípios \nna prática: Configurando os pesos de enlaces no OSPF”). O administrador pode optar por estabelecer todos os \ncustos de enlace em 1, conseguindo assim o roteamento com o mínimo de saltos, ou por designar para os enlaces \npesos inversamente proporcionais à capacidade do enlace, de modo a desencorajar o tráfego a usar enlaces de \nlargura de banda baixa. O OSPF não impõe uma política para o modo como são determinados os pesos dos en-\nlaces (essa tarefa é do administrador da rede); em vez disso, oferece os mecanismos (protocolo) para determinar \no caminho de roteamento de menor custo para um dado conjunto de pesos de enlaces.\nCom OSPF, um roteador transmite por difusão informações de roteamento a todos os outros roteadores \nno sistema autônomo, não apenas a seus vizinhos. Um roteador transmite informações de estado de enlace por \ndifusão sempre que houver uma mudança no estado de um enlace (por exemplo, uma mudança de custo ou uma \nmudança de estado para cima/para baixo). Também transmite o estado de um enlace periodicamente (pelo menos \na cada 30 min), mesmo que não tenha havido mudança. O RFC 2328 observa que “essa atualização periódica de \nanúncios de enlace adiciona robustez ao algoritmo de estado de enlace”\n. Anúncios OSPF são contidos em mensa-\ngens OSPF carregadas diretamente por IP, com um código protocolo de camada superior 89 para OSPF. Assim, o \npróprio protocolo OSPF tem de executar funcionalidades como transferência confiável de mensagem e transmis-\nsão de estado de enlace por difusão. O protocolo OSPF também verifica se os enlaces estão operacionais (via uma \nmensagem HELLO enviada a um vizinho ligado ao enlace) e permite que um roteador OSPF obtenha o banco de \ndados de um roteador vizinho referente ao estado do enlace no âmbito da rede.\nAlguns dos avanços incorporados ao OSPF são:\n• Segurança. Trocas entre roteadores OSPF (por exemplo, atualizações do estado de enlace) podem ser \nautenticadas. A autenticação garante que apenas roteadores de confiança conseguem participar do pro-\ntocolo OSPF dentro de um AS, evitando, assim, que intrusos mal-intencionados (ou estudantes de rede \nFigura 4.39  Implementação do RIP como um daemon routed\nRede \n(IP)\nTransporte \n(UDP)\nEnlace\nFísica\nTabelas de\nrepasse\nRouted\nTabelas de\nrepasse\nRouted\nRede \n(IP)\nTransporte \n(UDP)\nEnlace\nFísica\nA CAMADA  de REDE  287 \ntestando, por brincadeira, seu conhecimento recém-adquirido) injetem informações incorretas em ta-\nbelas de roteamento. Como padrão, pacotes OSPF entre roteadores não são autenticados e poderiam ser \nforjados. Dois tipos de autenticação podem ser configurados — simples e MD5 (veja o Capítulo 8 para \nobter uma discussão sobre MD5 e autenticação em geral). Com autenticação simples, a mesma senha é \nconfigurada em cada roteador. Quando um roteador envia um pacote OSPF, inclui a senha em texto claro \n(não criptografado). Logicamente, a autenticação simples não é segura. A autenticação MD5 é baseada \nem chaves secretas compartilhadas que são configuradas em todos os roteadores. Para cada pacote OSPF \nenviado, o roteador calcula o hash MD5 do conteúdo do pacote adicionado com a chave secreta. (Con-\nsulte a discussão sobre códigos de autenticação de mensagem no Capítulo 7.) Então, o roteador inclui \nno pacote OSPF o valor de hash resultante. O roteador receptor, usando a chave secreta pré-configurada, \ncalculará um hash MD5 do pacote e o comparará com o valor de hash que este transporta, verificando \nassim sua autenticidade. Números de sequência também são utilizados com autenticação MD5 para pro-\nteção contra ataques por reenvio.\n• Caminhos múltiplos com o mesmo custo. Quando vários caminhos até o destino têm o mesmo custo, o \nOSPF permite que sejam usados diversos (isto é, não é preciso escolher um único para carregar todo o \ntráfego quando existem vários de igual custo).\n• Suporte integrado para roteamento individual e em grupo (unicast e multicast). O multicast­\n OSPF (MOS-\nPF) [RFC 1584] fornece extensões simples ao OSPF para prover roteamento em grupo (um tópico que \nexaminaremos com mais profundidade na Seção 4.7.2). O MOSPF usa o banco de dados de enlaces \nexistente no OSPF e acrescenta um novo tipo de anúncio de estado de enlace ao mecanismo OSPF de \ntransmissão de estado de enlace por difusão.\nConfigurando os pesos de enlaces no OSPF\nNossa discussão de roteamento de estado de en-\nlace admitiu implicitamente que os pesos dos enlaces \nsão determinados, que um algoritmo de roteamento \ncomo o OSPF é rodado e que o tráfego flui de acor-\ndo com as tabelas calculadas pelo algoritmo LS. Em \ntermos de causa e efeito, os pesos dos enlaces são \ndados (isto é, vêm em primeiro lugar) e resultam (via \nalgoritmo de Dijkstra) em caminhos de roteamento \nque minimizam o custo geral. Desse ponto de vista, \npesos de enlaces refletem o custo da utilização de \num enlace (por exemplo, se os pesos forem inversa-\nmente proporcionais à capacidade, então a utilização \nde enlaces de alta capacidade teria pesos menores \ne, assim, seriam mais atraentes do ponto de vista de \nroteamento) e o algoritmo de Dijkstra serve para mini-\nmizar o custo geral.\nNa prática, a relação causa e efeito entre pesos \nde enlaces e caminhos de roteamento pode ser in-\nvertida — operadores de rede configuram pesos de \nenlaces de modo a obter caminhos de roteamento \nque cumpram certas metas de engenharia de tráfego \n[Fortz, 2000; Fortz, 2002]. Por exemplo, suponha que \num operador de rede tenha uma estimativa de fluxo do \ntráfego que entra na rede em cada ponto de ingresso \ne destinado a cada ponto de saída da rede. Então, \no operador poderia querer instituir um roteamento de \ntráfego do ponto de ingresso até o ponto de saída que \nminimizasse a utilização máxima em todos os enla-\nces da rede. Porém, com um algoritmo de roteamento \ncomo o OSPF\n, os principais controles de que o ope-\nrador dispõe para ajustar o roteamento de fluxos pela \nrede são os pesos dos enlaces. Assim, para cumprir \na meta de minimizar a utilização máxima do enlace, o \noperador tem de descobrir o conjunto de pesos de en-\nlaces que alcance esse objetivo. Essa é uma inversão \nda relação causa e efeito — o roteamento de fluxos \ndesejado é conhecido e os pesos dos enlaces OSPF \ntêm de ser encontrados de um modo tal que o algorit-\nmo de roteamento OSPF resulte nesse roteamento de \nfluxos desejado.\nPrincípios na prática\n   Redes de computadores e a Internet\n288\n• Suporte para hierarquia dentro de um único domínio de roteamento. Talvez o avanço mais significativo do \nOSPF seja a capacidade de estruturar em hierarquia um sistema autônomo. Na Seção 4.5.3, vimos as muitas \nvantagens de estruturas de roteamento hierárquicas. Examinaremos a execução do roteamento OSPF hie-\nrárquico no restante desta seção.\nUm sistema autônomo OSPF pode ser configurado hierarquicamente em áreas. Cada área roda seu pró-\nprio algoritmo de roteamento de estado de enlace OSPF, e cada roteador em uma área transmite seu estado de \nenlace, por difusão, a todos os outros roteadores daquela área. Dentro de cada área, um ou mais roteadores de \nborda de área são responsáveis pelo roteamento de pacotes fora da área. Por fim, exatamente uma área OSPF \nno AS é configurada para ser a área de backbone. O papel primordial da área de backbone é rotear tráfego entre \nas outras áreas do AS. O backbone sempre contém todos os roteadores de borda de área que estão dentro do \nAS e pode conter também roteadores que não são de borda. O roteamento interárea dentro do AS requer que \no pacote seja roteado primeiro até um roteador de borda de área (roteamento intra-área), em seguida roteado \npor meio do backbone até o roteador de borda de área que está na área de destino e, então, roteado até seu \ndestino final.\nO OSPF é um protocolo bastante complexo e, aqui, nosso tratamento teve de ser breve; Huitema [1998]; \nMoy [1998] e RFC 2328 oferecem detalhes adicionais.\n4.6.3  Roteamento inter-AS: BGP\nAcabamos de aprender como ISPs utilizam RIP e OSPF para determinar caminhos ótimos para pares ori-\ngem-destino internos ao mesmo AS. Agora vamos examinar como são determinados caminhos para pares ori-\ngem-destino que abrangem vários ASs. A versão 4 do protocolo de roteador de borda (Border Gateway Pro-\ntocol — BGP), especificada no RFC 4271 (veja também [RFC 4274]), é o padrão, na prática, para roteamento \nentre sistemas autônomos na Internet de hoje. Esse protocolo é em geral denominado BGP4 ou apenas BGP. Na \nqualidade de um protocolo de roteamento inter-ASs (veja Seção 4.5.3), o BGP oferece a cada AS meios de:\n1.\t Obter de ASs vizinhos informações de alcançabilidade de sub-redes.\n2.\t Propagar a informação de alcançabilidade a todos os roteadores internos ao AS.\n3.\t Determinar rotas “boas” para sub-redes com base na informação de alcançabilidade e na política do AS.\nO BGP, sobretudo, permite que cada sub-rede anuncie sua existência ao restante da Internet. Uma sub-re-\nde grita “Eu existo e estou aqui” e o BGP garante que todos os ASs da Internet saibam de sua existência e como \nchegar até ela. Não fosse o BGP, cada sub-rede ficaria isolada — sozinha e desconhecida pelo restante da Internet.\nO básico do BGP\nO BGP é de altíssima complexidade; livros inteiros foram dedicados ao assunto e muitas questões ainda não \nestão claras [Yannuzzi, 2005]. Além disso, mesmo após ler os livros e os RFCs, ainda assim você talvez ache difícil \ndominar o BGP por completo sem ter trabalhado com ele na prática durante muitos meses (se não anos) como \nprojetista ou administrador de um ISP de nível superior. Não obstante, como o BGP é um protocolo absoluta-\nmente crítico para a Internet — na essência, é o que agrega tudo —, precisamos ao menos entender os aspectos \nbásicos do seu funcionamento. Começamos descrevendo como o BGP poderia funcionar no contexto do exem-\nplo de rede simples que já estudamos na Figura 4.32. Nesta descrição, baseamo-nos em nossa discussão sobre \nroteamento hierárquico na Seção 4.5.3; aconselhamos que você leia outra vez esse material.\nNo BGP, pares de roteadores trocam informações de roteamento por conexões TCP semipermanentes usan-\ndo a porta 179. Essas conexões da Figura 4.32 são mostradas na Figura 4.40. Normalmente há uma conexão BGP \nTCP para cada enlace que liga diretamente dois roteadores que estão em dois ASs diferentes; assim, na Figura \n4.40 há uma conexão TCP entre os roteadores de borda 3a e 1c e outra entre os roteadores de borda 1b e 2a. Tam-\nA CAMADA  de REDE  289 \nbém há conexões BGP TCP semipermanentes entre roteadores dentro de um AS. Em particular, a Figura 4.40 \napresenta uma configuração comum de uma conexão TCP para cada par de roteadores internos a um AS, criando \numa malha de conexões TCP em cada AS. Os dois roteadores nas extremidades da conexão são denominados \npares BGP, e a conexão TCP, junto com todas as mensagens BGP enviadas pela conexão, é denominada sessão \nBGP. Além disso, uma sessão BGP que abranja dois ASs é denominada sessão BGP externa (eBGP) e uma ses-\nsão BGP ­\nentre roteadores no mesmo AS é chamada uma sessão BGP interna (iBGP). Na Figura 4.40, as sessões \neBGP são indicadas pelas linhas de traços longos; as sessões iBGP são representadas pelas linhas de traços curtos. \nNote que as linhas das sessões BGP na Figura 4.40 nem sempre correspondem aos enlaces físicos na Figura 4.32.\nO BGP permite que cada AS conheça quais destinos podem ser alcançados por meio de seus ASs vizinhos. No \nBGP\n, os destinos não são hospedeiros, mas prefixos ciderizados, e cada prefixo representa uma sub-rede ou um con-\njunto delas. Assim, por exemplo, suponha que haja quatro sub-redes conectadas ao AS2: 138.16.64/24, 138.16.65/24, \n138.16.66/24 e 138.16.67/24. O AS2 então poderia agregar os prefixos dessas quatro sub-redes e utilizar o BGP para \nanunciar ao AS1 o prefixo único 138.16.64/22. Como outro exemplo, suponha que apenas as primeiras três estão em \nAS2 e que a quarta sub-rede, 138.16.67/24, está em AS3. Então, como descrito no quadro Princípios na Prática na \nSeção 4.4.2, como os roteadores usam combinação de prefixo mais longo para repassar datagramas, o AS3 poderia \nanunciar a AS1 o prefixo mais específico 138.16.67/24 e o AS2 ainda poderia anunciar ao AS1 o prefixo agregado \n138.16.64/22.\nAgora vamos examinar como o BGP distribuiria informações sobre a alcançabilidade de prefixos pelas ses-\nsões BGP mostradas na Figura 4.40. Como é de se esperar, usando a sessão eBGP entre os roteadores de borda \n3a e 1c, AS3 envia a AS1 a lista de prefixos que podem ser alcançados a partir de AS3; e AS1 envia a AS3 a lista \nde prefixos que podem ser alcançados a partir de AS1. De modo semelhante, AS1 e AS2 trocam informações de \nalcançabilidade de prefixos por meio de seus roteadores de borda 1b e 2a. E, como também era de esperar, quando \num roteador de borda (em qualquer AS) recebe prefixos conhecidos pelo BGP, ele usa suas sessões iBGP para \ndistribuí-los aos outros roteadores no AS. Assim, todos os roteadores no AS1 se informarão sobre os prefixos de \nAS3, incluindo o roteador de borda 1b. O roteador de borda 1b (em AS1) pode reanunciar a AS2 os prefixos de \nAS3. Quando um roteador (seja ou não de borda) fica sabendo de um novo prefixo, cria um registro para ele em \nsua tabela de repasse, como descrito na Seção 4.5.3.\nObtendo presença na Internet: juntando o quebra-cabeça\nSuponha que você tenha acabado de criar uma \npequena rede com diversos servidores, incluindo \num servidor Web público, que descreve os produtos \ne serviços da sua empresa, um de correio, do qual \nseus funcionários obtêm suas mensagens de correio \neletrônico, e um de DNS. Claro, você gostaria que o \nmundo inteiro pudesse navegar em seu site para des-\ncobrir seus incríveis produtos e serviços. Além do \nmais, gostaria que seus funcionários pudessem enviar \ne receber correio eletrônico para clientes em potencial \nno mundo inteiro.\nPara tender a esses objetivos, primeiro você precisa \nobter conectividade com a Internet, o que é feito con-\ntratando e conectando-se a um ISP local. Sua empresa \nterá um roteador de borda, que estará conectado a um \nroteador no seu ISP local. Essa conexão poderia ser \numa DSL pela infraestrutura telefônica, uma linha pri-\nvada com o roteador do ISP\n, ou uma das muitas outras \nsoluções de acesso descritas no Capítulo 1. Seu ISP \nlocal também lhe oferecerá uma faixa de endereços IP\n, \npor exemplo, uma faixa de endereços /24 consistindo \nem 245 endereços. Ao obter sua conectividade física e \nsua faixa de endereços IP\n, você designará um dos seus \nendereços IP (na sua faixa de endereços) para o seu \nservidor Web, um para o seu servidor de correio, um \npara o seu servidor DNS, um para o seu roteador de \nborda e outros endereços IP para outros servidores e \ndispositivos na rede da sua empresa.\nAlém de contratar um ISP\n, você também precisa-\nrá contratar um registrador da Internet para obter um \nnome de domínio para a sua empresa, conforme des-\ncrevemos no Capítulo 2. Por exemplo, se a sua em-\npresa tiver o nome Xanadu Inc., você decerto tentará \nobter o nome de domínio xanadu.com, por exemplo. \nPrincípios na prática\n   Redes de computadores e a Internet\n290\nAtributos de caminho e rotas BGP\nAgora que já temos um conhecimento preliminar do BGP, vamos um pouco mais fundo (e, mesmo assim, \nestaremos varrendo para baixo do tapete alguns dos detalhes menos importantes!). No BGP, um sistema autô-\nnomo é identificado por seu número de sistema autônomo (ASN) globalmente exclusivo [RFC 1930]. (Tecni-\ncamente, nem todo AS tem um ASN. Em particular, um stub AS que carregue somente tráfego do qual é uma \norigem ou um destino em geral não terá um ASN; ignoramos essa tecnicidade em nossa discussão para que os \ndetalhes não nos impeçam de observar melhor o quadro geral.) Números de ASs, como endereços IP, são desig-\nnados por entidades regionais ICANN de registro [ICANN, 2012].\nQuando um roteador anuncia um prefixo para uma sessão BGP, inclui vários atributos BGP juntamente \ncom o prefixo. Na terminologia do BGP, um prefixo, junto com seus atributos, é denominado uma rota. Assim, \npares BGP anunciam rotas uns aos outros. Dois dos atributos mais importantes são AS-PATH e NEXT-HOP:\n• AS-PATH. Esse atributo contém os ASs pelos quais passou o anúncio para o prefixo. Quando um prefixo é \npassado para dentro de um AS, este adiciona seu ASN ao atributo AS-PATH. Por exemplo, considere a Figura \n4.40 e suponha que o prefixo 138.16.64/24 seja anunciado primeiro de AS2 para AS1; se AS1 então anunciar \no prefixo a AS3, o AS-PATH seria AS2 AS1. Roteadores usam o atributo AS-PATH para detectar e evitar \n \nlooping de anúncios; em especial, se um roteador perceber que seu AS está contido na lista de caminhos, re-\njeitará o anúncio. Como discutiremos em breve, roteadores também usam o atributo AS-PATH ao escolher \nentre vários caminhos para o mesmo prefixo.\n• Fornecendo o enlace crítico entre os protocolos de roteamento inter-AS e intra-AS, o atributo NEXT-HOP \npossui uma sutil mas importante utilidade. O NEXT-HOP é a interface do roteador que inicia o AS-PATH. \nSua empresa também deverá obter presença no siste-\nma DNS. Especificamente, como as pessoas de fora \ndesejarão entrar em contato com seu servidor DNS \npara obter os endereços IP dos seus servidores, você \ntambém precisará oferecer ao seu registrador o ende-\nreço IP do seu servidor DNS. Seu registrador, então, \ncolocará um registro para o seu servidor DNS (nome \nde domínio e endereço IP correspondente) nos ser-\nvidores do domínio .com de nível superior, conforme \ndescrevemos no Capítulo 2. Concluída essa etapa, \nqualquer usuário que saiba seu nome de domínio (por \nexemplo, xanadu.com) poderá obter o endereço IP do \nseu servidor DNS por meio do sistema DNS.\nPara que as pessoas consigam descobrir os ende-\nreços IP do seu servidor Web, você terá de incluir nele \nregistros que mapeiem o nome de hospedeiro do ser-\nvidor (por exemplo, www.xanadu.com) ao endereço IP\n. \nVocê desejará ter registros semelhantes para outros \nservidores publicamente disponíveis em sua empresa, \nincluindo o de correio. Dessa forma, se Alice quiser \nnavegar pelo seu servidor Web, o sistema DNS entrará \nem contato com seu servidor DNS, achará o endereço \nIP do seu servidor Web e o dará a Alice. Assim, ela po-\nderá estabelecer uma conexão TCP diretamente com \no seu servidor Web.\nTodavia, ainda resta uma etapa necessária e decisi-\nva para permitir que outros do mundo inteiro acessem \nseu servidor Web. Considere o que acontece quando \nAlice, que conhece o endereço IP do seu servidor Web, \nenvia um datagrama IP (por exemplo, um segmento \nTCP SYN) a esse endereço IP\n. Esse datagrama será \ndirecionado pela Internet, visitando uma série de rote-\nadores em muitos ASs diferentes, para enfim alcançar \nseu servidor Web. Quando qualquer um dos roteadores \nrecebe o datagrama, ele procura um registro em sua \ntabela de repasse para determinar em qual porta de \nsaída deverá encaminhá-lo. Portanto, cada roteador \nprecisa saber a respeito do prefixo /24 da sua empre-\nsa (ou de algum registro agregado). Como um roteador \npode saber o prefixo da sua empresa? Como já vimos, \nisso é feito por meio do BGP! Especificamente, quando \nsua empresa contrata um ISP local e recebe um pre-\nfixo (ou seja, uma faixa de endereços), seu ISP local \nusará o BGP para anunciar esse prefixo aos ISPs aos \nquais se conecta. Tais ISPs, por sua vez, usarão o BGP \npara propagar o anúncio. Por fim, todos os roteadores \nda Internet saberão a respeito do seu prefixo (ou sobre \nalgum agregado que o inclua) e, desse modo, pode-\nrão repassar datagramas destinados a seus servidores \nWeb e de correio de forma apropriada.\nA CAMADA  de REDE  291 \nFigura 4.40  Sessões eBGP e iBGP\nAS1\nAS3\n3b\nSessão eBGP\nLegenda:\nSessão iBGP\n3c\n3a\n1a\n1c\n1b\n1d\nAS2\n2a\n2c\n2b\nPara compreender mais esse atributo, vamos, de novo, nos referir à Figura 4.40. Considere o que acontece \nquando o roteador de borda 3a em AS3 anuncia uma rota a um roteador de borda 1c em AS1, utilizando \neBGP. A rota inclui o prefixo anunciado, que chamaremos de x, e um AS-PATH para o prefixo. O anúncio \ntambém inclui o NEXT-HOP, endereço IP da interface 3a do roteador que conduz a 1c. (Lembre-se de que \num roteador possui diversos endereços IP\n, um para cada uma das interfaces.) Considere agora o que ocorre \nquando o roteador 1d é informado sobre essa rota pelo iBGP\n. Após descobrir essa rota para x, o roteador 1d \npode querer encaminhar pacotes para x por ela, ou seja, o roteador 1d pode querer incluir o registro (x, l) \nem sua tabela de repasse, na qual l representa sua interface que inicia o caminho de menor custo partindo \nde 1d em direção ao roteador de borda 1c. Para determinar l, 1d fornece o endereço IP no atributo NEXT­\n‑HOP a seu módulo de roteamento intra-AS. Observe que o algoritmo de roteamento intra-AS determinou \no enlace de menor custo entre 1c e 3a. Desse caminho de menor custo de 1d à sub-rede 1c-3a, 1d determina \nsua interface do roteador l que inicia esse caminho e, então, adiciona a entrada (x, l) à sua tabela de repasse. \nAté que enfim! Em resumo, o atributo NEXT-HOP é usado por ­\nroteadores para configurar suas tabelas de \nrepasse adequadamente.\n• A Figura 4.41 ilustra outra situação em que o NEXT-HOP é necessário. Na figura, o AS1 e o AS2 estão \nconectados por dois enlaces interconectados. Um roteador em AS1 ­\npoderia descobrir duas rotas diferentes \nFigura 4.41  \u0007\nAtributos NEXT-HOP em anúncios servem para determinar qual enlace \ninterconectado utilizar\nAS2\nAS1\nDois enlaces \ninterconectados \nentre AS2 e AS1\nO roteador é \ninformado sobre \numa rota para x\nO roteador é informado \nsobre outra rota para x\nLegenda:\nMensagem de anúncios de rota \npara o destino x\n   Redes de computadores e a Internet\n292\nJuntando as partes: como um registro entra na tabela de repasse de um roteador?\nLembre-se de que um registro na tabela de repas-\nse de um roteador consiste em um prefixo (por exem-\nplo, 138.16.64/22) e uma porta de saída de roteador \ncorrespondente (por exemplo, porta 7). Quando um \npacote chega ao roteador, o endereço IP de destino \ndo pacote é comparado com os prefixos na tabela \nde repasse para achar aquele com a combinação de \nprefixo mais longa. O pacote é então repassado (den-\ntro do roteador) para a porta do roteador associada a \nesse prefixo. Em seguida, vamos explicar como um \nregistro de roteamento (prefixo e porta associada) é \ninserido em uma tabela de repasse. Este exercício \nsimples juntará grande parte daquilo que aprendemos \nsobre roteamento e repasse. Para tornar as coisas \nmais interessantes, vamos supor que o prefixo seja \num “prefixo externo”, ou seja, ele não pertence ao AS \ndo roteador, mas a algum outro AS.\nPara que um prefixo entre na tabela de repasse \ndo roteador, este precisa primeiro conhecer o prefixo \n(correspondente a uma sub-rede ou uma agregação \nde sub-redes). Como já vimos, o roteador passa a co-\nnhecer o prefixo por meio de um anúncio de rota BGP\n. \nEsse anúncio pode ser enviado a ele por uma sessão \neBGP (de um roteador em outro AS) ou por uma ses-\nsão iBGP (de um roteador no mesmo AS).\nDepois que o roteador conhece o prefixo, ele \nprecisa determinar a porta de saída apropriada para \nonde os datagramas destinados a ele serão repas-\nsados, antes que possa entrar com esse prefixo na \ntabela de repasse. Se o roteador receber mais de um \nanúncio de rota para o prefixo, ele usa o processo \nde seleção de rota do BGP\n, conforme descrito antes \nnesta subseção, para achar a “melhor” rota. Supo-\nnha que essa melhor rota tenha sido selecionada. \nComo já dissemos, a rota selecionada inclui um atri-\nbuto NEXT-HOP\n, que é o endereço IP do primeiro ro-\nteador fora do AS do roteador ao longo dessa melhor \nrota. Como vimos, o roteador usa então seu proto-\ncolo de roteamento intra-AS (em geral, OSPF) para \ndeterminar o caminho mais curto para o roteador do \nNEXT-HOP. O roteador por fim determina o número \nde porta para associar ao prefixo, identificando o pri-\nmeiro enlace nesse caminho mais curto. O roteador \npode, então (enfim!), entrar com o par prefixo-porta \nem sua tabela de repasse! A tabela de repasse cal-\nculada pelo processador de roteamento (ver Figura \n4.6) é então colocada nas placas de linha da porta de \nentrada do roteador.\nPrincípios na prática\npara o mesmo prefixo x. Essas duas rotas poderiam ter o mesmo AS-PATH para x, mas poderiam ter dife-\nrentes valores de NEXT-HOP correspondentes aos diferentes enlaces interconectados. Utilizando os valores \nde ­\nNEXT-HOP e o algoritmo de roteamento intra-AS, o roteador pode determinar o custo do caminho \npara cada enlace interconectado e, então, aplicar o roteamento batata quente (consulte Seção 4.5.3) para \ndeterminar a interface apropriada.\nO BGP também inclui atributos que permitem que roteadores designem a métrica de sua preferência às \nrotas, e um atributo que indica como o prefixo foi inserido no BGP no AS de origem. Para obter uma discussão \ncompleta de atributos de rota, consulte Griffin [2012]; Stewart [1999]; Halabi [2000]; Feamster [2004]; RFC 4271.\nQuando um roteador de borda recebe um anúncio de roteador, ele utiliza sua política de importação \npara decidir se aceita ou filtra a rota e se estabelece certos atributos, tal como a métrica de preferência do rote-\nador. A política de importação pode filtrar uma rota porque o AS talvez não queira enviar tráfego por um dos \nASs no AS-PATH. O roteador de borda também pode filtrar uma rota porque já conhece uma rota preferencial \npara o mesmo prefixo.\nSeleção de rota do BGP\nComo já descrito antes nesta seção, o BGP usa eBGP e iBGP para distribuir rotas a todos os roteadores \ndentro de ASs. Com essa distribuição, um roteador pode conhecer mais do que uma rota para qualquer prefixo \ndeterminado, quando deverá selecionar uma das possíveis. O dado usado por esse processo de seleção é o con-\nA CAMADA  de REDE  293 \njunto de todas as rotas que o roteador descobriu e aceitou. Se houver duas ou mais para o mesmo prefixo, então \no BGP invoca sequencialmente as seguintes regras de eliminação, até sobrar apenas uma:\n• Rotas recebem, como um de seus atributos, um valor de preferência local. A preferência local de uma rota \npode ter sido estabelecida pelo roteador ou ter sido descoberta por um outro roteador no mesmo AS. \nEssa é uma decisão política que fica a cargo do administrador de rede do AS. (Mais adiante discutiremos \ndetalhes sobre questões de política do BGP.) São selecionadas as rotas que têm os valores de preferência \nlocal mais altos.\n• No caso das outras rotas remanescentes (todas com o mesmo valor de preferência local), é selecionada a \nque tenha o AS-PATH mais curto. Se essa fosse a única regra de seleção, então o BGP estaria usando um \nalgoritmo DV para determinação de caminho no qual a métrica da distância utiliza o número de saltos \nde AS em vez do número de saltos de roteadores.\n• Dentre as rotas remanescentes (todas com o mesmo valor de preferência local e com o mesmo com-\nprimento de AS-PATH) é selecionada a que tenha o roteador NEXT-HOP mais próximo. Nesse caso, \nmais próximo significa o que tenha o menor custo de caminho de menor custo, determinado pelo \nalgoritmo intra-AS. Esse processo, como discutimos na Seção 4.5.3, é em geral denominado rotea-\nmento da batata quente.\n• Se ainda restar mais de uma rota, o roteador usa identificadores BGP para selecionar a rota; veja Stewart, \n[1999].\nAs regras de eliminação são ainda mais complicadas do que descrevemos aqui. Para evitar pesadelos com o \nBGP, é melhor aprender suas regras de seleção em pequenas doses!\nPolítica de roteamento\nVamos ilustrar alguns dos conceitos básicos da política de roteamento BGP com um exemplo simples. A \nFigura 4.42 mostra seis sistemas autônomos interconectados: A, B, C, W, X e Y. É importante notar que A, B, C, \nW, X e Y são ASs, e não roteadores. Vamos admitir que os sistemas autônomos W, X e Y são redes stub e que A, B \ne C são redes provedoras de backbone. Vamos supor também que A, B e C, todos conectados uns aos outros, for-\nnecem informação completa sobre o BGP a suas redes cliente. Todo o tráfego que entrar em uma rede stub deve \nser destinado a essa rede, e todo o tráfego que sair da rede stub deve ter sido originado naquela rede. W e Y são \nclaramente redes stub. X é uma rede stub com múltiplas interconexões, visto que está ligado ao resto da rede por \nmeio de dois provedores diferentes (um cenário que está se tornando cada vez mais comum na prática). Todavia, \ntal como W e Y, o próprio X deve ser a origem/destino de todo o tráfego que entra/sai de X. Porém, como esse \ncomportamento da rede stub será executado e imposto? Como X será impedido de repassar tráfego entre B e C? \nIsso pode ser conseguido facilmente controlando o modo como as rotas BGP são anunciadas. Em particular, X \nfuncionará como uma rede stub se anunciar (a seus vizinhos B e C) que não há nenhum caminho para quaisquer \noutros destinos a não ser ele mesmo. Isto é, mesmo que X conheça um caminho, digamos, XCY, que chegue até \na rede Y, ele não anunciará esse caminho a B. Como B não fica sabendo que X tem um caminho para Y, B nunca \nrepassaria tráfego destinado a Y (ou a C) por meio de X. Esse exemplo simples ilustra como uma política seletiva \nde anúncio de rota pode ser usada para implementar relacionamentos de roteamento cliente/provedor.\nEm seguida, vamos focalizar uma rede provedora, digamos, o AS B. Suponha que B ficasse sabendo (por A) \nque A tem um caminho AW para W. Assim, B pode instalar a rota BAW em sua base de informações de roteamen-\nto. É claro que B também quer anunciar o caminho BAW a seu cliente, X, de modo que X saiba que pode rotear \npara W via B. Porém, B deveria anunciar o caminho BAW a C? Se o fizer, então C poderia rotear tráfego para W \nvia CBAW. Se A, B e C forem todos provedores de backbone, então B poderia sentir-se no direito de achar que não \ndeveria ter de suportar a carga (e o custo!) de transportar o tráfego em trânsito entre A e B. B poderia sentir-se no \ndireito de achar que é de A e C o trabalho (e o custo!) de garantir que C possa rotear de/para clientes de A por meio \nde uma conexão direta entre A e C. Hoje não existe nenhum padrão oficial que determine como ISPs de backbone \n   Redes de computadores e a Internet\n294\ndevem rotear entre si. Todavia, os ISPs comerciais adotam uma regra prática que diz que qualquer tráfego que es-\nteja fluindo por uma rede de backbone de um ISP deve ter ou uma origem ou um destino (ou ambos) em uma rede \nque seja cliente daquele ISP; caso contrário, o tráfego estaria pegando uma carona gratuita na rede do ISP. Acordos \nindividuais de parceria (peering) (para reger questões como as levantadas) costumam ser negociados entre pares de \nISPs e, em geral, são confidenciais; Huston [1999a] provê uma discussão interessante sobre acordos de parceria. Se \nquiser uma descrição detalhada sobre como a política de roteamento reflete os relacionamentos comerciais entre \nFigura 4.42  Um cenário BGP simples\nLegenda:\nA\nW\nX\nY\nB\nRede do \nprovedor\nRede do \ncliente\nC\nPor que há diferentes protocolos de roteamento inter-AS e intra-AS?\nAgora que já examinamos os detalhes de protoco-\nlos de roteamento inter-AS e intra-AS específicos uti-\nlizados pela Internet, vamos concluir considerando a \nquestão talvez mais fundamental que, antes de tudo, \npoderíamos levantar sobre esses protocolos (espera-\nmos que você tenha estado preocupado com isso o \ntempo todo e que não tenha deixado de enxergar o \nquadro geral por causa dos detalhes!). Por que são \nusados diferentes protocolos de roteamento inter-AS e \nintra-AS?\nA resposta a essa pergunta expõe o âmago da di-\nferença entre os objetivos do roteamento dentro de \num AS e entre ASs:\n• \nPolítica. Entre ASs, as questões políticas dominam. \nPode até ser importante que o tráfego que se origi-\nna em determinado AS não possa passar por outro \nAS específico. De maneira semelhante, determina-\ndo AS pode muito bem querer controlar o tráfego \nem trânsito que ele carrega entre outros ASs. Vimos \nque o BGP carrega atributos de caminho e ofere-\nce distribuição controlada de informação de rotea­\nmento, de modo que as decisões de roteamento \nbaseadas em políticas possam ser tomadas. Dentro \nde um AS, tudo está nominalmente no mesmo con-\ntrole administrativo. Assim, as questões de políticas \nde roteamento desempenham um papel bem me-\nnos importante na escolha de rotas no AS.\n• Escalabilidade. A escalabilidade de um algoritmo \nde roteamento e de suas estruturas de dados para \nmanipular o roteamento para/entre grandes núme-\nros de redes é uma questão fundamental para o \nroteamento inter-AS. Dentro de um AS, a escala-\nbilidade é uma preocupação menor. Isso porque, \nse um único domínio administrativo ficar muito \ngrande, é sempre possível dividi-lo em dois ASs e \nrealizar roteamento inter-AS entre esses dois no-\nvos ASs. (Lembre-se de que o OSPF permite que \nessa hierarquia seja construída dividindo um AS \nem áreas.)\n• Desempenho. Dado que o roteamento inter-AS \né bastante orientado pelas políticas, a qualidade \n(por exemplo, o desempenho) das rotas usadas é \nmuitas vezes uma preocupação secundária (isto é, \numa rota mais longa ou de custo mais alto que sa-\ntisfaça a certos critérios políticos pode muito bem \nprevalecer sobre uma que é mais curta, mas que \nnão satisfaz a esses critérios). Na verdade, vimos \nque entre ASs não há nem mesmo a ideia de cus-\nto associado às rotas (exceto a contagem de sal-\ntos do AS). Dentro de um AS individual, contudo, \nessas preocupações com as políticas têm menos \nimportância, permitindo que o roteamento se con-\ncentre mais no nível de desempenho atingido em \numa rota.\nPrincípios na prática\nA CAMADA  de REDE  295 \nISPs, veja Gao [2001]; Dmitiropoulos [2007]. Para ver uma abordagem recente sobre políticas de roteamento BGP, \nde um ponto de vista do ISP, consulte Caesar [2005b].\nComo já observamos, o BGP é um padrão na prática para roteamento inter-AS na Internet pública. Para ver o \nconteúdo de várias tabelas de roteamento BGP (grandes!) extraídas de roteadores pertencentes a ISPs de nível 1, con-\nsulte <http://www.routeviews.org>. Tabelas de roteamento BGP em geral contêm dezenas de milhares de prefixos e \natributos correspondentes. Estatísticas sobre tamanho e características de tabelas de roteamento BGP são apresentadas \nem Potaroo [2012].\nCom isso concluímos nossa breve introdução ao BGP. Entender esse protocolo é importante porque ele \ndesempenha um papel central na Internet. Aconselhamos você a consultar as referências Griffin [2012]; Stewart \n[1999]; Labovitz [1997]; Halabi [2000]; Huitema [1998]; Gao [2001]; Feamster [2004], Caesar [2005b]; Li [2007] \npara aprender mais sobre BGP.\n4.7  Roteamento por difusão e para um grupo\nAté este ponto do capítulo, focalizamos protocolos de roteamento que suportam comunicação individual \n(isto é, ponto a ponto), em que um único nó de origem envia um pacote a um único nó de destino. Nesta seção, \nvoltaremos a atenção a protocolos de roteamento por difusão (broacast) e para um grupo (multicast). No rotea-\nmento por difusão a camada de rede provê um serviço de entrega de pacote enviado de um nó de origem a todos \nos outros nós da rede; o roteamento para um grupo habilita um único nó de origem a enviar a cópia de um paco-\nte a um subconjunto de nós das outras redes. Na Seção 4.7.1 consideraremos algoritmos de roteamento por difu-\nsão e sua incorporação em protocolos de roteamento. Examinaremos roteamento para um grupo na Seção 4.7.2.\n4.7.1  Algoritmos de roteamento por difusão (broadcast)\nTalvez o modo mais direto de conseguir comunicação por difusão seja o nó remetente enviar uma cópia \nseparada do pacote para cada destino, como mostra a Figura 4.43(a). Dados N nós de destino, o nó de origem \nsimplesmente faz N cópias do pacote, endereça cada cópia a um destino diferente e então transmite N cópias \naos N destinos usando roteamento individual. Essa abordagem individual de N caminhos da transmissão por \ndifusão é simples — não é preciso nenhum novo protocolo de roteamento de camada de rede, nem duplicação \nde pacotes, nem funcionalidade de repasse. Porém, tem várias desvantagens. A primeira é sua ineficiência. Se \no nó de origem estiver conectado ao resto da rede por um único enlace, então N cópias separadas do (mesmo) \npacote transitarão por esse único enlace. Evidentemente seria mais eficiente enviar apenas uma única cópia de \num pacote até esse primeiro salto e então fazer o nó na outra extremidade do primeiro salto produzir e transmitir \nquaisquer cópias adicionais necessárias. Isto é, seria mais eficiente que os próprios nós da rede (em vez de apenas \no nó de origem) criassem cópias duplicadas de um pacote. Por exemplo, na Figura 4.43(b), somente uma única \ncópia de um pacote transita pelo enlace R1-R2. Aquele pacote, então, é duplicado em R2, e uma única cópia será \nenviada pelos enlaces R2-R3 e R2-R4.\nAs desvantagens adicionais da abordagem individual de N caminhos talvez sejam mais sutis, mas nem por isso \nmenos importantes. Uma premissa implícita desse tipo de roteamento é que o remetente conheça os destinatários \nde difusão e seus endereços. No entanto, como essa informação é obtida? Muito provavelmente, seriam exigidos \nmecanismos de protocolo adicionais (tais como associação ao grupo de difusão ou protocolo de registro de destino), \no que acrescentaria sobrecarga e, o que é importante, complexidade adicional a um protocolo que, de início, parecia \nbastante simples. Uma desvantagem final da abordagem individual de N caminhos está relacionada aos propósitos \npara os quais a difusão seria utilizada. Na Seção 4.5 aprendemos que protocolos de roteamento de estado de enlace \nusam difusão para propagar informações de estado de enlace que são usadas para calcular rotas individuais. É claro \nque, em situações em que o grupo de difusão é usado para criar e atualizar rotas individuais, seria imprudente (no \nmínimo!) confiar em infraestrutura de roteamento individual para implantar grupos de difusão.\n   Redes de computadores e a Internet\n296\nDadas as diversas desvantagens da difusão de N caminhos, logicamente são de interesse abordagens nas \nquais os próprios nós da rede desempenham um papel ativo na duplicação de pacotes, no repasse de pacotes e no \ncálculo de rotas de difusão. A seguir, examinaremos diversas dessas abordagens e, mais uma vez, adotaremos a \nnotação de grafo apresentada na Seção 4.5. Modelaremos novamente a rede como um grafo G = (N, E), onde N é \num conjunto de nós e uma coleção E de arestas, e cada aresta é um par de nós de N. Não seremos muito exigentes \nquanto à notação e adotaremos N para indicar ambos os conjuntos de nós, e a cardinalidade (|N|) ou o tamanho \ndaquele conjunto, quando não houver possibilidade de confusão.\nInundação não controlada\nA técnica mais óbvia para conseguir difusão é uma abordagem de inundação na qual o nó de origem envia uma \ncópia do pacote a todos os seus vizinhos. Quando um nó recebe um pacote de difusão, ele o duplica e repassa a todos os \nseus vizinhos (exceto ao vizinho do qual recebeu o pacote). É claro que, se o grafo for conectado, esse esquema enfim \nentregará uma cópia do pacote de difusão a todos os nós no grafo. Embora esse esquema seja simples e elegante, tem \numa falha fatal (antes de continuar, tente imaginar qual): se o grafo tiver ciclos, então uma ou mais cópias de cada pa-\ncote de difusão permanecerão em ciclo indefinidamente. Por exemplo, na Figura 4.43, R2 inundará R3, R3 inundará \nR4, R4 inundará R2 e R2 inundará (novamente!) R3 e assim por diante. Esse cenário simples resulta no ciclo sem fim \nde dois pacotes de difusão, um em sentido horário e um em sentido anti-horário. Mas pode haver uma falha fatal ainda \nmais catastrófica: quando um nó estiver conectado a mais de dois outros nós, criará e repassará várias cópias do pacote \n \nde difusão, cada uma das quais criará várias cópias de si mesmas (em outros nós com mais de dois vizinhos) e as-\nsim por diante. Essa tempestade de difusão, gerada pela infindável multiplicação depacotes de difusão, acabaria \nresultando na criação de uma quantidade tão grande de pacotes de difusão que a rede ficaria inutilizada. (Consulte \nnos exercícios de fixação, ao final do capítulo, um problema que analisa a velocidade com que essa tempestade de \ndifusão cresce.)\nInundação controlada\nA chave para evitar uma tempestade de difusão é um nó escolher de modo sensato quando repassa um \npacote (por exemplo, se já tiver recebido e repassado uma cópia anterior do pacote) e quando não repassa um \npacote. Na prática, há vários modos de fazer isso.\nNa inundação controlada por número de sequência, um nó de origem coloca seu endereço (ou outro iden-\ntificador exclusivo), bem como um número de sequência de difusão em um pacote de difusão e então envia o \npacote a todos os seus vizinhos. Cada nó mantém uma lista de endereços de origem e números de sequência para \ncada pacote de difusão que já recebeu, duplicou e repassou. Quando um nó recebe um pacote de difusão, primeiro \nverifica se o pacote está nessa lista. Se estiver, é descartado; se não estiver, é duplicado e repassado para todos os \nvizinhos do nó (exceto para o nó de quem o pacote acabou de ser recebido). O protocolo Gnutella, discutido no \nFigura 4.43  Duplicação na origem versus duplicação dentro da rede\nCriação/transmissão de duplicatas\nR2\nR4\nR3\na.\nb.\nR1\nR4\nR3\nR1\nR2\nDuplicar\nDuplicar\nA CAMADA  de REDE  297 \nCapítulo 2, usa inundação controlada com número de sequência para fazer a transmissão por difusão de consultas \nem sua rede de sobreposição. (Em Gnutella, a duplicação e repasse de mensagens é realizada na camada de aplica-\nção, e não na camada de rede.)\nUma segunda abordagem da inundação controlada é conhecida como repasse pelo caminho inverso (re-\nverse path forwarding — RPF) [Dalal, 1978], às vezes também denominado difusão pelo caminho inverso (reverse \npath broadcast — RPB). A ideia por trás do repasse pelo caminho inverso é simples, mas elegante. Quando um \nroteador recebe um pacote de difusão com determinado endereço de origem, ele transmite o pacote para todos \nos seus enlaces de saí­\nda (exceto para aquele do qual o pacote foi recebido) somente se o pacote chegou pelo en-\nlace que está em seu próprio caminho individual mais curto de volta ao remetente. Caso contrário, o roteador \napenas descarta o pacote que está entrando sem repassá-lo para nenhum de seus enlaces de saída. O pacote pode \nser descartado porque o roteador sabe que receberá, ou já recebeu, uma cópia no enlace que está em seu próprio \ncaminho mais curto de volta ao remetente. (É provável que você esteja querendo se convencer de que isso, de fato, \nacontecerá, e que não ocorrerão nem looping nem tempestades de difusão.) Note que o RPF não usa roteamento \nde difusão individual para entregar um pacote a um destino, nem exige que um roteador conheça o caminho mais \ncurto completo entre ele mesmo e a origem. O RPF precisa conhecer apenas o próximo vizinho em seu caminho \nindividual mais curto até o remetente; usa a identidade desse vizinho apenas para determinar se repassa ou não \nrepassa um pacote de difusão recebido.\nA Figura 4.44 ilustra o RPF. Suponha que os enlaces com as linhas mais grossas representem os caminhos de \nmenor custo desde os destinatários até a origem (A). O nó A inicialmente faz a transmissão por difusão de um paco-\nte da origem A até os nós C e B. O nó B repassará o pacote da origem A que recebeu de A (uma vez que A está em seu \ncaminho de menor custo até A) para C e D. B ignorará (descartará sem repassar) quaisquer pacotes da origem A que \nreceba de quaisquer outros nós (por exemplo, dos roteadores C ou D). Vamos considerar agora o nó C, que receberá \num pacote da origem A direto de A e também de B. Como B não está no caminho mais curto de C de retorno a A, C \nignorará quaisquer pacotes da origem A que recebam de B. Por outro lado, quando C receber um pacote da origem \nA direto de A, ele o repassará aos nós B, E e F.\nDifusão por spanning tree\nConquanto a inundação controlada por números de sequência e o RPF impeçam tempestades de difusão, \neles não evitam completamente a transmissão de pacotes por difusão redundantes. Por exemplo, na Figura \n4.44, os nós B, C, D e F recebem ou um ou dois pacotes redundantes. Idealmente, cada nó deveria receber ape-\nnas uma cópia do pacote de difusão. Examinando a árvore formada pelos nós conectados por linhas grossas \nna Figura 4.45(a), podemos ver que, se os pacotes de difusão fossem repassados por enlaces dessa árvore, cada \nFigura 4.44  Repasse pelo caminho inverso\nLegenda:\nA\nB\nD\nG\nC\nF\nE\npacote será repassado\npacote não será repassado além do roteador receptor\n   Redes de computadores e a Internet\n298\nnó da rede receberia exatamente uma cópia do pacote de difusão — exatamente a solução que procurávamos! \nEsse é um exemplo de spanning tree — uma árvore que contém todo e qualquer nó em um grafo. Mais for-\nmalmente, uma spanning tree de um grafo G = (N, E) é um grafo G' = (N, E') tal que E' é um subconjunto de \nE, G' é conectado, G' não contém nenhum ciclo e G' contém todos os nós originais em G. Se cada enlace tiver \num custo associado e o custo de uma árvore for a soma dos custos dos enlaces, então uma árvore cujo custo \nseja o mínimo entre todas as spanning trees do gráfico é denominada uma spanning tree mínima (o que não é \nnenhuma surpresa).\nAssim, outra abordagem para o fornecimento de difusão é os nós da rede construírem uma spanning tree, \nem primeiro lugar. Quando um nó de origem quiser enviar um pacote por difusão, enviará o pacote por todos \nos enlaces incidentes que pertençam à spanning tree. Um nó que receba um pacote por difusão então o repassa \na todos os seus vizinhos na spanning tree (exceto para o vizinho do qual recebeu o pacote). A spanning tree não \napenas elimina pacotes por difusão redundantes mas, uma vez instalada, pode ser usada por qualquer nó para \niniciar uma difusão, como mostram as figuras 4.45(a) e 4.45(b). Note que um nó não precisa estar ciente da \nárvore inteira; ele precisa apenas saber qual de seus vizinhos em G são vizinhos que pertencem à spanning tree.\nA principal complexidade associada com a abordagem de spanning tree é sua criação e manutenção. Fo-\nram desenvolvidos numerosos algoritmos distribuídos de spanning tree [Gallager, 1983; Gartner, 2003]. Aqui, \nconsideramos apenas um algoritmo simples. Na abordagem de nó central da construção de uma spanning tree, \né definido um nó central (também conhecido como ponto de encontro ou núcleo). Os nós então transmitem \nmensagens de adesão individualmente à árvore, endereçadas ao nó central. Uma mensagem de adesão à árvore é \nrepassada usando roteamento individual em direção ao centro até que ela chegue a um roteador que já pertence \nà spanning tree ou até que chegue ao centro. Em qualquer um dos casos, o caminho que a mensagem de adesão à \nárvore seguiu define o ramo da spanning tree entre o nó da borda que enviou a mensagem de adesão à árvore e o \ncentro. Esse novo caminho pode ser visto como um enxerto à spanning tree existente.\nA Figura 4.46 ilustra a construção de uma spanning tree baseada em um centro. Suponha que o nó E seja \nselecionado como centro da árvore. Imagine que o nó F primeiro se junte à árvore e repasse a E uma men-\nsagem de adesão à árvore. O único enlace EF se torna a spanning tree inicial. O nó B então se junta à span-\nning tree enviando a E sua mensagem de adesão à árvore. Suponha que a rota do caminho individual de E para \nB seja por D. Nesse caso, a mensagem de adesão à árvore resulta no enxerto do caminho BDE à spanning tree. \nEm seguida, o nó A se junta ao grupo crescente repassando sua mensagem de adesão à árvore em direção a E. \n \nSe o caminho individual de A a E passar por B, então, uma vez que B já se juntou à spanning tree, a chegada da \nmensagem de adesão à árvore de A em B resultará no enxerto imediato do enlace AB à spanning tree. Em seguida, \no nó C se junta à spanning tree repassando sua mensagem de adesão à árvore diretamente a E. Por fim, como o \nroteamento individual de G para E tem de passar pelo nó D, quando G enviar sua mensagem de adesão à árvore \na E, o enlace GD será enxertado à spanning tree no nó D.\nFigura 4.45  Difusão por spanning tree\nA\nB\nC\nD\nG\nE\nF\nA\nB\nC\nD\nG\nE\nF\na. Difusão iniciada em A\nb. Difusão iniciada em D\nA CAMADA  de REDE  299 \nAlgoritmos de difusão na prática\nProtocolos de difusão são usados na prática nas camadas de aplicação e de rede. Gnutella [2009] usa a difusão \nno nível de aplicação para fazer transmissão por difusão de consultas de conteúdo entre pares Gnutella. Nesse caso, \num enlace entre dois processos pares distribuídos no nível de aplicação na rede Gnutella é, na verdade, uma conexão \nTCP. O Gnutella usa uma forma de inundação controlada com número de sequência na qual um identificador de \n16 bits e um descritor de carga útil de 16 bits (que identifica o tipo de mensagem Gnutella) são usados para detectar \nse uma consulta por difusão recebida já foi recebida, duplicada e repassada antes. O Gnutella usa um campo de \ntempo de vida (TTL) para limitar o número de saltos pelos quais uma consulta inundada será repassada. Quando \num processo Gnutella recebe e duplica uma consulta, ele decrementa o campo de TTL antes de repassá-la. Assim, \numa consulta Gnutella inundada alcançará apenas pares que estão dentro de determinado número (o valor inicial \ndo TTL) de saltos em nível de aplicação desde o iniciador da consulta. Por isso, o mecanismo de consultas por inun-\ndação do Gnutella às vezes é denominado inundação de escopo limitado.\nUma forma de inundação controlada com número de sequência também é usada para fazer a transmissão \npor difusão de anúncios de estado de enlace (link-state advertisements — LSAs) no algoritmo de roteamento OSPF \n[RFC 2328; Perlman, 1999] e no algoritmo de ­\nroteamento Sistema Intermediário a Sistema Intermediário (Interme-\ndiate-System-to-Intermediate-System — IS-IS) [RFC 1142; Perlman, 1999]. O OSPF usa um número de sequência \n \nde 32 bits, bem como um campo de idade de 16 bits para identificar anúncios de estado de enlace (LSAs). \nLembre-se de que um nó OSPF faz broadcast periódico de LSAs para os enlaces ligados a ele quando o custo de \nenlace até um vizinho muda ou quando um enlace ativa ou desativa. Números de sequência LSA são usados para \ndetectar LSAs duplicados, mas também cumprem uma segunda função importante no OSPF. Com a inundação, é \npossível que um LSA gerado pela origem no tempo t chegue após um LSA mais novo que foi gerado pela mesma \norigem no tempo t + δ. Os números de sequência usados pelo nó da origem permitem que um LSA mais antigo \nseja distinguido de um mais novo. O campo de idade cumpre uma finalidade semelhante à de um valor TTL. \nO valor inicial do campo de idade é estabelecido em zero e é incrementado a cada salto à medida que este é re-\ntransmitido, e também é incrementado enquanto fica na memória de um roteador esperando para ser inundado. \nEmbora nossa descrição do algoritmo de inundação LSA tenha sido apenas breve, observamos que, na verdade, \nprojetar protocolos de difusão LSA pode ser um negócio muito delicado. RFC 789 e Perlman [1999] descrevem \num incidente no qual LSAs transmitidos incorretamente por dois roteadores defeituosos fizeram uma versão \nantiga do algoritmo de inundação LSA derrubar a ARPAnet inteira!\n4.7.2  Serviço para um grupo (multicast)\nVimos, na seção anterior, que, com o serviço de difusão, pacotes são entregues a cada um e a todos \nos nós em uma rede. Nesta seção, voltamos nossa atenção para o serviço para um grupo, no qual um pa-\nFigura 4.46  Construção de uma spanning tree com centro\n3\n2\n4\n1\n5\nA\nB\nC\nD\nG\nE\nF\nA\nB\nC\nD\nG\nE\nF\na. Construção da spanning tree passo a passo\nb. Spanning tree construída\n   Redes de computadores e a Internet\n300\ncote é entregue a apenas um subgrupo de nós de rede. Uma série de aplicações emergentes de rede requer \na entrega de pacotes de um ou mais remetentes a um grupo de destinatários. Entre essas aplicações estão a \ntransferência de dados em grandes volumes (por exemplo, a transferência de uma atualização de software \ndo desenvolvedor do software para os usuários que necessitam da atualização), mídia de fluxo contínuo (por \nexemplo, transferência de áudio, vídeo e texto de uma palestra ao vivo para um conjunto de participantes \ndistribuídos), as aplicações de dados compartilhados (por exemplo, quadro branco ou videoconferência, \ncompartilhados por muitos participantes distribuídos), a alimentação de dados (por exemplo, cotação de \nações), a atualização de cache da Web e os jogos interativos (por exemplo, ambientes virtuais interativos \ndistribuídos ou jogos multiusuários).\nNa comunicação para um grupo, enfrentamos imediatamente dois problemas — como identificar os \ndestinatários de um pacote desse tipo e como endereçar um pacote enviado a um desses destinatários. No \ncaso da comunicação individual, o endereço IP do destinatário (receptor) é levado em cada datagrama IP \nindividual e identifica o único destinatário; no caso do serviço para um grupo, temos vários destinatários. \nTem sentido que cada pacote desse tipo carregue os endereços IP de todos os vários destinatários? Embora \nessa abordagem possa ser funcional com um número pequeno de destinatários, não pode ser expandida com \nfacilidade para o caso de centenas de milhares de destinatários; a quantidade de informações de endereça-\nmento no datagrama ultrapassaria a quantidade de dados que é de fato carregada no campo de carga útil do \npacote. A identificação explícita dos destinatários pelo remetente também exige que o remetente conheça as \nidentidades e os endereços de todos os destinatários. Veremos em breve que há casos em que essa exigência \npode ser indesejável.\nPor essas razões, na arquitetura da Internet (e em outras arquiteturas de rede como ATM [Black, 1995]), \num pacote para um grupo é endereçado usando endereço indireto, isto é, um único identificador é utilizado \npara o grupo de destinatários e uma cópia do pacote que é ­\nendereçada ao grupo usando esse único identifica-\ndor é entregue a todos os destinatários associados ao grupo. Na Internet, o identificador único que representa \num grupo de destinatários é um endereço IP classe D para um grupo. O grupo de destinatários associados a \num endereço classe D é denominado grupo multicast. A abstração do grupo é ilustrada na Figura 4.47. Na \nfigura, quatro hospedeiros (no tom mais claro) estão associados ao endereço de grupo 226.17.30.197 e recebe-\nrão todos os datagramas endereçados a esse endereço de grupo. A dificuldade que ainda temos de enfrentar é \no fato de que cada hospedeiro tem um endereço individual exclusivo que é completamente independente do \nendereço do grupo do qual ele está participando.\nEmbora seja simples, a abstração do grupo provoca uma série de perguntas. Como um grupo começa e como \ntermina? Como é escolhido o endereço? Como novos hospedeiros são incorporados (seja como remetentes, seja \ncomo destinatários)? Qualquer um pode se juntar ao grupo (e enviar para esse grupo e receber dele) ou a participa-\nção no grupo é limitada e, se for limitada, quem a limita? Os membros do grupo ficam conhecendo as identidades \ndos outros membros como parte do protocolo de camada de rede? Como os nós da rede interagem para entregar \num datagrama de grupo a todos os membros desse grupo? Para a Internet, as respostas a todas essas perguntas \nenvolvem o Protocolo de Gerenciamento de Grupo da Internet (Internet Group Management Protocol — IGMP \n[RFC 3376]). Assim, vamos considerar rapidamente o protocolo IGMP. Mais adiante, voltaremos a essas perguntas \nde caráter mais geral.\nInternet Group Management Protocol\nO protocolo IGMP, versão 3 [RFC 3376] opera entre um hospedeiro e o roteador diretamente conectado a \nele (em termos informais, pense no roteador diretamente conectado ao hospedeiro como o roteador de primeiro \nsalto que um hospedeiro veria no caminho até ­\nqualquer hospedeiro externo à sua própria rede local, ou como \no roteador de último salto em qualquer caminho até esse hospedeiro), conforme mostra a Figura 4.48. A figura \nmostra os três roteadores do primeiro salto, cada um conectado ao hospedeiro ao qual está ligado por uma inter-\nface local de saída. Essa interface local está ligada a uma LAN nesse exemplo e, embora cada LAN tenha vários \nA CAMADA  de REDE  301 \nhospedeiros ligados a ela, no máximo apenas alguns deles comumente pertencerão a um dado grupo a qualquer \ndeterminado instante.\nO IGMP oferece os meios para um hospedeiro informar ao roteador conectado a ele que uma aplicação que \nestá funcionando no hospedeiro quer se juntar a um grupo específico. Dado que o escopo de interação do IGMP \né limitado a um hospedeiro e seu roteador conectado, outro protocolo é decerto necessário para coordenar os \nroteadores de grupo (incluindo os conectados) por toda a Internet, de modo que os datagramas de grupo sejam \nroteados a seus destinos finais. Esta última funcionalidade é realizada por algoritmos de roteamento de grupo \nda camada de rede, como os que veremos em breve. Assim, o serviço de grupo na camada de rede da Internet \nconsiste em dois componentes complementares: IGMP e protocolos de roteamento de grupo.\nO IGMP tem apenas três tipos de mensagens. Como as mensagens ICMP, as mensagens IGMP são \ntransportadas (encapsuladas) dentro de um datagrama IP, com um número de ­\nprotocolo igual a 2. A men-\nsagem membership_query é enviada por um roteador a todos os ­\nhospedeiros em uma interface co-\nnectada (por exemplo, para todos os hospedeiros em uma rede ­\nlocal) para determinar o conjunto de todos \nos grupos aos quais se ligaram os hospedeiros naquela interface. Os hospedeiros respondem a uma men-\nsagem membership_query com uma mensagem IGMP membership_report. As mensagens \n \nmembership_report também podem ser geradas por um hospedeiro quando uma aplicação se junta \npela primeira vez ao ­\ngrupo sem esperar por uma mensagem membership_query vinda do roteador. O úl-\ntimo tipo de mensagem IGMP é a mensagem leave_group. O interessante é que essa mensagem é opcional. \nMas, se é opcional, como um roteador detecta quando um hospedeiro sai do ­\ngrupo? A resposta é que o roteador \ndeduz que o hospedeiro não está mais ligado a determinado grupo quando nenhum hospedeiro responde a uma \nmensagem membership_query com esse endereço de grupo. Isso é um exemplo do que às vezes é deno-\nminado estado flexível em um protocolo da Internet. Em um protocolo de estado flexível, o estado (no caso do \nFigura 4.47  \u0007\nO serviço para um grupo: um datagrama endereçado ao grupo é entregue a todos \nos membros do grupo\n128.119.40.186\n128.34.108.63\n128.34.108.60\n128.59.16.20\ngrupo \n226.17.30.197\nRoteador ao qual está ligado \num membro do grupo\nRoteador ao qual nenhum \nmembro do grupo está ligado\nLegenda:\n   Redes de computadores e a Internet\n302\nIGMP, o fato de haver hospedeiros ligados a um dado grupo) será encerrado por um evento de esgotamento de \ntemporização (nesse caso, por uma mensagem membership_query periódica emitida pelo roteador) se não for ex-\nplicitamente renovado (nesse caso, por uma mensagem membership_report vinda de um hospedeiro ­\nconectado).\nO termo estado flexível (soft state) foi criado por Clark [1988], que descreveu a noção de mensagens pe-\nriódicas de atualização de estado enviadas por um sistema final, e sugeriu que, com tais mensagens, o estado \npoderia ser perdido em uma falha qualquer e depois restaurado automaticamente por mensagens de renovação \nsubsequentes — tudo de forma transparente ao sistema final e sem chamar qualquer procedimento explícito de \nrecuperação de falhas:\n(…) a informação de estado não seria crítica na manutenção do tipo desejado de serviço associado ao fluxo. \nEm vez disso, esse tipo de serviço seria imposto pelas extremidades, que periodicamente enviariam mensagens \npara garantir que o tipo de serviço apropriado estava sendo associado ao fluxo. Desse modo, a informação \nde estado associada ao fluxo poderia ser perdida em uma falha qualquer sem a interrupção permanente dos \nrecursos sendo utilizados. Chamo esse conceito de “estado flexível”, e ele pode muito bem nos permitir alcançar \nnossos objetivos principais de capacidade de sobrevivência e flexibilidade...\nArgumenta-se que os protocolos de estado flexível resultam em um controle mais simples do que os de \nestado rígido, que não apenas exigem que o estado seja acrescentado e removido explicitamente, mas também \nmecanismos para se recuperar da situação onde a entidade responsável por remover o estado tenha terminado \nprematuramente ou falhado de alguma maneira. Discussões interessantes sobre o estado flexível podem ser vistas \nem Raman [1999]; Ji [2003]; Lui [2004].\nAlgoritmos de roteamento para grupos\nO problema de roteamento para  grupos (multicast) está ilustrado na Figura 4.49. Os hospedeiros agre-\ngados a esse grupo são os de tom mais claro; os roteadores imediatamente conectados a eles também estão nesse \ntom. Como mostra a figura, apenas um subconjunto desses roteadores (os que têm hospedeiros conectados que \nse juntaram ao grupo) realmente precisa receber o tráfego para o grupo. Ou seja, apenas os roteadores A, B, E \ne F precisam receber esse tráfego. Uma vez que nenhum dos hospedeiros conectados ao roteador D está ligado \nao grupo e o roteador C não tem hospedeiros conectados, nem C nem D precisam receber o tráfego do grupo. \nA meta do roteamento para um grupo é encontrar uma árvore de enlaces que conecte todos os roteadores que \ntêm hospedeiros conectados pertencentes ao ­\ngrupo. ­\nEntão, pacotes para um grupo serão roteados ao longo dessa \nFigura 4.48  \u0007\nOs dois componentes de grupo da camada de rede: IGMP e protocolos de \nroteamento para um grupo\nIGMP\nIGMP\nIGMP\nIGMP\nRoteamento para \ngrupo de longa \ndistância\nA CAMADA  de REDE  303 \nárvore desde o remetente até todos os hospedeiros pertencentes à árvore de grupo. Claro, a árvore pode conter \nroteadores que não têm hospedeiros conectados pertencentes ao grupo (por exemplo, na Figura 4.48, é impossí-\nvel conectar os roteadores A, B, E e F em uma árvore sem envolver o roteador C ou D).\nNa prática, duas abordagens foram adotadas para determinar a árvore de roteamento para um grupo e \nambas já foram estudadas no contexto do roteamento por difusão, portanto vamos apenas mencioná-las. A di-\nferença entre as duas abordagens refere-se ao uso de uma única árvore compartilhada pelo grupo para distribuir \no tráfego para todos os remetentes do grupo ou à construção de uma árvore de roteamento específica para cada \nremetente individual:\n• Roteamento multicast usando uma árvore compartilhada pelo grupo. Como no caso da difusão por span-\nning tree, o roteamento para um grupo por uma árvore compartilhada por esse grupo é baseado na \nconstrução de uma árvore que inclui todos os roteadores de borda cujos hospedeiros a ele conectados \npertencem ao grupo. Na prática, é usada uma abordagem centralizada para construir a árvore de ro-\nteamento para o grupo — roteadores de aresta cujos hospedeiros a eles ligados pertencem ao grupo \nenviam (individualmente) mensagens de adesão endereçadas ao nó central. Como no caso da difusão, \numa mensagem de adesão é transmitida usando roteamento individual em direção ao centro até que a \nmensagem chegue a um roteador que já pertence à árvore de grupo ou chegue até o centro. Todos os ro-\nteadores do caminho percorrido pela mensagem de adesão então transmitirão os pacotes recebidos pelo \ngrupo ao roteador de aresta que iniciou a adesão. Uma questão crítica para o roteamento para um grupo \npor meio de árvore baseado em um centro é o processo utilizado para selecionar o centro. Algoritmos \nde seleção de centro são discutidos em Wall [1980]; Thaler [1997]; Estrin [1997].\n• Roteamento para um grupo usando uma árvore baseada na origem. Enquanto o roteamento para um gru-\npo por meio de árvore compartilhada pelo grupo constrói uma ­\núnica árvore compartilhada para rotear \npacotes de todos os remetentes, a segunda abordagem constrói uma árvore de roteamento para um grupo \npara cada origem no grupo. Na ­\nprática, é usado um algoritmo RPF (com nó de origem x) para construir \numa árvore de transmissão para um grupo a fim de enviar datagramas que venham da origem x. Para ser \nusado em um grupo, o algoritmo RPF que estudamos precisa sofrer alguma adaptação. Para ver por que, \nconsidere o roteador D na Figura 4.50. Com difusão RPF, o algoritmo repassaria pacotes ao roteador G, \nFigura 4.49  Hospedeiros do grupo, seus roteadores conectados e outros roteadores\nA\nC\nF\nB\nD\nE\n   Redes de computadores e a Internet\n304\nmesmo que este não tivesse nenhum hospedeiro conectado a ele pertencente ao grupo. Embora isso não \nseja tão ruim para este caso, em que D tem apenas um roteador adiante dele, G, imagine o que aconteceria \nse houvesse milhares de roteadores adiante de D! Cada um dos milhares de roteadores ­\nreceberia ­\npacotes \n­\nindesejados para um grupo. (Esse cenário não está tão longe da realidade como parece. A Mbone inicial \n[Casner, 1992; Macedonia, 1994], a primeira rede de grupo global, sofria exatamente desse problema no \ninício.) A solução para o problema do recebimento de pacotes de grupo indesejados sob RPF é conhecida \ncomo poda. Um roteador para um grupo que receba pacotes de grupo e não tenha nenhum hospedeiro \nconectado a ele pertencente àquele grupo enviará uma mensagem de poda ao roteador anterior a ele. Se \num roteador receber mensagens de poda de cada um dos roteadores que estão adiante dele, então pode \nrepassar uma mensagem de poda aos que estão antes dele.\nRoteamento para  grupos na Internet\nO primeiro protocolo de roteamento para  grupos usado na Internet foi o Protocolo de Roteamento para \num Grupo por Vetor de Distâncias (Distance-Vector Multicast Routing Protocol — DVMRP) [RFC 1075]. \nO DVMRP executa árvores específicas de origem com repasse de caminho inverso e poda. Esse protocolo usa \num algoritmo RPF com poda, como acabamos de ver. Talvez o protocolo de roteamento para um grupo mais \nutilizado na Internet seja o protocolo de roteamento para um grupo independente de protocolo (PIM — Pro-\ntocol-Independent Multicast), o qual reconhece explicitamente dois cenários de distribuição para um grupo. \nNo modo denso [RFC 3973], os membros do grupo multicast são densamente localizados; ou seja, muitos ou a \nmaioria dos roteadores na área precisam estar envolvidos nos datagramas de roteamento para um grupo. O PIM \nde modo denso é uma técnica de repasse de caminho inverso do tipo “inundar e podar”\n, semelhante em espírito \nao DVMRP.\nNo modo esparso [RFC 4601], o número de roteadores com membros do grupo é pequeno em relação \nao número total de roteadores; os membros do grupo são amplamente dispersos. O PIM de modo esparso usa \npontos de encontro para definir a árvore de distribuição multicast. No grupo de origem específica (SSM — \nFigura 4.50  Repasse pelo caminho inverso, no caso do serviço para um grupo\nA\nC\nF\nE\nG\nB\nD\nLegenda:\npacote será repassado\npacote não será repassado além do roteador destinatário\nS: origem\nA CAMADA  de REDE  305 \nsource-specific multicast) [RFC 3569, RFC 4607], só um remetente pode enviar tráfego para a árvore de grupo, \nsimplificando de modo considerável a construção e a manutenção da árvore.\nQuando o PIM e o DVMRP são usados dentro de um domínio, o operador de rede pode configurar rotea-\ndores IP de grupo dentro do domínio, da mesma forma que protocolos de roteamento individual intradomínio \ncomo RIP, IS-IS e OSPF podem ser configurados. Mas o que acontece quando são necessárias rotas de grupo \nentre domínios diferentes? Existe algum equivalente ao protocolo BGP interdomínio para grupos? A resposta é \n(literalmente) sim. [RFC 4271] define as extensões de protocolos múltiplos para o BGP que lhe permite carregar \ninformações sobre roteamento para outros protocolos, incluindo informações de grupo. O protocolo MSDP \n(Multicast Source Discovery Protocol) [RFC 3618, RFC 4611] pode ser usado para conectar os pontos de en-\ncontro em domínios de diferentes PIMs de modo esparso. Uma visão geral interessante sobre o atual estado do \nroteamento para um grupo na Internet é o [RFC 5110].\nVamos finalizar nossa discussão sobre IP para um grupo observando que ele ainda tem de decolar de forma \nsignificativa. Para ver discussões interessantes sobre o modelo de serviço de grupo da Internet e questões sobre \nimplementação, consulte Diot [2000]; Sharma [2003]. Não obstante, apesar da falta de emprego generalizado, \no uso de grupos em nível de rede está longe de “morrer”\n. O tráfego de grupo foi conduzido por muitos anos na \nInternet 2, assim como as redes com as quais ela se interconecta [Internet2 Multicast, 2012]. No Reino Unido, \na BBC está envolvida em testes de distribuição de conteúdo pelo uso do IP de grupo [BBC Multicast, 2012]. Ao \nmesmo tempo, o uso de grupos em nível de aplicação, como vimos em PPLive, no Capítulo 2, e outros sistemas \npeer-to-peer, como o End System Multicast [Chu, 2002], oferecem distribuição de conteúdo para um grupo entre \npares que utilizam protocolos de grupo da camada de aplicação (em vez da camada de rede). Os serviços de gru-\npo do futuro serão primeiro executados na camada de rede (no núcleo da rede) ou na de aplicação (na borda da \nrede)? Enquanto a atual mania de distribuição de conteúdo peer-to-peer influencia a favor da camada de aplicação \nde grupo pelo menos em um futuro próximo, a evolução continua a caminhar no IP de grupo, e às vezes, no fim \ndas contas, devagar se vai ao longe.\n4.8  Resumo\nNeste capítulo, iniciamos nossa viagem rumo ao núcleo da rede. Aprendemos que a camada de rede envolve \ntodos os roteadores da rede. Por causa disso, os protocolos de camada de rede estão entre os mais desafiadores \nda pilha de protocolos.\nAprendemos que um roteador pode precisar processar milhões de fluxos de pacotes entre diferentes pares \norigem-destino ao mesmo tempo. Para que um roteador consiga processar toda essa quantidade de fluxos, os \nprojetistas de redes aprenderam, com o passar dos anos, que as tarefas do roteador devem ser as mais simples \npossíveis. Muitas medidas podem ser tomadas para facilitar a tarefa do roteador, incluindo a utilização de uma \ncamada de rede de datagramas, em vez de uma camada de rede de circuitos virtuais, a utilização de um cabeçalho \naprimorado de tamanho fixo (como no IPv6), a eliminação da fragmentação (também feita no IPv6) e o forne-\ncimento de um único serviço de melhor esforço. Talvez o truque mais importante aqui seja não monitorar fluxos \nindividuais, mas, em vez disso, tomar decisões de roteamento com base exclusivamente em endereços de destino \nhierarquicamente estruturados nos datagramas. É interessante notar que o serviço dos correios vem usando essa \nmesma tática há muitos anos.\nNeste capítulo, examinamos também os princípios básicos dos algoritmos de roteamento. Aprendemos como \nesses algoritmos fazem a abstração da rede de computadores em um grafo com nós e enlaces. Com essa abstração, \npodemos pesquisar a rica teoria do roteamento de caminho mais curto em grafos, que foi desenvolvida nos últi-\nmos 40 anos nas comunidades de pesquisa operacional e de algoritmos. Vimos que há duas abordagens amplas: \numa centralizada (global), em que cada nó obtém o mapeamento completo da rede e aplica independentemente o \nalgoritmo de roteamento de caminho mais curto, e uma descentralizada, em que nós individuais têm apenas um \nquadro parcial da rede inteira e, mesmo assim, trabalham em conjunto para entregar pacotes ao longo das rotas \nmais curtas. Também estudamos como a hierarquia é utilizada para lidar com o problema da escala, dividindo redes \n   Redes de computadores e a Internet\n306\nde grande porte em domínios administrativos independentes denominados sistemas autônomos (ASs). Cada AS \ndireciona independentemente seus datagramas pelo sistema, assim como cada país distribui correspondência postal \npelo seu território. Aprendemos como abordagens centralizadas, descentralizadas e hierárquicas estão incorporadas \nnos principais protocolos de roteamento da Internet: RIP, OSPF e BGP. Concluímos nosso estudo de algoritmos de \nroteamento considerando o roteamento por difusão (broadcast) e para um grupo (multicast).\nAgora que concluímos nosso estudo da camada de rede, nossa jornada segue rumo a um nível mais baixo da \npilha de protocolos, isto é, à camada de enlace. Como a camada de rede, a camada de enlace é também parte do \nnúcleo da rede. Mas veremos no próximo capítulo que a camada de enlace tem a tarefa muito mais localizada de \nmovimentar pacotes entre nós no mesmo enlace ou rede local. Embora essa tarefa possa à primeira vista parecer \ntrivial quando comparada às tarefas da camada de rede, veremos que a camada de enlace envolve uma série de \nquestões importantes e fascinantes que podem nos manter ocupados por muito tempo.\nExercícios\nde fixação e perguntas\nQuestões de revisão do Capítulo 4\nSEÇÕES 4.1–4.2\n\t\nR1.\t Vamos rever um pouco da terminologia usada neste livro. Lembre-se de que o nome de um pacote na \ncamada de transporte é segmento e que o nome de um pacote na camada de enlace é quadro. Qual é o nome \nde um pacote de camada de rede? Lembre-se de que roteadores e comutadores da camada de enlace são \ndenominados comutadores de pacotes. Qual é a diferença fundamental entre um roteador e um comutador \nda camada de enlace? Lembre-se de que usamos o termo roteadores tanto para redes de datagramas quanto \npara redes de CVs.\n\t\nR2.\t Quais são as duas funções mais importantes de camada de rede em uma rede de datagramas? Quais são as três \nfunções mais importantes de camada de rede em uma rede com circuitos virtuais?\n\t\nR3.\t Qual é a diferença entre rotear e repassar?\n\t\nR4.\t Os roteadores nas redes de datagramas e nas redes de circuitos virtuais usam tabelas de repasse? Caso usem, \ndescreva as tabelas de repasse para ambas as classes de redes.\n\t\nR5.\t Descreva alguns serviços hipotéticos que a camada de rede poderia oferecer a um pacote individual. Faça o \nmesmo para um fluxo de pacotes. Alguns dos serviços hipotéticos que você descreveu são fornecidos pela \ncamada de rede da Internet? Alguns são fornecidos pelo modelo de serviço ATM CBR? Alguns são fornecidos \npelo modelo de serviço ATM ABR?\n\t\nR6.\t Cite algumas aplicações que poderiam se beneficiar do modelo de serviço ATM CBR.\nSEÇÃO 4.3\n\t\nR7.\t Discuta por que cada porta de entrada em um roteador de alta velocidade armazena uma cópia de sombra da \ntabela de repasse.\n\t\nR8.\t Três tipos de elementos de comutação são discutidos na Seção 4.3. Cite e descreva brevemente cada tipo. Qual \n(se houver algum) pode enviar múltiplos pacotes em paralelo pelo elemento?\n\t\nR9.\t Descreva como pode ocorrer perda de pacotes em portas de entrada. Descreva como a perda de pacotes pode \nser eliminada em portas de entrada (sem usar buffers infinitos).\n\t\nR10.\t Descreva como pode ocorrer perda de pacotes em portas de saída. Essa perda poderia ser impedida \naumentando a velocidade de fábrica do comutador?\n\t\nR11.\t O que é bloqueio HOL? Ele ocorre em portas de saída ou em portas de entrada?\nA CAMADA  de REDE  307 \nSEÇÃO 4.4\n\t\nR12.\t Roteadores têm endereços IP? Em caso positivo, quantos?\n\t\nR13.\t Qual é o equivalente binário de 32 bits para o endereço IP 223.1.3.27?\n\t\nR14.\t Visite um hospedeiro que usa DHCP para obter seu endereço IP, máscara de rede, roteador de default e \nendereço IP de seu servidor DNS local. Faça uma lista desses valores.\n\t\nR15.\t Suponha que haja três roteadores entre os hospedeiros de origem e de destino. Ignorando a fragmentação, um \ndatagrama IP enviado do hospedeiro de origem até o hospedeiro de destino transitará por quantas interfaces? \nQuantas tabelas de repasse serão indexadas para deslocar o datagrama desde a origem até o destino?\n\t\nR16.\t Suponha que uma aplicação gere blocos de 40 bytes de dados a cada 20 ms e que cada bloco seja encapsulado em \num segmento TCP e, em seguida, em um datagrama IP. Que porcentagem de cada datagrama será sobrecarga e \nque porcentagem será dados de aplicação?\n\t\nR17.\t Suponha que o hospedeiro A envie ao hospedeiro B um segmento TCP encapsulado em um datagrama IP. \nQuando o hospedeiro B recebe o datagrama, como sua camada de rede sabe que deve passar o segmento (isto \né, a carga útil do datagrama) para TCP e não para UDP ou qualquer outra coisa?\n\t\nR18.\t Suponha que você compre um roteador sem fio e o conecte a seu modem a cabo. Suponha também que \nseu ISP designe dinamicamente um endereço IP a seu dispositivo conectado (isto é, seu roteador sem fio). \nSuponha ainda que você tenha cinco PCs em casa e que usa 802.11 para conectá-los sem fio ao roteador. \nComo são designados endereços IP aos cinco PCs? O roteador sem fio usa NAT? Por quê?\n\t\nR19.\t Compare os campos de cabeçalho do IPv4 e do IPv6 e aponte suas diferenças. Eles têm algum campo em comum?\n\t\nR20.\t Afirma-se que, quando o IPv6 implementa túneis via roteadores IPv4, o IPv6 trata os túneis IPv4 como \nprotocolos de camada de enlace. Você concorda com essa afirmação? Explique sua resposta.\nSEÇÃO 4.5\n\t\nR21.\t Compare e aponte as diferenças entre os algoritmos de roteamento de estado de enlace e por vetor de \ndistâncias.\n\t\nR22.\t Discuta como a organização hierárquica da Internet possibilitou estender seu alcance para milhões de \nusuários.\n\t\nR23.\t É necessário que todo sistema autônomo use o mesmo algoritmo de roteamento intra-AS? Justifique sua \nresposta.\nSEÇÃO 4.6\n\t\nR24.\t Considere a Figura 4.37. Começando com a tabela original em D, suponha que D receba de A o seguinte \nanúncio:\nSub-rede de destino\nRoteador seguinte\nNúmero de saltos até o destino\nz\nC\n10\nw\n—\n1\nx\n—\n1\n. . . .\n. . . .\n. . . .\n\t\n\t A tabela em D mudará? Em caso afirmativo, como?\n\t\nR25.\t Compare os anúncios utilizados por RIP e OSPF e aponte suas diferenças.\n\t\nR26.\t Complete: anúncios RIP em geral anunciam o número de saltos até vários destinos. Atualizações BGP, por \noutro lado, anunciam _________________ aos diversos destinos.\n   Redes de computadores e a Internet\n308\n\t\nR27.\t Por que são usados protocolos inter-AS e intra-AS diferentes na Internet?\n\t\nR28.\t Por que considerações políticas são tão importantes para protocolos intra-AS, como o OSPF e o RIP, quanto \npara um protocolo de roteamento inter-AS, como BGP?\n\t\nR29\t\t Defina e aponte as diferenças entre os seguintes termos: sub-rede, prefixo e rota BGP.\n\t\nR30.\t Como o BGP usa o atributo NEXT-HOP? Como ele usa o atributo AS-PATH?\n\t\nR31.\t Descreva como um administrador de rede de um ISP de nível superior pode executar uma política ao \nconfigurar o BGP.\nSEÇÃO 4.7\n\t\nR32.\t Cite uma diferença importante entre a execução da abstração de difusão por meio de múltiplas transmissões \nindividuais e a de uma única difusão com suporte da rede (roteador).\n\t\nR33.\t Para cada uma das três abordagens gerais que estudamos para a comunicação por difusão (inundação não \ncontrolada, inundação controlada e difusão por spanning tree), as seguintes declarações são verdadeiras ou \nfalsas? Você pode considerar que não há perda de pacotes por estouro de buffers e que todos os pacotes são \nentregues em um enlace na ordem em que foram enviados.\na.\t Um nó pode receber várias cópias do mesmo pacote.\nb.\t Um nó pode repassar várias cópias de um pacote pelo mesmo enlace de saída.\n\t\nR34.\t Quando um hospedeiro se junta a um grupo, ele deve mudar seu endereço IP para o endereço do grupo ao \nqual está se juntando?\n\t\nR35.\t Quais são os papéis desempenhados pelo protocolo IGMP e por um protocolo de roteamento para um grupo \nde longa distância?\n\t\nR36.\t Qual é a diferença entre uma árvore compartilhada por um grupo e uma árvore de origem no contexto do \nroteamento para um grupo?\nproblemas\n\t\nP1.\t Nesta questão, consideramos alguns dos prós e dos contras de redes de circuitos virtuais e redes de datagramas.\na.\t Suponha que roteadores foram submetidos a condições que poderiam levá-los a falhar com muita \nfrequência. Isso seria um argumento em favor de um CV ou arquitetura de datagrama? Por quê?  \nb.\t Suponha que um nó de origem e um de destino solicitem que uma quantidade fixa de capacidade esteja \nsempre disponível em todos os roteadores no caminho entre o nó de origem e de destino, para o uso \nexclusivo de fluxo de tráfego entre esse nós. Essas ações favorecem uma arquitetura de circuitos virtuais \nou de datagramas? Por quê?\nc.\t Suponha que os enlaces e os roteadores da rede nunca falhem e que os caminhos de roteamento usados \nentre as duplas de origem/destino permaneçam constantes. Nesse cenário, a arquitetura de circuitos \nvirtuais ou de datagramas possui mais sobrecarga de tráfego de controle? Por quê?\n\t\nP2.\t Considere uma rede de circuitos virtuais. Suponha que o número do CV é um campo de 8 bits.\na.\t Qual é o número máximo de circuitos virtuais que pode ser transportado por um enlace?\nb.\t Suponha que um nó central determine caminhos e números de CVs no estabelecimento da conexão. \nSuponha que o mesmo número de CV seja usado em cada enlace no caminho do CV. Descreva como \no nó central poderia determinar o número do CV no estabelecimento da conexão. É possível haver um \nnúmero de CVs ativos menor do que o máximo determinado na parte (a) e, mesmo assim, não haver \nnenhum número de CV livre em comum?\nc.\t Suponha que sejam permitidos números diferentes de CVs em cada enlace no caminho de um CV. \nDurante o estabelecimento da conexão, após a determinação de um caminho fim a fim, descreva como \nA CAMADA  de REDE  309 \nos enlaces podem escolher seus números de CVs e configurar suas tabelas de repasse de uma maneira \ndescentralizada, sem depender de um nó central.\n\t\nP3.\t Uma tabela de repasse bem básica em uma rede de CVs tem quatro colunas. O que significam os valores em \ncada uma dessas colunas? Uma tabela de repasse bem básica em uma rede de datagramas tem duas colunas. \nO que significam os valores em cada coluna?\n\t\nP4.\t Considere a rede a seguir.\na.\t Suponha que seja uma rede de datagramas. Mostre a tabela de repasse no roteador A, de modo que todo \no tráfego destinado ao hospedeiro H3 seja encaminhado pela interface 3.\nb.\t Suponha que esta rede seja uma rede de datagramas. Você consegue compor uma tabela de repasse no \nroteador A, de modo que todo o tráfego de H1 destinado ao hospedeiro H3 seja encaminhado pela \ninterface 3, enquanto todo o tráfego de H2 destinado ao hospedeiro H3 seja encaminhado pela interface \n4? (Dica: esta é uma pergunta capciosa.)\nc.\t Suponha, agora, que esta rede seja uma rede de circuitos virtuais e que haja uma chamada em andamento \nentre H1 e H3, e outra chamada em andamento entre H2 e H3. Elabore uma tabela de repasse no roteador \nA, de modo que todo o tráfego de H1 destinado ao hospedeiro H3 seja encaminhado pela interface 3, \nenquanto todo o tráfego de H2 destinado ao hospedeiro H3 seja encaminhado pela interface 4.\nd.\t Admitindo o mesmo cenário de (c), elabore tabelas de repasse nos nós B, C e D.\nB\nA\n1\n3\n2\n4\n2\nD\n1\n2\n3\nH3\nH1\nH2\n1\n1\n2\nC\n\t\nP5.\t Considere uma rede de circuito virtual cujo campo de número de CV tenha 2 bits. Suponha que a rede queira \nestabelecer um circuito virtual por quatro enlaces: A, B, C e D. Imagine que, nesse momento, cada enlace \nesteja carregando dois outros circuitos virtuais e que os números desses outros CVs são os seguintes:\nEnlace A\nEnlace B\nEnlace C\nEnlace D\n00\n01\n10\n11\n01\n10\n11\n00\n\t\n\t Ao responder às perguntas a seguir, tenha em mente que cada um dos CVs pode estar atravessando apenas \num dos quatro enlaces.\na.\t Se cada CV tiver de usar o mesmo número de CV em todos os enlaces ao longo de seu caminho, qual \nnúmero de CV poderia ser designado ao novo CV?\nb.\t Se fosse permitido que cada CV tivesse um número de CV diferente nos diferentes enlaces em seu caminho \n(de modo que a tabela de repasse tenha de realizar tradução de número de CV), quantas combinações \ndiferentes de quatro números de CVs (um para cada enlace) poderiam ser usadas?\n\t\nP6.\t No texto usamos o termo serviço orientado para conexão para descrever a camada de transporte e serviço de \nconexão para a camada de rede. Por que essa sutileza na terminologia?\n\t\nP7.\t Suponha que dois pacotes cheguem a duas portas de entrada diferentes de um roteador exatamente ao mesmo \ntempo. Suponha também que não haja outros pacotes em lugar algum no roteador.\na.\t Suponha que os dois pacotes devam ser repassados a duas portas de saída diferentes. É possível repassar \nos dois pacotes pelo elemento de comutação ao mesmo tempo quando o elemento usa um barramento \ncompartilhado?\n   Redes de computadores e a Internet\n310\nb.\t Imagine que os dois pacotes devam ser repassados a duas portas de saída diferentes. É possível repassar os \ndois pacotes pelo elemento de comutação ao mesmo tempo quando o elemento usa o tipo crossbar?\nc.\t Considere que os dois pacotes devam ser repassados para a mesma porta de saída. É possível repassar os \ndois pacotes pelo elemento de comutação ao mesmo tempo quando o elemento usa um tipo crossbar?\n\t\nP8.\t Na Seção 4.3, observamos que o atraso máximo de fila é (n–1)D se o elemento de comutação for n vezes mais \nrápido do que as taxas das linhas de entrada. Suponha que todos os pacotes tenham o mesmo comprimento, n \npacotes chegam ao mesmo tempo às n portas de entrada e todos os n pacotes querem ser encaminhados para \ndiferentes portas de saída. Qual é o atraso máximo para um pacote para (a) a memória, (b) o barramento e (c) os \nelementos de comutação do tipo crossbar?\n\t\nP9.\t Considere o comutador a seguir. Suponha que todos os datagramas possuam o mesmo comprimento, que o \ncomutador opere de uma maneira segmentada e síncrona, e que em um intervalo de tempo (time slot) um \ndatagrama possa ser transferido de uma porta de entrada para uma porta de saída. A malha de comutação é \num crossbar no qual, no máximo, um datagrama possa ser transferido para uma determinada porta de saída \nem um intervalo de tempo, mas portas de saída diferentes podem receber datagramas de portas de entrada \ndiferentes em um único intervalo de tempo. Qual é o número mínimo de intervalos de tempo necessário para \ntransferir os pacotes mostrados das portas de entrada para suas portas de saída, admitindo qualquer ordem \nde escalonamento de fila que você quiser (ou seja, não é necessário o bloqueio HOL)? Qual é o maior número \nde intervalos necessários, admitindo uma ordem de escalonamento de pior caso e que uma fila de entrada não \nvazia nunca fica ociosa?\nX Y\nX\nY\nZ\nPorta de saída X\nPorta de saída Y\nPorta de saída Z\nElemento de \ncomutação\n\t\nP10.\t Considere uma rede de datagramas que usa endereços de hospedeiro de 32 bits. Suponha que um roteador \ntenha quatro enlaces, numerados de 0 a 3, e que os pacotes têm de ser repassados para as interfaces de enlaces \ndesta forma:\nFaixa do endereço de destino\nInterface de enlace\n11100000 00000000 00000000 00000000\naté\n0\n11100000 00111111 11111111 11111111\n11100000 01000000 00000000 00000000\naté\n1\n11100000 01000000 11111111 11111111\n11100000 01000001 00000000 00000000\naté\n2\n11100001 01111111 11111111 11111111\nsenão\n3\na.\t Elabore uma tabela de repasse que tenha cinco registros, use correspondência do prefixo mais longo e \nrepasse pacotes para as interfaces de enlace corretas.\nA CAMADA  de REDE  311 \nb.\t Descreva como sua tabela de repasse determina a interface de enlace apropriada para datagramas com os \nseguintes endereços:\n\t\n11001000 10010001 01010001 01010101\n\t\n11100001 01000000 11000011 00111100\n\t\n11100001 10000000 00010001 01110111\n\t\nP11.\t Considere uma rede de datagramas que utiliza endereços de hospedeiro de 8 bits. Suponha que um roteador \nutilize a correspondência do prefixo mais longo e tenha a seguinte tabela de repasse:\nPrefixo a comparar\nInterface\n00\n0\n010\n1\n011\n2\n10\n2\n11\n3\n\t\n\t Para cada uma das quatro interfaces, forneça a faixa associada de endereços de hospedeiro de destino e o \nnúmero de endereços na faixa.\n\t\nP12.\t Considere uma rede de datagramas que usa endereços de hospedeiros de 8 bits. Suponha que um roteador \nuse a correspondência do prefixo mais longo e tenha a seguinte tabela de repasse:\nPrefixo a comparar\nInterface\n1\n0\n10\n1\n111\n2\nsenão\n3\n\t\n\t Para cada uma das quatro interfaces, forneça a faixa associada de endereços de hospedeiro de destino e o \nnúmero de endereços na faixa.\n\t\nP13.\t Considere um roteador que interconecta três sub-redes: 1, 2 e 3. Suponha que todas as interfaces de cada uma \ndessas três sub-redes tenha de ter o prefixo 223.1.17/24. Suponha também que a sub-rede 1 tenha de suportar \naté 60 interfaces, a sub-rede 2 tenha de suportar até 90 interfaces e a sub-rede 3, 12 interfaces. Dê três endereços \nde rede (da forma a.b.c.d/x) que satisfaçam essas limitações.\n\t\nP14.\t Na Seção 4.2.2 é dado um exemplo de tabela de repasse (usando a correspondência de prefixo mais longo). \nReescreva a tabela usando a notação a.b.c.d/x em vez da notação de cadeia binária.\n\t\nP15.\t No Problema P10, solicitamos que você elaborasse uma tabela de repasse (usando a correspondência de \nprefixo mais longo). Reescreva a tabela usando a notação a.b.c.d/x em vez da notação de cadeia binária.\n\t\nP16.\t Considere uma sub-rede com prefixo 128.119.40.128/26. Dê um exemplo de um endereço IP (na forma \nxxx.xxx.xxx.xxx) que possa ser designado para essa rede. Suponha que um ISP possua o bloco de endereços \nna forma 128.119.40.64/26. Suponha que ele queira criar quatro sub-redes a partir desse bloco, e que cada bloco \ntenha o mesmo número de endereços IP. Quais são os prefixos (na forma a.b.c.d/x) para as quatro sub-redes?\n\t\nP17.\t Considere a topologia mostrada na Figura 4.17. Denomine as três sub-redes com hospedeiros (começando \nem sentido horário, a partir da posição das 12h) como A, B e C. Denomine as sub-redes sem hospedeiros \ncomo D, E e F.\n   Redes de computadores e a Internet\n312\na.\t Designe endereços de rede a cada uma das seis sub-redes, com as seguintes restrições: todos os endereços \ndeverão ser alocados a partir de 214.97.254/23; a sub-rede A deve ter endereços suficientes para suportar \n250 interfaces; a sub-rede B deve ter endereços suficientes para suportar 120 interfaces e a sub-rede C deve \nter endereços suficientes para suportar 120 interfaces. É claro que cada uma das sub-redes D, E e F deve \npoder suportar duas interfaces. Para cada sub-rede, a designação deve tomar a forma a.b.c.d/x ou a.b.c.d/x \n– e.f.g.h/y.\nb.\t Usando a resposta dada no item (a), elabore as tabelas de repasse (usando a correspon­\ndência de prefixo mais \nlongo) para cada um dos três roteadores.\n\t\nP18.\t Use o serviço whois no American Registry for Internet Numbers (http://www.arin.net/whois) para determinar \nos blocos de endereço IP para três universidades. Os serviços whois podem ser usados para determinar com \ncerteza o local geográfico de um endereço IP específico? Use www.maxmind.com para determinar os locais \ndos servidores Web em cada universidade.\n\t\nP19.\t Considere enviar um datagrama de 2.400 bytes por um enlace que tem uma MTU de 700 bytes. Suponha que \no datagrama original esteja marcado com o número de identificação 422. Quantos fragmentos são gerados? \nQuais são os valores em vários campos dos datagramas IP gerados em relação à fragmentação?\n\t\nP20.\t Suponha que entre o hospedeiro de origem A e o hospedeiro destinatário B os datagramas estejam limitados \na 1.500 bytes (incluindo cabeçalho). Admitindo um cabeçalho IP de 20 bytes, quantos datagramas seriam \nnecessários para enviar um arquivo MP3 de 5 milhões de bytes? Explique como você obteve a resposta.\n\t\nP21.\t Considere a configuração de rede da Figura 4.22. Suponha que o ISP designe ao roteador o endereço \n24.34.112.235 e que o endereço da rede residencial seja 192.168.1/24.\na.\t Designe endereços a todas as interfaces na rede residencial.\nb.\t Suponha que haja duas conexões TCP em curso em cada hospedeiro, todas para a porta 80 no hospedeiro \n128.119.40.86. Forneça os seis registros correspondentes na tabela de tradução NAT.\n\t\nP22.\t Suponha que você esteja interessado em detectar o número de hospedeiros por trás da NAT. Você observa \nque a camada IP traz um número de identificação, de modo sequencial, em cada pacote IP. O número de \nidentificação do primeiro pacote IP gerado por um hospedeiro é aleatório, e os números de identificação \nsubsequentes são determinados sequencialmente. Admita que todos os pacotes IP gerados por hospedeiros \npor trás da NAT sejam enviados para o mundo exterior.\na.\t Com base nessa observação e admitindo que você pode analisar todos os pacotes enviados para fora pela \nNAT, você pode descrever uma técnica simples que detecte o número de hospedeiros únicos por trás da \nNAT? Justifique sua resposta.\nb.\t Se os números de identificação não são determinados de maneira sequencial, e sim aleatória, sua técnica \nfuncionaria? Justifique sua resposta.\n\t\nP23.\t Neste problema estudaremos o impacto das NATs sobre aplicações P2P. Suponha que um parceiro com nome \nde usuário Arnold descubra, por meio de consulta, que um parceiro com nome de hospedeiro Bernard tem \num arquivo que ele, Arnold, quer descarregar. Suponha também que Bernard e Arnold estejam por trás de \numa NAT. Tente elaborar uma técnica que permita a Arnold estabelecer uma conexão TCP com Bernard sem \na configuração da NAT específica da aplicação. Se você tiver dificuldade na elaboração dessa técnica, discuta \no motivo.\n\t\nP24.\t Considerando a Figura 4.27, enumere os caminhos de y a u que não tenham laços.\n\t\nP25.\t Repita o Problema P24 considerando os caminhos de x a z, z a u e z a w.\n\t\nP26.\t \u0007\nConsidere a seguinte rede. Com os custos de enlace indicados, use o algoritmo do caminho mais curto de \nDijkstra para calcular o caminho mais curto de x até todos os nós da rede. Mostre como o algoritmo funciona \ncalculando uma tabela semelhante à Tabela 4.3.\nA CAMADA  de REDE  313 \nx\nv\nt\ny\nz\nu\nw\n6\n12\n8\n7\n8\n3\n6\n4\n3\n2\n4\n3\n\t\nP27.\t Considere a rede mostrada no Problema P26. Usando o algoritmo de Dijkstra e mostrando seu trabalho \nusando uma tabela semelhante à Tabela 4.3, faça o seguinte:\na.\t Calcule o caminho mais curto de t até todos os nós da rede.\nb.\t Calcule o caminho mais curto de u até todos os nós da rede.\nc.\t Calcule o caminho mais curto de v até todos os nós da rede.\nd.\t Calcule o caminho mais curto de w até todos os nós da rede.\ne.\t Calcule o caminho mais curto de y até todos os nós da rede.\nf.\t Calcule o caminho mais curto de z até todos os nós da rede.\n\t\nP28.\t Considere a rede mostrada a seguir e admita que cada nó inicialmente conheça os custos até cada um de seus \nvizinhos. Considere o algoritmo de vetor de distâncias e mostre os registros na tabela de distâncias para o nó z.\nu\nz\nv\ny\n2\n3\n6\n2\n3\n1\nx\n\t\nP29.\t Considere uma topologia geral (isto é, não a rede específica mostrada antes) e uma versão síncrona do \nalgoritmo de vetor de distâncias. Suponha que, a cada iteração, um nó troque seus vetores de distâncias \ncom seus vizinhos e receba os vetores de distâncias deles. Supondo que o algoritmo comece com cada nó \nconhecendo apenas os custos até seus vizinhos imediatos, qual é o número máximo de iterações exigidas até \nque o algoritmo distribuído convirja? Justifique sua resposta.\n\t\nP30.\t Considere o fragmento de rede mostrado a seguir. x tem apenas dois vizinhos ligados a ele: w e y. w tem um \ncaminho de custo mínimo até o destino u (não mostrado) de 5 e y tem um caminho de custo mínimo u de 6. \nOs caminhos completos de w e de y até u (e entre w e y) não são mostrados. Todos os valores dos custos de \nenlace na rede são números inteiros estritamente positivos.\n   Redes de computadores e a Internet\n314\nx\ny\nw\n2\n2\n5\na.\t Dê os vetores de distâncias de x para os destinos w, y e u.\nb.\t Dê uma mudança de custo de enlace para c(x, w) ou para c(x, y) tal que x informará a seus vizinhos um \nnovo caminho de custo mínimo até u como resultado da execução do algoritmo de vetor de distâncias.\nc.\t Dê uma mudança de custo de enlace para c(x, w) ou para c(x, y) tal que x não informará a seus vizinhos \num novo caminho de custo mínimo até u como resultado da execução do algoritmo de vetor de distâncias.\n\t\nP31.\t Considere a topologia de três nós mostrada na Figura 4.30. Em vez de ter os custos de enlace da Figura 4.30, \nos custos de enlace são: c(x, y) = 3, c(y, z) = 6, c(z, x) = 4. Calcule as tabelas de distâncias após a etapa de \ninicialização e após cada iteração de uma versão síncrona do algoritmo de vetor de distâncias (como fizemos em \nnossa discussão anterior da Figura 4.30).\n\t\nP32.\t Considere o problema da contagem até o infinito no roteamento de vetor de distâncias. Esse problema \nocorrerá se reduzirmos o custo de um enlace? Por quê? E se conectarmos dois nós que não possuem enlace?\n\t\nP33.\t Demonstre que, para o algoritmo de vetor de distâncias na Figura 4.30, cada valor no vetor de distâncias D(x) \né não crescente e, consequentemente, se estabilizará em um número finito de etapas.\n\t\nP34.\t Considere a Figura 4.31. Suponha que haja outro roteador, w, conectado aos roteadores y e z. Os custos \nde todos os enlaces são: c(x, y) = 4, c(x, z) = 50, c(y, w) = 1, c(z, w) = 1, c(y, z) = 3. Suponha que a reversão \nenvenenada seja utilizada no algoritmo de roteamento de vetor de distâncias.\na.\t Quando o roteamento de vetor de distâncias é estabilizado, os roteadores w, y e z informam uns aos outros \nsobre suas distâncias para x. Que valores de distância eles podem informar uns aos outros?\nb.\t Agora suponha que o custo do enlace entre x e y aumente para 60. Haverá um problema de contagem até \no infinito mesmo se a reversão envenenada for utilizada? Por quê? Se houver um problema de contagem \naté o infinito, então quantas iterações são necessárias para o roteamento de vetor de distâncias alcançar \num estágio estável novamente? Justifique sua resposta.\nc.\t Como você modifica c(y,z) de modo que não haja qualquer problema de contagem até o infinito se c(y,x) \nmudar de 4 para 60?\n\t\nP35.\t Descreva como laços nos caminhos podem ser detectados com BGP.\n\t\nP36.\t Um roteador BGP sempre escolherá uma rota sem laços com o menor comprimento de AS-PATH? Justifique \nsua resposta.\n\t\nP37.\t Considere a rede a seguir. Suponha que AS3 e AS2 estejam rodando o OSPF para seu protocolo de roteamento \nintra-AS. Suponha que AS1 e AS4 estejam rodando o RIP para seu protocolo de roteamento intra-AS. Suponha \nque o eBGP e o iBGP sejam usados para o protocolo de roteamento intra-AS. Inicialmente, suponha que não \nhaja enlace físico entre AS2 e AS4.\na.\t O roteador 3c sabe sobre o prefixo x por qual protocolo de roteamento: OSPF, RIP, eBGP ou iBGP?\nb.\t O roteador 3a sabe sobre o prefixo x por qual protocolo de roteamento?\nc.\t O roteador 1c sabe sobre o prefixo x por qual protocolo de roteamento?\nd.\t O roteador 1d sabe sobre o prefixo x por qual protocolo de roteamento?\nA CAMADA  de REDE  315 \nAS4\nAS3\nAS1\nAS2\nx\n4b\n4c\n4a\n3c\n3b\n3a\n1c\n1b\n1d\n1a\nI1\nI2\n2c\n2a\n2b\n\t\nP38.\t Referindo-se ao problema anterior, uma vez que o roteador 1d sabe sobre x, ele inserirá uma entrada (x, I) em \nsua tabela de repasse.\na.\t I será igual a I1 ou I2 para essa entrada? Justifique a resposta em uma frase.\nb.\t Agora suponha que haja um enlace físico entre AS2 e AS4, ilustrado pela linha pontilhada. Suponha que \no roteador 1d saiba que x é acessível por meio de AS2 e de AS3. I será definido para I1 ou I2? Justifique a \nresposta em uma frase.\nc.\t Agora suponha que haja outro AS, denominado AS5, que fica no caminho entre AS2 e AS4 (não ilustrado \nno diagrama). Suponha que o roteador 1d saiba que x é acessível por meio de AS2 AS5 AS4, bem como de \nAS3 AS4. I será definido para I1 ou I2 ? Em uma frase, explique o motivo.\n\t\nP39.\t Considere a rede a seguir. O ISP B provê serviço nacional de backbone ao ISP regional A. O ISP C provê \nserviço nacional de backbone ao ISP regional D. Cada ISP consiste em um AS. Usando BGP, B e C se \nemparelham em dois lugares. Considere o tráfego na direção de A a D. B preferiria passar esse tráfego \npara C na Costa Oeste (de modo que C teria de absorver o custo de transportar o tráfego através do país), \nenquanto C preferiria receber o tráfego via seu ponto de emparelhamento com B na Costa Leste (de modo \nque B transportaria o tráfego através do país). Qual mecanismo BGP poderia ser usado por C de modo que \nB entregasse o tráfego de A a D em seu ponto de emparelhamento na Costa Leste? Para responder a essa \npergunta, você precisará estudar muito bem a especificação do BGP.\nISP B\nISP C\nISP D\nISP A\n   Redes de computadores e a Internet\n316\n\t\nP40.\t Na Figura 4.42, considere a informação de caminho que chega às sub-redes stub W, X e Y. Com base na \ninformação disponível em W e X, quais são as respectivas visões da topologia da rede? Justifique sua resposta. \nA topologia vista de Y é mostrada a seguir.\nW\nY\nX\nA\nC\nTopologia\nvista pela\nrede stub Y\n\t\nP41.\t Considere a Figura 4.42. B nunca encaminharia tráfego destinado a Y via X com base no roteamento \nBGP. Mas existem muitas aplicações conhecidas para as quais os pacotes de dados vão primeiro para \nX e depois fluem para Y. Identifique tal aplicação e descreva como os pacotes de dados percorrem um \ncaminho não determinado pelo roteamento BGP.\n\t\nP42.\t Na Figura 4.42, suponha que haja outra rede stub V que seja cliente do ISP A. Suponha que B e C tenham \numa relação de emparelhamento, e que A seja cliente de B e de C. Suponha, ainda, que A gostaria de ter o \ntráfego destinado para W vindo apenas de B, e o tráfego destinado para V vindo de B ou C. Como A deveria \nanunciar suas rotas para B e C? Quais rotas AS são recebidas por C?\n\t\nP.43.\t Suponha que os ASs X e Z não estejam conectados diretamente, mas estejam conectados pelo AS Y. Suponha \nainda que X tenha um acordo de emparelhamento com Y, e que Y tenha um acordo de emparelhamento \ncom Z. Por fim, suponha que Z queira transitar todo o tráfego de Y, mas não queira transitar o tráfego de \nX. O BGP permite que Z execute essa política?\n\t\nP44.\t Considere a rede de sete nós (com nós rotulados de t a z) do Problema 26. Mostre a árvore de custo mínimo \ncom raiz em z que inclua (como hospedeiros finais) os nós u, v, w e y. Justifique informalmente por que sua \nárvore é uma árvore de custo mínimo.\n\t\nP45.\t Considere as duas abordagens básicas identificadas para fazer a difusão: emulação de transmissão individual \ne difusão de camada de rede (isto é, assistido por roteador) e suponha que seja usada difusão de spanning \ntree para fazer difusão de camada de rede. Considere um único remetente e 32 destinatários. Suponha que \no remetente esteja conectado aos destinatários por uma árvore binária de roteadores. Qual é o custo para \nenviar um pacote por difusão nos casos da emulação de transmissão individual e de difusão de camada de \nrede para essa topologia? Aqui, cada vez que um pacote (ou cópia de um pacote) é enviado por um único \nenlace, ele incorre em uma unidade de custo. Qual topologia para interconectar o remetente, os destinatários \ne os roteadores fará que os custos da emulação individual e de difusão verdadeira da camada de rede fiquem \no mais longe possível um do outro? Você pode escolher quantos roteadores quiser.\n\t\nP46.\t Considere a operação do algoritmo de repasse de caminho inverso (RPF) na Figura 4.44. Usando a mesma \ntopologia, descubra um conjunto de caminhos de todos os nós até o nó de origem A (e indique esses caminhos \nem um grafo usando linhas grossas como as da Figura 4.44) de modo que, se esses caminhos forem os de \nmenor custo, então o nó B receberia uma cópia das mensagens de difusão de A dos nós A, C e D sob RPF.\n\t\nP47.\t Considere a topologia ilustrada na Figura 4.44. Suponha que todos os enlaces tenham custo unitário e que \no nó E é a origem da difusão. Usando setas como as mostradas na Figura 4.44, indique enlaces pelos quais \npacotes serão repassados usando RPF e enlaces pelos quais pacotes não serão repassados, dado que o nó E é \na origem.\n\t\nP48.\t Repita o Problema P47 utilizando o gráfico do Problema P26. Suponha que z seja a origem da difusão e que \nos custos do enlace são aqueles mostrados no Problema P26.\n\t\nP49.\t Considere a topologia mostrada na Figura 4.46 e suponha que cada enlace tenha custo unitário. Suponha \nque o nó C seja escolhido como o centro de um algoritmo de roteamento para um grupo baseado em centro. \nSupondo que cada roteador conectado use seu caminho de menor custo até o nó C para enviar mensagens de \nA CAMADA  de REDE  317 \nadesão a C, desenhe a árvore de roteamento baseada no centro resultante. É uma árvore de custo mínimo? \nJustifique sua resposta.\n\t\nP50.\t Repita o Problema P49, usando o gráfico do Problema P26. Admita que o nó central seja v.\n\t\nP51.\t Na Seção 4.5.1 estudamos o algoritmo de roteamento de estado de enlace de Dijkstra para calcular os \ncaminhos individuais que são os de menor custo da origem até todos os destinos. Poderíamos imaginar que a \nunião desses caminhos forme uma árvore individual de caminho de menor custo (ou uma árvore individual \nde caminho mais curto, se todos os custos de enlaces fossem idênticos). Construindo um contraexemplo (um \nexemplo que nega essa afirmação), mostre que a árvore de caminho de menor custo nem sempre é a mesma \nque uma spanning tree mínima.\n\t\nP52.\t Considere uma rede na qual todos os nós estão conectados a três outros nós. Em uma única fase de tempo, \num nó pode receber de seus vizinhos todos os pacotes transmitidos por difusão, duplicar os pacotes e enviá-\nlos a todos os seus vizinhos (exceto ao nó que enviou um dado pacote). Na próxima fase, nós vizinhos podem \nreceber, duplicar e repassar esses pacotes e assim por diante. Suponha que seja utilizada a inundação não \ncontrolada para prover difusão a essa rede. Na fase de tempo t, quantas cópias do pacote broadcast serão \ntransmitidas, admitindo que, durante a fase 1, um único pacote broadcast é transmitido pelo nó de origem a \nseus três vizinhos?\n\t\nP53.\t Vimos na Seção 4.7 que não há um protocolo de camada de rede que possa ser usado para identificar os \nhospedeiros que participam de um grupo (multicast). Isto posto, como aplicações de transmissão para um \ngrupo podem aprender as identidades dos hospedeiros que estão participando de um grupo?\n\t\nP54.\t Projete (dê uma descrição em pseudocódigo) um protocolo de nível de aplicação que mantenha os endereços \nde hospedeiros para todos os hospedeiros participantes de um grupo (multicast). Identifique o serviço de \nrede (individual ou para um grupo) que é usado por seu protocolo e indique se seu protocolo está enviando \nmensagens dentro ou fora da banda (com relação ao fluxo de dados de aplicação entre os participantes do \ngrupo) e por quê.\n\t\nP55.\t Qual é o tamanho do espaço de endereços de grupo? Suponha agora que dois grupos escolham aleatoriamente \num endereço de grupo. Qual é a probabilidade de que escolham o mesmo endereço? Suponha agora que mil \ngrupos estejam em operação ao mesmo tempo e escolham seus endereços de grupo aleatoriamente. Qual é a \nprobabilidade de que uns interfiram nos outros?\nTarefas de programação de sockets\nAo final do Capítulo 2, existem quatro tarefas de programação de sockets. A seguir, você verá uma quinta \ntarefa que emprega ICMP, um protocolo discutido neste capítulo.\nTarefa 5: ICMP ping\nPing é uma aplicação popular para redes, usada para testar, de um local remoto, se determinado hospedeiro \nestá ativo e pode ser alcançado. Em geral é usada para medir a latência entre o hospedeiro cliente e o hospedeiro \nde destino. Ele funciona enviando pacotes ICMP de “requisição de eco” (isto é, pacotes ping) ao hospedeiro de \ndestino e escutando as “respostas de eco” ICMP (isto é, pacotes pong). Ping mede o RTT, registra perda de pacote \ne calcula um resumo estatístico de diversas trocas ping-pong (o mínimo, média, máximo e desvio-padrão dos \ntempos de viagem de ida e volta).\nNeste laboratório, você escreverá sua própria aplicação Ping em Python. Sua aplicação usará ICMP. Mas, \npara simplificar seu programa, você não seguirá exatamente a especificação oficial no RFC 1739. Observe que só \nprecisará escrever o lado cliente do programa, pois a funcionalidade necessária no lado servidor já está embutida \nem todos os sistemas operacionais. Você poderá achar todos os detalhes desta tarefa, bem como trechos impor-\ntantes do código em Python, no site de apoio do livro.\n   Redes de computadores e a Internet\n318\nTarefas de programação\nNesta tarefa de programação, você escreverá um conjunto “distribuído” de procedimentos que executam \num roteamento de vetor de distâncias assíncrono distribuído para a rede mostrada a seguir.\nVocê deve escrever as seguintes rotinas que “rodarão” assincronamente dentro do ambiente emulado forne-\ncido para essa tarefa. Para o nó 0, escreverá estas rotinas:\n3\n2\n0\n1\n7\n3\n1\n2\n1\n• rtinit0(). Essa rotina será chamada uma vez no início da emulação. rtinit0() não tem argumentos. Você \ndeve inicializar sua tabela de distâncias no nó 0 para refletir os custos diretos de 1, 3 e 7 até os nós 1, 2 e 3, \nrespectivamente. Na figura anterior, todos os enlaces são bidirecionais e os custos em ambas as direções \nsão idênticos. Após inicializar a tabela de distâncias e quaisquer outras estruturas de dados necessárias às \nrotinas de seu nó 0, este deve então enviar a seus vizinhos diretamente ligados (nesse caso, 1, 2 e 3) o custo \nde seus caminhos de custo mínimo para todos os outros nós da rede. Essa informação do custo mínimo é \nenviada aos nós vizinhos em um pacote de atualização de roteamento chamando a rotina tolayer2(), con-\nforme descrito na tarefa completa. O formato do pacote de atualização de roteamento também está descrito \nna tarefa completa.\n• rtupdate0(struct rtpkt *rcvdpkt). Essa rotina será chamada quando o nó 0 receber um pacote de roteamento \nque foi enviado a ele por um de seus vizinhos diretamente conectados. O parâmetro *rcvdpkt é um indica-\ndor para o pacote que foi recebido. rtupdate0() é o “coração” do algoritmo de vetor de distâncias. Os valores \nque ele recebe em um pacote de atualização de roteamento de algum outro nó i contêm os custos correntes \ndo caminho mais curto de i para todos os outros nós da rede. rtupdate0() usa esses valores recebidos para \natualizar sua própria tabela de distâncias (conforme especificado pelo algoritmo de vetor de distâncias). Se \nseu próprio custo mínimo até outro nó mudar como resultado da atualização, o nó 0 informará essa mu-\ndança no custo mínimo a seus vizinhos diretamente conectados enviando-lhes um pacote de roteamento. \nLembre-se de que no algoritmo de vetor de distâncias apenas os nós conectados diretamente trocarão pa-\ncotes de roteamento. Assim, os nós 1 e 2 vão se comunicar, mas os nós 1 e 3, não.\nRotinas semelhantes são definidas para os nós 1, 2 e 3. Assim, você escreverá oito procedimentos ao todo: \nrtinit0(), rtinit1(), rtinit2(), rtinit3(), rtupdate0(), rtupdate1(), rtupdate2() e rtupdate3(). Juntas, essas rotinas exe-\ncutarão um cálculo assíncrono, distribuído, das tabelas de distâncias para a topologia e os custos mostrados na \nfigura apresentada anteriormente.\nNo site de apoio do livro você poderá encontrar todos os detalhes da tarefa de programação, bem como o \ncódigo em C de que precisará para criar o ambiente simulado de hardware/software. Também está disponível \numa versão da tarefa em Java.\nWireshark Lab\nNo site deste livro você encontrará duas tarefas de laboratório Wireshark, em inglês. A primeira examina a ope-\nração do protocolo IP e, em particular, o formato do datagrama IP\n. A segunda explora a utilização do protocolo ICMP \nnos comandos ping e Traceroute.\nA CAMADA  de REDE  319 \nO que o fez se decidir pela especialização em \nredes?\nEu trabalhava como programador na UCLA no fi-\nnal da década de 1960 com o patrocínio da Advanced \nResearch Projects Agency do Departamento de Defesa \ndos Estados Unidos (na época conhecida como ARPA \ne hoje, como DARPA). Meu trabalho era desenvolvi-\ndo no laboratório do professor Leonard Kleinrock no \nNetwork Measurement Center da recém-criada ARPA-\nnet. O primeiro nó da ARPAnet foi instalado na UCLA \nem 1o de setembro de 1969. Eu era responsável pela \nprogramação de um computador utilizado para coletar \ninformações de desempenho da ARPAnet e passá-las \npara comparação com modelos matemáticos e previ-\nsões de desempenho da rede.\nEu e vários outros alunos de pós-graduação éramos \nresponsáveis pelo trabalho com o que era conhecido \ncomo protocolos de nível de hospedeiro da ARPAnet — \n \nos procedimentos e formatos que permitiriam a inte-\nração dos muitos tipos diferentes de computadores na \nrede. Era uma exploração fascinante de um novo mundo \n(para mim) de computação e comunicação distribuídas.\nQuando começou a projetar o IP\n, você imaginava \nque esse protocolo tornar-se-ia tão predominante \nquanto é hoje?\nQuando Bob Kahn e eu começamos a trabalhar nis-\nso, em 1973, acho que estávamos muito mais preocu-\npados com a questão central: como fazer redes de pa-\ncotes heterogêneas interagirem umas com as outras, \nadmitindo que não poderíamos modificá-las. Espe-\nrávamos descobrir um modo que permitisse que um \nconjunto arbitrário de redes de comutação de pacotes \nfosse interligado de maneira transparente, de modo \nque os computadores componentes das redes pudes-\nsem se comunicar fim a fim sem precisar de nenhuma \ntradução entre eles. Acho que sabíamos que estáva-\nmos lidando com uma tecnologia poderosa e expansí-\nvel, mas duvido que tivéssemos uma ideia muito clara \ndo que seria o mundo com centenas de milhões de \ncomputadores todos interligados com a Internet.\nEm sua opinião, qual é o futuro das redes e da In-\nternet? Quais são os grandes obstáculos/desafios \nque estão no caminho do seu desenvolvimento?\nAcredito que a Internet, em particular, e as redes, em \ngeral, continuarão a proliferar. Hoje já existem evidên-\ncias convincentes de que haverá bilhões de dispositivos \nhabilitados para a Internet, entre eles equipamentos \ncomo telefones celulares, refrigeradores, PDAs, servi-\ndores ­\nresidenciais, ­\naparelhos de televisão, bem como a \ncostumeira coleção de notebooks, servidores e assim \npor ­\ndiante. Entre os grandes desafios estão o suporte \npara a mobilidade, a duração das baterias, a capacidade \ndos enlaces de acesso à rede e a escalabilidade ilimitada \ndo núcleo óptico da rede. A tarefa com a qual estou \nVinton G. Cerf\nVinton G. Serf é vice-presidente e evangelista-chefe da Internet para o Google. \nEle trabalhou por mais 16 anos na MCI, ocupando diversos cargos, sendo o último \ncomo vice-presidente sênior de Estratégia de Tecnologia. É muito conhecido pela \ncoautoria dos protocolos TCP/IP e da arquitetura da Internet. Durante sua gestão de \n1976 a 1982 na Advanced Research Projects Agency do Departamento de Defesa \ndos Estados Unidos (DARPA), desempenhou um papel fundamental na liderança do \ndesenvolvimento da Internet e de pacotes de dados e técnicas de segurança rela-\ncionadas com a Internet. Em 2005, ele recebeu a Medalha Presidencial de Liberdade \ndos EUA, e a Medalha Nacional de Tecnologia dos EUA em 1997. Ele é bacharel em \nMatemática pela Stanford University e doutor em ciência da computação pela UCLA.\nENTREVISTA\n   Redes de computadores e a Internet\n320\nprofundamente envolvido no Jet Propulsion Labora-\ntory é o projeto de uma extensão interplanetária da In-\nternet. Precisaremos descobrir um atalho para passar \ndo IPv4 [endereços de 32 bits] para o IPv6 [128 bits]. \nA lista é comprida!\nQuais pessoas o inspiraram profissionalmente?\nMeu colega Bob Kahn; o orientador de minha tese, \nGerald Estrin; meu melhor amigo, Steve ­\nCrocker \n(nós nos conhecemos na escola secundária e ele me \napresentou aos computadores em 1960!); e os milha-\nres de engenheiros que continuam a evoluir a Inter-\nnet ainda hoje.\nVocê pode dar algum conselho aos estudantes \nque estão ingressando no campo das redes/\nInternet?\nNão limitem seu pensamento aos sistemas existen-\ntes — imaginem o que poderia ser possível e então co-\nmecem a trabalhar para descobrir um meio de sair do \nestado atual das coisas e chegar lá. Ousem sonhar; eu \ne alguns colegas do Jet Propulsion Laboratory estamos \ntrabalhando no projeto de uma extensão interplanetária \nda Internet terrestre. A implementação dessa rede pode \nlevar décadas, uma missão por vez, mas, usando uma \nparáfrase: “O homem deve tentar alcançar o que está \nfora do seu alcance; senão, para que existiria o céu?”\n.\nNo capítulo anterior, aprendemos que a camada de rede fornece um serviço de comunicação entre dois hos-\npedeiros quaisquer da rede. Entre eles, os datagramas trafegam por uma série de enlaces de comunicação, alguns \ncom fio e alguns sem, começando no hospedeiro de origem, passando por uma série de comutadores de pacotes \n(comutadores e roteadores) e terminando no hospedeiro de destino. À medida que continuamos a descer a pilha \nde protocolos, da camada de rede até a camada de enlace, é natural que imaginemos como pacotes são enviados \npelos enlaces individuais no caminho de comunicação fim a fim. Como os datagramas da camada de rede são \nencapsulados nos quadros da camada de enlace para transmissão por um único enlace? Diferentes protocolos \nda camada de enlace são usados em diversos enlaces no caminho de comunicação? Como os conflitos de trans-\nmissão nos enlaces de difusão podem ser resolvidos? Existe endereçamento na camada de enlace e, se houver, \ncomo o endereçamento da camada de enlace opera com o endereçamento da camada de rede, que aprendemos no \nCapítulo 4? E qual é exatamente a diferença entre um comutador e um roteador? Neste capítulo responderemos \na essas e a outras perguntas importantes.\nAo discutir a camada de enlace, descobriremos que há dois tipos de canais completamente diferentes dessa \ncamada. O primeiro são os canais de difusão (broadcast), que conectam múltiplos hospedeiros em LANs sem \nfio, redes por satélite e redes de acesso híbridas de cabo coaxial e de fibra (HFC). Como muitos hospedeiros são \nconectados ao mesmo canal de comunicação por difusão, é necessário um protocolo, denominado de acesso \nao meio, para coordenar a transmissão de quadros. Em alguns casos, um controlador central pode ser usado \npara coordenar as transmissões; em outros, os próprios hospedeiros as coordenam. O segundo tipo é o enlace \nde comunicação ponto a ponto, tal como o existente entre dois roteadores conectados por um enlace de longa \ndistância, ou entre um computador no escritório de um usuário e o comutador Ethernet próximo ao qual ele está \nconectado. Coordenar o acesso a um enlace ponto a ponto é mais simples; o material de referência no site deste li-\nvro possui uma discussão detalhada do Point-to-Point Protocol (PPP), que é usado em configurações que variam \ndesde o serviço discado por uma linha telefônica até o transporte de quadros ponto a ponto de alta velocidade \npor enlaces de fibra ótica.\nNeste capítulo estudaremos diversas tecnologias importantes da camada de enlace. Examinaremos em de-\ntalhes a detecção e correção de erros, um assunto que abordamos por alto no Capítulo 3. Consideraremos as \nredes de acesso múltiplo e as LANs comutadas, incluindo Ethernet, de longe a tecnologia predominante para \nLANs com fio. Estudaremos também as LANs virtuais e as redes de datacenter. Embora Wi-Fi e, mais frequente, \nas LANs sem fio sejam tópicos da camada de enlace, deixaremos nosso estudo desses assuntos importantes para \no Capítulo 6.\ncamada de enlace:\nacesso e redes locais\nenlaces, redes de\nc\n \na\n \np\n \ní\n \nt\n \nu\n \nl\n \no\n1\n3\n6\n8 9\n7\n2\n45\n   Redes de computadores e a Internet\n322\n5.1  Introdução à camada de enlace\nVamos começar com um pouco de terminologia útil. Achamos que, neste capítulo, é conveniente nos refe-\nrirmos a qualquer dispositivo que rode um protocolo da camada de enlace (isto é, camada 2) como um nó. Os \nnós incluem hospedeiros, roteadores, comutadores e pontos de acesso Wi-Fi (discutidos no Capítulo 6). Tam-\nbém nos referiremos aos canais de comunicação que conectam nós adjacentes nos caminhos de comunicação \ncomo enlaces. Para levar um datagrama de um hospedeiro de origem até um de destino, o datagrama tem de ser \ntransportado sobre cada um dos enlaces individuais existentes no caminho fim a fim. Como um exemplo, na rede \ncorporativa mostrada na parte inferior da Figura 5.1, considere o envio de um datagrama de um dos hospedeiros \nsem fio para um dos servidores. Esse datagrama, na verdade, passará por seis enlaces: um Wi-Fi entre o hospe-\ndeiro remetente e o ponto de acesso Wi-Fi, um Ethernet entre o ponto de acesso e um comutador da camada de \nenlace; um entre o comutador da camada de enlace e o roteador, um entre os dois roteadores; um Ethernet entre \no roteador e um comutador da camada de enlace e, por fim, um enlace Ethernet entre o comutador e o servidor. \nFigura 5.1  Seis saltos da camada de enlace entre hospedeiro sem fio e servidor\nRede móvel\nKR 05.01.eps\nAW/Kurose and Ross\nISP nacional\nou global\nISP local\nou regional\nRede corporativa\nRede doméstica\ncamada de enlace: enlaces, redes de acesso e redes locais  323 \nConsiderando dado enlace, um nó transmissor encapsula o datagrama em um quadro da camada de enlace e o \ntransmite para dentro do enlace.\nPara uma boa compreensão da camada de enlace e de como ela se relaciona com a camada de rede, vamos \nconsiderar uma analogia com um sistema de transporte. Imagine um agente de viagens que planeja uma viagem \npara um turista de Princeton, em Nova Jersey, até Lausanne, na Suíça. O agente decide que é mais conveniente \npara o turista pegar uma limusine de Princeton até o aeroporto JFK, em seguida um avião até o aeroporto de \nGenebra e, por fim, um trem até a estação ferroviária de Lausanne. Assim que o agente fizer as três reservas, é \nresponsabilidade da empresa de limusines conduzir o turista de Princeton ao aeroporto JFK; é responsabilidade \nda companhia aérea transportar o turista do aeroporto JFK a Genebra; e é responsabilidade do trem suíço levar \no turista de Genebra a Lausanne. Cada um dos três segmentos da viagem é “direto” entre duas localidades “ad-\njacentes”\n. Note que os três segmentos de transporte são administrados por empresas diferentes e usam meios de \ntransporte completamente diferentes (limusine, avião e trem). Embora os meios de transporte sejam diferentes, \ncada um fornece o serviço básico de levar passageiros de uma localidade a outra adjacente. Nessa comparação \ncom o transporte, o turista seria um datagrama; cada segmento de transporte, um enlace de comunicação; o meio \nde transporte, um protocolo da camada de enlace; e o agente de viagens, um protocolo de roteamento.\n5.1.1  Os serviços fornecidos pela camada de enlace\nEmbora o serviço básico de qualquer camada de enlace seja mover um datagrama de um nó até um nó ad-\njacente por um único enlace de comunicação, os detalhes do serviço podem variar de um protocolo da camada \nde enlace para outro. Entre os serviços que podem ser oferecidos por um protocolo da camada de enlace, estão:\n• Enquadramento de dados. Quase todos os protocolos da camada de enlace encapsulam cada datagrama \nda camada de rede dentro de um quadro da camada de enlace antes de transmiti-lo pelo enlace. Um \nquadro consiste em um campo de dados no qual o datagrama da camada de rede é inserido, e em uma \nsérie de campos de cabeçalho. A estrutura do quadro é especificada pelo protocolo da camada de enlace. \nVeremos diversos formatos de quadros diferentes quando examinarmos os protocolos da camada de \nenlace específicos na segunda metade deste capítulo.\n• Acesso ao enlace. Um protocolo de controle de acesso ao meio (medium access control — MAC) especifica \nas regras segundo as quais um quadro é transmitido pelo enlace. Para enlaces ponto a ponto que têm um \núnico remetente em uma extremidade do enlace e um único receptor na outra, o protocolo MAC é simples \n(ou inexistente) — o remetente pode enviar um quadro sempre que o enlace estiver ocioso. O caso mais \ninteressante é quando vários nós compartilham um único enlace de difusão — o denominado problema de \nacesso múltiplo. Aqui, o protocolo MAC serve para coordenar as transmissões de quadros dos muitos nós.\n• Entrega confiável. Quando um protocolo da camada de enlace fornece serviço confiável de entrega, ele \ngarante que vai transportar sem erro cada datagrama da camada de rede pelo enlace. Lembre-se de que \ncertos protocolos da camada de transporte (como o TCP) também fornecem um serviço confiável de \nentrega. Semelhante ao que acontece com um serviço confiável de entrega da camada de transporte, con-\nsegue-se um serviço confiável de entrega da camada de enlace com reconhecimentos e retransmissões \n(veja a Seção 3.4). Um serviço confiável de entrega da camada de enlace é muito usado por enlaces que \ncostumam ter altas taxas de erros, como é o caso de um enlace sem fio, com a finalidade de corrigir um \nerro localmente, no enlace no qual o erro ocorre, em vez de forçar uma retransmissão fim a fim dos dados \npor um protocolo da camada de transporte ou de aplicação. Contudo, a entrega confiável da camada de \nenlace pode ser considerada uma sobrecarga desnecessária para enlaces de baixa taxa de erros, incluindo \nenlaces de fibra, enlaces coaxiais e muitos enlaces de pares de fios trançados de cobre. Por essa razão, \nmuitos protocolos da camada de enlace com fio não fornecem um serviço de entrega confiável.\n• Detecção e correção de erros. O hardware da camada de enlace de um nó receptor pode decidir incorre-\ntamente que um bit de um quadro é zero quando foi transmitido como 1 e vice-versa. Esses erros de bits \n   Redes de computadores e a Internet\n324\nsão introduzidos por atenuação de sinal e ruído eletromagnético. Como não há necessidade de repassar \num datagrama que tem um erro, muitos protocolos da camada de enlace oferecem um mecanismo para \ndetectar a presença de tais erros. Isso é feito obrigando o nó transmissor a enviar bits de detecção de \nerros no quadro e o nó receptor a realizar uma verificação de erros. Lembre-se de que nos Capítulos 3 e \n4 dissemos que as camadas de transporte e de rede da Internet também fornecem um serviço limitado \nde detecção de erros — a soma de verificação da Internet. A detecção de erros na camada de enlace ge-\nralmente é mais sofisticada e é executada em hardware. A correção de erros é semelhante à detecção de \nerros, exceto que um receptor não só detecta quando ocorreram os erros no quadro, mas também deter-\nmina exatamente em que lugar do quadro ocorreram (e, então, os corrige).\n5.1.2  Onde a camada de enlace é implementada?\nAntes de mergulharmos em nosso detalhado estudo sobre a camada de enlace, vamos considerar a questão \nde onde esta é implementada. Daqui em diante focaremos em um sistema final, visto que já aprendemos no Ca-\npítulo 4 como a camada de enlace é implementada em uma placa de linha de um roteador. A camada de enlace \nde um computador deve ser implementada no hardware ou no software? Deve ser implementada em uma placa \nou chip separado e como ocorre a interface com o resto do hardware de um hospedeiro e com os componentes \nde sistemas operacionais?\nA Figura 5.2 mostra a arquitetura típica de um hospedeiro. Na maior parte, a camada de enlace é implemen-\ntada em um adaptador de rede, às vezes também conhecido como placa de interface de rede (NIC). No núcleo \ndo adaptador de rede está o controlador da camada de enlace, em geral um único chip de sistema especial, que \nexecuta vários serviços da camada de enlace (enquadramento, acesso ao enlace, detecção de erros etc.). Dessa \nforma, muito da funcionalidade do controlador da camada de enlace é realizado em hardware. Por exemplo, o \ncontrolador da Intel 8254x [Intel, 2012] implementa os protocolos Ethernet, os quais estudaremos na Seção 5.5; \no controlador Atheros AR5006 [Atheros, 2012] implementa os protocolos Wi-Fi 802.11 que estudaremos no Ca-\npítulo 6. Até o final dos anos 1990, a maioria dos adaptadores de rede eram placas fisicamente separadas (como a \nplaca PCMCIA ou uma placa plug-in que se encaixa em um compartimento para cartão PCI de um computador), \nporém agora cada vez mais adaptadores de rede estão sendo integrados à placa-mãe do hospedeiro — uma con-\nfiguração chamada LAN-na-placa-mãe.\nNo lado transmissor, o controlador separa um datagrama que foi criado e o armazena na memória do \nhospedeiro por camadas mais altas da pilha de protocolos, encapsula o datagrama em um quadro da camada de \nenlace (preenchendo os vários campos do quadro), e então transmite o quadro para um enlace de comunicação, \nseguindo o protocolo de acesso ao enlace. No lado receptor, um controlador recebe todo o quadro e extrai o \ndatagrama da camada de rede. Se a camada de enlace efetuar uma verificação de erros, é o controlador transmis-\nsor que estabelece os bits de detecção de erros no cabeçalho de quadro e é o controlador receptor que executa a \nverificação de erros.\nA Figura 5.2 mostra um adaptador de rede conectado ao barramento do computador (por exemplo, barra-\nmento PCI ou PCI-X), que se parece muito com qualquer outro dispositivo de entrada/saída dos outros compo-\nnentes do hospedeiro. A Figura 5.2 mostra também que, enquanto a maior parte da camada de enlace é executada \nem hardware, parte dela é implementada em software que é executada na CPU do hospedeiro. Os componentes \ndo software da camada de enlace implementam uma funcionalidade da camada de enlace de um nível mais alto, \nmontando informações de endereçamento da camada de enlace e ativando o hardware do controlador. No lado \nreceptor, o software da camada de enlace responde a interrupções do controlador (por exemplo, pelo recebimen-\nto de um ou mais quadros), lida com condições de erro e passa o datagrama para a camada de rede, mais acima. \nAssim, a camada de enlace é uma combinação de hardware e software — o lugar na pilha de protocolos, onde o \nsoftware encontra o hardware. Intel [2012] fornece um panorama legível (assim como descrições detalhadas) do \ncontrolador 8254x do ponto de vista de uma programação de software.\ncamada de enlace: enlaces, redes de acesso e redes locais  325 \n5.2  Técnicas de detecção e correção de erros\nNa seção anterior, observamos que detecção e correção de erros no nível de bits — detecção e correção da \nalteração de bits em um quadro da camada de enlace enviado de um nó para outro nó vizinho fisicamente ligado \na ele — são dois serviços com fornecidos frequência pela camada de enlace. Vimos no Capítulo 3 que serviços \nde detecção e correção de erros também são frequentemente oferecidos na camada de transporte. Nesta seção, \nexaminaremos algumas das técnicas mais simples que podem ser usadas para detectar e, em alguns casos, cor-\nrigir esses erros de bits. Como a teoria e a implementação dessas técnicas é um assunto tratado detalhadamente \nem muitos livros (como Schwartz [1980] ou Bertsekas [1991]), nossa abordagem terá de ser breve. Nossa meta é \ndesenvolver uma visão intuitiva das capacidades que as técnicas de detecção e correção de erros fornecem e ver \ncomo algumas técnicas simples funcionam e são usadas na prática na camada de enlace.\nA Figura 5.3 ilustra o cenário de nosso estudo. No nó remetente, para que os dados, D, fiquem protegidos con-\ntra erros de bits, eles são aumentados com bits de detecção e de correção (error detection-and-correction — EDC). \nEm geral, os dados que devem ser protegidos incluem não somente o datagrama passado para baixo a partir da \ncamada de rede para transmissão pelo enlace, mas também informações de endereçamento da camada de enlace, \nnúmeros de sequência e outros campos do cabeçalho do quadro de enlace. Tanto D como EDC são enviados ao nó \nreceptor em um quadro no nível de enlace. No nó receptor, são recebidas sequências de bits, D′ e EDC′. Note que D′ \ne EDC′ podem ser diferentes dos D e EDC originais, como resultado de inversões nos bits em trânsito.\nO desafio do receptor é determinar se D′ é ou não igual ao D original, uma vez que recebeu apenas D′ e \nEDC′. A exata sintaxe da decisão do receptor na Figura 5.4 (perguntamos se um erro foi detectado, e não se um \nerro foi cometido!) é importante. Técnicas de detecção e correção permitem que o receptor descubra a ocorrência \nde erros de bits às vezes, mas não sempre. Mesmo com a utilização de bits de detecção de erros, ainda pode haver \nerros de bits não detectados, isto é, o receptor pode não perceber que a informação recebida contém erros de \nbits. Por conseguinte, o receptor poderá entregar um datagrama corrompido à camada de rede ou não perceber \nque o conteúdo de um campo no cabeçalho do quadro foi corrompido. Assim, é preciso escolher um esquema \nde detecção de erros para o qual a probabilidade dessas ocorrências seja pequena. Em geral, técnicas mais sofisti-\ncadas de detecção e correção de erros (isto é, as que têm uma probabilidade menor de permitir erros de bits não \ndetectados) ficam sujeitas a uma sobrecarga maior — é preciso mais processamento para calcular e transmitir um \nnúmero maior de bits de detecção e correção de erros.\nFigura 5.2  \u0007\nAdaptador de rede: seu relacionamento com o resto dos componentes do \nhospedeiro e a funcionalidade da pilha de protocolos\nHospedeiro\nMemória\nBarramento do\nhospedeiro (por\nexemplo, PCI)\nCPU\nControlador\nTransmissão\nfísica\nAdaptador de rede\nEnlace\nFísica\nTransporte\nRede\nEnlace\nAplicação\n   Redes de computadores e a Internet\n326\nVamos examinar agora três técnicas de detecção de erros nos dados transmitidos — verificações de parida-\nde (para ilustrar as ideias básicas da detecção e correção de erros), métodos de soma de verificação (que são mais \nempregados na camada de transporte) e verificações de redundância cíclica (CRCs) (que são em geral emprega-\ndas na camada de enlace, nos adaptadores).\n5.2.1  Verificações de paridade\nTalvez a maneira mais simples de detectar erros seja utilizar um único bit de paridade. Suponha que a \ninformação a ser enviada, D na Figura 5.4, tenha d bits. Em um esquema de paridade par, o remetente apenas \ninclui um bit adicional e escolhe o valor desse bit de modo que o número total de “1” nos d + 1 bits (a informação \noriginal mais um bit de paridade) seja par. Em esquemas de paridade ímpar, o valor do bit de paridade é escolhido \npara que haja um número ímpar de “1”\n. A Figura 5.4 ilustra um esquema de paridade par com o bit de paridade \narmazenado em um campo separado.\nCom um único bit de paridade, a operação do receptor também é simples. O receptor precisa apenas contar \nquantos “1” há nos d + 1 bits recebidos. Se, utilizando um esquema de paridade par, for encontrado um número \nímpar de bits de valor 1, o receptor saberá que ocorreu pelo menos um erro de bit. Mais precisamente, ele saberá \nque ocorreu algum número ímpar de erros de bit.\nMas o que acontecerá se ocorrer um número par de erros de bit? É bom que você se convença de que isso \nresultaria em um erro não detectado. Se a probabilidade de erro de bits for pequena e se for razoável admitir que \nos erros ocorrem independentemente entre um bit e o bit seguinte, a probabilidade de haver vários erros de bits \nem um pacote seria bastante pequena. Nesse caso, um único bit de paridade poderia ser suficiente. Contudo, \nmedições demonstraram que, em vez de acontecerem independentemente, os erros em geral se aglomeram em \nFigura 5.3  Cenário de detecção e correção de erros\nEDC'\nD'\nErro detectado\nDatagrama\nEDC\nD\nd bits\nde dados\nEnlace sujeito a erros de bits\nTodos\nos bits em D'\nestão OK\n?\nN\nS\nKR 05.03.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p1 Wide x 17p5 Deep\n11/16/11 rossi\nDatagrama\nHI\nFigura 5.4  Paridade par usando um bit\n0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1\n1\nd bits de dados\nBit de\nparidade\nKR 05.04.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n13p Wide x 4p6 Deep\n11/16/11 rossi\ncamada de enlace: enlaces, redes de acesso e redes locais  327 \n“rajadas”\n. Na condição de rajada de erros, a probabilidade de haver erros não detectados em um quadro protegido \npor um esquema de paridade de bit único pode chegar perto de 50% [Spragins, 1991]. Claro que é necessário um \nesquema de detecção de erros mais robusto (e, felizmente, é o que se usa na prática!). Mas, antes de examinarmos \nos esquemas usados na prática, vamos considerar uma generalização simples da paridade de bit único que nos \ndará uma ideia das técnicas de correção.\nA Figura 5.5 mostra uma generalização bidimensional do esquema de paridade de bit único. Nessa figura, \nos d bits de D são divididos em i linhas e j colunas. Um valor de paridade é calculado para cada linha e para \ncada coluna. Os i + j + 1 bits de paridade resultantes compreendem os bits de detecção de erros do quadro da \ncamada de enlace.\nSuponha agora que ocorra um erro de bit único nos d bits originais de informação. Com esse esquema de \nparidade bidimensional, tanto a paridade da coluna quanto a da linha que contiver o bit modificado estarão com \nerro. O receptor então não só pode detectar que ocorreu um erro de um bit único, mas também usar os índices da \nlinha e da coluna com erros de paridade para realmente identificar o bit que foi corrompido e corrigir aquele erro! \nA Figura 5.5 mostra um exemplo no qual o bit com valor 1 na posição (2, 2) está corrompido e mudou para um \n0 — um erro que não somente é detectável, como também corrigível no receptor. Embora nossa discussão tenha \nfocalizado os d bits originais de informação, um erro único nos próprios bits de paridade também é detectável \ne corrigível. A paridade bidimensional também pode detectar (mas não corrigir!) qualquer combinação de dois \nerros em um pacote. Outras propriedades do esquema de paridade bidimensional são discutidas nos exercícios \nao final deste capítulo.\nA capacidade do receptor para detectar e corrigir erros é conhecida como correção de erros antecipada \n(forward error correction — FEC). Essas técnicas são usadas na armazenagem de áudio e em equipamentos de \nreprodução, como CDs de áudio. Em um ambiente de rede, as técnicas FEC podem ser usadas isoladamente ou \nem conjunto com as técnicas ARQ, que examinamos no Capítulo 3. As técnicas FEC são valiosas porque podem \nreduzir o número exigido de retransmissões do remetente. Talvez o mais importante seja que elas permitem ime-\ndiata correção de erros no receptor. Isso evita ter de esperar pelo atraso de propagação da viagem de ida e volta de \nque o remetente precisa para receber um pacote NAK e para que um pacote retransmitido se propague de volta \nFigura 5.5  Paridade par bidimensional\n1  0  1  0  1  1\n1  1  1  1  0  0\n0  1  1  1  0  1\n0  0  1  0  1  0\n1  0  1  0  1  1\n1  0  1  1  0  0\n0  1  1  1  0  1\n0  0  1  0  1  0\nParidade de linha\nErro de\nparidade\nErro de\nparidade\nNenhum erro\nErro de bit\núnico corrigível\nd1,1\nd2,1\n. . .\ndi,1\ndi+1,1\n. . .\n. . .\n. . .\n. . .\n. . .\nd1, j\nd2, j\n. . .\ndi, j\ndi+1, j\nd1, j+1\nd2, j+1\n. . .\ndi, j+1\ndi+1, j+1\nParidade de coluna\nKR 05 05\n   Redes de computadores e a Internet\n328\nao receptor — uma vantagem potencialmente importante para aplicações de rede em tempo real [Rubenstein, \n1998] ou enlaces (como enlaces no espaço sideral) com longos atrasos de propagação. Entre as pesquisas que \nexaminaram a utilização da FEC em protocolos de controle de erros estão as de Biersack [1992]; Nonnenmacher \n[1998]; Byers [1998]; Shacham [1990].\n5.2.2  Métodos de soma de verificação\nEm técnicas de soma de verificação, os d bits de dados na Figura 5.4 são tratados como uma sequência de \nnúmeros inteiros de k bits. Um método simples de soma de verificação é somar esses inteiros de k bits e usar \no total resultante como bits de detecção de erros. A soma de verificação da Internet é baseada nessa técnica \n— bytes de dados são tratados como inteiros de 16 bits e somados. O complemento de 1 dessa soma forma, \nentão, a soma de verificação da Internet, que é carregada no cabeçalho do segmento. Como discutido na Seção \n3.3, o receptor verifica a soma de verificação calculando os complementos de 1 da soma dos dados recebidos \n(inclusive a soma de verificação) e averiguando se o resultado contém somente bits 1. Se qualquer um dos bits \nfor 0, isso indicará um erro. O RFC 1071 discute em detalhes o algoritmo da soma de verificação da Internet \ne sua implementação. Nos protocolos TCP e UDP, a soma de verificação da Internet é calculada sobre todos \nos campos (incluindo os de cabeçalho e de dados). No IP, a soma de verificação é calculada sobre o cabeçalho \nIP (já que o segmento UDP ou TCP tem sua própria soma de verificação). Em outros protocolos, o XTP, por \nexemplo Strayer [1992], uma soma de verificação é calculada sobre o cabeçalho e outra soma de verificação é \ncalculada sobre o pacote inteiro.\nMétodos de soma de verificação exigem relativamente pouca sobrecarga no pacote. Por exemplo, as de \nTCP e UDP utilizam apenas 16 bits. Contudo, oferecem proteção um tanto baixa contra erros comparados com \na verificação de redundância cíclica, discutida mais adiante, que é utilizada com frequência na camada de enlace. \nUma pergunta que surge naturalmente neste ponto é: por que a soma de verificação é utilizada na camada de \ntransporte e a verificação de redundância cíclica é utilizada na camada de enlace? Lembre-se de que a camada \nde transporte em geral é executada em software, como parte do sistema operacional de um hospedeiro. Como a \ndetecção de erros na camada de transporte é realizada em software, é importante que o esquema de detecção de \nerros seja simples e rápido como a soma de verificação. Por outro lado, a detecção de erro na camada de enlace é \nimplementada em hardware dedicado nos adaptadores, que podem rodar muito rápido as mais complexas ope-\nrações de CRC. Feldmeier [1995] apresenta técnicas de implementação rápida em software não só para códigos \nde soma de verificação ponderada, mas também para CRC (veja a seguir) e outros códigos.\n5.2.3  Verificação de redundância cíclica (CRC)\nUma técnica de detecção de erros muito usada nas redes de computadores de hoje é baseada em códigos de \nverificação de redundância cíclica (cyclic redundancy check — CRC). Códigos de CRC também são conhecidos \ncomo códigos polinomiais, já que é possível considerar a cadeia de bits a ser enviada como um polinômio cujos \ncoeficientes são os valores 0 e 1 na cadeia, sendo as operações interpretadas como aritmética polinomial.\nCódigos de CRC funcionam da seguinte forma. Considere a parcela de d bits de dados, D, que o nó reme-\ntente quer enviar para o nó receptor. O remetente e o receptor devem, primeiro, concordar com um padrão de \nr + 1 bits, conhecido como um gerador, que denominaremos G. Vamos exigir que o bit mais significativo (o da \nextrema esquerda) de G seja um 1. A ideia fundamental por trás dos códigos de CRC é mostrada na Figura 5.6. \nPara dada parcela de dados, D, o remetente escolherá r bits adicionais, R, e os anexará a D de modo que o padrão \nde d + r bits resultante (interpretado como um número binário) seja divisível exatamente por G (por exemplo, \nsem nenhum remanescente), usando aritmética de módulo 2. Assim, o processo de verificação de erros com CRC \né simples: o receptor divide os d + r bits recebidos por G. Se o resto for diferente de zero, o receptor saberá que \nocorreu um erro; caso contrário, os dados são aceitos como corretos.\ncamada de enlace: enlaces, redes de acesso e redes locais  329 \nTodos os cálculos de CRC são feitos por aritmética de módulo 2 sem “vai 1” nas adições nem “empresta 1” nas \nsubtrações. Isso significa que a adição e a subtração são idênticas e ambas são equivalentes à operação ou exclusivo \n(XOR) bit a bit dos operandos. Por exemplo,\n1011 XOR 0101 = 1110\n1001 XOR 1101 = 0100\nTambém de modo semelhante temos:\n1011 – 0101 = 1110\n1001 – 1101 = 0100\nA multiplicação e a divisão são as mesmas da base 2, exceto que, em qualquer adição ou subtração exigida, \nnão se emprestam nem se tomam emprestadas casas. Como na aritmética binária comum, a multiplicação por 2k \ndesloca um padrão de bits para a esquerda por k casas. Assim, dados D e R, a quantidade D · 2r XOR R dá como \nresultado o padrão de bits d + r mostrado na Figura 5.6. Usaremos a representação algébrica do padrão de bits d \n+ r da Figura 5.6 em nossa discussão a seguir.\nVamos agora voltar à questão crucial de como o remetente calcula R. Lembre-se de que queremos descobrir \num R tal que exista um n tal que\nD ∙ 2r XOR R = nG\nIsto é, devemos escolher um R tal que G divida D ∙ 2r XOR sem resto. Se usarmos a operação XOR (isto é, \nadicionarmos com módulo 2, sem vai 1) de R com ambos os lados da equação anterior, teremos:\nD · 2r = nG XOR R\nEssa equação nos diz que, se dividirmos D ∙ 2r por G, o valor do resto será exatamente R. Em outras palavras, \npodemos calcular R como\nR\nresto\n=\nD · 2r \nG\nA Figura 5.7 ilustra esses cálculos para o caso de D = 101110, d = 6, G = 1001 e r = 3. Os 9 bits transmitidos \nnesse caso são 101110 011. Você deve fazer esses cálculos e também verificar se, na verdade, D ∙ 2r = 101011 ∙ G \nXOR R.\nPadrões internacionais foram definidos para geradores G de 8, 12, 16 e 32 bits. O padrão CRC-32 de 32 bits, \nque foi adotado em uma série de protocolos do IEEE da camada de enlace, usa um gerador igual a\nGCRC-32 = 100000100110000010001110110110111\nCada padrão de CRC pode detectar erros de rajada de menos do que r + 1 bits. (Isso significa que todos \nos erros de bits consecutivos de r bits ou menos serão detectados.) Além disso, em hipóteses apropriadas, uma \nrajada de comprimento maior do que r + 1 bits é detectada com probabilidade de 1 – 0,5r. Cada um dos padrões \nde CRC também pode detectar qualquer número ímpar de erros de bits. Veja em Williams [1993] uma discussão \nFigura 5.6  Códigos de CRC\nd bits\nr bits\nD: bits de dados a enviar\nD • 2r  XOR    R\nR: bits de CRC\nPadrão de bits\nFórmula matemática\nKR 05.06.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n22p1 Wide x 5p1 Deep\n11/16/11 rossi\n   Redes de computadores e a Internet\n330\nsobre a realização de verificações de CRC. A teoria por trás dos códigos de CRC e de códigos até mais poderosos \nultrapassa o escopo deste livro. O livro de Schwartz [1980] oferece uma excelente introdução a esse tópico.\nFigura 5.7  Um exemplo de cálculo de CRC\n1  0  0  1\n1  0  1  1  1  0  0  0  0\n1  0  1  0  1  1  \n1  0  0  1  \n1  0  1\n0  0  0\n1  0  1  0\n1  0  0  1\n1  1  0\n0  0  0\n1  1  0  0\n1  0  0  1\n1  0  1  0\n1  0  0  1\n 0  1  1\nG\nD\nR\nKR 05.07.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n11p3 Wide x 15p0 Deep\n11/16/11 rossi\n5.3  Enlaces e protocolos de acesso múltiplo\nNa introdução deste capítulo, observamos que há dois tipos de enlaces de redes: ponto a ponto e enlaces \nde difusão. Um enlace ponto a ponto consiste em um único remetente em uma extremidade do enlace e um \núnico receptor na outra. Muitos protocolos da camada de enlace foram projetados para enlaces ponto a ponto; \no PPP (protocolo ponto a ponto) e um controle de ligação de dados de alto nível (HDCL) são alguns que exa-\nminaremos mais adiante neste capítulo. O segundo tipo, o enlace de difusão, pode ter vários nós remetentes e \nreceptores, todos conectados ao mesmo canal de transmissão único e compartilhado. O termo difusão é usado \naqui porque, quando qualquer um dos nós transmite um quadro, o canal propaga o quadro por difusão e cada \nnó recebe uma cópia. A Ethernet e as LANs sem fio são exemplos de tecnologias de difusão da camada de en-\nlace. Nesta seção, vamos nos afastar um pouco dos protocolos específicos dessa camada e examinar, primeiro, \num problema de importância fundamental para a camada de enlace de dados: como coordenar o acesso de \nvários nós remetentes e receptores a um canal de difusão compartilhado — o problema do acesso múltiplo. \nCanais de difusão são muito usados em LANs, redes que estão geograficamente concentradas em um único \nprédio (ou empresa, ou campus). Assim, no final da seção, examinaremos também como os canais de acesso \nmúltiplo são usados em LANs.\nTodos estamos familiarizados com a noção de difusão — a televisão usa essa tecnologia desde que foi in-\nventada. Mas a televisão tradicional é uma difusão unidirecional (isto é, um nó fixo que transmite para muitos \nnós receptores), ao passo que os nós em um canal de difusão de uma rede de computadores tanto podem enviar \nquanto receber. Talvez uma analogia humana mais apropriada para um canal de difusão seja um coquetel, no qual \nmuitas pessoas se encontram em uma sala grande para falar e ouvir (e o ar fornece o meio de transmissão). Outra \nboa analogia é algo com que muitos leitores estão familiarizados — uma sala de aula, onde professor(es) e estu-\ndante(s) compartilham, de maneira semelhante, o mesmo e único meio de transmissão por difusão. Um proble-\nma fundamental em ambos os cenários é determinar quem fala (isto é, quem transmite pelo canal) e quando fala. \nComo seres humanos, desenvolvemos uma série elaborada de protocolos para compartilhar o canal de difusão:\ncamada de enlace: enlaces, redes de acesso e redes locais  331 \n“Dê a todos uma oportunidade de falar.”\n“Não fale até que alguém fale com você.”\n“Não monopolize a conversa.”\n“Levante a mão se tiver uma pergunta a fazer.”\n“Não interrompa uma pessoa quando ela estiver falando.”\n“Não durma quando alguém estiver falando.”\nRedes de computadores têm protocolos semelhantes — denominados protocolos de acesso múltiplo —, \npelos quais os nós regulam sua transmissão pelos canais de difusão compartilhados. Como ilustra a Figura 5.8, \nos protocolos de acesso múltiplo são necessários em diversos cenários de rede, que inclui redes de acesso com \nfio e sem fio, além de redes por satélite. Embora tecnicamente cada nó acesse o canal de difusão por meio de seu \nadaptador, nesta seção vamos nos referir ao nó como o dispositivo de envio e de recepção. Na prática, centenas \nou até milhares de nós podem se comunicar diretamente por um canal de difusão.\nComo todos os nós têm a capacidade de transmitir quadros, mais do que dois podem transmitir quadros ao \nmesmo tempo. Quando isso acontece, todos os nós recebem vários quadros ao mesmo tempo, isto é, os quadros \ntransmitidos colidem em todos os receptores. Em geral, quando há uma colisão, nenhum dos nós receptores \nconsegue perceber algum sentido nos quadros que foram transmitidos; de certo modo, os sinais dos quadros que \ncolidem ficam inextricavelmente embaralhados. Assim, todos os quadros envolvidos na colisão são perdidos e \no canal de difusão é desperdiçado durante o intervalo de colisão. É claro que, se muitos nós querem transmitir \nquadros com frequência, muitas transmissões resultarão em colisões e grande parte da largura de banda do canal \nde difusão será desperdiçada.\nPara assegurar que o canal de difusão realize trabalho útil quando há vários nós ativos, é preciso coordenar, \nde algum modo, as transmissões desses nós ativos. Essa tarefa de coordenação é de responsabilidade do protocolo \nde acesso múltiplo. Durante os últimos 40 anos, milhares de artigos e centenas de teses foram escritos sobre tais \nprotocolos; um levantamento abrangente dos primeiros 20 anos desse volume de trabalho pode ser encontrado \nCompartilhado com ﬁo \n(por exemplo, rede de acesso a cabo)\nCompartilhado sem ﬁo\n(por exemplo, Wi-Fi)\nSatélite\nCoquetel\nKR 05.08.eps\nK\nd R\nTerminal\nblá, blá, blá\nFigura 5.8  Vários canais de acesso múltiplo\n   Redes de computadores e a Internet\n332\nem Rom [1990]. Além disso, a pesquisa ativa sobre protocolos de acesso múltiplo continua por causa do surgi-\nmento contínuo de novos tipos de enlaces, em particular novos enlaces sem fio.\nDurante anos, dezenas de protocolos de acesso múltiplo foram executados em diversas tecnologias da ca-\nmada de enlace. Não obstante, podemos classificar praticamente qualquer protocolo de acesso múltiplo em uma \ndas seguintes categorias: protocolos de divisão de canal, protocolos de acesso aleatório e protocolos de reve-\nzamento. Examinaremos essas categorias nas três subseções seguintes.\nVamos concluir essa visão geral mencionando que, idealmente, um protocolo de acesso múltiplo para um \ncanal de difusão com velocidade de R bits por segundo tem as seguintes características desejáveis:\n1.\t Quando apenas um nó tem dados para enviar, esse nó tem uma vazão de R bit/s.\n2.\t Quando M nós têm dados para enviar, cada um desses nós tem uma vazão de R/M bits/s. Isso não significa \nnecessariamente que cada um dos M nós sempre terá uma velocidade instantânea de R/M, mas que cada \nnó deverá ter uma velocidade média de transmissão de R/M durante algum intervalo de tempo adequada-\nmente definido.\n3.\t O protocolo é descentralizado, isto é, não há um nó mestre que represente um único ponto de falha para \na rede.\n4.\t O protocolo é simples para que sua implementação seja barata.\n5.3.1  Protocolos de divisão de canal\nLembre-se de que na Seção 1.3 dissemos que a multiplexação por divisão de tempo (TDM) e a multiple-\nxação por divisão de frequência (FDM) são duas técnicas que podem ser usadas para dividir a largura de banda \nde um canal de difusão entre todos os nós que compartilham esse canal. Como exemplo, suponha que o canal \nsuporte N nós e que a velocidade de transmissão do canal seja R bits/s. O protocolo TDM divide o tempo em \nquadros temporais, os quais depois divide em N compartimentos de tempo. (O quadro temporal TDM não \ndeve ser confundido com a unidade de dados da camada de enlace trocada entre adaptadores remetentes e recep-\ntores, que também é denominada um quadro. Para diminuir a confusão, nesta seção vamos nos referir à unidade \nde dados trocada na camada de enlace como um pacote.) Cada compartimento de tempo é, então, atribuído a \num dos N nós. Sempre que um nó tiver um pacote para enviar, ele transmite os bits do pacote durante o com-\npartimento atribuído a ele no quadro rotativo TDM. Normalmente, os tamanhos dos quadros são escolhidos de \nmodo que um único quadro possa ser transmitido durante um compartimento de tempo. A Figura 5.9 mostra um \nexemplo de TDM simples de quatro nós. Voltando à analogia do coquetel, um coquetel regulamentado por TDM \npermitiria que um dos convidados falasse durante um período de tempo fixo; em seguida, permitiria que outro \nconvidado falasse pelo mesmo período de tempo e assim por diante. Quando todos tivessem tido sua chance de \nfalar, o padrão seria repetido.\nO protocolo TDM é atraente, pois elimina colisões e é perfeitamente justo: cada nó ganha uma velocidade de \ntransmissão dedicada de R/N bits/s durante cada quadro temporal. Contudo, ele tem duas desvantagens importan-\ntes. A primeira é que um nó fica limitado a uma velocidade média de R/N bits/s, mesmo quando ele é o único nó \ncom pacotes para enviar. A segunda é que o nó deve sempre esperar sua vez na sequência de transmissão — de novo, \nmesmo quando ele é o único com um quadro a enviar. Imagine um convidado que é o único que tem algo a dizer (e \nimagine uma situação ainda mais rara, em que todos na festa querem ouvir o que aquela pessoa tem a dizer). É óbvio \nque o protocolo TDM seria uma má escolha para um protocolo de acesso múltiplo para essa festa em particular.\nEnquanto o protocolo TDM compartilha o canal de difusão no tempo, o protocolo FDM divide o canal de \nR bits/s em frequências diferentes (cada uma com uma largura de banda de R/N) e reserva cada frequência a um \ndos N nós, criando, desse modo, N canais menores de R/N bits/s a partir de um único canal maior de R bits/s. O \nprotocolo FDM compartilha as vantagens do protocolo TDM — evita colisões e divide a largura de banda com \njustiça entre os N nós. Porém, também compartilha uma desvantagem principal com o protocolo TDM — um nó \né limitado a uma largura de banda R/N, mesmo quando é o único nó que tem pacotes a enviar.\ncamada de enlace: enlaces, redes de acesso e redes locais  333 \nUm terceiro protocolo de divisão de canal é o protocolo de acesso múltiplo por divisão de código (code division \nmultiple access — CDMA). Enquanto os protocolos TDM e FDM atribuem aos nós intervalos de tempo e frequências, \nrespectivamente, CDMA atribui um código diferente a cada nó. Então, cada nó usa seu código exclusivo para \ncodificar os bits de dados que envia. Se os códigos forem escolhidos com cuidado, as redes CDMA terão a maravi-\nlhosa propriedade de permitir que nós diferentes transmitam simultaneamente e, ainda assim, consigam que seus \nreceptores respectivos recebam corretamente os bits codificados pelo remetente (supondo que o receptor conheça \no código do remetente), a despeito das interferências causadas pelas transmissões dos outros nós. O CDMA vem \nsendo usado em sistemas militares há algum tempo (por suas propriedades anti-interferências) e agora está bastan-\nte difundido para uso civil. Como a utilização do CDMA está muito ligada a canais sem fio, adiaremos a discussão \nde seus detalhes técnicos para o Capítulo 6. Por enquanto, basta saber que códigos CDMA, assim como compar-\ntimentos de tempo em TDM e frequências em FDM, podem ser alocados a usuários de canais de múltiplo acesso.\n5.3.2  Protocolos de acesso aleatório\nA segunda classe geral de protocolos de acesso múltiplo são os protocolos de acesso aleatório. Com um pro-\ntocolo de acesso aleatório, um nó transmissor sempre transmite à taxa total do canal, isto é, R bits/s. Quando há \numa colisão, cada nó envolvido nela retransmite repetidamente seu quadro (isto é, pacote) até que este passe sem \ncolisão. Mas, quando um nó sofre uma colisão, ele nem sempre retransmite o quadro de imediato. Em vez disso, \nele espera um tempo aleatório antes de retransmitir o quadro. Cada nó envolvido em uma colisão escolhe atrasos \naleatórios independentes. Como após uma colisão os tempos de atraso são escolhidos de modo independente, é \npossível que um dos nós escolha um atraso mais curto o suficiente do que os atrasos dos outros nós em colisão e, \nportanto, consiga passar seu quadro discretamente para dentro do canal, sem colisão.\nHá dezenas, se não centenas, de protocolos de acesso aleatório descritos na literatura [Rom, 1990; Bertsekas, \n1991]. Nesta seção, descreveremos alguns dos mais usados — os protocolos ALOHA [Abramson, 1970; 1985] e \nos de acesso múltiplo com detecção de portadora (CSMA) [Kleinrock, 1975b]. Ethernet [Metcalfe, 1976] é uma \nvariante popular e muito disseminada do protocolo CSMA.\nSlotted ALOHA\nVamos começar nosso estudo de protocolos de acesso aleatório com um dos mais simples deles, o slotted \nALOHA. Em nossa descrição, admitiremos o seguinte:\nFigura 5.9  Um exemplo de TDM e FDM de quatro nós\n4KHz\nFDM\nTDM\nEnlace\n4KHz\nCompartimento\nTodos os compartimentos com rótulos “2” são dedicados\na um par remetente/receptor especíﬁco.\nQuadro\n1\n2\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nLegenda:\n   Redes de computadores e a Internet\n334\n• Todos os quadros consistem em exatamente L bits.\n• O tempo é dividido em intervalos (slots) de tamanho L/R segundos (isto é, um intervalo é igual ao tempo \nde transmissão de um quadro).\n• Os nós começam a transmitir quadros somente no início dos intervalos.\n• Os nós são sincronizados de modo que cada nó sabe onde os intervalos começam.\n• Se dois ou mais nós colidirem em um intervalo, então todos os nós detectarão o evento de colisão antes \ndo término do intervalo.\nSeja p uma probabilidade, isto é, um número entre 0 e 1. O funcionamento do slotted ALOHA em cada nó \né simples:\n• Quando o nó tem um novo quadro para enviar, espera até o início do próximo intervalo e transmite o \nquadro inteiro no intervalo.\n• Se não houver colisão, o nó terá transmitido seu quadro com sucesso e, assim, não precisará considerar a \nretransmissão. (Ele pode preparar um novo quadro para transmitir, se tiver algum.)\n• Se houver uma colisão, o nó a detectará antes do final do intervalo. Ele retransmitirá seu quadro em cada \nintervalo subsequente com probabilidade p até que o quadro seja transmitido sem colisão.\nPor retransmissão com probabilidade p, queremos dizer que o nó de fato joga uma moeda viciada; coroa \ncorresponde a “retransmitir”\n, o que ocorre com probabilidade p, enquanto cara corresponde a “pule o intervalo e \njogue a moeda novamente no próximo intervalo”\n, o que ocorre com probabilidade (1 – p). Todos os nós envolvi-\ndos na colisão jogam suas moedas independentemente.\nNa aparência o slotted ALOHA teria muitas vantagens. Ao contrário da divisão de canal, esse protocolo \npermite que um único nó transmita continuamente à taxa total do canal, R, quando ele for o único nó ativo. \n(Diz-se que um nó é ativo quanto tem quadros a enviar.) O slotted ALOHA também é altamente descentralizado, \nporque cada nó detecta colisões e decide de modo independente quando retransmitir. (No entanto, requer que os \nintervalos sejam sincronizados nos nós; em breve discutiremos uma versão sem intervalos do protocolo ALOHA \n[unslotted ALOHA], bem como protocolos CSMA — nenhum deles requer essa sincronização e, portanto, são \ntotalmente descentralizados.) O slotted ALOHA é também um protocolo extremamente simples.\nO slotted ALOHA funciona bem quando há apenas um nó ativo, mas qual é sua eficiência quando há vários? \nNesse caso, há duas preocupações possíveis quanto à eficiência. A primeira, como mostra a Figura 5.10, é que, \nquando há vários nós ativos, certa fração dos intervalos terá colisões e, portanto, será “desperdiçada”\n. A segunda é \nque outra fração dos intervalos estará vazia porque todos os nós ativos evitarão transmitir como resultado da po-\nFigura 5.10  \u0007\nNós 1, 2 e 3 colidem no primeiro intervalo. O nó 2 finalmente é bem-sucedido no \nquarto intervalo, o nó 1 no oitavo intervalo e o nó 3 no nono intervalo\nNó 3\nLegenda:\nC = Intervalo de colisão\nE = Intervalo vazio\nS = Intervalo bem-sucedido\nNó 2\nNó 1\n2\n2\n2\n1\n1\n1\n1\n3\n3\n3\nTempo\nC\nE\nC\nS\nE\nC\nE\nS\nS\ncamada de enlace: enlaces, redes de acesso e redes locais  335 \nlítica probabilística de transmissão. Os únicos intervalos “não desperdiçados” serão aqueles em que exatamente \num nó transmite. Um intervalo em que exatamente um nó transmite é denominado um intervalo bem-sucedido. \nA eficiência de um protocolo de acesso múltiplo com intervalos é definida como a fração (calculada durante um \nlongo tempo) de intervalos bem-sucedidos no caso de haver grande número de nós ativos, cada qual tendo sem-\npre grande número de quadros a enviar. Note que, se não fosse usado nenhum tipo de controle de acesso e cada \nnó retransmitisse logo após a colisão, a eficiência seria zero. É claro que o slotted ALOHA aumenta a eficiência \npara além de zero, mas em quanto?\nVamos agora esboçar a derivação da eficiência máxima do slotted ALOHA. Para manter a simplicidade des-\nsa derivação, vamos modificar um pouco o protocolo e admitir que cada nó tenta transmitir um quadro em cada \nintervalo com probabilidade p. (Isto é, admitimos que cada nó sempre tenha um quadro para enviar, e transmita \ncom probabilidade p tanto para um quadro novo como para um quadro que já sofreu uma colisão.) Suponha que \nhaja N nós. Então, a probabilidade de que determinado intervalo seja um intervalo bem-sucedido é a probabilida-\nde de que um dos nós transmita e os restantes N – 1 nós, não. A probabilidade de que determinado nó transmita \né p; a de que os nós restantes não transmitam é (1 – p)N–1. Por conseguinte, a probabilidade de que um dado nó \ntenha sucesso é p(1 – p)N–1. Como há N nós, a probabilidade de um nó arbitrário ter sucesso é Np(1 – p)N–1.\nAssim, quando há N nós ativos, a eficiência do slotted ALOHA é Np(1 – p)N–1. Para obtermos a eficiência má-\nxima para N nós ativos, temos de encontrar um p* que maximize essa expressão. (Veja os exercícios ao final deste \ncapítulo para um esboço geral dessa derivação.) E, para obtermos a eficiência máxima para um grande número \nde nós ativos, consideramos o limite de Np*(1 – p*)N–1 quando N tende ao infinito. (Novamente, veja os exercícios \nao final deste capítulo.) Após esses cálculos, descobriremos que a eficiência máxima do protocolo é dada por 1/e \n= 0,37. Isto é, quando um grande número de nós tem muitos quadros a transmitir, então (na melhor das hipó-\nteses) apenas 37% dos intervalos realiza um trabalho útil. Assim, a taxa efetiva de transmissão do canal não é R \nbits/s, mas apenas 0,37 R bits/s! Uma análise semelhante também demonstra que 37% dos intervalos ficam vazios \ne 26% apresentam colisões. Imagine o pobre administrador de rede que comprou um sistema slotted ALOHA de \n100 Mbits/s esperando poder usar a rede para transmitir dados entre um grande número de usuários a uma taxa \nagregada de, digamos, 80 Mbits/s! Embora o canal seja capaz de transmitir um dado quadro à taxa máxima do \ncanal de 100 Mbits/s, no final, a vazão que se consegue com esse canal é de menos de 37 Mbits/s.\nAloha\nO protocolo slotted ALOHA requer que todos os nós sincronizem suas transmissões para que comecem no \ninício de um intervalo. O primeiro protocolo ALOHA [Abramson, 1970] era, na realidade, um protocolo sem \nintervalos e totalmente descentralizado. No ALOHA puro, quando um quadro chega pela primeira vez (isto é, \num datagrama da camada de rede é passado para baixo a partir da camada de rede no nó remetente), o nó ime-\ndiatamente transmite o quadro inteiro ao canal de difusão. Se um quadro transmitido sofrer uma colisão com \numa ou mais transmissões, o nó retransmitirá de imediato (após ter concluído a transmissão total do quadro que \nsofreu a colisão) o quadro com probabilidade p. Caso contrário, o nó esperará por um tempo de transmissão de \nquadro. Após essa espera, ele então retransmite o quadro com probabilidade p ou espera (permanecendo ocioso) \npor outro tempo de quadro com probabilidade 1 – p.\nPara determinar a eficiência máxima do ALOHA puro, vamos focalizar um nó individual. Consideraremos \nas mesmas premissas que adotamos na análise do slotted ALOHA e tomaremos o tempo de transmissão do quadro \ncomo a unidade de tempo. A qualquer momento, a probabilidade de que um nó esteja transmitindo um quadro é \np. Suponha que esse quadro comece a transmitir no tempo t0. Como ilustra na Figura 5.11, para que ele seja trans-\nmitido com sucesso, nenhum dos outros nós pode começar sua transmissão no intervalo de tempo [t0 – 1, t0]. \nEsta se sobreporia ao início da transmissão do quadro do nó i. A probabilidade de que todos os outros nós não \niniciem uma transmissão nesse intervalo é (1 – p)N–1. De maneira semelhante, nenhum outro nó pode iniciar \numa transmissão enquanto o nó i estiver transmitindo, pois essa transmissão se sobreporia à parte final da do nó \ni. A probabilidade de que todos os outros nós não iniciem uma transmissão nesse intervalo é também (1 – p)N–1. \n   Redes de computadores e a Internet\n336\nAssim, a probabilidade de que um dado nó tenha uma transmissão bem-sucedida é p(1 – p)2(N–1). Levando ao \nlimite, como fizemos no caso do slotted ALOHA, descobrimos que a eficiência máxima do protocolo ALOHA \npuro é de apenas 1/(2e) — exatamente a metade da eficiência do slotted ALOHA. É este o preço que se paga por \num protocolo ALOHA totalmente descentralizado.\nNorm Abramsom e a ALOHAnet\nNorm Abramsom, um doutor em engenharia, era \napaixonado por surfe e interessado na comutação \nde pacotes. Essa combinação de interesses o levou \nà Universidade do Havaí em 1969. O Havaí é forma-\ndo por muitas ilhas montanhosas, o que dificulta a \ninstalação e a operação de redes terrestres. Quando \nnão estava surfando, Abramson ficava pensando em \ncomo projetar uma rede que fizesse comutação de \npacotes por rádio. A rede que ele projetou tinha um \nhospedeiro central e diversos nós secundários espa-\nlhados pelas ilhas havaianas. A rede tinha dois canais, \ncada um usando uma faixa de frequência diferente. O \ncanal na direção dos nós secundários fazia difusão de \npacotes do hospedeiro central para os secundários; e \no canal na direção contrária enviava pacotes dos hos-\npedeiros secundários para o central. Além de enviar \npacotes de informação, o hospedeiro central também \nenviava pelo canal na direção dos nós secundários um \nreconhecimento para cada pacote recebido com su-\ncesso dos secundários.\nComo os hospedeiros secundários transmitiam \npacotes de maneira descentralizada, inevitavelmente \nocorriam colisões no canal entre eles e o hospedeiro \ncentral. Essa observação levou Abramson a inventar, \nem 1970, o protocolo ALOHA puro, descrito neste capí-\ntulo. Em 1970, com o financiamento contínuo da ARPA, \nele conectou sua ALOHAnet à ARPAnet. O trabalho de \nAbramson é importante não somente porque foi o pri-\nmeiro exemplo de uma rede de pacotes por rádio, mas \ntambém porque inspirou Bob Metcalfe. Alguns anos \ndepois, Metcalfe modificou o protocolo ALOHA e criou \no protocolo CSMA/CD e a rede local Ethernet.\nHistória\nCSMA (acesso múltiplo com detecção de portadora)\nTanto no slotted ALOHA quanto no ALOHA puro, a decisão de transmitir é tomada por um nó indepen-\ndentemente da atividade dos outros nós ligados ao canal de difusão. Em particular, um nó não se preocupa se \npor acaso outro está transmitindo quando ele começa a transmitir nem para de transmitir se outro nó começar \na interferir em sua transmissão. Em nossa analogia do coquetel, os protocolos ALOHA se parecem muito com \num convidado mal-educado que continua a tagarelar mesmo quando outras pessoas estão falando. Como seres \nhumanos, temos protocolos que nos levam não somente a nos comportar com mais civilidade, mas também a \nFigura 5.11 \nTransmissões interferentes no ALOHA puro\nTempo\nVai se sobrepor\nao início do\nquadro de i\nt0 – 1\nt0\nt0 + 1\nVai se sobrepor\nao ﬁnal do\nquadro de i\nQuadro do nó i\nKR 05.11.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p1 Wide x 11p10 Deep\n11/16/11 rossi\ncamada de enlace: enlaces, redes de acesso e redes locais  337 \nreduzir o tempo que gastamos “colidindo” com outros durante a conversação e, por conseguinte, a aumentar a \nquantidade de dados que trocamos durante nossas conversas. Especificamente, há duas regras importantes que \nregem a conversação educada entre seres humanos:\n• Ouça antes de falar. Se uma pessoa estiver falando, espere até que ela tenha terminado. No mundo das re-\ndes, isso é denominado detecção de portadora — um nó ouve o canal antes de transmitir. Se um quadro \nde outro nó estiver atualmente sendo transmitido para dentro do canal, o nó então esperará até que não \ndetecte transmissões por um período de tempo curto, e então iniciará a transmissão.\n• Se alguém começar a falar ao mesmo tempo que você, pare de falar. No mundo das redes, isso é deno-\nminado detecção de colisão — um nó que está transmitindo ouve o canal enquanto transmite. Se esse \nnó detectar que outro nó está transmitindo um quadro interferente, ele para de transmitir e espera por \nalgum tempo antes de repetir o ciclo de detectar-e-transmitir-quando-ocioso.\nEssas duas regras estão incorporadas na família de protocolos de acesso múltiplo com detecção de porta-\ndora (CSMA — carrier sense multiple access) e CSMA com detecção de colisão (CSMA/CD) [Kleinrock, 1975b; \nMetcalfe, 1976; Lam, 1980; Rom, 1990]. Foram propostas muitas variações do CSMA e do CSMA/CD. Nesta \nseção, consideraremos algumas das características mais importantes e fundamentais do CSMA e do CSMA/CD.\nA primeira pergunta que se poderia fazer sobre o CSMA é a seguinte: se todos os nós realizam detecção de \nportadora, por que ocorrem colisões no primeiro lugar? Afinal, um nó vai abster-se de transmitir sempre que \nperceber que outro está transmitindo. A resposta a essa pergunta pode ser ilustrada utilizando diagramas espaço/\ntempo [Molle, 1987]. A Figura 5.12 apresenta um diagrama espaço/tempo de quatro nós (A, B, C, D) ligados a \num barramento linear de transmissão. O eixo horizontal mostra a posição de cada nó no espaço; o eixo vertical \nrepresenta o tempo.\nNo tempo t0, o nó B percebe que o canal está ocioso, pois nenhum outro nó está transmitindo no momento. \nAssim, o nó B começa a transmitir e seus bits se propagam em ambas as direções ao longo do meio de transmis-\nsão. A propagação para baixo dos bits de B na Figura 5.12 com o aumento do tempo indica que é preciso uma \nquantidade de tempo de valor diferente de zero para que os bits de B de fato se propaguem (apesar de quase à \nvelocidade da luz) ao longo do meio de transmissão. No tempo t1 (t1 > t0), o nó D tem um quadro para enviar. \nEmbora o nó B esteja transmitindo no tempo t1, os bits que estão sendo transmitidos por B ainda não alcançaram \nD. Assim, D percebe o canal como ocioso em t1. De acordo com o protocolo CSMA, D começa então a transmitir \nseu quadro. Pouco tempo depois, a transmissão de B passa a interferir na transmissão de D em D. Fica evidente, \npela Figura 5.12, que o tempo de atraso de propagação fim a fim de canal para um canal de difusão — o tempo \nque leva para que um sinal se propague de um dos extremos do canal para outro — desempenhará um papel cru-\ncial na determinação de seu desempenho. Quanto mais longo for esse atraso de propagação, maior será a chance \nde um nó que detecta portadora ainda não poder perceber uma transmissão que já começou em outro nó da rede.\nCarrier Sense Multiple Access with Collision Detection (CSMA/CD)\nNa Figura 5.12, os nós não realizam detecção de colisão; ambos, B e D, continuam a transmitir seus quadros \nintegralmente mesmo que ocorra uma colisão. Quando um nó realiza detecção de colisão, ele cessa a transmissão \nimediatamente. A Figura 5.13 mostra o mesmo cenário da Figura 5.12, exceto que cada um dos dois nós aborta \nsua transmissão pouco tempo após detectar uma colisão. É claro que adicionar detecção de colisão a um proto-\ncolo de acesso múltiplo ajudará o desempenho do protocolo por não transmitir inteiramente um quadro inútil, \ncujo conteúdo está corrompido (pela interferência de um quadro de outro nó).\nAntes de analisar o protocolo CSMA/CD, vamos resumir sua operação do ponto de vista de um adaptador \n(em um nó) ligado a um canal de difusão:\n1.\t O adaptador obtém um datagrama da camada de rede, prepara um quadro da camada de enlace e coloca \no quadro no buffer do adaptador.\n   Redes de computadores e a Internet\n338\nFigura 5.12  Diagrama espaço/tempo de dois nos CSMA com colisão de transmissões\nA\nTempo\nTempo\nEspaço\nt 0\nt 1\nB\nC\nD\nKR 05.12.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n26p8 Wide x 30p Deep\n11/16/11 rossi\nFigura 5.13  CSMA com detecção de colisão\nA\nTempo\nTempo\nTempo de\ndetecção de\ncolisão/abortamento\nEspaço\nt 0\nt 1\nB\nC\nD\ncamada de enlace: enlaces, redes de acesso e redes locais  339 \n2.\t Se o adaptador detectar que o canal está ocioso (ou seja, não há energia de sinal entrando nele a partir do \ncanal), ele começa a transmitir o quadro. Por outro lado, se detectar que o canal está ocupado, ele espera \naté que não detecte energia de sinal, para então começar a transmitir o quadro.\n3.\t Enquanto transmite, o adaptador monitora a presença de energia de sinal vinda de outros adaptadores \nusando o canal de difusão.\n4.\t Se transmitir o quadro inteiro sem detectar energia de sinal de outros adaptadores, o adaptador terá \nterminado com o quadro. Por outro lado, se detectar energia de sinal de outros adaptadores enquanto \ntransmite, ele aborta a transmissão (ou seja, para de transmitir seu quadro).\n5.\t Depois de abortar, o adaptador espera por um tempo aleatório e depois retorna à etapa 2.\nEspera­\n‑se que a necessidade de esperar por um tempo aleatório (e não fixo) seja clara — se dois nós \ntransmitissem quadros ao mesmo tempo e depois ambos esperassem pelo mesmo período de tempo fixo, eles \ncontinuariam colidindo indefinidamente. Mas qual é um bom intervalo de tempo para escolher um tempo de \nespera aleatório? Se o intervalo for grande e o número de nós colidindo for pequeno, é provável que os nós \nesperem muito tempo (com o canal permanecendo ocioso) antes de repetir a etapa de detectar­\n‑e­\n‑transmitir­\n‑quando­\n‑ocioso. Por outro lado, se o intervalo for pequeno e o número de nós colidindo for grande, é provável \nque os valores aleatórios escolhidos sejam quase os mesmos, e os nós transmitindo colidirão de novo. Gos-\ntaríamos de ter um intervalo que seja curto quando o número de nós colidindo for pequeno, porém longo \nquando for grande.\nO algoritmo de recuo exponencial binário, usado na Ethernet e também nos protocolos de acesso múl-\ntiplo de rede a cabo DOCSIS [DOCSIS, 2011], resolve esse problema de forma elegante. Especificamente, ao \ntransmitir um quadro que já tenha experimentado n colisões, um nó escolhe o valor de K aleatoriamente a \npartir de {0, 1, 2, . . . , 2n – 1}. Assim, quanto mais colisões um quadro experimentar, maior o intervalo do qual \nK é escolhido. Para Ethernet, a quantidade de tempo real que um nó recua é K ∙ 512 tempos de bit (isto é, K \nvezes a quantidade de tempo necessária para enviar 512 bits para a Ethernet) e o valor máximo que n pode \ntomar é limitado a 10.\nVejamos um exemplo. Suponha que um nó tente transmitir um quadro pela primeira vez e, enquanto trans-\nmite, ele detecta uma colisão. O nó, então, escolhe K = 0 com probabilidade 0,5 ou escolhe K = 1 com probabili-\ndade 0,5. Se o nó escolhe K = 0, então ele de imediato começa a detectar o canal. Se o nó escolhe K = 1, ele espera \n512 tempos de bit (por exemplo, 0,01 ms para uma Ethernet a 100 Mbits/s) antes de iniciar o ciclo de detectar­\n‑e­\n‑transmitir­\n‑quando­\n‑ocioso. Após uma segunda colisão, K é escolhido com probabilidade igual dentre {0,1,2,3}. \nApós três colisões, K é escolhido com probabilidade igual dentre {0,1,2,3,4,5,6,7}. Após dez ou mais colisões, K \né escolhido com probabilidade igual dentre {0,1,2, . . . , 1023}. Assim, o tamanho dos conjuntos dos quais K é \nescolhido cresce exponencialmente com o número de colisões; por esse motivo, esse algoritmo é denominado \nrecuo exponencial binário.\nObservamos aqui também que, toda vez que um nó prepara um novo quadro para transmissão, ele roda \no algoritmo CSMA/CD, não levando em conta quaisquer colisões que possam ter ocorrido no passado recente. \nAssim, é possível que um nó com um novo quadro possa escapar imediatamente em uma transmissão bem-suce-\ndida enquanto vários outros nós estão no estado de recuo exponencial.\nEficiência do CSMA/CD\nQuando somente um nó tem um quadro para enviar, esse nó pode transmitir na velocidade total do \ncanal (por exemplo, para Ethernet, as velocidades típicas são 10 Mbits/s, 100 Mbits/s ou 1 Gbit/s). Porém, se \nmuitos nós tiverem quadros para transmitir, a velocidade de transmissão eficaz do canal pode ser muito me-\nnor. Definimos a eficiência do CSMA/CD como a fração de tempo (por um período longo) durante a qual \nos quadros estão sendo transmitidos no canal sem colisões quando há um grande número de nós ativos, \ncom cada nó tendo um grande número de quadros para enviar. Para apresentar uma aproximação fechada \nda eficiência da Ethernet, considere que dprop indica o tempo máximo que uma energia de sinal leva para ser \n   Redes de computadores e a Internet\n340\npropagada entre dois adaptadores quaisquer. Considere que dtrans seja o tempo para transmitir um quadro de \ntamanho máximo (cerca de 1,2 ms para uma Ethernet a 10 Mbits/s). Uma derivação da eficiência do CSMA/\nCD está fora do escopo deste livro (ver Lam [1980] e Bertsekas [1991]). Aqui, indicamos simplesmente a \nseguinte aproximação:\nEfciência =\n1\n1 + 5dprop/dtrans\nPor esta fórmula, vemos que, à medida que dprop tende a 0, a eficiência tende a 1. Isso corresponde à intuição \nde que, se o atraso de propagação for zero, os nós colidindo serão imediatamente abortados, sem desperdiçar o \ncanal. Além disso, à medida que dtrans se torna muito grande, a eficiência tende a 1. Isso também é intuitivo, pois, \nquando um quadro agarra o canal, prende-se a ele por um longo tempo; assim, o canal estará realizando trabalho \nprodutivo por quase todo o tempo.\n5.3.3  Protocolos de revezamento\nLembre-se de que duas propriedades desejáveis de um protocolo de acesso múltiplo são: (1) quando apenas \num nó está ativo, este tem uma vazão de R bits/s; (2) quando M nós estão ativos, então cada nó ativo tem uma \nvazão de mais ou menos R/M bits/s. Os protocolos ALOHA e CSMA têm a primeira propriedade, mas não a \nsegunda. Isso motivou os pesquisadores a criarem outra classe de protocolos — os protocolos de revezamento. \nComo acontece com os de acesso aleatório, há dezenas de protocolos de revezamento, e cada um deles tem muitas \nvariações. Discutiremos nesta seção dois dos mais importantes. O primeiro é o protocolo de polling (seleção). \nEle requer que um dos nós seja designado como nó mestre. Este seleciona cada um dos nós por alternância cir-\ncular. Em particular, ele envia primeiro uma mensagem ao nó 1 dizendo que ele (o nó 1) pode transmitir até certo \nnúmero máximo de quadros. Após o nó 1 transmitir alguns quadros, o nó mestre diz ao nó 2 que ele (o nó 2) \npode transmitir até certo número máximo de quadros. (O nó mestre pode determinar quando um nó terminou \nde enviar seus quadros observando a ausência de um sinal no canal.) O procedimento continua dessa maneira, \ncom o nó mestre escolhendo cada um dos nós de maneira cíclica.\nO protocolo de polling elimina as colisões e os intervalos vazios que atormentam os protocolos de acesso \naleatório, e isso permite que ele tenha uma eficiência muito maior. Mas também tem algumas desvantagens. A \nprimeira é que o protocolo introduz um atraso de seleção — o período de tempo exigido para notificar um nó que \nele pode transmitir. Se, por exemplo, apenas um nó estiver ativo, então ele transmitirá a uma velocidade menor \ndo que R bits/s, pois o nó mestre tem de escolher cada um dos nós ociosos por vez, cada vez que um nó ativo tiver \nenviado seu número máximo de quadros. A segunda desvantagem é potencialmente mais séria: se o nó mestre \nfalhar, o canal inteiro ficará inoperante. O Protocolo 802.15 e o protocolo Bluetooth, que estudaremos na Seção \n6.3, são exemplos de protocolos de polling.\nO segundo protocolo de revezamento é o protocolo de passagem de permissão. Nele, não há nó mestre. \nUm pequeno quadro de finalidade especial conhecido como uma permissão (token) é passado entre os nós obe-\ndecendo a uma determinada ordem fixa. Por exemplo, o nó 1 poderá sempre enviar a permissão ao nó 2, o nó 2 \npoderá sempre enviar a permissão ao nó 3, o nó N poderá sempre enviar a permissão ao nó 1. Quando um nó \nrecebe uma permissão, ele a retém apenas se tiver alguns quadros para transferir, caso contrário, imediatamente \na repassa para o nó seguinte. Se um nó tiver quadros para transmitir quando recebe a permissão, ele enviará um \nnúmero máximo de quadros e, em seguida, passará a permissão para o nó seguinte. A passagem de permissão é \ndescentralizada e tem uma alta eficiência. Mas também tem seus problemas. Por exemplo, a falha de um nó pode \nderrubar o canal inteiro. Ou, se um nó acidentalmente se descuida e não libera a permissão, então é preciso cha-\nmar algum procedimento de recuperação para recolocar a permissão em circulação. Durante muitos anos, foram \ndesenvolvidos muitos protocolos de passagem de permissão, e cada um deles teve de enfrentar esses e outros as-\nsuntos delicados, incluindo o protocolo FDDI (Fiber Distributed Data Interface) [Jain, 1994] e o protocolo token \nring IEEE 802.5 [IEEE 802.5, 2012].\ncamada de enlace: enlaces, redes de acesso e redes locais  341 \n5.3.4  \u0007\nDOCSIS: o protocolo da camada de enlace para acesso à Internet a \ncabo\nNas três subseções anteriores, aprendemos a respeito de três classes gerais de protocolos de acesso múltiplo: \nprotocolos de divisão de canal, protocolos de acesso aleatório e protocolos de revezamento. Aqui, uma rede de \nacesso a cabo servirá como um excelente estudo de caso, pois veremos aspectos de cada uma dessas três classes \nde protocolos de acesso múltiplo com a rede de acesso a cabo!\nLembre-se de que, na Seção 1.2.1, vimos que uma rede de acesso a cabo em geral conecta milhares de \nmodems a cabo residenciais a um sistema de término de modem a cabo (CMTS) no terminal de distribuição da \nrede a cabo. Data-Over-Cable Service Interface Specifications (DOCSIS) [DOCSIS, 2011] especifica a arquitetura \nde rede de dados a cabo e seus protocolos. DOCSIS utiliza FDM para dividir os segmentos de rede em direção \nao modem (downstream) e em direção ao CMTS (upstream) em canais de múltiplas frequências. Cada canal do \nCMTS ao modem tem 6 MHz de largura, com uma vazão máxima de cerca de 40 Mbits/s por canal (embora, na \nprática, essa velocidade de dados raramente seja vista em um modem a cabo); cada canal do modem ao CMTS \ntem uma largura de canal máxima de 6,4 MHz, e uma vazão máxima de mais ou menos 30 Mbits/s. Cada um \ndeles é um canal de difusão. Os quadros transmitidos no canal do CMTS ao modem são recebidos por todos os \nmodems a cabo que recebem esse canal; porém, como há apenas um CMTS transmitindo para o canal em direção \nao modem, não há problema de acesso múltiplo. A direção contrária, no entanto, é mais interessante e tecnica-\nmente desafiadora, pois vários modems a cabo compartilham o mesmo canal (frequência) em direção ao CMTS, \ne com isso potencialmente haverá colisões.\nConforme ilustra a Figura 5.14, cada canal do modem ao CMTS é dividido em intervalos de tempo (tipo \nTDM), cada um com uma sequência de mini­\n‑intervalos, durante os quais os modems a cabo podem transmitir \nao CMTS. O CMTS concede permissão explicitamente aos modems individuais para transmitir durante mini­\n‑intervalos específicos. O CMTS realiza isso enviando uma mensagem de controle (conhecida como mensagem \nMAP) em um canal em direção ao modem, para especificar qual modem a cabo (com dados para enviar) pode \ntransmitir durante qual mini­\n‑intervalo durante o tempo especificado na mensagem de controle. Como os mini­\n‑intervalos são alocados explicitamente aos modems a cabo, o CMTS pode garantir que não haverá transmissões \ncolidindo durante um mini­\n‑intervalo.\nMas como o CMTS sabe quais modems a cabo possuem dados para enviar em primeiro lugar? Isso é feito \npelo envio de quadros de requisição de mini-intervalo dos modems ao CMTS, durante um conjunto especial \nde mini-intervalos dedicados para essa finalidade, como mostra a Figura 5.14. Esses quadros de requisição \nde mini-intervalo são transmitidos em uma forma de acesso aleatório e, portanto, podem colidir uns com os \noutros. Um modem a cabo não consegue detectar se o canal até o CMTS está ocupado nem detectar colisões. \nFigura 5.14  Canais entre o CMTS e os modems a cabo\nResidências com\nmodems a cabo\nMini-intervalos\ncontendo quadros\nde requisição de\nmini-intervalo\nMini-intervalos\ndesignados contendo\nquadros de dados\ndo modem a cabo\nTerminal de distribuição\nQuadro MAP para\nintervalo [t1,t2]\nKR 05.14.eps\nKurose and Ross\nComputer Networking 6/e\nCMTS\nCanal i do CMTS ao modem\nCanal j do modem ao CMTS\nt1\nt2\n   Redes de computadores e a Internet\n342\nEm vez disso, ele deduz que seu quadro de requisição de mini-intervalo teve uma colisão se não receber uma \nresposta à alocação requisitada na próxima mensagem de controle do CMTS ao modem. Ao deduzir uma \ncolisão, um modem a cabo usa o recuo exponencial binário para adiar a retransmissão do seu quadro de re-\nquisição de mini-intervalo para um período de tempo no futuro. Quando há pouco tráfego no canal de subida \naté o CMTS, um modem a cabo pode de fato transmitir quadros de dados durante os intervalos designados \nnominalmente para os quadros de requisição de mini-intervalo (evitando, assim, ter que esperar por uma \ndesignação de mini-intervalo).\nUma rede de acesso a cabo, portanto, serve como um excelente exemplo de protocolos de acesso múl-\ntiplo em ação — FDM, TDM, acesso aleatório e intervalos de tempo alocados de forma central, tudo dentro \nde uma rede!\n5.4  Redes locais comutadas\nTendo explicado as redes de difusão e os protocolos de acesso múltiplo na seção anterior, vamos voltar \nnossa atenção em seguida às redes locais comutadas. A Figura 5.15 mostra uma rede local comutada conec-\ntando três departamentos, dois servidores e um roteador com quatro comutadores. Como esses comutadores \noperam na camada de enlace, eles comutam quadros da camada de enlace (em vez de datagramas da camada \nde rede), não reconhecem endereços da camada de rede e não utilizam algoritmos de roteamento, como RIP \nou OSPF, para determinar caminhos pela rede de comutadores da camada 2. Em vez de usar endereços IP, logo \nveremos que eles usam endereços da camada de enlace para repassar quadros da camada de enlace pela rede \nde comutadores. Vamos começar nosso estudo das LANs comutadas explicando primeiro o endereçamento \nna camada de enlace (Seção 5.4.1). Depois, examinaremos o famoso protocolo Ethernet (Seção 5.4.2). Depois \nde examinar o endereçamento na camada de enlace e Ethernet, veremos como operam os comutadores da ca-\nmada de enlace (Seção 5.4.3) e depois (na Seção 5.4.4) como esses comutadores normalmente são usados para \nmontar LANs em grande escala.\nFigura 5.15  Uma rede institucional conectada por quatro comutadores\nServidor\nde correio\nÀ Internet\nexterna\n1 Gbit/s\n1\n2\n3\n4\n5\n6\n1 Gbit/s\n1 Gbit/s\nEngenharia Elétrica\nCiência da Computação\n100 Mbits/s\n(ﬁbra)\n100 Mbits/s\n(ﬁbra)\n100 Mbits/s\n(ﬁbra)\nMistura de 10 Mbits/s,\n100 Mbits/s, 1 Gbit/s,\ncabo Cat 5\nServidor Web\nEngenharia da Computação\ncamada de enlace: enlaces, redes de acesso e redes locais  343 \n5.4.1  Endereçamento na camada de enlace e ARP\nHospedeiros e roteadores têm endereços da camada de enlace. Você talvez fique surpreso com isso ao lem-\nbrar que dissemos, no Capítulo 4, que nós também têm endereços da camada de rede e talvez se pergunte por \nque, afinal de contas, é preciso ter endereços na camada de rede e na de enlace. Nesta seção, além de descrever-\nmos a sintaxe e a função dos endereços da camada de enlace, esperamos esclarecer por que as duas camadas de \nendereços são úteis e, na verdade, indispensáveis. Estudaremos também o Protocolo de Resolução de Endereços \n(Address Resolution Protocol — ARP), que oferece um mecanismo que habilita os nós a traduzirem endereços IP \npara endereços da camada de enlace.\nEndereços MAC\nNa verdade, não é o nó (isto é, o hospedeiro ou o roteador) que tem um endereço da camada de enlace, mas \no adaptador do nó. Um hospedeiro ou roteador com várias interfaces de rede, portanto, terá vários endereços da \ncamada de enlace associados a ele, assim como também teria vários endereços IP associados. Porém, é impor-\ntante observar que os comutadores da camada de enlace não têm endereços da camada de enlace associados às \nsuas interfaces, que se conectam aos hospedeiros e roteadores. Isso porque a função do comutador da camada de \nenlace é transportar datagramas entre hospedeiros e roteadores; um comutador faz isso de modo transparente, \nou seja, sem que o hospedeiro ou roteador tenha que endereçar o quadro explicitamente para o comutador inter-\nmediário. Isso é ilustrado na Figura 5.16. Um endereço da camada de enlace é também denominado endereço de \nLAN, endereço físico ou endereço MAC (Media Access Control — controle de acesso ao meio). Como a expres-\nsão endereço MAC parece ser a mais popular, daqui em diante nos referiremos a endereços da camada de enlace \ncomo endereços MAC. Para a maior parte das LANs (incluindo a Ethernet e as LANs 802.11 sem fio), o endereço \nMAC tem 6 bytes de comprimento, o que dá 248 endereços MAC possíveis. Como ilustrado na Figura 5.16, tais \nendereços de 6 bytes costumam ser expressos em notação hexadecimal, com cada byte do endereço mostrado \ncomo um par de números hexadecimais. Apesar de os endereços MAC serem projetados como permanentes, \nagora é possível mudar o endereço de MAC de um adaptador via software. No entanto, pelo resto desta seção, \nvamos considerar que o endereço MAC de um adaptador é fixo.\nUma propriedade interessante dos endereços MAC é que não existem dois adaptadores com o mesmo en-\ndereço. Isso pode parecer surpreendente, dado que os adaptadores são fabricados em muitos países por inúmeras \nempresas diferentes. Como uma empresa fabricante de adaptadores em Taiwan se certifica de que está usando \nendereços diferentes dos usados por um fabricante de adaptadores na Bélgica? A resposta é que o IEEE gerencia \no espaço físico de endereços MAC. Em particular, quando uma empresa quer produzir adaptadores, compra, por \nFigura 5.16  Cada interface conectada à LAN tem um endereço MAC exclusivo\n88-B2-2F-54-1A-0F\n5C-66-AB-90-75-B1\n1A-23-F9-CD-06-9B\n49-BD-D2-C7-56-2A\n   Redes de computadores e a Internet\n344\numa taxa nominal, uma parcela do espaço de endereços que consiste em 224 endereços. O IEEE aloca a parcela de \n224 endereços fixando os primeiros 24 bits de um endereço MAC e permitindo que a empresa crie combinações \nexclusivas com os últimos 24 bits para cada adaptador.\nO endereço MAC de um adaptador tem uma estrutura linear (oposta à estrutura hierárquica) e nunca \nmuda, não importando para onde vá o adaptador. Um notebook com um cartão Ethernet tem sempre o mesmo \nendereço MAC, não importando aonde o computador vá. Um smartphone com uma interface 802.11 tem sempre \no mesmo endereço MAC aonde quer que vá. Lembre-se de que, ao contrário, um endereço IP tem uma estrutura \nhierárquica (isto é, uma parte que é da rede e outra que é do hospedeiro) e que o endereço IP de um nó precisa \nser trocado quando o hospedeiro troca de lugar, por exemplo, muda a rede à qual está conectado. O endereço \nMAC de um adaptador é semelhante ao número do CPF de uma pessoa, que também tem uma estrutura linear e \nnão muda, não importando aonde a pessoa vá. Um endereço IP é semelhante ao endereço postal de uma pessoa, \nque é hierárquico e precisa ser trocado quando a pessoa muda de endereço. Exatamente como uma pessoa pode \nachar útil ter um endereço postal, bem como um número de CPF, também é útil para um nó ter um endereço da \ncamada de rede, assim como um endereço MAC.\nQuando um adaptador quer enviar um quadro para algum adaptador de destino, o remetente insere no qua-\ndro o endereço MAC do destino e envia o quadro para dentro da LAN. Como veremos em breve, um comutador \nàs vezes transmite por difusão um quadro que chega para todas as suas interfaces. Veremos, no Capítulo 6, que \na LAN 802.11 também transmite quadros por difusão. Assim, um adaptador pode receber um quadro que não \nestá endereçado a ele. Desse modo, quando o adaptador receber um quadro, ele verificará se o endereço MAC \nde destino combina com seu próprio endereço MAC. Se ambos combinarem, o adaptador extrairá o datagrama \nencerrado no quadro e o passará para cima na pilha de protocolos. Se não combinarem, o adaptador descartará o \nquadro sem passar o datagrama da camada de rede para cima na pilha de protocolos. Assim, somente o destino \nserá interrompido quando receber um quadro.\nNo entanto, às vezes um adaptador remetente quer que todos os outros adaptadores na LAN recebam e \nprocessem o quadro que ele está prestes a enviar. Nesse caso, o adaptador remetente insere um endereço de difu-\nsão MAC especial no campo de endereço do destinatário do quadro. Para LANs que usam endereços de 6 bytes \n(como a Ethernet e 802.11), o endereço de difusão é uma cadeia de 48 bits 1 consecutivos (isto é, FF-FF-FF-FF-\nFF-FF em notação hexadecimal).\nMantendo a independência das camadas\nHá diversas razões por que os nós têm endere-\nços MAC além de endereços da camada de rede. \nPrimeiro, LANs são projetadas para protocolos da \ncamada de rede arbitrários, e não apenas para IP \ne para a Internet. Se os adaptadores recebessem \nendereços IP\n, e não os endereços MAC “neutros”, \neles não poderiam suportar com facilidade outros \nprotocolos da camada de rede (por exemplo, IPX ou \nDECNet). Segundo, se adaptadores usassem ende-\nreços da camada de rede — em vez de endereços \nMAC — o endereço da camada de rede teria de ser \narmazenado na RAM do adaptador e reconfigura-\ndo toda vez que este mudasse de local (ou fosse \nligado). Outra opção é não usar nenhum endereço \nnos adaptadores e fazer que cada um deles pas-\nse os dados (em geral, um datagrama IP) de cada \nquadro que recebe para cima na pilha de protoco-\nlos. A camada de rede poderia, então, verificar se o \nendereço combina com o da camada de rede. Um \nproblema com essa opção é que o hospedeiro seria \ninterrompido por cada quadro enviado à LAN, inclu-\nsive pelos destinados a outros nós na mesma LAN \nde difusão. Em resumo, para que as camadas sejam \nblocos de construção praticamente independentes \nem uma arquitetura de rede, diferentes camadas \nprecisam ter seu próprio esquema de endereça-\nmento. Já vimos até agora três tipos diferentes de \nendereços: nomes de hospedeiros para a camada \nde aplicação, endereços IP para a camada de rede \ne endereços MAC para a camada de enlace.\nPrincípios na prática\ncamada de enlace: enlaces, redes de acesso e redes locais  345 \nARP (protocolo de resolução de endereços)\nComo existem endereços da camada de rede (por exemplo, IP da Internet) e da camada de enlace (isto é, \nendereços MAC), é preciso fazer a tradução de um para o outro. Para a Internet, esta é uma tarefa do protocolo \nde resolução de endereços (Address Resolution Protocol — ARP) [RFC 826].\nPara compreender a necessidade de um protocolo como o ARP, considere a rede mostrada na Figura 5.17. \nNesse exemplo simples, cada hospedeiro e roteador tem um único endereço IP e um único endereço MAC. Como \nsempre, endereços IP são mostrados em notação decimal com pontos e endereços MAC, em notação hexadecimal. \nPara os propósitos desta discussão, vamos considerar nesta seção que o comutador transmite todos os quadros por \ndifusão; isto é, sempre que um comutador recebe um quadro em uma interface, ele o repassa para todas as suas \noutras interfaces. Na próxima seção, vamos dar uma explicação mais precisa sobre como os comutadores trabalham.\nAgora, suponha que o nó com endereço IP 222.222.222.220 queira mandar um datagrama IP para o nó \n222.222.222.222. Nesse exemplo, os nós de origem e de destino estão na mesma sub-rede, no sentido do ende-\nreçamento estudado na Seção 4.4.2. Para enviar um datagrama, o nó de origem deve dar a seu adaptador não \nsomente o datagrama IP, mas também o endereço MAC para o nó de destino 222.222.222.222. O adaptador do nó \nremetente montará então um quadro da camada de enlace contendo o endereço MAC do nó receptor e enviará \no quadro para a LAN.\nA pergunta importante considerada nesta seção é: como o nó remetente determina o endereço MAC para o \nnó com endereço IP 222.222.222.222? Como você já deve ter adivinhado, ele usa o ARP. Um módulo ARP no nó \nremetente toma como entrada qualquer endereço IP na mesma LAN e retorna o endereço MAC correspondente. \nNesse exemplo, o nó remetente 222.222.222.220 fornece a seu módulo ARP o endereço IP 222.222.222.222 e o \nmódulo ARP retorna o endereço MAC correspondente, 49-BD-D2-C7-56-2A.\nAssim, vemos que o ARP converte um endereço IP para um endereço MAC. Em muitos aspectos, o ARP é \nsemelhante ao DNS (estudado na Seção 2.5), que converte nomes de hospedeiros para endereços IP\n. Contudo, uma \nimportante diferença entre os dois conversores é que o DNS converte nomes de hospedeiros para máquinas em qual-\nquer lugar da Internet, ao passo que o ARP converte endereços IP apenas para nós na mesma sub-rede. Se um nó na \nCalifórnia tentasse usar o ARP para converter o endereço IP de um nó no Mississippi, o ARP devolveria um erro.\nAgora que já explicamos o que o ARP faz, vamos ver como ele funciona. Cada nó (hospedeiro ou roteador) \ntem em sua RAM uma tabela ARP que contém mapeamentos de endereços IP para endereços MAC. A Figura \n5.18 mostra como seria uma tabela ARP no nó 222.222.222.220. Essa tabela também contém um valor de tempo \nde vida (TTL) que indica quando cada mapeamento será apagado. Note que a tabela não contém necessariamente \num registro para cada hospedeiro da sub-rede; alguns podem jamais ter sido registrados, enquanto outros podem \nter expirado. Um tempo de remoção típico para um registro é de 20 minutos a partir do momento em que foi \ncolocado em uma tabela ARP.\nFigura 5.17  Cada interface em uma LAN tem um endereço IP e um endereço MAC\nIP:222.222.222.221\nIP:222.222.222.220\nIP:222.222.222.223\nIP:222.222.222.222\n5C-66-AB-90-75-B1\n1A-23-F9-CD-06-9B\n49-BD-D2-C7-56-2A\n88-B2-2F-54-1A-0F\nA\nB\nC\n   Redes de computadores e a Internet\n346\nSuponha agora que o hospedeiro 222.222.222.220 queira enviar um datagrama que tem endereço IP para \noutro nó daquela sub-rede. O nó remetente precisa obter o endereço MAC do nó de destino, dado o endereço \nIP daquele mesmo nó. Essa tarefa será fácil se a tabela ARP do nó remetente tiver um registro para esse nó de \ndestino. Mas, e se, naquele momento, a tabela ARP não tiver um registro para o destinatário? Em particular, \nsuponha que 222.222.222.220 queira enviar um datagrama para 222.222.222.222. Nesse caso, o remetente usa o \nprotocolo ARP para converter o endereço. Primeiro, monta um pacote especial denominado pacote ARP. Um \npacote ARP tem diversos campos, incluindo os endereços IP e MAC de envio e de recepção. Os pacotes ARP de \nconsulta e de resposta têm o mesmo formato. A finalidade do pacote de consulta ARP é pesquisar todos os outros \nhospedeiros e roteadores na sub-rede para determinar o endereço MAC correspondente ao endereço IP que está \nsendo convertido.\nVoltando a nosso exemplo, o nó 222.222.222.220 passa um pacote de consulta ARP ao adaptador junto com \numa indicação de que este deve enviar o pacote ao endereço MAC de difusão, a saber, FF-FF-FF-FF-FF-FF. O \nadaptador encapsula o pacote ARP em um quadro da camada de enlace, usa o endereço de difusão como ende-\nreço de destino do quadro e transmite o quadro para a sub-rede. Retomando nossa analogia de número do CPF/\nendereço postal, note que a consulta ARP equivale a uma pessoa gritar em uma sala cheia de baias em alguma \nempresa (digamos, a empresa AnyCorp): “Qual é o número do CPF da pessoa cujo endereço é Baia 13, Sala 112, \nAnyCorp, Palo Alto, Califórnia?”\n. O quadro que contém a consulta ARP é recebido por todos os outros adaptado-\nres na sub-rede e (por causa do endereço de difusão) cada adaptador passa o pacote ARP dentro do quadro para \no seu módulo ARP. Cada um desses módulos ARP verifica se seu endereço IP corresponde ao endereço IP de \ndestino no pacote ARP. O único nó que atende a essa condição devolve um pacote ARP de resposta ao hospedeiro \nque fez a consulta, com o mapeamento desejado. O hospedeiro que fez a consulta (222.222.222.220) pode, então, \natualizar sua tabela ARP e enviar seu datagrama IP, revestido com um quadro da camada de enlace, cujo endereço \nMAC de destino é aquele do hospedeiro ou roteador que respondeu à consulta ARP anterior.\nO protocolo ARP apresenta algumas características interessantes. Primeiro, a mensagem de consulta ARP \né enviada dentro de um quadro de difusão, ao passo que a mensagem de resposta ARP é enviada dentro de um \nquadro padrão. Antes de continuar a leitura, é bom que você pense por que isso acontece. Segundo, o ARP é do \ntipo plug-and-play, isto é, a tabela de um nó ARP é construída automaticamente — ela não tem de ser configurada \npor um administrador de sistemas. E, se um nó for desligado da sub-rede, seu registro será enfim apagado das \noutras tabelas ARP na sub-rede.\nEstudantes se perguntam se o ARP é como um protocolo da camada de enlace ou um protocolo da camada \nde rede. Como vimos, um pacote ARP é encapsulado dentro de um quadro da camada de enlace e, assim, en-\ncontra-se acima da camada de enlace, do ponto de vista da arquitetura. No entanto, um pacote ARP tem campos \nque contêm endereços da camada de rede, dessa forma é também comprovadamente um protocolo da camada de \nrede. Em suma, o ARP é talvez mais bem considerado um protocolo que fica em cima do limite entre as camadas \nde enlace e de rede — não se adequando perfeitamente na simples pilha de protocolos que estudamos no Capítulo \n1. Os protocolos do mundo real têm complexidades desse tipo!\nEnvio de um datagrama para fora da sub-rede\nAgora já deve estar claro como o ARP funciona quando um nó quer enviar um datagrama a outro nó na \nmesma sub-rede. Mas vamos examinar uma situação mais complicada, em que um hospedeiro de uma sub-rede \nFigura 5.18  Uma possível tabela ARP no nó 222.222.222.220\nEndereço IP\nEndereço MAC\nTTL\n222.222.222.221\n88-B2-2F-54-1A-0F\n13:45:00\n222.222.222.223\n5C-66-AB-90-75-B1\n13:52:00\ncamada de enlace: enlaces, redes de acesso e redes locais  347 \nquer enviar um datagrama da camada de rede para um nó que está fora da sub-rede (isto é, passa por um roteador \ne entra em outra sub-rede). Vamos discutir essa questão no contexto da Figura 5.19, que mostra uma rede simples \nconstituída de duas sub-redes interconectadas por um roteador.\nHá diversos pontos interessantes a notar na Figura 5.19. Cada hospedeiro tem exatamente um endereço IP \ne um adaptador. Mas, como vimos no Capítulo 4, um roteador tem um endereço IP para cada uma de suas inter-\nfaces. Para cada interface de roteador também há um módulo ARP (dentro do roteador) e um adaptador. Como \no roteador da Figura 5.19 tem duas interfaces, ele tem dois endereços IP, dois módulos ARP e dois adaptadores. \nÉ claro que cada adaptador na rede tem seu próprio endereço MAC.\nNote também que a Sub-rede 1 tem endereços de rede 111.111.111/24 e que a Sub-rede 2 tem endereços de \nrede 222.222.222/24. Assim, todas as interfaces conectadas à Sub-rede 1 têm o formato 111.111.111.xxx e todas \nas conectadas à Sub-rede 2 têm o formato 222.222.222.xxx.\nAgora, vamos examinar como um hospedeiro na Sub-rede 1 enviaria um datagrama a um hospedeiro na \nSub-rede 2. Especificamente, suponha que o hospedeiro 111.111.111.111 queira enviar um datagrama IP ao hos-\npedeiro 222.222.222.222. O hospedeiro remetente passa o datagrama a seu adaptador, como sempre. Mas ele deve \ntambém indicar a seu adaptador um endereço MAC de destino apropriado. E que endereço MAC o adaptador \ndeveria usar? Poderíamos arriscar o palpite de que é aquele do adaptador do hospedeiro 222.222.222.222, a saber, \n49-BD-D2-C7-56-2A. Mas esse palpite estaria errado! Se o adaptador remetente usasse aquele endereço MAC, \nnenhum dos adaptadores da Sub-rede 1 se preocuparia em passar os datagramas IP para cima, para sua camada \nde rede, já que o endereço de destino do quadro não combinaria com o endereço MAC de nenhum adaptador na \nSub-rede 1. O datagrama apenas morreria e iria para o céu dos datagramas.\nSe examinarmos cuidadosamente a Figura 5.19, veremos que, para um datagrama ir de 111.111.111.111 \naté um nó da Sub­\n‑rede 2, ele teria de ser enviado primeiro à interface de roteador 111.111.111.110, que é o \nendereço IP do roteador do primeiro salto no caminho até o destino final. Assim, o endereço MAC apropriado \npara o quadro é o endereço do adaptador para a interface de roteador 111.111.111.110, a saber, E6­\n‑E9­\n‑00­\n‑17­\n‑BB­\n‑4B. Como o hospedeiro remetente consegue o endereço MAC para a interface 111.111.111.110? Usando \no ARP, é claro! Tão logo tenha esse endereço MAC, o adaptador remetente cria um quadro (contendo o data-\ngrama endereçado para 222.222.222.222) e o envia para a Sub­\n‑rede 1. O adaptador do roteador na Sub­\n‑rede 1 \nverifica que o quadro da camada de enlace está endereçado a ele e, por conseguinte, o passa para a camada de \nrede do roteador. Viva! O datagrama IP foi transportado com sucesso do hospedeiro de origem para o rotea-\ndor! Mas não acabamos. Ainda temos de levar o datagrama do roteador até o destino. O roteador agora tem \nde determinar a interface correta para a qual o datagrama deve ser repassado. Como discutimos no Capítulo \n4, isso é feito pela consulta a uma tabela de repasse no roteador. A tabela de repasse indica o roteador para o \nqual o datagrama deve ser repassado via interface de roteador 222.222.222.220. Essa interface, então, passa o \ndatagrama a seu adaptador, que o encapsula em um novo quadro e envia o quadro para a Sub­\n‑rede 2. Dessa vez, \no endereço MAC de destino do quadro é, na verdade, o endereço MAC do destino final. E de onde o roteador \nobtém esse endereço MAC de destino? Do ARP, é claro!\nO ARP para Ethernet está definido no RFC 826. Uma boa introdução ao ARP é dada no tutorial do TCP/IP, \nRFC 1180. Exploraremos o ARP mais detalhadamente nos exercícios ao final deste capítulo.\nFigura 5.19  Duas sub-redes interconectadas por um roteador\nIP:111.111.111.110\nIP:111.111.111.111\nIP:111.111.111.112\nIP:222.222.222.221\nIP:222.222.222.222\n74-29-9C-E8-FF-55\nCC-49-DE-D0-AB-7D\nE6-E9-00-17-BB-4B\n1A-23-F9-CD-06-9B\nIP:222.222.222.220\n88-B2-2F-54-1A-0F\n49-BD-D2-C7-56-2A\n   Redes de computadores e a Internet\n348\n5.4.2  Ethernet\nA Ethernet praticamente tomou conta do mercado de LANs com fio. Na década de 1980 e início da década \nde 1990, ela enfrentou muitos desafios de outras tecnologias LAN, incluindo token ring, FDDI e ATM. Algumas \ndessas outras tecnologias conseguiram conquistar uma parte do mercado de LANs durante alguns anos. Mas, \ndesde sua invenção, em meados da década de 1970, a Ethernet continuou a se desenvolver e crescer e conservou \nsua posição dominante no mercado. Hoje, é de longe a tecnologia preponderante de LAN com fio e deve conti-\nnuar assim no futuro previsível. Podemos dizer que a Ethernet está sendo para a rede local o que a Internet tem \nsido para a rede global.\nHá muitas razões para o sucesso da Ethernet. Primeiro, ela foi a primeira LAN de alta velocidade ampla-\nmente disseminada. Como foi disponibilizada cedo, os administradores de rede ficaram bastante familiarizados \ncom a Ethernet — com suas maravilhas e sutilezas — e relutaram em mudar para outras tecnologias LAN quando \nestas apareceram em cena. Segundo, token ring, FDDI e ATM são tecnologias mais complexas e mais caras do \nque a Ethernet, o que desencorajou ainda mais os administradores na questão da mudança. Terceiro, a razão \nmais atraente para mudar para uma outra tecnologia LAN (como FDDI e ATM) era em geral a velocidade mais \nalta da nova tecnologia; contudo, a Ethernet sempre se defendeu produzindo versões que funcionavam a veloci-\ndades iguais, ou mais altas. E, também, a Ethernet comutada foi introduzida no início da década de 1990, o que \naumentou ainda mais suas velocidades efetivas de dados. Por fim, como se tornou muito popular, o hardware \npara Ethernet (em particular, adaptadores e comutadores) passou a ser mercadoria comum, de custo muito baixo.\nA LAN Ethernet original foi inventada em meados da década de 1970 por Bob Metcalfe e David Boggs, e \nusava um barramento coaxial para interconectar os nós. As topologias de barramento da Ethernet persistiram \ndurante toda a década de 1980 e até metade da década de 1990. Com uma topologia de barramento, a Ethernet é \numa LAN de transmissão por difusão — todos os quadros transmitidos movem-se para, e são processados por, \ntodos os adaptadores conectados ao barramento. Lembre-se de que vimos o protocolo de acesso múltiplo CSMA/\nCD da Ethernet com o recuo exponencial binário na Seção 5.3.2.\nNo fim da década de 1990, a maioria das empresas e universidades já tinha substituído suas LANs por ins-\ntalações Ethernet usando topologia de estrela baseada em um hub (repetidor). Nessas instalações, os hospedeiros \n(e roteadores) estão diretamente conectados a um hub com um cabo de pares trançados de cobre. Um hub é um \ndispositivo de camada física que atua sobre bits individuais e não sobre quadros. Quando um bit, representando \n0 ou 1, chega de uma interface, o hub apenas recria o bit, aumenta a energia e o transmite para todas as outras \ninterfaces. Sendo assim, Ethernet com uma topologia de estrela baseada em um hub também é uma LAN de \ndifusão — sempre que um hub recebe um bit de uma de suas interfaces, ele envia uma cópia para todas as outras \ninterfaces. Em particular, se um hub recebe quadros de duas diferentes interfaces ao mesmo tempo, ocorre uma \ncolisão e os nós que criaram os quadros precisam retransmitir.\nNo começo dos anos 2000, Ethernet passou por outra grande mudança evolucionária. As instalações \nEthernet continuaram a usar a topologia de estrela, mas o hub no núcleo foi substituído por um comutador. \nExaminaremos o comutador da Ethernet com mais atenção em outro ponto deste capítulo. Por enquanto, só \nmencionaremos que um comutador é não apenas “sem colisões”\n, mas também um autêntico comutador de paco-\ntes do tipo armazenar-e-repassar; mas, ao contrário dos roteadores, que operam até a camada 3, um comutador \nopera até a camada 2.\nEstrutura do quadro Ethernet\nPodemos aprender muito sobre a Ethernet examinando o quadro mostrado na Figura 5.20. Para colocar \nnossa discussão de quadros Ethernet em um contexto tangível, vamos considerar o envio de um datagrama IP \nde um hospedeiro a outro, estando os dois na mesma LAN Ethernet (por exemplo, a da Figura 5.17). (Embora \na carga útil do nosso quadro Ethernet seja um diagrama IP, notamos que um quadro Ethernet também pode \ncamada de enlace: enlaces, redes de acesso e redes locais  349 \ncarregar outros pacotes da camada de rede.) Seja o adaptador do remetente, adaptador A, com o endereço MAC \nAA-AA-AA-AA-AA-AA; seja o adaptador receptor, adaptador B, com o endereço MAC BB-BB-BB-BB-BB-BB. \nO adaptador remetente encapsula o datagrama IP dentro de um quadro Ethernet, o qual passa à camada física. \nO adaptador receptor recebe o quadro da camada física, extrai o datagrama IP e o passa para a de rede. Nesse \ncontexto, vamos examinar os seis primeiros campos do quadro Ethernet, como ilustrados na Figura 5.20.\nFigura 5.20  Estrutura do quadro Ethernet\nPreâmbulo\nCRC\nEndereço\nde destino\nEndereço\nde origem\nTipo\nDados\nKR 05.20.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p Wide x 3p4 Deep\n11/17/11, 11/21/11 rossi\n• Campo de dados (46 a 1.500 bytes). Esse campo carrega o datagrama IP. A unidade máxima de transferên-\ncia (MTU) da Ethernet é 1.500 bytes. Isso significa que, se o datagrama IP exceder 1.500 bytes, o hospe-\ndeiro terá de fragmentar o datagrama, como discutimos na Seção 4.4.1. O tamanho mínimo do campo de \ndados é 46 bytes. Isso significa que, se um datagrama IP tiver menos do que 46 bytes, o campo de dados \nterá de ser “preenchido” de modo a completar os 46 bytes. Quando se usa o preenchimento, os dados \npassados à camada de rede contêm preenchimento, bem como um datagrama IP. A camada de rede usa o \ncampo de comprimento do cabeçalho do datagrama IP para remover o preenchimento.\n• Endereço de destino (6 bytes). Esse campo contém o endereço MAC do adaptador de destino, BB­\n‑BB­\n‑BB­\n‑BB­\n‑BB­\n‑BB. Quando o adaptador B recebe um quadro Ethernet cujo endereço de destino é ou BB­\n‑BB­\n‑BB­\n‑BB­\n‑BB­\n‑BB, ou o endereço MAC de difusão, ele passa o conteúdo do campo de dados para a camada \nde rede. Se receber um quadro com qualquer outro endereço MAC, ele o descarta.\n• Endereço de origem (6 bytes). Esse campo contém o endereço MAC do adaptador que transmite o quadro \npara a LAN, neste exemplo, AA-AA-AA-AA-AA-AA.\n• Campo de tipo (2 bytes). O campo de tipo permite que a Ethernet multiplexe protocolos da camada \nde rede. Para entender isso, é preciso ter em mente que hospedeiros podem usar outros protocolos \nda camada de rede além do IP. Na verdade, um hospedeiro pode suportar vários protocolos da ca-\nmada de rede e usar protocolos diferentes para aplicações diversas. Por essa razão, quando o quadro \nEthernet chega ao adaptador B, o adaptador B precisa saber para qual protocolo da camada de rede \nele deve passar (isto é, demultiplexar) o conteúdo do campo de dados. O IP e outros protocolos da \ncamada de rede (por exemplo, Novell IPX ou AppleTalk) têm seu próprio número de tipo padroni-\nzado. Além disso, o protocolo ARP (discutido na seção anterior) tem seu próprio número de tipo, e \nse o quadro que chegar contiver um pacote ARP (por exemplo, tem um campo de tipo 0806 hexade-\ncimal), o pacote ARP será demultiplexado até o protocolo ARP. Note que o campo de tipo é seme-\nlhante ao campo de protocolo no datagrama da camada de rede e aos campos de número de porta \nno segmento da camada de transporte; todos eles servem para ligar um protocolo de uma camada a \num protocolo da camada acima.\n• Verificação de redundância cíclica (CRC) (4 bytes). Como discutido na Seção 5.2.3, a finalidade do campo \nde CRC é permitir que o adaptador receptor, o adaptador B, detecte se algum erro de bit foi introduzido \nno quadro.\n• Preâmbulo (8 bytes). O quadro Ethernet começa com um campo de preâmbulo de 8 bytes. Cada um dos \nprimeiros 7 bytes do preâmbulo tem um valor de 10101010; o último byte é 10101011. Os primeiros 7 \nbytes do preâmbulo servem para “despertar” os adaptadores receptores e sincronizar seus relógios com o \nrelógio do remetente. Por que os relógios poderiam estar fora de sincronia? Não esqueça que o adaptador \nA visa transmitir o quadro a 10 Mbits/s, 100 Mbits/s ou 1 Gbit/s, dependendo do tipo de LAN Ethernet. \nContudo, como nada é absolutamente perfeito, o adaptador A não transmitirá o quadro exatamente à \n   Redes de computadores e a Internet\n350\nmesma velocidade-alvo; sempre haverá alguma variação em relação a ela, uma variação que não é conhe-\ncida a priori pelos outros adaptadores na LAN. Um adaptador receptor pode sincronizar com o relógio \ndo adaptador A apenas sincronizando os bits dos primeiros 7 bytes do preâmbulo. Os dois últimos bits \ndo oitavo byte do preâmbulo (os primeiros dois 1s consecutivos) alertam o adaptador B de que “algo \nimportante” está chegando.\nTodas as tecnologias Ethernet fornecem serviço não orientado para conexão à camada de rede. Isto é, \nquando o adaptador A quer enviar um datagrama ao adaptador B, o adaptador A encapsula o datagrama em \num quadro Ethernet e envia o quadro à LAN, sem se apresentar previamente a B. Esse serviço de camada 2 \nnão orientado para conexão é semelhante ao serviço de datagrama de camada 3 do IP e ao serviço de camada \n4 não orientado para conexão do UDP.\nAs tecnologias Ethernet fornecem um serviço não confiável à camada de rede. Especificamente, quando o \nadaptador B recebe um quadro do adaptador A, ele submete o quadro a uma verificação de CRC, mas não envia \num reconhecimento quando um quadro passa na verificação de CRC nem um reconhecimento negativo quando \no quadro não passa na verificação. Quando um quadro não passa na verificação de CRC, o adaptador B simples-\nmente o descarta. Assim, o adaptador A não tem a mínima ideia se o quadro que transmitiu passou na verificação \nde CRC. Essa falta de transporte confiável (na camada de enlace) ajuda a tornar a Ethernet simples e barata. Mas \ntambém significa que a sequência de datagramas passada à camada de rede pode ter lacunas.\nSe houver lacunas por causa de quadros Ethernet descartados, a aplicação no hospedeiro B também verá \nessas lacunas? Como aprendemos no Capítulo 3, isso depende exclusivamente de a aplicação estar usando UDP \nou TCP. Se estiver usando UDP, então a aplicação no hospedeiro B verá de fato lacunas nos dados. Por outro lado, \nse a aplicação estiver usando TCP, então o TCP no hospedeiro B não reconhecerá os dados contidos em quadros \ndescartados, fazendo que o TCP no hospedeiro A retransmita. Note que, quando o TCP retransmite dados, estes \neventualmente retornarão ao adaptador Ethernet no qual foram descartados. Assim, nesse sentido, a Ethernet \nBob Metcalfe e a Ethernet\nQuando era estudante de doutorado na Universida-\nde Harvard, no início da década de 1970, Bob Metcalfe \ntrabalhava na ARPAnet no MIT. Durante seus estudos, \nele tomou conhecimento do trabalho de Abramson com \no ALOHA e os protocolos de acesso aleatório. Após ter \nconcluído seu doutorado e pouco antes de começar um \ntrabalho no PARC (Palo Alto Research Center) da Xerox, \nfez uma visita de três meses a Abramson e seus colegas \nda Universidade do Havaí, quando pôde observar, em \nprimeira mão, a ALOHAnet. No PARC da Xerox, Met-\ncalf conheceu os computadores Alto, que, em muitos \naspectos, foram os predecessores dos equipamentos \npessoais da década de 1980. Ele entendeu a necessi-\ndade de montar esses computadores em rede de um \nmodo que não fosse dispendioso. Assim, munido com \no que conhecia sobre ARPAnet, ALOHAnet e protoco-\nlos de acesso aleatório, Metcalfe, junto com seu colega \nDavid Boggs, inventou a Ethernet.\nA Ethernet original de Metcalfe e Boggs executa-\nva a 2,94 Mbits/s e interligava até 256 hospedeiros a \ndistâncias de até 1,5 km. Metcalfe e Boggs conse-\nguiram que a maioria dos pesquisadores do PARC \nda Xerox se comunicasse por meio de seus com-\nputadores Alto. Então, Metcalfe forjou uma aliança \nentre a Xerox, a Digital e a Intel para estabelecer a \nEthernet de 10 Mbits/s como padrão, ratificado pelo \nIEEE. A Xerox não demonstrou muito interesse em \ncomercializar a Ethernet. Em 1979, Metcalfe abriu \nsua própria empresa, a 3Com, para desenvolver e \ncomercializar tecnologia de rede, incluindo a tecno-\nlogia Ethernet. Em particular, a 3Com desenvolveu \ne comercializou placas Ethernet no início da década \nde 1980 para os então popularíssimos PCs da IBM. \nMetcalfe deixou a 3Com em 1990, quando a em-\npresa tinha dois mil funcionários e 400 milhões de \ndólares de receita.\nHistória\ncamada de enlace: enlaces, redes de acesso e redes locais  351 \nrealmente retransmite dados, embora não saiba se está transmitindo um datagrama novo com dados novos, ou \num datagrama que contém dados que já foram transmitidos pelo menos uma vez.\nTecnologias Ethernet\nEm nossa discussão anterior, nos referimos à Ethernet como se fosse um único protocolo-padrão. Mas, \nna verdade, ela aparece em diferentes versões, com acrônimos um pouco confusos como 10BASE-T, 10BASE-2, \n100BASE-T, 1000BASE-LX e 10GBASE-T. Essas e muitas outras tecnologias Ethernet foram padronizadas através \ndos anos pelos grupos de trabalho IEEE 802.3. Apesar de esses acrônimos parecerem confusos, existe uma ordem \nseguida. A primeira parte do acrônimo se refere à velocidade-padrão: 10, 100, 1.000 ou 10G, por 10 Megabits (por \nsegundo), 100 Megabits e 10 Gigabits Ethernet, respectivamente. “BASE” se refere à banda-base, significando que \na mídia física só suporta o tráfego da Ethernet; quase todos os padrões 802.3 são para banda-base. A parte final \ndo acrônimo se refere à mídia física em si; a Ethernet é uma camada de enlace e uma camada física que inclui um \ncabo coaxial, fio de cobre e fibra. Em geral um “T” se refere a um cabo de par trançado de fios de cobre.\nHistoricamente, uma Ethernet era de início concebida como um segmento de um cabo coaxial. Os primei-\nros padrões 10BASE-2 e 10BASE-5 especificavam a Ethernet a 10 Mbits/s sobre dois tipos de cabos coaxiais, cada \num limitado a um comprimento de 500 m. Extensões mais longas podiam ser obtidas usando um repetidor — \num dispositivo da camada de enlace que recebe um sinal no lado de entrada, e regenera o sinal no lado de saída. \nUm cabo coaxial, como na Figura 5.20, corresponde muito bem a nossa visão da Ethernet como um meio de \ndifusão — todos os quadros transmitidos por uma interface são recebidos em outras interfaces, e seu protocolo \nCDMA/CD resolve de maneira satisfatória o problema de acesso múltiplo. Os nós são apenas conectados ao cabo, \ne voilà, temos uma rede local!\nA Ethernet passou por uma série de etapas de evolução ao longo dos anos, e a atual é muito diferente do \nprojeto original da topologia de barramento que usava cabos coaxiais. Na maioria das instalações de hoje, os nós \nsão conectados a um comutador via segmentos ponto a ponto feitos de cabos de pares trançados de fios de cobre \nou cabos de fibra ótica, como demonstrado nas figuras 5.15-5.17.\nNo meio da década de 1990, a Ethernet foi padronizada em 100 Mbits/s, dez vezes mais rápida do que a \nde 10 Mbits/s. O formato de quadro e o protocolo Ethernet MAC original foram preservados, mas camadas \nfísicas de alta velocidade foram definidas para fios de cobre (100BASE­\n‑T) e fibra (100BASE­\n‑FX, 100BASE­\n‑SX, \n100BASE­\n‑BX). A Figura 5.21 mostra esses diferentes padrões e os formatos de quadro e protocolo Ethernet MAC \ncomum. A Ethernet de 100 Mpbs é limitada a 100 m de distância por um cabo de par trançado, e vários quilôme-\ntros por fibra, o que permite a conexão de comutadores Ethernet em diferentes prédios.\nA Gigabit Ethernet é uma extensão dos muito bem-sucedidos padrões 10 Mbits/s e 100 Mbits/s. Oferecendo \numa velocidade bruta de 1.000 Mbits/s, mantém total compatibilidade com a imensa base instalada de equipa-\nmentos Ethernet. O padrão para a Gigabit Ethernet, formalmente conhecido como IEEE 802.3z, faz o seguinte:\n• Usa o formato-padrão do quadro Ethernet (Figura 5.20) e é compatível com as tecnologias 10BASE-T \ne 100BASE-T. Isso permite fácil integração da Gigabit Ethernet com a base instalada de equipamentos \nEthernet.\nFigura 5.21  \u0007\nPadrões Ethernet de 100 Mbits/s: uma camada de enlace comum, diferentes \ncamadas físicas\nFísica\nTransporte\nRede\nEnlace\nAplicação\nKR 05.21.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n100BASE-TX\n100BASE-T4\n100BASE-T2\nProtocolo MAC e\nformato de quadro\n100BASE-SX\n100BASE-FX\n100BASE-BX\n   Redes de computadores e a Internet\n352\n• Permite enlaces ponto a ponto, bem como canais de difusão compartilhados. Tais enlaces usam comuta-\ndores, ao passo que canais de difusão usam hubs, como descrito antes. No jargão da Gigabit Ethernet, os \nhubs são denominados distribuidores com buffer.\n• Utiliza CSMA/CD para canais de difusão compartilhados. Para conseguir eficiência aceitável, a distância \nmáxima entre os nós deve ser severamente limitada.\n• Permite operação full-duplex a 1.000 Mbits/s em ambas as direções para canais ponto a ponto.\nNo início operando através de fibra ótica, a Gigabit Ethernet está disponível para ser instalada por meio de \ncabeamento UTP categoria 5. A Ethernet a 10 Gbits/s (10GBASE-T) foi padronizada em 2007, oferecendo uma \ncapacidade de LAN Ethernet ainda mais alta.\nVamos concluir nossa discussão sobre a tecnologia Ethernet considerando uma dúvida que pode estar in-\ncomodando você. Na época da topologia de barramento e da topologia de estrela baseada em hub, a Ethernet era \nevidentemente um enlace de difusão (como definido na Seção 5.3), onde colisões de quadro ocorriam quando \nnós transmitiam ao mesmo tempo. Para lidar com essas colisões, o padrão Ethernet incluiu o protocolo CSMA/\nCD, que é de particular eficácia para transmissões de LAN abrangendo uma pequena região geográfica. Mas se o \nuso atual prevalente da Ethernet é baseado em comutadores com a topologia de estrela, usando o modo de comu-\ntação armazenar-e-repassar, existe mesmo a necessidade de se usar um protocolo Ethernet MAC? Como veremos \nem breve, um comutador coordena suas transmissões e nunca repassa mais de um quadro por vez na mesma in-\nterface. Além disso, comutadores modernos são full-duplex, de modo que um comutador e um nó possam enviar \nquadros um ao outro ao mesmo tempo sem interferência. Em outras palavras, em uma LAN Ethernet baseada em \ncomutador, não há colisões e, portanto, não existe a necessidade de um protocolo MAC!\nComo vimos, a Ethernet atual é muito diferente da original concebida por Metcalfe e Boggs há mais de 30 \nanos — as velocidades tiveram um aumento de três ordens de grandeza, os quadros Ethernet são transportados \npor uma variedade de mídias, as Ethernets comutadas se tornaram dominantes e até mesmo o protocolo MAC \né muitas vezes desnecessário! Será que tudo isso realmente ainda é Ethernet? A resposta é, obviamente, “sim, \npor definição”\n. No entanto, é interessante observar que, por todas essas mudanças, existiu uma constante que \ncontinuou inalterada por mais de 30 anos — o formato do quadro Ethernet. Talvez esta seja a única peça central \nverdadeira e eterna do padrão Ethernet.\n5.4.3  Comutadores da camada de enlace\nAté aqui, temos sido deliberadamente vagos sobre como um comutador* trabalha e o que ele faz. A função \nde um comutador é receber quadros da camada de enlace e repassá-los para enlaces de saída; estudaremos essa \nfunção de repasse detalhadamente em breve. O comutador em si é transparente aos hospedeiros e roteadores \nna sub-rede; ou seja, um nó endereça um quadro a outro nó (em vez de endereçar o quadro ao comutador) que \nalegremente envia o quadro à LAN, sem saber que um comutador receberá o quadro e o repassará. A velocidade \ncom que os quadros chegam a qualquer interface de saída do comutador pode temporariamente exceder a capa-\ncidade do enlace daquela interface. Para resolver esse problema, interfaces de saídas do comutador têm buffers, \nda mesma forma que uma interface de saída de um roteador tem buffers para datagramas. Vamos agora observar \nmais atentamente o funcionamento de um comutador.\nRepasse e filtragem\nFiltragem é a capacidade de um comutador que determina se um quadro deve ser repassado para alguma \ninterface ou se deve apenas ser descartado. Repasse é a capacidade de um comutador que determina as interfaces \npara as quais um quadro deve ser dirigido e então dirigir o quadro a essas interfaces. Filtragem e repasse por \n*\t Embora o termo “switch” seja jargão corrente entre os profissionais de rede preferimos traduzi-lo por “comutador”\n, já que “switch” pode ser \nempregado em diferentes sentidos e não foi padronizado pelo IEEE (N. R.)\ncamada de enlace: enlaces, redes de acesso e redes locais  353 \ncomutadores são feitos com uma tabela de comutação. A tabela de comutação contém registros para alguns hos-\npedeiros e roteadores da LAN, mas não necessariamente para todos. Um registro na tabela de comutação contém \n(1) o endereço MAC, (2) a interface do comutador que leva em direção a esse endereço MAC e (3) o horário em \nque o registro foi colocado na tabela. Um exemplo de tabela de comutação para a LAN da Figura 5.15 é mostra-\ndo na Figura 5.22. Embora essa descrição de repasse de quadros possa parecer semelhante à nossa discussão de \nrepasse de datagramas no Capítulo 4, logo veremos que há diferenças significativas. Uma diferença importante é \nque os comutadores repassam pacotes baseados em endereços MAC, em vez de endereços IP. Também veremos \nque uma tabela de comutação é montada de maneira diferente da tabela de roteamento de um roteador.\nFigura 5.22  \u0007\nParte de uma tabela de comutação para o comutador mais acima na Figura 5.15\nEndereço\nInterface\nHorário\n62-FE-F7-11-89-A3\n1\n9:32\n7C-BA-B2-B4-91-10\n3\n9:36\n....\n....\n....\nPara entender como funcionam a filtragem e o repasse por comutadores, suponha que um quadro com en-\ndereço de destino DD-DD-DD-DD-DD-DD chegue ao comutador na interface x. O comutador indexa sua tabela \ncom o endereço MAC DD-DD-DD-DD-DD-DD. Há três casos possíveis:\n• Não existe entrada na tabela para DD-DD-DD-DD-DD-DD. Nesse caso, o comutador repassa cópias do \nquadro para os buffers de saída que precedem todas as interfaces, exceto a interface x. Em outras palavras, \nse não existe entrada para o endereço de destino, o comutador transmite o quadro por difusão.\n• Existe uma entrada na tabela, associando DD-DD-DD-DD-DD-DD com a interface x. Nesse caso, o \nquadro está vindo de um segmento da LAN que contém o adaptador DD-DD-DD-DD-DD-DD. Não \nhavendo necessidade de repassar o quadro para qualquer outra interface, o comutador realiza a função \nde filtragem ao descartar o quadro.\n• Existe uma entrada na tabela, associando DD-DD-DD-DD-DD-DD com a interface y ≠ x. Nesse caso, \no quadro precisa ser repassado ao segmento da LAN conectado à interface y. O comutador realiza sua \nfunção de repasse ao colocar o quadro em um buffer de saída que precede a interface y.\nVamos examinar essas regras para a rede da Figura 5.15 e sua tabela de comutação na Figura 5.22 Suponha \nque um quadro com endereço de destino 62-FE-F7-11-89-A3 chegue ao comutador vindo da interface 1. O \ncomutador examina sua tabela e vê que o destino está no segmento de LAN conectado à interface 1 (isto é, do \ndepartamento de engenharia elétrica). Isso significa que o quadro já foi transmitido por difusão no segmento de \nLAN que contém o destino. Por conseguinte, o comutador filtra (isto é, descarta) o quadro. Agora suponha que \num quadro com o mesmo endereço de destino chegue da interface 2. O comutador novamente examina sua tabe-\nla e verifica que o destino está na direção da interface 1; por conseguinte, ele repassa o quadro para o buffer de saí-\nda que precede a interface 1. Com este exemplo, deve ficar claro que, enquanto a tabela de comutação permanecer \ncompleta e precisa, o comutador encaminha quadros até seus destinos sem qualquer transmissão por difusão.\nAssim, nesse sentido, o comutador é “mais esperto” do que um hub. Mas como uma tabela de comutação é \nconfigurada afinal? Existem equivalentes de camadas de enlace a protocolos de roteamento da camada de rede? \nOu um gerente sobrecarregado de serviço deve configurar manualmente a tabela de comutação?\nAutoaprendizagem\nUm comutador tem a maravilhosa propriedade (em especial, para o administrador de rede, que quase sem-\npre está sobrecarregado) de montar sua tabela de modo automático, dinâmico e autônomo — sem nenhuma \nintervenção de um administrador de rede ou de um protocolo de configuração. Em outras palavras, comutadores \nsão autodidatas. Essa capacidade é obtida da seguinte forma:\n   Redes de computadores e a Internet\n354\n1.\t A tabela de comutação inicialmente está vazia.\n2.\t Para cada quadro recebido em uma interface, o comutador armazena em sua tabela (1) o endereço MAC \nque está no campo de endereço de origem do quadro, (2) a interface da qual veio o quadro e (3) o horário \ncorrente. Dessa maneira, o comutador registra em sua tabela o segmento da LAN no qual reside o nó re-\nmetente. Se cada hospedeiro na LAN mais cedo ou mais tarde enviar um quadro, então cada um deles por \nfim estará registrado na tabela.\n3.\t O comutador apagará um endereço na tabela se nenhum quadro que tenha aquele endereço como ende-\nreço de origem for recebido após certo período de tempo (o tempo de envelhecimento). Desse modo, se \num PC for substituído por outro (com um adaptador diferente), o endereço MAC do PC original acabará \nsendo expurgado da tabela de comutação.\nVamos examinar a propriedade de aprendizagem automática para a rede da Figura 5.15 e sua tabela de co-\nmutação correspondente, apresentada na Figura 5.22. Suponha que, no horário 9h39min, um quadro com ende-\nreço de origem 01-12-23-34-45-56 venha da interface 2. Suponha também que esse endereço não esteja na tabela \nde comutação. Então, o comutador anexa um novo registro à tabela, conforme mostra a Figura 5.23.\nContinuando com esse mesmo exemplo, suponha ainda que o tempo de envelhecimento para esse comuta-\ndor seja 60 minutos e que nenhum quadro com endereço de origem 62­\n‑FE­\n‑F7­\n‑11­\n‑89­\n‑A3 chegue ao comutador \nentre 9h32min e 10h32min. Então, no horário 10h32min, o comutador remove esse endereço de sua tabela.\nComutadores são dispositivos do tipo plug-and-play porque não requerem a intervenção de um admi-\nnistrador ou de um usuário da rede. Um administrador de rede que quiser instalar um comutador não precisa \nfazer nada mais do que conectar os segmentos de LAN às interfaces do comutador. O administrador não pre-\ncisa configurar as tabelas de comutação na hora da instalação nem quando um hospedeiro é removido de um \ndos segmentos de LAN. Comutadores também são full-duplex, ou seja, qualquer interface do comutador pode \nenviar e receber ao mesmo tempo.\nFigura 5.23  \u0007\nO comutador aprende a localização do adaptador com endereço  \n01-12-23-34-45-56\nEndereço\nInterface\nHorário\n01-12-23-34-45-56\n2\n9:39\n62-FE-F7-11-89-A3\n1\n9:32\n7C-BA-B2-B4-91-10\n3\n9:36\n....\n....\n....\nPropriedades de comutação da camada de enlace\nTendo descrito as operações básicas da comutação da camada de enlace, vamos considerar suas proprie-\ndades e funcionalidades. Podemos identificar diversas vantagens no uso de comutadores, em vez de se usarem \nenlaces de difusão como barramentos ou topologias de estrela baseadas em hub:\n• Eliminação de colisões: Em uma LAN montada com comutadores (e sem hubs), não existe desperdício de \nbanda causado por colisões! Os comutadores armazenam os quadros e nunca transmitem mais de um \nquadro em um segmento ao mesmo tempo. Como em um roteador, a vazão máxima agregada de um co-\nmutador é a soma da velocidade de todas as interfaces do comutador. Portanto, os comutadores oferecem \numa melhoria de desempenho significativa em relação às LANs com enlaces de difusão.\n• Enlaces heterogêneos: Uma vez que o comutador isola um enlace do outro, os diferentes enlaces na \nLAN conseguem operar em diferentes velocidades e podem ser executados por diferentes mídias. \n \nPor exemplo, o comutador mais acima na Figura 5.22 poderia ter três enlaces de cobre 1000BASE-T de \ncamada de enlace: enlaces, redes de acesso e redes locais  355 \n1 Gbit/s, dois enlaces de fibra 100BASE-FX de 100 Mbits/s e um enlace de cobre 100BASE-T. Assim, \num comutador é ideal para misturar equipamento legado e novo.\n• Gerenciamento. Além de oferecer mais segurança (ver a nota “Foco na segurança”), um comutador tam-\nbém facilita o gerenciamento da rede. Por exemplo, se um adaptador apresenta defeito e envia continua-\nmente quadros Ethernet (chamado adaptador tagarela), um comutador pode detectar o problema e des-\nconectar internamente o adaptador com defeito. Com esse recurso, o administrador da rede não precisa \nse levantar de madrugada e dirigir até o trabalho para corrigir o problema. De modo semelhante, um \ncabo cortado desconecta apenas o hospedeiro que o estava usando para conectar o comutador. Nos dias \ndo cabo coaxial, muitos gerentes de rede gastavam horas “percorrendo as linhas” (ou, mais precisamente, \n“arrastando-se pelo chão”) para achar a interrupção que paralisou a rede inteira. Conforme discutimos \nno Capítulo 9 (Gerenciamento de rede), os comutadores também colhem estatísticas sobre uso da largu-\nra de banda, taxas de colisão e tipos de tráfego, e tornam essa informação disponível para o gerente da \nrede. Tal informação pode ser usada para depurar e corrigir problemas, além de planejar como a LAN \ndeverá evoluir no futuro. Os pesquisadores estão explorando a inclusão de ainda mais funcionalidade de \ngerenciamento para as LANs Ethernet nos protótipos implementados [Casado, 2007; Koponen, 2011].\nComutadores versus roteadores\nComo aprendemos no Capítulo 4, roteadores são comutadores de pacotes do tipo armazena-e-repassa, que \ntransmitem pacotes usando endereços da camada de rede. Embora um comutador também seja um comutador \nde pacotes do tipo armazena-e-repassa, ele é em essência diferente de um roteador, pois repassa pacotes usando \nendereços MAC. Enquanto um roteador é um comutador de pacotes da camada 3, um comutador opera com \nprotocolos da camada 2.\nMesmo sendo fundamentalmente diferentes, é comum que os administradores de rede tenham de optar entre \num comutador e um roteador ao instalar um dispositivo de interconexão. Por exemplo, para a rede da Figura 5.15, \nAnalisando uma LAN comutada: envenenamento de comutador\nQuando um hospedeiro é conectado a um comu-\ntador, em geral só recebe quadros que estão sendo \nenviados explicitamente a ele. Por exemplo, conside-\nre uma LAN comutada na Figura 5.17. Quando o hos-\npedeiro A envia um quadro ao hospedeiro B, e há um \nregistro para B na tabela de comutação, então o comu-\ntador repassa o quadro somente para B. Se o hospe-\ndeiro C estiver executando um analisador de quadros, \no hospedeiro C não poderá analisar esse quadro de \nA-para-B. Assim, em um ambiente de LAN comutada \n(ao contrário de um ambiente de enlace de difusão, \ncomo LANs 802.11 ou LANs Ethernet baseadas em \nhub), é mais difícil que um invasor analise quadros. \nPorém, como o comutador envia por difusão quadros \nque possuem endereços de destino que não estão na \ntabela de comutação, o invasor em C ainda poderá \nsondar alguns quadros que não são endereçados de \nmodo explícito a C. Além disso, um farejador poderá \nvasculhar todos os quadros de difusão Ethernet com \nendereço de destino de difusão FF–FF–FF–FF–FF–FF\n. \nUm ataque bem conhecido contra um comutador, de-\nnominado envenenamento de comutador, é enviar \numa grande quantidade de pacotes ao comutador com \nmuitos endereços MAC de origem falsos e diferentes, \nenchendo assim a tabela de comutação com registros \nfalsos e não deixando espaço para os endereços MAC \ndos hospedeiros legítimos. Isso faz o comutador enviar \na maioria dos quadros por difusão, podendo então ser \napanhados pelo esquadrinhador [Skoudis, 2006]. Vis-\nto que esse ataque é bem complexo, mesmo para um \ninvasor sofisticado, os comutadores são muito menos \nvulneráveis à análise do que os hubs e as LANs sem fio.\nSegurança em foco\n   Redes de computadores e a Internet\n356\no administrador de rede poderia com facilidade ter optado por usar um roteador, em vez de um comutador, para \nconectar as LANs de departamento, servidores e roteador de borda da Internet. Na verdade, um roteador per-\nmitiria a comunicação interdepartamental sem criar colisões. Dado que ambos, comutadores e roteadores, são \ncandidatos a dispositivos de interconexão, quais são os prós e os contras das duas técnicas?\nVamos considerar, inicialmente, os prós e os contras de comutadores. Como já dissemos, comutadores são do \ntipo plug-and-play, uma propriedade que é apreciada por todos os administradores de rede atarefados do mundo. \nEles também podem ter velocidades relativamente altas de filtragem e repasse — como ilustra a Figura 5.24, têm de \nprocessar quadros apenas até a camada 2, enquanto roteadores têm de processar pacotes até a camada 3. Por outro \nlado, para evitar a circulação dos quadros por difusão, a topologia de uma rede de comutação está restrita a uma \nspanning tree. E mais, uma rede de comutação de grande porte exigiria, nos hospedeiros e roteadores, tabelas ARP \ntambém grandes, gerando tráfego e processamento ARP substanciais. Além do mais, comutadores são suscetíveis a \ntempestades de difusão — se um hospedeiro se desorganiza e transmite uma corrente sem fim de quadros Ethernet \npor difusão, os comutadores repassam todos esses quadros, causando o colapso da rede inteira.\nAgora, vamos considerar os prós e os contras dos roteadores. Como na rede o endereçamento muitas vezes é \nhierárquico (e não linear, como o MAC), os pacotes em geral não ficam circulando nos roteadores, mesmo quan-\ndo a rede tem trajetos redundantes. (Na verdade, eles podem circular quando as tabelas de roteadores estão mal \nconfiguradas; mas, como aprendemos no Capítulo 4, o IP usa um campo de cabeçalho de datagrama especial para \nlimitar a circulação.) Assim, pacotes não ficam restritos a uma topologia de spanning tree e podem usar o melhor \ntrajeto entre origem e destino. Como roteadores não sofrem essa limitação, eles permitiram que a Internet fosse \nmontada com uma topologia rica que inclui, por exemplo, múltiplos enlaces ativos entre a Europa e a América \ndo Norte. Outra característica dos roteadores é que eles fornecem proteção de firewall contra as tempestades de \ndifusão da camada 2. Talvez sua desvantagem mais significativa seja o fato de não serem do tipo plug-and-play \n— eles e os hospedeiros que a eles se conectam precisam de seus endereços IP para ser configurados. Além disso, \nroteadores muitas vezes apresentam tempo de processamento por pacote maior do que comutadores, pois têm de \nprocessar até os campos da camada 3. \nDado que comutadores e roteadores têm seus prós e contras (como resumido na Tabela 5.1), quando uma \nrede institucional (por exemplo, de um campus universitário ou uma rede corporativa) deveria usar comutadores \ne quando deveria usar roteadores? Em geral, redes pequenas, com algumas centenas de hospedeiros, têm uns \npoucos segmentos de LAN. Para essas, comutadores serão satisfatórios, pois localizam o tráfego e aumentam a \nvazão agregada sem exigir nenhuma configuração de endereços IP. Mas redes maiores, com milhares de hospe-\ndeiros, em geral incluem roteadores (além de comutadores). Roteadores fornecem isolamento de tráfego mais \nrobusto, controlam tempestades de difusão e usam rotas “mais inteligentes” entre os hospedeiros da rede.\nPara mais informações sobre os prós e os contras das redes comutadas e roteadas, bem como sobre como \na tecnologia de LAN comutada pode ser estendida para acomodar duas ordens de magnitude de hospedeiros a \nmais que a Ethernet atual, consulte Meyers [2004]; Kim [2008].\nFigura 5.24  Processamento de pacotes em comutadores, roteadores e hospedeiros\nHospedeiro\nAplicação\nHospedeiro\nTransporte\nRede\nEnlace\nFísica\nEnlace\nFísica\nRede\nComutador\nRoteador\nEnlace\nFísica\nAplicação\nTransporte\nRede\nEnlace\nFísica\ncamada de enlace: enlaces, redes de acesso e redes locais  357 \n5.4.4  Redes locais virtuais (VLANs)\nNa discussão anterior sobre a Figura 5.15, notamos que as LANs institucionais modernas com frequência \nsão configuradas hierarquicamente, cada grupo de trabalho (departamento) tendo seu próprio comutador de \nLAN conectado ao comutador de LAN de outros grupos via uma hierarquia de comutadores. Embora tal confi-\nguração funcione bem em um mundo ideal, o mundo real é bem diferente. Três desvantagens podem ser identi-\nficadas na configuração da Figura 5.15:\n• Falta de isolamento do tráfego. Apesar de a hierarquia localizar o tráfego de grupos dentro de um único \ncomutador, o tráfego de difusão (por exemplo, quadros carregando mensagens ARP e DHCP ou quadros \ncom endereços de destino que ainda não foram descobertos por um comutador com a autoaprendiza-\ngem) tem que ainda percorrer toda a rede institucional. Limitar o escopo desse tráfego de difusão apri-\nmoraria o desempenho da LAN. Talvez mais importante que isso, também seria desejável limitar esse \ntráfego por razões de segurança e privacidade. Por exemplo, se um grupo contém a equipe da gerência \nexecutiva de uma empresa e outro grupo contém funcionários decepcionados executando o analisador \nde pacotes Wireshark, o gerente de rede talvez prefira que o tráfego da gerência executiva não alcance \nos computadores dos funcionários. Esse tipo de isolamento pode ser substituído trocando o comutador \ncentral da Figura 5.15 por um roteador. Em breve veremos que esse isolamento também pode ser reali-\nzado por meio de uma solução de comutação (camada 2).\n• Uso ineficiente de comutadores. Se, em vez de três, a instituição tivesse dez grupos, os dez comutadores \nde primeiro nível seriam necessários. Se cada grupo fosse pequeno, digamos, com menos de dez pessoas, \num comutador de 96 portas seria suficiente para atender a todos, mas esse único comutador não fornece \nisolamento de tráfego.\n• Gerenciamento de usuários. Se um funcionário se locomove entre os grupos, o cabeamento físico deve \nser mudado para conectá-lo a um comutador diferente na Figura 5.15. Funcionários pertencentes a dois \ngrupos dificultam o problema.\nFelizmente, cada uma dessas dificuldades pode ser resolvida com um comutador que suporte Redes Locais \nVirtuais (VLANs). Como o nome já sugere, um comutador que suporta VLANs permite que diversas redes \nlocais virtuais sejam executadas por meio de uma única infraestrutura física de uma rede local virtual. Hospe-\ndeiros dentro de uma VLAN se comunicam como se eles (e não outros hospedeiros) estivessem conectados ao \ncomutador. Em uma VLAN baseada em portas, as portas (interfaces) do comutador são divididas em grupos \npelo gerente da rede. Cada grupo constitui uma VLAN, com as portas em cada VLAN formando um domínio de \ndifusão (por exemplo, o tráfego de difusão de uma porta só pode alcançar outras portas no grupo). A Figura 5.25 \nmostra um único comutador com 16 portas. As interfaces de 2 a 8 pertencem à VLAN EE, enquanto as de 9 a 15 \npertencem à VLAN CS (portas de 1 e 16 não são atribuídas). Essa VLAN soluciona todas as dificuldades citadas \n— quadros de VLAN EE e CS são isolados uns dos outros, os dois comutadores na Figura 5.15 foram substituídos \npor um único comutador, e se o usuário da porta de comutador 8 se juntar ao Departamento CS, o operador da \nrede apenas reconfigura o software da VLAN para que a porta 8 seja associada com a VLAN CS. Qualquer um \npoderia imaginar facilmente como o comutador VLAN opera e é configurado — o gerente de rede declara uma \ninterface pertencente a uma dada VLAN (com portas não declaradas pertencentes à VLAN padrão) usando um \nTabela 5.1  \u0007\nComparação entre as características típicas de dispositivos de interconexão \npopulares\nHubs\nRoteadores\nComutadores\nIsolamento de tráfego \nNão\nSim\nSim\nPlug-and-play\nSim\nNão\nSim\nRoteamento ideal\nNão\nSim\nNão\n   Redes de computadores e a Internet\n358\nsoftware de gerenciamento de comutadores, uma tabela de mapeamento porta-VLAN é mantida dentro do co-\nmutador; e um hardware de comutação somente entrega quadros entre as portas pertencentes à mesma VLAN.\nMas, isolando completamente as duas VLANs, criamos um novo problema! Como o tráfego do departa-\nmento EE pode ser enviado para o departamento CS? Uma maneira de se lidar com isso seria conectar uma porta \nde comutação da VLAN (exemplo, porta 1 na Figura 5.25) a um roteador externo, configurando aquela porta \npara que pertença às VLANs de ambos os departamentos EE e CS. Nesse caso, apesar de eles compartilharem um \nmesmo roteador físico, a configuração faria parecer que têm comutadores diferentes conectados por um roteador. \nUm datagrama IP, indo do departamento EE para o CS, passaria primeiro pela VLAN EE para alcançar o roteador \ne depois seria encaminhado de volta pelo roteador através da VLAN CS até o hospedeiro CS. Felizmente, os for-\nnecedores de comutadores facilitam as configurações para o gerente de rede. Eles montam um dispositivo único \nque contém um comutador VLAN e um roteador, para que um roteador externo não seja necessário. Uma lição \nde casa ao final do capítulo explora esse exemplo mais em detalhes.\nVoltando à Figura 5.15, vamos supor que, em vez de termos um departamento separado de Engenharia da \nComputação, parte do corpo docente de EE e de CS esteja alojada em um prédio separado, onde (é claro!) eles \nprecisariam de acesso à rede, e (é claro!) gostariam de fazer parte do VLAN de seu departamento. A Figura 5.26 \nmostra um segundo comutador com 8 entradas, onde as entradas do comutador foram definidas como perten-\ncentes à VLAN EE ou CS, conforme necessário. Mas como esses dois comutadores seriam interconectados? Uma \nsolução fácil seria definir uma entrada como pertencente à VLAN CS em cada comutador (e da mesma forma \npara a VLAN EE) e conectá-las umas às outras, como demonstrado na Figura 5.26(a). No entanto, essa solução \nnão permite crescimento, já que N VLANs exigiriam N portas em cada comutador para simplesmente interco-\nnectar os dois comutadores.\nUma abordagem mais escalável para interconectar os comutadores das VLANs é conhecido como entronca-\nmento de VLANs. Na técnica de entrocamento de VLANs, mostrada na Figura 5.26(b), uma porta especial em cada \ncomutador (porta 16 no comutador esquerdo e porta 1 no direito) é configurada como uma porta de tronco para \ninterconectar os dois comutadores da VLAN. A porta de tronco pertence a todas as VLANs, e quadros enviados \na qualquer VLAN são encaminhados pelo enlace de tronco ao outro comutador. Mas isso gera mais uma dúvida: \ncomo um comutador “sabe” que um quadro que está chegando a uma porta de tronco pertence a uma VLAN espe-\ncífica? O IEEE definiu um formato de quadro estendido, 802.1Q, para quadros atravessando o tronco VLAN. Con-\nforme mostrado na Figura 5.27, o quadro 802.1Q consiste no quadro padrão Ethernet com um rótulo de VLAN \nde quatro bytes adicionado no cabeçalho que transporta a identidade da VLAN à qual o quadro pertence. O rótulo \nda VLAN é adicionado ao quadro pelo comutador no lado de envio do tronco de VLAN, analisado, e removido \npelo comutador no lado de recebimento do tronco. O próprio rótulo da VLAN consiste em um campo de 2 bytes \nchamado Rótulo de Identificação de Protocolo (Tag Protocol Identifier — TPID) (com um valor hexadecimal fixo de \n81-00), um campo de 2 bytes de Controle de Informação de Rótulo contendo um campo de identificação de VLAN \ncom 12 bits, e um campo de prioridade com 3 bits semelhante em propósito ao campo TOS do datagrama IP.\nFigura 5.25  Comutador único com duas VLANs configuradas\nKR 05.25.eps\nKUROSE/ROSS\nComputer Networking 6/e\n18p4 Wide x 11p6 Deep\n11/17/11, 11/21/11 ROSSI ILLUSTRATION\n1\nEngenharia Elétrica\n(portas VLAN 2–8)\nCiência da Computação\n(portas VLAN 9–15)\n9\n15\n2\n4\n8\n10\n16\ncamada de enlace: enlaces, redes de acesso e redes locais  359 \nFigura 5.26  \u0007\nConectando 2 comutadores da VLAN a duas VLANs: (a) 2 cabos (b) entroncados\nKR 05.26.eps\nKUROSE/ROSS\nComputer Networking 6/e\n36p0 Wide x 21p7 Deep\n11/17/11,  11/21/11 ROSSI ILLUSTRATION\n1\n16\n1\n8\n1\nEngenharia Elétrica\n(portas VLAN 2–8)\nb.\na.\nEngenharia Elétrica\n(portas VLAN 2, 3, 6)\nEnlace de\ntronco\nCiência da Computação\n(portas VLAN 9–15)\n9\n15\n2\n4\n8\n10\n16\n1\n2\n3\n4\n5\n6\n8\n7\nCiência da Computação\n(portas VLAN 4, 5, 7)\nFigura 5.27  \u0007\nQuadro Ethernet original (no alto); quadro VLAN Ethernet 802.1Q-tagged (embaixo)\nPreâmbulo\nCRC\nEndereço\nde destino\nEndereço\nde origem\nTipo\nDados\nPreâmbulo\nCRC'\nEndereço\nde destino\nEndereço\nde origem\nTipo\nControle de Informação de Rótulo\nIdentiﬁcador de Protocolo de Rótulo\nCRC\nrecalculado\nDados\nKR 05.27.eps\nKUROSE/ROSS\nComputer Networking 6/e\n32p1 Wide x 9p3 Deep\n11/17/11, 11/21/11 ROSSI ILLUSTRATION\nNesta análise, só fizemos uma breve citação sobre VLANs e focamos em VLANs baseadas em portas. De-\nveríamos mencionar também que as VLANs podem ser definidas de diversas maneiras. Em uma VLAN baseada \nem MAC, o administrador de rede especifica o grupo de endereços MAC que pertence a cada VLAN; quando \num dispositivo é conectado a uma porta, esta é conectada à VLAN apropriada com base no endereço MAC do \ndispositivo. As VLANs também podem ser definidas por protocolos da camada de rede (por exemplo, IPv4, IPv6 \nou Appletalk) e outros critérios. Veja o padrão 802.1Q [IEEE802.1q, 2005] para obter mais informações.\n5.5  \u0007\nVirtualização de enlace: uma rede como camada de \nenlace\nComo este capítulo trata de protocolos da camada de enlace, e dado que estamos chegando ao fim, vamos \nrefletir um pouco sobre como evoluiu o que entendemos como enlace. Começamos o capítulo considerando que \n   Redes de computadores e a Internet\n360\num enlace é um fio físico que conecta dois hospedeiros comunicantes. Quando estudamos protocolos de acesso \nmúltiplo, vimos que vários hospedeiros podiam ser conectados por um fio compartilhado e que o “fio” que co-\nnectava os hospedeiros podia ser o espectro de rádio ou qualquer outro meio. Isso nos levou a ver o enlace, de \nmodo um pouco mais abstrato, como um canal, em vez de um fio. Quando estudamos LANs Ethernet (Figura \n5.15) vimos que, na verdade, os meios de interconexão poderiam ser uma infraestrutura de comutação bastante \ncomplexa. Durante toda essa evolução, entretanto, os hospedeiros sempre mantiveram a visão do meio de co-\nnexão apenas como um canal da camada de enlace conectando dois ou mais hospedeiros. Vimos, por exemplo, \nque um hospedeiro Ethernet pode facilmente ficar inconsciente do fato de estar ligado a outros hospedeiros de \nLAN por um único segmento curto de LAN (Figura 5.17) ou por uma LAN comutada geograficamente dispersa \n(Figura 5.15) ou pela VLAN (Figura 5.26).\nNo caso de uma conexão com modem discado entre dois hospedeiros, o enlace que conecta os dois é, na \nverdade, a rede de telefonia — uma rede global de telecomunicações logicamente separada, com seus próprios \ncomutadores, enlaces e pilhas de protocolos para transferência e sinalização de dados. Entretanto, do ponto de \nvista da camada de rede da Internet, a conexão discada por meio da rede de telefonia é vista como um simples \n“fio”\n. Nesse sentido, a Internet virtualiza a rede de telefonia, considerando-a uma tecnologia da camada de enlace \nque provê conectividade da camada de enlace entre dois hospedeiros da Internet. Lembre-se de que, quando dis-\ncutimos redes de sobreposição no Capítulo 2, dissemos que, de modo semelhante, essa rede vê a Internet como \num meio de prover conectividade entre nós sobrepostos, procurando sobrepor-se à Internet do mesmo modo que \na Internet se sobrepõe à rede de telefonia.\nNesta seção consideraremos redes MPLS (Multiprotocol Label Switching). Diferentemente da rede de te-\nlefonia de comutação de circuitos, as redes MPLS, são, de direito, redes de comutação de pacotes por circuitos \nvirtuais. Elas têm seus próprios formatos de pacotes e comportamentos de repasse. Assim, de um ponto de vista \npedagógico, é bem coerente discutir MPLS quando estudamos a camada de rede ou a de enlace. Todavia, do pon-\nto de vista da Internet, podemos considerar a MPLS, assim como a rede de telefonia e a Ethernet comutada, tec-\nnologias da camada de enlace que servem para interconectar dispositivos IP. Assim, consideremos as redes MPLS \nao discutirmos a camada de enlace. Redes frame-relay e ATM também podem ser usadas para interconectar dis-\npositivos IP, embora representem uma tecnologia ligeiramente mais antiga (mas ainda disponibilizada), que não \nserá discutida aqui; se quiser saber mais detalhes consulte Goralski [1999], um livro de fácil leitura. Nosso estudo \nde MPLS terá de ser breve, pois livros inteiros podem ser escritos (e foram) sobre essas redes. Recomendamos \nDavie [2000] para obter detalhes sobre MPLS. Aqui, focalizaremos principalmente como tais redes servem para \ninterconectar dispositivos IP, embora as tecnologias subjacentes também sejam examinadas com um pouco mais \nde profundidade.\n5.5.1  Comutação de Rótulos Multiprotocolo (MPLS)\nA Comutação de Rótulos Multiprotocolo (MPLS — Multiprotocol Label Switching) evoluiu dos inúmeros \nesforços realizados pela indústria de meados ao final da década de 1990 para melhorar a velocidade de repasse \nde roteadores IP, adotando um conceito fundamental do mundo das redes de circuitos virtuais: um rótulo de \ntamanho fixo. O objetivo não era abandonar a infraestrutura de repasse de datagramas IP com base no destino \nem favor de rótulos de tamanho fixo e circuitos virtuais, mas aumentá-la rotulando datagramas seletivamente e \npermitindo que roteadores repassassem datagramas com base em rótulos de tamanho fixo (em vez de endereços \nde destino IP), quando possível. O importante é que essas técnicas trabalhavam de mãos dadas com o IP, usando \nendereçamento e roteamento IP. A IETF reuniu esses esforços no protocolo MPLS [RFC 3031, RFC 3032] mistu-\nrando de fato técnicas de circuitos virtuais em uma rede de datagramas com roteadores.\nVamos começar nosso estudo do MPLS considerando o formato de um quadro da camada de enlace que \né manipulado por um roteador habilitado para MPLS. A Figura 5.28 mostra que um quadro da camada de en-\nlace transmitido entre dispositivos habilitados para MPLS tem um pequeno cabeçalho MPLS adicionado entre \no cabeçalho de camada 2 (por exemplo, Ethernet) e o cabeçalho de camada 3 (isto é, IP). O RFC 3032 define o \ncamada de enlace: enlaces, redes de acesso e redes locais  361 \nformato do cabeçalho MPLS para esses enlaces; cabeçalhos de redes ATM e de frame relay também são definidos \nem outros RFCs. Entre os campos no cabeçalho MPLS estão o rótulo (que desempenha o papel de identificador \nde circuito virtual que estudamos na Seção 4.2.1), 3 bits reservados para uso experimental, um único bit, S, que é \nusado para indicar o final de uma série de rótulos MPLS “empilhados” (um tópico avançado que não será abor-\ndado aqui) e um campo de tempo de vida.\nA Figura 5.28 deixa logo evidente que um quadro melhorado com MPLS só pode ser enviado entre roteado-\nres habilitados para MPLS (já que um roteador não habilitado para MPLS ficaria bastante confuso ao encontrar \num cabeçalho MPLS onde esperava encontrar o IP!). Um roteador habilitado para MPLS é em geral denominado \nroteador de comutação de rótulos, pois repassa um quadro MPLS consultando o rótulo MPLS em sua tabela de \nrepasse e, então, passando imediatamente o datagrama para a interface de saída apropriada. Assim, o roteador ha-\nbilitado para MPLS não precisa extrair o endereço de destino e executar uma busca para fazer a compatibilização \ncom o prefixo mais longo na tabela de repasse. Mas como um roteador sabe se seu vizinho é realmente habilitado \npara MPLS e como sabe qual rótulo associar com determinado destino IP? Para responder a essas perguntas, \nprecisaremos estudar a interação entre um grupo de roteadores habilitados para MPLS.\nNo exemplo da Figura 5.29, os roteadores R1 a R4 são habilitados para MPLS. R5 e R6 são roteadores IP \npadrão. R1 anunciou a R2 e R3 que ele (R1) pode rotear para o destino A, e que um quadro recebido com rótulo \nMPLS 6 será repassado ao destino A. O roteador R3 anunciou ao roteador R4 que ele (R4) pode rotear para os \ndestinos A e D e que os quadros que estão chegando e que portam os rótulos MPLS 10 e 12 serão comutados na \ndireção desses destinos. O roteador R2 também anunciou ao roteador R4 que ele (R2) pode alcançar o destino A e \nque um quadro recebido portando o rótulo MPLS 8 será comutado na direção de A. Note que o roteador R4 agora \nFigura 5.28  \u0007\nCabeçalho MPLS: localizado entre os cabeçalhos da camada de enlace e da \ncamada de rede\nCabeçalho PPP\nou Ethernet\nCabeçalho MPLS\nCabeçalho IP\nRestante do quadro\nda camada de enlace\nKR 05.28.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p0 Wide x 6p0 Deep\n11/17/11, 11/21/11, 11/22/11 rossi\nRótulo\nExp\nS TTL\nFigura 5.29  Repasse melhorado com MPLS\nR4\nrótulo de\nentrada\nrótulo de\nsaída\n10\n12\n8\nA\nD\nA\n0\n0\n1\ndestino\ninterface\nde saída\nR6\nR5\nR3\nR2\nD\nA\n0\n0\n0\n1\n1\n0\nR1\nrótulo de\nentrada\nrótulo de\nsaída\n6\n9\nA\nD\n1\n0\n10\n12\ndestino\ninterface\nde saída\nrótulo de\nentrada\nrótulo de\nsaída\n–\nA\n0\n6\ndestino\ninterface\nde saída\nrótulo de\nentrada\nrótulo\nde saída\n6\nA\n0\n8\ndestino\ninterface\nde saída\nKR 05.29.eps\nAW/K\nd R\n   Redes de computadores e a Internet\n362\nestá na interessante posição de ter dois caminhos MPLS para chegar até A: por meio da interface 0 com rótulo \nMPLS de saída 10, e da interface 1 com um rótulo MPLS 8. O quadro geral apresentado na Figura 5.29 é que os \ndispositivos IP R5, R6, A e D estão conectados em conjunto via uma infraestrutura MPLS (roteadores habilitados \na MPLS R1, R2, R3 e R4) praticamente do mesmo modo como uma LAN comutada ou uma rede ATM podem \nconectar dispositivos IP entre si. E, do mesmo modo que uma LAN comutada ou uma rede ATM, os roteadores \nR1 a R4 habilitados para MPLS fazem isso sem jamais tocar o cabeçalho IP de um pacote.\nNessa discussão, não especificamos o protocolo utilizado para distribuir rótulos entre roteadores habilita-\ndos para MPLS, pois os detalhes dessa sinalização estariam muito além do escopo deste livro. Observamos, entre-\ntanto, que o grupo de trabalho da IETF para o MPLS especificou no [RFC 3468] que uma extensão do protocolo \nRSVP, conhecida como RSVP-TE [RFC 3209], será o foco de seus esforços para a sinalização MPLS. Também não \ndiscutimos como o MPLS realmente calcula os caminhos para pacotes entre roteadores habilitados para MPLS, \nnem como ele reúne informações de estado do enlace (por exemplo, quantidade de largura de banda do enlace \nnão reservada pelo MPLS) para usar nesses cálculos de caminho. Os algoritmos de roteamento de estado de en-\nlace (por exemplo, OSPF) foram estendidos para inundar essa informação aos roteadores habilitados para MPLS. \nÉ interessante que os algoritmos reais de cálculo de caminho não são padronizados, e são atualmente específicos \ndo fornecedor.\nAté aqui, a ênfase de nossa discussão sobre o MPLS tem sido o fato de que esse protocolo executa comutação \ncom base em rótulos, sem precisar considerar o endereço IP de um pacote. As verdadeiras vantagens do MPLS e \na razão do atual interesse por ele, contudo, não estão nos aumentos substanciais nas velocidades de comutação, \nmas nas novas capacidades de gerenciamento de tráfego que o MPLS proporciona. Como já vimos, R4 tem dois \ncaminhos MPLS até A. Se o repasse fosse executado até a camada IP tendo como base o endereço IP, os protocolos \nde roteamento IP que estudamos no Capítulo 4 especificariam um único caminho de menor custo até A. Assim, o \nMPLS provê a capacidade de repassar pacotes por rotas, o que não seria possível usando protocolos padronizados \nde roteamento IP. Essa é só uma forma simples de engenharia de tráfego usando o MPLS [RFC 3346; RFC 3272; \nRFC 2702; Xiao, 2000], com a qual um operador de rede pode suplantar o roteamento IP normal e obrigar que \numa parte do tráfego dirigido a um dado destino siga por um caminho, e que outra parte do tráfego dirigido ao \nmesmo destino siga por outro (seja por política, por desempenho ou por alguma outra razão).\nTambém é possível utilizar MPLS para muitas outras finalidades. O protocolo pode ser usado para realizar \nrestauração rápida de caminhos de repasse MPLS, por exemplo, mudar a rota do tráfego que passa por um cami-\nnho previamente calculado, restabelecido, em resposta à falha de enlace [Kar, 2000; Huang, 2002; RFC 3469]. Por \nfim, observamos que o MPLS pode ser, e tem sido, utilizado para implementar as denominadas redes privadas \nvirtuais (virtual private networks — VPN). Ao executar uma VPN para um cliente, um ISP utiliza uma rede \nhabilitada para MPLS para conectar as várias redes do cliente. O MPLS também pode ser usado para isolar os \nrecursos e o endereçamento utilizados pela VPN do cliente dos outros usuários que estão cruzando a rede do IS; \npara obter mais detalhes, veja DeClercq [2002].\nNossa discussão sobre o MPLS foi necessariamente breve e aconselhamos o leitor a consultar as referências \nque mencionamos. Observamos que são tantas as utilizações ­\npossíveis do MPLS que ele parece estar se tornando \no canivete suíço da engenharia de tráfego da Internet!\n5.6  Redes do datacenter\nNos últimos anos, empresas de Internet como Google, Microsoft, Facebook e Amazon (bem como suas \nequivalentes na Ásia e na Europa) construíram datacenters maciços, cada um abrigando dezenas a centenas de \nmilhares de hospedeiros e dando suporte simultâneo a muitas aplicações distintas na nuvem (por exemplo, busca, \ncorreio eletrônico, redes sociais e comércio eletrônico). Cada datacenter tem sua própria rede do datacenter que \ninterconecta seus hospedeiros e liga o datacenter à Internet. Nesta seção, faremos uma rápida introdução às redes \ndo datacenter para aplicações de nuvem.\ncamada de enlace: enlaces, redes de acesso e redes locais  363 \nO custo de um grande datacenter é imenso, ultrapassando US$ 12 milhões por mês para um datacenter de \n100 mil hospedeiros [Greenberg, 2009a]. Desses, 45% podem ser atribuídos aos próprios hospedeiros (que pre-\ncisam ser substituídos a cada 3-4 anos); 25% à infraestrutura, incluindo transformadores, “no-breaks”\n, geradores \npara faltas de energia prolongadas e sistemas de resfriamento; 15% para custos com consumo de energia elétrica; \ne 15% para redes, incluindo dispositivos (comutadores, roteadores e balanceadores de carga), enlaces externos e \ncustos de tráfego de dados. (Nessas porcentagens, os custos com equipamento são amortizados, assim uma mé-\ntrica de custo comum é aplicada para compras de única vez e despesas contínuas, como energia.) Embora o uso \nde redes não seja o maior dos custos, sua inovação é a chave para reduzir o custo geral e maximizar o desempenho \n[Greenberg, 2009a].\nAs abelhas trabalhadoras em um datacenter são os hospedeiros: eles servem o conteúdo (por exemplo, \npáginas Web e vídeos), armazenam mensagens de correio eletrônico e documentos, e realizam coletivamente \ncálculos maciçamente distribuídos (por exemplo, cálculos de índice distribuídos para mecanismos de busca). Os \nhospedeiros nos datacenters, chamados lâminas e semelhantes a embalagens de pizza, são em geral hospedei-\nros básicos incluindo CPU, memória e armazenamento de disco. Os hospedeiros são empilhados em estantes, \ncom cada uma normalmente tendo de 20 a 40 lâminas. No topo de cada estante há um comutador, devidamente \ndenominado comutador do topo da estante (TOR — Top Of Rack), que interconecta os hospedeiros entre si e \ncom outros comutadores no datacenter. Especificamente, cada hospedeiro na estante tem uma placa de interface \nde rede que se conecta ao seu comutador TOR, e cada comutador TOR tem portas adicionais que podem ser co-\nnectadas a outros comutadores. Embora os hospedeiros de hoje geralmente tenham conexões Ethernet a 1 Gbit/s \ncom seus comutadores TOR, conexões de 10 Gbits/s poderão se tornar comuns. Cada hospedeiro também recebe \nseu próprio endereço IP interno ao datacenter.\nA rede do datacenter aceita dois tipos de tráfego: tráfego fluindo entre clientes externos e hospedeiros in-\nternos, e tráfego fluindo entre hospedeiros internos. Para tratar dos fluxos entre os clientes externos e os hospe-\ndeiros internos, a rede do datacenter inclui um ou mais roteadores de borda, conectando a rede do datacenter à \nInternet pública. Portanto, a rede do datacenter interconecta as estantes umas com as outras e conecta as estantes \naos roteadores de borda. A Figura 5.30 mostra um exemplo de uma rede do datacenter. O projeto de rede do \nFigura 5.30  Uma rede do datacenter com uma topologia hierárquica\nKR 05.30.eps\nAW/Kurose and Ross\nComputer Networking, 6/e\nInternet\nA\n1\n2\n3\n4\n5\n6\n7\n8\nC\nB\nEstantes de servidores\nComutadores TOR\nComutadores da camada 2\nComutadores da camada 1\nRoteador de acesso\nRoteador de borda\nBalanceador\nde carga\n   Redes de computadores e a Internet\n364\ndatacenter, a arte de projetar a rede de interconexão e os protocolos que conectam as estantes entre si e com os \nroteadores de borda, tornou-se um ramo importante da pesquisa sobre redes de computadores nos últimos anos \n[Al-Fares, 2008; Greenberg, 2009a; Greenberg, 2009b; Mydotr, 2009; Guo, 2009; Chen, 2010; Abu-Libdeh, 2010; \nAlizadeh, 2010; Wang, 2010; Farrington, 2010; Halperin, 2011; Wilson, 2011; Mudigonda, 2011; Ballani, 2011; \nCurtis, 2011; Raiciu, 2011].\nBalanceamento de carga\nUm datacenter de nuvem, como um datacenter da Google ou da Microsoft, oferece muitas aplicações si-\nmultaneamente, como aplicações de busca, correio eletrônico e vídeo. Para dar suporte a solicitações de clientes \nexternos, cada aplicação é associada a um endereço IP publicamente visível, ao qual clientes enviam suas solicita-\nções e do qual eles recebem respostas. Dentro do datacenter, as solicitações externas são direcionadas primeiro a \num balanceador de carga, cuja função é distribuir as solicitações aos hospedeiros, equilibrando a carga entre os \nhospedeiros como uma função de sua carga atual. Um grande datacenter normalmente terá vários balanceadores \nde carga, cada um dedicado a um conjunto de aplicações de nuvem específicas. Esse balanceador de carga às \nvezes é conhecido como “comutador da camada 4”\n, pois toma decisões com base no número da porta de destino \n(camada 4), bem como no endereço IP de destino no pacote. Ao receber uma solicitação por uma aplicação em \nparticular, o balanceador de carga a encaminha para um dos hospedeiros que trata da aplicação. (Um hospedeiro \npode, então, invocar os serviços de outros hospedeiros, para ajudar a processar a solicitação.) Quando o hospe-\ndeiro termina de processar a solicitação, ele envia sua resposta de volta ao balanceador de carga, que por sua vez \nrepassa a resposta de volta ao cliente externo. O balanceador de carga não só equilibra a carga de trabalho entre \nos hospedeiros, mas também oferece uma função tipo NAT, traduzindo o endereço IP externo, público, para o \nendereço IP interno do hospedeiro apropriado, e depois traduzindo de volta os pacotes que trafegam na direção \ncontrária, de volta aos clientes. Isso impede que os clientes entrem em contato direto com os hospedeiros, o que \ntem o benefício para a segurança de ocultar a estrutura de rede interna e impedir que clientes interajam direta-\nmente com os hospedeiros.\nArquitetura hierárquica\nPara um datacenter pequeno, abrigando apenas alguns milhares de hospedeiros, uma rede simples, que con-\nsiste em um roteador de borda, um balanceador de carga e algumas dezenas de estantes, todas interconectadas \npor um único comutador Ethernet, possivelmente seria suficiente. Mas, para escalar para dezenas a centenas de \nmilhares de hospedeiros, um datacenter normalmente emprega uma hierarquia de roteadores e comutadores, \ncomo a topologia mostrada na Figura 5.30. No topo da hierarquia, o roteador de borda se conecta aos roteadores \nde acesso (somente dois aparecem na Figura 5.30, mas pode haver muito mais). Abaixo de cada roteador de aces-\nso há três camadas de comutadores. Cada roteador de acesso se conecta a um comutador da camada superior, e \ncada comutador da camada superior se conecta a vários comutadores da segunda camada e a um balanceador \nde carga. Cada comutador da segunda camada, por sua vez, se conecta a várias estantes por meio dos comutado-\nres TOR das estantes (comutadores da terceira camada). Todos os enlaces em geral utilizam Ethernet para seus \nprotocolos da camada de enlace e da camada física, com uma mistura de cabeamento de cobre e fibra. Com esse \nprojeto hierárquico, é possível escalar um datacenter até centenas de milhares de hospedeiros.\nComo é crítico para um provedor de aplicação de nuvem oferecer aplicações continuamente com alta dis-\nponibilidade, os datacenters também incluem equipamento de rede redundante e enlaces redundantes em seus \nprojetos (isso não está incluído na Figura 5.30). Por exemplo, cada comutador TOR pode se conectar a dois co-\nmutadores da camada 2, e cada roteador de acesso, comutador da camada 1 e comutador da camada 2 pode ser \nduplicado e integrado ao projeto [Cisco, 2012; Greenberg, 2009b]. No projeto hierárquico da Figura 5.30, observe \nque os hospedeiros abaixo de cada roteador de acesso formam uma única sub-rede. Para localizar o tráfego de \ndifusão ARP, cada uma dessas sub-redes é dividida ainda mais em sub-redes de VLAN menores, cada uma com-\npreendendo algumas centenas de hospedeiros [Greenberg, 2009a].\ncamada de enlace: enlaces, redes de acesso e redes locais  365 \nEmbora a arquitetura hierárquica convencional que acabamos de descrever resolva o problema de escala, \nela sofre de capacidade limitada de hospedeiro-a-hospedeiro [Greenberg, 2009b]. Para entender essa limitação, \nconsidere novamente a Figura 5.30 e suponha que cada hospedeiro se conecte ao seu comutador TOR com \num enlace de 1 Gbit/s, enquanto os enlaces entre os comutadores são enlaces Ethernet de 10 Gbits/s. Dois \nhospedeiros na mesma estante sempre podem se comunicar com 1 Gbit/s completo, limitados apenas pela ve-\nlocidade das placas de interface de rede dos hospedeiros. Porém, se houver muitos fluxos simultâneos na rede \ndo datacenter, a velocidade máxima entre dois hospedeiros em estantes diferentes pode ser muito menor. Para \nter uma ideia desse problema, considere um padrão de tráfego consistindo em 40 fluxos simultâneos entre 40 \npares de hospedeiros em diferentes estantes. Especificamente, suponha que cada um dos 10 hospedeiros na \nestante 1 da Figura 5.30 envie um fluxo a um hospedeiro correspondente na estante 5. De modo semelhante, \nhá dez fluxos simultâneos entre pares de hospedeiros nas estantes 2 e 6, dez fluxos simultâneos entre as estantes \n3 e 7, e dez fluxos simultâneos entre as estantes 4 e 8. Se cada fluxo compartilha uniformemente a capacidade \nde um enlace com outros fluxos atravessando esse enlace, então os 40 fluxos cruzando o enlace de 10 Gbits/s \nde A-para-B (bem como o enlace de 10 Gbits/s de B-para-C) receberão, cada um, apenas 10 Gbits/s / 40 = \n250 Mbits/s, que é muito menor do que a velocidade de 1 Gbit/s da placa de interface de rede. O problema \nse torna ainda mais grave para fluxos entre hospedeiros que precisam trafegar por uma camada mais alta na \nhierarquia. Uma solução possível para essa limitação é empregar comutadores e roteadores com velocidade \nmais alta. Mas isso aumentaria significativamente o custo do datacenter, pois os comutadores e roteadores com \ngrandes velocidades de porta são muito caros.\nO suporte à comunicação com alta largura de banda de hospedeiro-a-hospedeiro é importante porque \num requisito básico nos datacenters é a flexibilidade no posicionamento de computação e serviços [Greenberg, \n2009b; Farrington, 2010]. Por exemplo, um mecanismo de busca da Internet em grande escala pode ser executa-\ndo em milhares de hospedeiros espalhados por várias estantes com requisitos de largura de banda significativos \nentre todos os pares de hospedeiros. De modo semelhante, um serviço de computação de nuvem, como EC2, \npode querer colocar as diversas máquinas virtuais que compreendem um serviço do cliente nos hospedeiros físi-\ncos com mais capacidades, independentemente do seu local no datacenter. Se esses hospedeiros físicos estiverem \nespalhados por várias estantes, gargalos na rede, como descrevemos no parágrafo anterior, podem ocasionar um \ndesempenho fraco.\nTendências para as redes no datacenter\nPara reduzir o custo dos datacenters, e ao mesmo tempo melhorar seu atraso e vazão, gigantes de nuvem \nda Internet, como Google, Facebook, Amazon e Microsoft, estão continuamente implantando novos projetos de \nrede do datacenter. Embora esses projetos sejam próprios, mesmo assim, muitas tendências importantes podem \nser identificadas.\nFigura 5.31  Topologia de rede de dados altamente interconectada\nKR 05.31.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n1\n2\n3\n4\n5\n6\n7\n8\nEstantes de servidores\nComutadores TOR\nComutadores da camada 2\nComutadores da camada 1\n   Redes de computadores e a Internet\n366\nUma dessas tendências é executar novas arquiteturas de interconexão e protocolos de rede que contornem \nas desvantagens dos projetos hierárquicos tradicionais. Uma tática desse tipo é substituir a hierarquia de comu-\ntadores e roteadores por uma topologia totalmente conectada [Al-Fares, 2008; Greenberg, 2009b; Guo, 2009], \ncomo aquela mostrada na Figura 5.31. Nesse projeto, cada comutador da camada 1 se conecta a todos os comuta-\ndores da camada 2, de modo que (1) o tráfego de hospedeiro-a-hospedeiro nunca precisa subir além das camadas \nde comutadores e (2) com n comutadores da camada 1, entre dois quaisquer comutadores da camada 2, existem \nn caminhos separados. Esse projeto pode melhorar significativamente a capacidade de hospedeiro-a-hospedeiro. \nPara ver isso, considere novamente nosso exemplo de 40 fluxos. A topologia na Figura 5.31 pode lidar com tal \npadrão de fluxos, pois há quatro caminhos distintos entre o primeiro comutador da camada 2 e o segundo co-\nmutador dessa camada, oferecendo em conjunto uma capacidade agregada de 40 Gbits/s entre os dois primeiros \ncomutadores da camada 2. Tal projeto não só alivia a limitação de capacidade de hospedeiro-a-hospedeiro, mas \ntambém cria um ambiente de computação e serviço mais flexível, no qual a comunicação entre duas estantes \nquaisquer não conectadas ao mesmo comutador é logicamente equivalente, independentemente de seus locais \nno datacenter.\nOutra tendência importante é empregar datacenters modulares (MDCs) baseados em contêineres [You-\nTube, 2009; Waldrop, 2007]. Em um MDC, uma fábrica monta, dentro de um contêiner de navio padrão de \n12 m, um “mini-datacenter” e envia o contêiner ao local do datacenter. Cada contêiner tem até alguns milhares \nde hospedeiros, empilhados em dezenas de estantes, que são dispostas próximas umas das outras. No local do \ndatacenter, vários contêineres são interconectados entre si e também com a Internet. Quando um contêiner pré-\nfabricado é implantado em um datacenter, normalmente, é difícil dar assistência técnica. Assim, cada contêiner \né projetado para realizar a degradação de desempenho controlada: quando os componentes (servidores e co-\nmutadores) falham com o tempo, ele continua a operar, mas com um desempenho diminuído. Quando muitos \ncomponentes tiverem falhado e o desempenho tiver caído abaixo de um patamar, o contêiner inteiro é removido \ne substituído por um novo.\nA montagem de um datacenter a partir de contêineres cria novos desafios de rede. Com um MDC, existem \ndois tipos de redes: as redes internas ao contêiner, dentro de cada contêiner, e a rede central conectando cada \ncontêiner [Guo, 2009; Farrington, 2010]. Dentro de cada um, na escala de até alguns milhares de hospedeiros, é \npossível montar uma rede totalmente conectada (como já explicamos) usando comutadores Gigabit Ethernet co-\nmuns, pouco dispendiosos. Porém, o projeto da rede central, que interconecta centenas a milhares de contêineres \nenquanto oferece alta largura de banda de hospedeiro-a-hospedeiro entre os contêineres para cargas de trabalho \ntípica, continua sendo um problema desafiador. Uma arquitetura de comutador híbrido eletro-ótico para inter-\nconectar os contêineres é proposta em Farrington [2010].\nAo usar topologias altamente interconectadas, um dos principais problemas é o projeto de algoritmos de \nroteamento entre os comutadores. Uma possibilidade [Greenberg, 2009b] é usar uma forma de roteamento alea-\ntório. Outra possibilidade [Guo, 2009] é implementar múltiplas placas de interface de rede em cada hospedeiro, \nconectar cada um a vários comutadores de baixo custo e permitir que os próprios hospedeiros direcionem o \ntráfego de modo inteligente entre os comutadores. Hoje, variações e extensões dessas técnicas estão sendo execu-\ntadas em datacenters contemporâneos. Muito mais inovações no projeto de datacenter provavelmente surgirão; \nos leitores interessados poderão ler os muitos artigos recentes sobre projeto de redes do datacenter.\n5.7  Um dia na vida de uma solicitação de página Web\nAgora que já cobrimos a camada de enlace neste capítulo, e da rede, de transporte e a camada de aplicação \nem capítulos anteriores, nossa jornada pela pilha de protocolos está completa! Bem no início deste livro (Seção \n1.1), escrevemos que “grande parte deste livro trata de protocolos de redes de computadores” e, nos primeiros \ncinco capítulos, vimos que de fato isso é verdade. Antes de nos dirigirmos aos tópicos dos próximos capítulos, \ngostaríamos de finalizar nossa jornada pela pilha de protocolos considerando uma visão integrada e holística \ncamada de enlace: enlaces, redes de acesso e redes locais  367 \ndos que vimos até agora. Uma forma de termos essa visão geral é identificarmos os vários (vários!) protocolos \nenvolvidos na satisfação de simples pedidos, como fazer o download de uma página Web. A Figura 5.32 ilustra a \nimagem que queremos passar: um estudante, Bob, conecta seu notebook ao comutador Ethernet da sua escola e \nfaz o download de uma página Web (digamos que é a página principal de www.google.com). Como já sabemos, \nexiste muito mais sob a superfície do que se imagina para realizar esta solicitação que parece simples. O labora-\ntório Wireshark, ao final deste capítulo, examina cenários de comunicação mais em detalhes, contendo vários \npacotes envolvidos em situações parecidas.\n5.7.1  Começando: DHCP, UDP, IP e Ethernet\nSuponha que Bob liga seu notebook e o conecta via um cabo Ethernet ao comutador Ethernet da escola, \nque por sua vez é conectado ao roteador da escola, como demonstrado na Figura 5.32. O roteador da escola é \nconectado a um ISP, que neste exemplo é comcast.net. Neste exemplo, a comcast.net fornece o serviço DNS para \na escola; dessa forma, o servidor DNS se localiza em uma rede da Comcast, e não na rede da escola. Vamos supor \nque servidor DHCP está sendo executado dentro do roteador, como costuma acontecer.\nQuando Bob conecta seu notebook à rede pela primeira vez, não consegue fazer nada (por exemplo, bai-\nxar uma página Web) sem um endereço IP. Assim, a primeira ação relacionada à rede, tomada pelo notebook, é \nexecutar o protocolo DHCP para obter um endereço IP, bem como outras informações do servidor DHCP local:\n1.\t O sistema operacional do notebook de Bob cria uma mensagem de solicitação DHCP (Seção 4.4.2) e \na coloca dentro do segmento UDP (Seção 3.3) com a porta de destino 67 (servidor DHCP) e porta de \norigem 68 (cliente DHCP). O segmento é então colocado dentro de um datagrama IP (Seção 4.4.1) com \num endereço IP de destino por difusão (255.255.255.255) e um endereço IP de origem 0.0.0.0, já que o \nnotebook do Bob ainda não tem um endereço IP.\n2.\t O datagrama IP contendo uma mensagem de solicitação DHCP é colocado dentro de um quadro Ether-\nnet (Seção 5.4.2). O quadro Ethernet tem endereços de destino MAC FF:FF:FF:FF:FF:FF de modo que o \nquadro será transmitido a todos os dispositivos conectados ao comutador (onde se espera que haja um \nservidor DHCP); o quadro de origem do endereço MAC é do notebook do Bob, 00:16:D3:23:68:8A.\nFigura 5.32  Um dia na vida de uma solicitação de página Web: preparação e ações da rede\n00:22:6B:45:1F:1B\n68.85.2.1\n00:16:D3:23:68:8A\n68.85.2.101\nServidor DNS\nde comcast.net\n68.87.71.226\nServidor Web de\nwww.google.com\n64.233.169.105\nRede da escola\n68.80.2.0/24\nRede da Comcast\n68.80.0.0/13\nRede do Google\n64.233.160.0/19\nKR 05.32.eps\nKUROSE/ROSS\n1–7\n8–13\n18–24\n14–17\n   Redes de computadores e a Internet\n368\n3.\t O quadro de difusão Ethernet contendo a solicitação DHCP é o primeiro a ser enviado pelo notebook \nde Bob para o comutador Ethernet. O comutador transmite o quadro da entrada para todas as portas de \nsaída, incluindo a porta conectada ao roteador.\n4.\t O roteador recebe o quadro Ethernet transmitido, que contém a solicitação DHCP na sua interface com \nendereço MAC 00:22:6B:45:1F:1B, e o datagrama IP é extraído do quadro Ethernet. O endereço IP de \ndestino transmitido indica que este datagrama IP deveria ser processado por protocolos de camadas mais \nelevadas em seu nó, de modo que, dessa forma, a carga útil do datagrama (um segmento UDP) é demulti-\nplexada (Seção 3.2) até o UDP, e a mensagem de solicitação é extraída do segmento UDP. Agora o servidor \nDHCP tem a mensagem de solicitação DHCP.\n5.\t Suponhamos que o servidor DHCP que esteja sendo executado dentro de um roteador possa alocar o \nendereço IP no bloco CIDR (Seção 4.4.2) 68.85.2.0/24. Neste exemplo, todos os endereços IP usados na \nescola estão dentro do bloco de endereços da Comcast. Vamos supor que o servidor DHCP designe o \nendereço 68.85.2.101 ao notebook do Bob. O servidor DHCP cria uma mensagem de ACK DHCP (Seção \n4.4.2) contendo um endereço IP, assim como o endereço IP do servidor DNS (68.87.71.226), o endereço \nIP para o roteador de borda (gateway) default (68.85.2.1) e o bloco de sub-rede (68.85.2.0/24) (equivalente \nà “máscara de rede”). A mensagem DHCP é colocada dentro de um segmento UDP, o qual é colocado \ndentro de um datagrama IP, o qual é colocado dentro de um quadro Ethernet. O quadro Ethernet tem o \nendereço MAC de origem da interface do roteador na rede doméstica (00:22:6B:45:1F:1B) e um endereço \nMAC de destino do notebook do Bob (00:16:D3:23:68:8A).\n6.\t O quadro Ethernet contendo o ACK DHCP é enviado (individualmente) pelo roteador ao comutador. \nUma vez que o comutador realiza a autoaprendizagem (Seção 5.4.3) e que recebe previamente um quadro \ndo notebook de Bob (que contém a solicitação DHCP), o comutador sabe como encaminhar um quadro \nendereçado a 00:16:D3:23:68:8A apenas para a porta de saída que leva ao notebook do Bob.\n7.\t O notebook do Bob recebe o quadro Ethernet que contém o ACK DHCP, extrai o datagrama IP do quadro \nEthernet, extrai o segmento UDP do datagrama IP, e extrai a mensagem ACK DHCP do segmento UDP. \nEntão, o cliente DHCP do Bob grava seu endereço IP e o endereço IP do seu servidor DNS. Ele também \ninstala o endereço do roteador de borda default em sua tabela de repasse de IP (Seção 4.1). O notebook \nde Bob enviará todos os datagramas com endereços de destino fora de sua sub-rede 68.85.2.0/24 à saí-\nda-padrão. Nesse momento, o notebook do Bob inicializou os seus componentes de rede e está pronto \npara começar a processar a busca da página Web. (Observe que apenas as duas últimas etapas DHCP das \nquatro apresentadas no Capítulo 4 são de fato necessárias.)\n5.7.2  Ainda começando: DNS, ARP\nQuando Bob digita o URL www.google.com em seu navegador, ele inicia uma longa cadeia de eventos que \nno fim resultarão na exibição da página principal do Google no navegador. O navegador de Bob inicia o processo \nao criar um socket TCP (Seção 2.7) que será usado para enviar uma requisição HTTP (Seção 2.2) para www.\ngoogle.com. Para criar o socket, o notebook de Bob precisará saber o endereço IP de www.google.com. Aprende-\nmos, na Seção 2.5, que o protocolo DNS é usado para fornecer serviços de tradução de nomes para endereço IP.\n8.\t O sistema operacional do notebook de Bob cria então uma mensagem de consulta DNS (Seção 2.5.3), co-\nlocando a cadeia de caracteres “www.google.com” no campo de pergunta da mensagem DNS. Essa mensa-\ngem é então colocada dentro de um segmento UDP, com a porta de destino 53 (servidor DNS). O segmento \nUDP é então colocado dentro de um datagrama IP com um endereço de destino IP 68.87.71.226 (o ende-\nreço do servidor DNS retomado pelo ACK DHCP na etapa 5) e um endereço IP de origem 68.85.2.101.\n9.\t O notebook de Bob coloca então um datagrama contendo a mensagem de consulta DNS em um quadro \nEthernet. Este quadro será enviado (endereçado, na camada de enlace) ao roteador de borda da rede da \nescola de Bob. No entanto, apesar de o notebook de Bob conhecer o endereço IP do roteador de borda da \nrede da escola (68.85.2.1), via mensagem ACK DHCP na etapa 5 anterior, ele não sabe o endereço MAC \ncamada de enlace: enlaces, redes de acesso e redes locais  369 \ndo roteador de borda. Para que o notebook do Bob obtenha o endereço MAC do roteador de borda, ele \nprecisará usar o protocolo ARP (Seção 5.4.1).\n10.\t O notebook de Bob cria uma mensagem de consulta ARP direcionada ao endereço IP 68.85.2.1 (a por-\nta-padrão), coloca a mensagem ARP dentro do quadro Ethernet para ser transmitido por difusão ao \nendereço de destino (FF:FF:FF:FF:FF:FF) e envia o quadro Ethernet ao comutador, que entrega o quadro \na todos os dispositivos, incluindo o roteador de borda.\n11.\t O roteador de borda recebe um quadro contendo a mensagem de consulta ARP na interface da rede da es-\ncola, e descobre que o endereço IP de destino 68.85.2.1 na mensagem ARP corresponde ao endereço IP de \nsua interface. Então, o roteador de borda prepara uma resposta ARP, indicando que o seu endereço MAC \n00:22:6B:45:1F:1B corresponde ao endereço IP 68.85.2.1. Ele coloca a mensagem de resposta ARP em um \nquadro Ethernet, com o endereço de destino 00:16:D3:23:68:8A (notebook do Bob) e envia o quadro ao \ncomutador, que entrega o quadro ao notebook de Bob.\n12.\t O notebook de Bob recebe o quadro que contém a mensagem de resposta ARP e extrai o endereço MAC \ndo roteador de borda (00:22:6B:45:1F:1B) da mensagem de resposta ARP.\n13.\t O notebook de Bob pode agora (enfim!) endereçar o quadro Ethernet com a mensagem de consulta DNS \nao endereço MAC do roteador de borda. Observe que o datagrama nesse quadro tem o endereço IP de \ndestino 68.87.71.226 (servidor DNS), enquanto o quadro tem o endereço de destino 00:22:6B:45:1F:1B \n(roteador de borda). O notebook de Bob envia esse quadro ao comutador, que entrega o quadro ao rotea-\ndor de borda.\n5.7.3  \u0007\nAinda começando: roteamento intradomínio ao servidor DNS\n14.\t O roteador de borda recebe o quadro e extrai o datagrama IP que contém a consulta DNS. O roteador pro-\ncura o endereço de destino desse datagrama (68.87.71.226) e determina de sua tabela de repasse que ele deve \nser enviado ao roteador da extremidade esquerda na rede Comcast, como na Figura 5.32. O datagrama IP é \ncolocado em um quadro de uma camada de enlace apropriado ao enlace conectando o roteador da escola ao \nroteador Comcast da extremidade esquerda, e o quadro é enviado através desse enlace.\n15.\t O roteador da extremidade esquerda na rede Comcast recebe o quadro, extrai o datagrama IP, examina o en-\ndereço de destino do datagrama (68.87.71.226) e determina a interface de saída pela qual enviará o datagra-\nma ao servidor DNS de sua tabela de repasse, que foi preenchida pelo protocolo intradomínio da Comcast \n(como RIP, OSPF ou IS-IS, Seção 4.6), assim como o protocolo intradomínio da Internet, BGP.\n16.\t Por fim, o datagrama IP contendo a consulta DNS chega ao servidor DNS. Este extrai a mensagem de \nconsulta DNS, procura o nome em www.google.com na sua base de dados DNS (Seção 2.5), e encontra \no registro de recurso DNS que contém o endereço IP (64.233.169.105) para www.google.com (supon-\ndo-se que está em cache no servidor DNS). Lembre-se que este dado em cache foi originado no servidor \nDNS com autoridade (Seção 2.5.2) para google.com. O servidor DNS forma uma mensagem DNS de \nresposta contendo o mapeamento entre nome de hospedeiro e endereço IP, e coloca a mensagem DNS de \nresposta em um segmento UDP, e o segmento dentro do datagrama IP endereçado ao notebook do Bob \n(68.85.2.101). Esse datagrama será encaminhado de volta ao roteador da escolha por meio da rede Com-\ncast, e de lá ao notebook de Bob, via comutador Ethernet.\n17.\t O notebook de Bob extrai o endereço IP do servidor www.google.com da mensagem DNS. Enfim, depois \nde muito trabalho, o notebook de Bob está pronto para contatar o servidor www.google.com!\n5.7.4  Interação cliente-servidor Web: TCP e HTTP\n18.\t Agora que o notebook de Bob tem o endereço IP de www.google.com, ele está pronto para criar um socket \nTCP (Seção 2.7), que será usado para enviar uma mensagem HTTP GET (Seção 2.2.3) para www.google.com. \n \n   Redes de computadores e a Internet\n370\nQuando Bob cria um socket TCP, o TCP de seu notebook precisa primeiro executar uma apresentação \nde três vias (Seção 3.5.6) com o TCP em www.google.com. Então, o notebook de Bob primeiro cria um \nsegmento TCP SYN com a porta de destino 80 (para HTTP), coloca o segmento TCP dentro de um da-\ntagrama IP, com o endereço IP de destino 64.233.169.105 (www.google.com), coloca o datagrama dentro \nde um quadro com o endereço de destino 00:22:6B:45:1F:1B (o roteador de borda) e envia o quadro ao \ncomutador.\n19.\t Os roteadores da rede da escola, da rede Comcast e da rede do Google encaminham o datagrama conten-\ndo o TCP SYN até www.google.com, usando a tabela de repasse em cada roteador, como nas etapas 14-16. \nOs itens da tabela de repasse controlando o envio de pacotes interdomínio entre as redes da Comcast e do \nGoogle são determinados pelo protocolo BGP (Seção 4.6.3).\n20.\t Por fim, o datagrama contendo o TCP SYN chega em www.google.com. A mensagem TCP SYN é extraída \ndo datagrama e demultiplexada ao socket associado à porta 80. Um socket de conexão (Seção 2.7) é criado \npara a conexão TCP entre o servidor HTTP do Google e o notebook de Bob. Um segmento TCP SYNACK \n(Seção 3.5.6) é gerado, colocado dentro de um datagrama endereçado ao notebook de Bob, e enfim colo-\ncado em um quadro da camada de enlace apropriado ao enlace conectando www.google.com ao roteador \nde primeiro salto.\n21.\t O datagrama que contém o segmento TCP SYNACK é encaminhado através das redes do Google, Com-\ncast e da escola, finalmente chegando ao cartão Ethernet no computador de Bob. O datagrama é demulti-\nplexado dentro do sistema operacional e entregue ao socket TCP criado na etapa 18, que entra em estado \nde conexão.\n22.\t Agora, com o socket dentro do notebook de Bob (finalmente!), pronto para enviar bytes a www.google.\ncom, o navegador cria uma mensagem HTTP GET (Seção 2.2.3) contendo a URL a ser procurada. A \nmensagem HTTP GET é enviada ao socket, com a mensagem GET se tornando a carga útil do segmento \nTCP. O segmento TCP é colocado em um datagrama e enviado e entregue em www.google.com, como nas \netapas 18-20.\n23.\t O servidor HTTP www.google.com lê a mensagem HTTP GET do socket TCP, cria uma mensagem de \nresposta HTTP (Seção 2.2), coloca o conteúdo da página Web requisitada no corpo da mensagem de \nresposta HTTP, e envia a mensagem pelo socket TCP.\n24.\t O datagrama contendo a mensagem de resposta HTTP é encaminhado através das redes do Google, da \nComcast e da escola e chega ao notebook de Bob. O programa do navegador de Bob lê a resposta HTTP \ndo socket, extrai o html da página do corpo da resposta HTTP, e enfim (enfim!) mostra a página Web!\nNosso cenário abrangeu muito do fundamento das redes de comunicação! Se você entendeu a maior parte \nda representação, então também viu muito do fundamento desde que leu a Seção 1.1, onde escrevemos “grande \nparte deste livro trata de protocolos de redes de computadores” e você pode ter se perguntado o que na verdade \nera um protocolo! Por mais detalhado que o exemplo possa parecer, nós omitimos uma série de protocolos pos-\nsíveis (por exemplo, NAT executado no roteador de borda da escola, acesso sem fio à rede da escola, protocolos \nde segurança para acessar a rede da escola, ou segmentos ou datagramas codificados), e considerações (cache da \nWeb, hierarquia DNS) que poderíamos encontrar na Internet pública. Estudaremos a maioria desses tópicos na \nsegunda parte deste livro.\nPor fim, observamos que nosso exemplo era integrado e holístico, mas também muito resumido de muitos \nprotocolos que estudamos na primeira parte do livro. Este exemplo é mais focado nos aspectos de “como” e não \nno “por quê”\n. Para obter uma visão mais ampla e reflexiva dos protocolos de rede em geral, veja Clark [1988]; \n[RFC 5218].\n5.8  Resumo\nNeste capítulo, examinamos a camada de enlace — seus serviços, os princípios subjacentes à sua operação e \nvários protocolos específicos importantes que usam tais princípios na execução dos serviços da camada de enlace.\ncamada de enlace: enlaces, redes de acesso e redes locais  371 \nVimos que o serviço básico é mover um datagrama da camada de rede de um nó (hospedeiro, comutador, \nroteador, ponto de acesso Wi-Fi) até um nó adjacente. Vimos também que todos os protocolos da camada de \nenlace operam encapsulando um datagrama da camada de rede dentro de um quadro da camada de enlace antes \nde transmitir o quadro pelo enlace até o nó adjacente. Além dessa função comum de enquadramento, contudo, \naprendemos que diferentes protocolos da camada de enlace oferecem serviços muito diferentes de acesso ao \nenlace, entrega e transmissão. Essas diferenças decorrem, em parte, da vasta variedade de tipos de enlaces sobre \nos quais os protocolos de enlace devem operar. Um enlace ponto a ponto simples tem um único remetente e um \núnico receptor comunicando-se por um único “fio”\n. Um enlace de acesso múltiplo é compartilhado por muitos \nremetentes e receptores; por conseguinte, o protocolo da camada de enlace para um canal de acesso múltiplo \ntem um protocolo (seu protocolo de acesso múltiplo) para coordenar o acesso ao enlace. No caso de MPLS, o \n“enlace” que conecta dois nós adjacentes (por exemplo, dois roteadores IP adjacentes no sentido do IP — ou seja, \nsão roteadores IP do próximo salto na direção do destino) pode, na realidade, constituir uma rede em si e por si \npróprio. Em certo sentido, a ideia de uma rede ser considerada um “enlace” não deveria parecer estranha. Um \nenlace telefônico que conecta um modem/computador residencial a um modem/roteador remoto, por exemplo, \nna verdade é um caminho que atravessa uma sofisticada e complexa rede telefônica.\nDentre os princípios subjacentes à comunicação por camada de enlace, examinamos técnicas de detecção e \ncorreção de erros, protocolos de acesso múltiplo, endereçamento da camada de enlace, virtualização (VLANs) e \na construção de redes locais comutadas estendidas e redes do datacenter. Grande parte do foco atual na camada \nde enlace está sobre essas redes comutadas. No caso da detecção/correção de erros, examinamos como é possível \nadicionar bits ao cabeçalho de um quadro para detectar e algumas vezes corrigir erros de mudança de bits que \npodem ocorrer quando o quadro é transmitido pelo enlace. Analisamos esquemas simples de paridade e de soma \nde verificação, bem como o esquema mais robusto de verificação da redundância cíclica. Passamos, então, para o \ntópico dos protocolos de acesso múltiplo. Identificamos e estudamos três métodos amplos para coordenar o aces-\nso a um canal de difusão: métodos de divisão de canal (TDM, FDM), métodos de acesso aleatório (os protocolos \nALOHA e os CSMA) e métodos de revezamento (polling e passagem de permissão). Estudamos a rede de acesso \na cabo e descobrimos que ela usa muitos desses métodos de acesso múltiplo. Vimos que uma consequência do \ncompartilhamento de um canal de difusão por vários nós é a necessidade de prover endereços aos nós no nível \nda camada de enlace. Aprendemos que endereços da camada de enlace são bastante diferentes dos da camada de \nrede e que, no caso da Internet, um protocolo especial (o ARP — protocolo de resolução de endereço) é usado \npara fazer o mapeamento entre esses dois modos de endereçamento e estudamos o protocolo Ethernet, tremen-\ndamente bem-sucedido, com detalhes. Em seguida, examinamos como os nós que compartilham um canal de \ndifusão formam uma LAN e como várias LANs podem ser conectadas para formar uma LAN de maior porte \n— tudo isso sem a intervenção do roteamento da camada de rede para a interconexão desses nós locais. Também \naprendemos como múltiplas LANs virtuais podem ser criadas sobre uma única infraestrutura física de LAN.\nEncerramos nosso estudo da camada de enlace focalizando como redes MPLS fornecem serviços da ca-\nmada de enlace quando interconectadas com roteadores IP e com uma visão geral dos projetos de rede para os \nmaciços datacenters atuais. Concluímos este capítulo (e, sem dúvida, os cinco primeiros) identificando os muitos \nprotocolos que são necessários para buscar uma simples página Web. Com isso, concluímos nossa jornada pela \npilha de protocolos! É claro que a camada física fica abaixo da de enlace, mas provavelmente será melhor deixar \nos detalhes da camada física para outro curso. Contudo, discutimos brevemente vários aspectos da camada física \nneste capítulo e no Capítulo 1 (nossa discussão sobre meios físicos na Seção 1.2). Consideraremos novamente a \ncamada física quando estudarmos as características dos enlaces sem fio no próximo capítulo.\nEmbora nossa jornada pela pilha de protocolos esteja encerrada, o estudo sobre rede de computadores ain-\nda não chegou ao fim. Nos quatro capítulos seguintes, examinaremos redes sem fio, redes multimídia, segurança \nda rede e gerenciamento de redes. Esses quatro tópicos não se encaixam perfeitamente em uma única camada; na \nverdade, cada um atravessa muitas camadas. Assim, entender esses tópicos (às vezes tachados de “tópicos avan-\nçados” em alguns textos sobre redes) requer uma boa base sobre todas as camadas da pilha de protocolos — uma \nbase que se completou com nosso estudo sobre a camada de enlace de dados!\n   Redes de computadores e a Internet\n372\nExercícios\nde fixação e perguntas\nQuestões de revisão do Capítulo 5\nSEÇÕES 5.1–5.2\n\t\nR1.\t Considere a analogia de transporte na Seção 5.1.1. Se o passageiro é comparado com o datagrama, o que é \ncomparado com o quadro da camada de enlace?\n\t\nR2.\t Se todos os enlaces da Internet fornecessem serviço de entrega confiável, o serviço de entrega confiável do \nTCP seria redundante? Justifique sua resposta.\n\t\nR3.\t Quais alguns possíveis serviços um protocolo da camada de enlace pode oferecer à camada de rede? Quais \ndos serviços da camada de enlace têm correspondentes no IP? E no TCP?\nSEÇÃO 5.3\n\t\nR4.\t Suponha que dois nós comecem a transmitir ao mesmo tempo um pacote de comprimento L por um canal \nbroadcast de velocidade R. Denote o atraso de propagação entre os dois nós como dprop. Haverá uma colisão \nse dprop < L/R? Por quê?\n\t\nR5.\t Na Seção 5.3, relacionamos quatro características desejáveis de um canal de difusão. O slotted ALOHA tem \nquais dessas características? E o protocolo de passagem de permissão, tem quais dessas características?\n\t\nR6.\t No CSMA/CD, depois da quinta colisão, qual é a probabilidade de um nó escolher K = 4? O resultado K = 4 \ncorresponde a um atraso de quantos segundos em uma Ethernet de 10 Mbits/s?\n\t\nR7.\t Descreva os protocolos de polling e de passagem de permissão usando a analogia com as interações ocorridas \nem um coquetel.\n\t\nR8.\t Por que o protocolo de passagem de permissão seria ineficiente se uma LAN tivesse um perímetro muito \ngrande?\nSEÇÃO 5.4\n\t\nR9.\t Que tamanho tem o espaço de endereços MAC? E o espaço de endereços IPv4? E o espaço de endereços IPv6?\n\t\nR10.\t Suponha que cada um dos nós A, B e C esteja ligado à mesma LAN de difusão (por meio de seus adaptadores). \nSe A enviar milhares de datagramas IP a B com quadro de encapsulamento endereçado ao endereço MAC de \nB, o adaptador de C processará esses quadros? Se processar, ele passará os datagramas IP desses quadros para \nC? O que mudaria em suas respostas se A enviasse quadros com o endereço MAC de difusão?\n\t\nR11.\t Por que uma pesquisa ARP é enviada dentro de um quadro de difusão? Por que uma resposta ARP é enviada \nem um quadro com um endereço MAC de destino específico?\n\t\nR12.\t Na rede da Figura 5.19, o roteador tem dois módulos ARP, cada um com sua própria tabela ARP. É possível \nque o mesmo endereço MAC apareça em ambas?\n\t\nR13.\t Compare as estruturas de quadro das redes 10BASE-T, 100BASE-T e Gigabit Ethernet. Quais as diferenças \nentre elas?\n\t\nR14.\t Considere a Figura 5.15. Quantas sub-redes existem no sentido de endereçamento da Seção 4.4?\n\t\nR15.\t Qual o número máximo de VLANs que podem ser configuradas em um comutador que suporta o protocolo \n802.1Q? Por quê?\n\t\nR16.\t Imagine que N comutadores que suportam K grupos de VLAN serão conectados por meio de um protocolo \nde entroncamento. Quantas portas serão necessárias para conectar os comutadores? Justifique sua resposta.\ncamada de enlace: enlaces, redes de acesso e redes locais  373 \nproblemas\n\t\nP1.\t Suponha que o conteúdo de informação de um pacote seja o padrão de bits 1110 0110 1001 1101 e que um \nesquema de paridade par esteja sendo usado. Qual seria o valor do campo de soma de verificação para o caso \nde um esquema de paridade bidimensional? Sua resposta deve ser tal que seja usado um campo de soma de \nverificação de comprimento mínimo.\n\t\nP2.\t Dê um exemplo (que não seja o da Figura 5.5) mostrando que verificações de paridade bidimensional podem \ncorrigir e detectar um erro de bit único. Dê outro exemplo mostrando um erro de bit duplo que pode ser \ndetectado, mas não corrigido.\n\t\nP3.\t Suponha que a parte da informação de um pacote (D da Figura 5.3) contenha 10  bytes consistindo na \nrepresentação ASCII binária (8 bits) sem sinal da cadeia de caracteres “Networking”\n. Calcule a soma de \nverificação da Internet para esses dados.\n\t\nP4.\t Considere o problema anterior, mas suponha desta vez que esses 10 bytes contenham:\na.\t A representação binária dos números de 1 a 10.\nb.\t A representação ASCII das letras B até K (letras maiúsculas).\nc.\t A representação ASCII das letras B até K (letras minúsculas).\n\t\n\t Calcule a soma de verificação da Internet para esses dados.\n\t\nP5.\t Considere o gerador de 7 bits G =10011 e suponha que D tenha o valor de 1010101010. Qual é o valor de R?\n\t\nP6.\t Considere o problema acima, mas suponha que D tenha o valor de:\na.\t 1001010101.\nb.\t 0101101010.\nc.\t 1010100000.\n\t\nP7.\t Neste problema, exploramos algumas propriedades de CRC. Para o gerador G (=1001) dado na Seção 5.2.3, \nresponda às seguintes questões:\na.\t Por que ele pode detectar qualquer erro de bit único no dado D?\nb.\t Pode esse G detectar qualquer número ímpar de erros de bit? Por quê?\n\t\nP8.\t Na Seção 5.3, fornecemos um esboço da derivação da eficiência do slotted ALOHA. Neste problema, \nconcluiremos a derivação.\na.\t Lembre-se de que, quando há N nós ativos, a eficiência do slotted ALOHA é Np(1 – p)N-1. Ache o valor de p \nque maximize essa expressão.\nb.\t Usando o valor de p encontrado em (a), ache a eficiência do slotted ALOHA fazendo que N tenda ao \ninfinito. Dica: (1 – 1/N)N tende a 1/e quando N tende ao infinito.\n\t\nP9.\t Mostre que a eficiência máxima do ALOHA puro é 1/(2e). Obs.: Este problema será fácil se você tiver concluído \no anterior!\n\t\nP10.\t Considere dois nós, A e B, que usem um protocolo slotted ALOHA para competir pelo canal. Suponha que o \nnó A tenha mais dados para transmitir do que o B, e a probabilidade de retransmissão do nó A, pA, seja maior \ndo que a de retransmissão do nó B, pB.\na.\t Determine a fórmula para a vazão média do nó A. Qual é a eficiência total do protocolo com esses dois \nnós?\nb.\t Se pA = 2pB, a vazão média do nó A é duas vezes maior do que a do nó B? Por quê? Se não, como escolher \npA e pB para que isso aconteça?\nc.\t No geral, suponha que haja N nós, e entre eles o nó A tem a probabilidade de retransmissão 2p e todos os \noutros têm a probabilidade de retransmissão p. Determine as expressões para computar a vazão média do \nnó A e de qualquer outro nó.\n   Redes de computadores e a Internet\n374\n\t\nP11.\t Suponha que quatro nós ativos — nós A, B, C e D — estejam competindo pelo acesso a um canal usando \no slotted ALOHA. Imagine que cada nó tenha um número infinito de pacotes para enviar. Cada nó tenta \ntransmitir em cada intervalo (slot) com probabilidade p. O primeiro é numerado como 1, o segundo como 2, \ne assim por diante.\na.\t Qual a probabilidade que o nó A tenha sucesso pela primeira vez no intervalo 5?\nb.\t Qual a probabilidade que algum nó (A, B, C ou D) tenha sucesso no intervalo 4?\nc.\t Qual a probabilidade que o primeiro sucesso ocorra no intervalo 3?\nd.\t Qual a eficiência nesse sistema de quatro nós?\n\t\nP12.\t Desenhe um gráfico da eficiência do slotted ALOHA e do ALOHA puro como uma função de p, para os \nseguintes valores de N:\na.\t N = 15.\nb.\t N = 25.\nc.\t N = 35.\n\t\nP13.\t Considere um canal de difusão com N nós e uma taxa de transmissão de R bits/s. Suponha que o canal de \ndifusão use o polling (com um nó de polling adicional) para acesso múltiplo. Imagine que o intervalo de tempo \nentre o momento em que o nó conclui a transmissão e o momento em que o nó subsequente é autorizado a \ntransmitir (isto é, o atraso de polling) seja dpoll. Suponha ainda que, em uma rodada de polling, determinado \nnó seja autorizado a transmitir, no máximo, Q bits. Qual é a vazão máxima do canal de difusão?\n\t\nP14.\t Considere três LANs interconectadas por dois roteadores, como mostrado na Figura 5.33.\na.\t Atribua endereços IP a todas as interfaces. Para a Sub-rede 1, use endereços do tipo 192.168.1.xxx; para a \nSub-rede 2, use endereços do tipo 192.168.2.xxx, e para a Sub-rede 3 use endereços do tipo 192.168.3.xxx.\nb.\t Atribua endereços MAC a todos os adaptadores.\nc.\t Considere o envio de um datagrama IP do hospedeiro A ao hospedeiro F. Suponha que todas as tabelas \nARP estejam atualizadas. Enumere todas as etapas, como foi feito no exemplo de um único roteador na \nSeção 5.4.1.\nd.\t Repita (c), admitindo agora que a tabela ARP do hospedeiro remetente esteja vazia (e que as outras tabelas \nestejam atualizadas).\nFigura 5.33  Três sub-redes interconectadas por roteadores\nSub-rede 3\nE\nF\nC\nSub-rede 2\nD\nKR 05.33.eps\nKurose/Ross\nComputer Networking 6/e\nSize:  22p0 w  X  17p8 \nA\nB\nSub-rede 1\ncamada de enlace: enlaces, redes de acesso e redes locais  375 \n\t\nP15.\t Considere a Figura 5.33. Agora substituímos o roteador entre as sub-redes 1 e 2 pelo comutador S1, e \nchamamos de R1 o roteador entre as sub-redes 2 e 3.\na.  \u0007\nConsidere o envio de um datagrama IP do hospedeiro E ao hospedeiro F. O hospedeiro E pedirá ajuda ao \nroteador R1 para enviar o datagrama? Por quê? No quadro Ethernet que contém o datagrama IP, quais são \nos endereços IP e MAC de origem e de destino?\nb.  \u0007\nSuponha que E quisesse enviar um datagrama IP a B, e que o cache ARP de E não tenha o endereço MAC \nde B. E preparará uma consulta ARP para descobrir o endereço MAC de B? Por quê? No quadro Ethernet \n(que contém o datagrama IP destinado a B) entregue ao roteador R1, quais são os endereços de origem e \ndestino IP e MAC?\nc.\t Suponha que o hospedeiro A gostaria de enviar um datagrama IP ao hospedeiro B, e nem o cache ARP de \nA contém o endereço MAC de B, nem o cache ARP de B contém o endereço MAC de A. Suponha ainda \nque a tabela de encaminhamento do comutador S1 contenha entradas apenas para o hospedeiro B e para \no roteador R1. Dessa forma, A transmitirá por difusão uma mensagem de requisição ARP. Que ações o \ncomutador S1 tomará quando receber a mensagem de requisição ARP? O roteador R1 também receberá \nessa mensagem? Se sim, R1 a encaminhará para a Sub-rede 3? Assim que o hospedeiro B receber essa \nmensagem de requisição ARP, ele enviará a mensagem de resposta ARP de volta ao hospedeiro A. Mas \nenviará uma mensagem de consulta ARP para o endereço MAC de A? Por quê? O que o comutador S1 fará \nquando receber mensagem de resposta ARP do hospedeiro B?\n\t\nP16.\t Considere o problema anterior, mas suponha que o roteador entre as sub-redes 2 e 3 é substituído por um \ncomutador. Responda às questões de (a) a (c) do exercício anterior nesse novo contexto.\n\t\nP17.\t Lembre-se de que, com o protocolo CSMA/CD, o adaptador espera K ∙ 512 tempos de bits após uma colisão, \nonde K é escolhido aleatoriamente. Para K = 100, quanto tempo o adaptador espera até voltar à etapa 2 para \numa Ethernet de 10 Mbits/s? E para canal de difusão de 100 Mbits/s?\n\t\nP18.\t Suponha que os nós A e B estejam no mesmo canal de difusão de 10 Mbits/s e que o atraso de propagação \nentre os dois nós seja de 325 tempos de bit. Suponha que pacotes CSMA/CD e Ethernet sejam usados para \nesse canal de difusão. Imagine que o nó A comece a transmitir um quadro e que, antes de terminar, o nó B \ncomece a transmitir um quadro. O nó A pode terminar de transmitir antes de detectar que B transmitiu? Por \nquê? Se a resposta for sim, então A acredita, incorretamente, que seu quadro foi transmitido com sucesso, sem \nnenhuma colisão. Dica: suponha que no tempo t = 0 tempos de bit, A comece a transmitir um quadro. No pior \ndos casos, A transmite um quadro de tamanho mínimo de 512 + 64 tempos de bit. Portanto, A terminaria de \ntransmitir o quadro em t = 512 + 64 tempos de bit. Então, a resposta será não, se o sinal de B chegar a A antes \ndo tempo de bit t = 512 + 64 bits. No pior dos casos, quando o sinal de B chega a A?\n\t\nP19.\t Suponha que os nós A e B estejam no mesmo segmento de uma Ethernet de 10 Mbits/s e que o atraso de \npropagação entre os dois nós seja de 245 tempos de bit. Imagine que A e B enviem quadros ao mesmo tempo, \nque estes colidam e que, então, A e B escolham valores diferentes de K no algoritmo CSMA/CD. Admitindo \nque nenhum outro nó esteja ativo, as retransmissões de A e B podem colidir? Para nossa finalidade, é \nsuficiente resolver o seguinte exemplo. Suponha que A e B comecem a transmitir em t = 0 tempos de bit. \nAmbos detectam colisões em t = 245 tempos de bit. Suponha que KA = 0 e KB = 1. Em que tempo B programa \nsua retransmissão? Em que tempo A começa a transmissão? (Nota: os nós devem esperar por um canal ocioso \napós retornar à etapa 2 — veja o protocolo.) Em que tempo o sinal de A chega a B? B se abstém de transmitir \nem seu tempo programado?\n\t\nP20.\t Neste problema, você derivará a eficiência de um protocolo de acesso múltiplo semelhante ao CSMA/CD. \nNele, o tempo é segmentado e todos os adaptadores estão sincronizados com os intervalos. Entretanto, \ndiferentemente do slotted ALOHA, o comprimento de um intervalo (em segundos) é muito menor do que \num tempo de quadro (o tempo para transmitir um quadro). Seja S o comprimento de um intervalo. Suponha \nque todos os quadros tenham comprimento constante L = kRS, sendo R a taxa de transmissão do canal e k um \nnúmero inteiro grande. Suponha que haja N nós, cada um com um número infinito de quadros para enviar. \nAdmitimos também que dprop < S, de modo que todos os nós podem detectar uma colisão antes do final de um \nintervalo de tempo. O protocolo é o seguinte:\n   Redes de computadores e a Internet\n376\n• \nSe, para determinado intervalo, nenhum nó estiver de posse do canal, todos disputam o canal; em particular, \ncada nó transmite no intervalo com probabilidade p. Se exatamente um nó transmitir no intervalo, esse nó \ntomará posse do canal para os k – 1 intervalos subsequentes e transmitirá seu quadro inteiro.\n• \nSe algum nó estiver de posse do canal, todos os outros evitarão transmitir até que o nó que está de posse \ndo canal tenha terminado de transmitir seu quadro. Assim que esse nó tiver transmitido seu quadro, \ntodos os nós disputarão o canal.\n\t\n\t Note que o canal se alterna entre dois estados: o produtivo, que dura exatamente k intervalos, e o não \nprodutivo, que dura um número aleatório de intervalos. A eficiência do canal é, claramente, a razão k/(k + x), \nsendo x o número esperado de intervalos consecutivos não produtivos.\na.\t Para N e p fixos, determine a eficiência desse protocolo.\nb.\t Para N fixo, determine o p que maximiza a eficiência.\nc.\t Usando o p (que é uma função de N) encontrado em (b), determine a eficiência quando N tende ao \ninfinito.\nd.\t Mostre que essa eficiência se aproxima de 1 quando o comprimento do quadro é grande.\n\t\nP21.\t Considere a Figura 5.33 no problema P14. Determine os endereços MAC e IP para as interfaces do hospedeiro \nA, ambos os roteadores e do hospedeiro F. Determine os endereços MAC de origem e destino no quadro que \nencapsula esse datagrama IP, enquanto o quadro é transmitido (i) de A ao roteador esquerdo, (ii) do roteador \nesquerdo ao roteador direito, (iii) do roteador direito a F. Determine também os endereços IP de origem e \ndestino no datagrama IP encapsulado no quadro em cada um desses pontos no tempo.\n\t\nP22.\t Suponha que o roteador da extremidade esquerda da Figura 5.33 seja substituído por um comutador. Os \nhospedeiros A, B, C e D e o roteador direito têm uma conexão estrela com esse comutador. Determine \nos endereços MAC de origem e destino no quadro que encapsula esse datagrama IP enquanto o quadro \né transmitido (i) de A ao comutador, (ii) do comutador ao roteador direito, (iii) do roteador direito a F. \nDetermine também os endereços IP de origem e destino no datagrama IP encapsulado pelo quadro em cada \num desses pontos no tempo.\n\t\nP23.\t Considere a Figura 5.15. Suponha que todos os enlaces têm 100 Mbits/s. Qual é a vazão total máxima agregada \nque pode ser atingida entre os 9 hospedeiros e 2 servidores nessa rede? Você pode supor que qualquer \nhospedeiro ou servidor pode enviar a qualquer outro servidor ou hospedeiro. Por quê?\n\t\nP24.\t Suponha que três comutadores departamentais na Figura 5.15 são substituídos por hubs. Todos os enlaces \ntêm 100 Mbits/s. Agora responda às perguntas feitas no problema P23.\n\t\nP25.\t Suponha que todos os comutadores na Figura 5.15 sejam substituídos por hubs. Todos os enlaces têm \n100 Mbits/s. Agora responda às perguntas feitas no problema P23.\n\t\nP26.\t Vamos considerar a operação de aprendizagem do comutador no contexto de uma rede em que 6 nós, rotulados \nde A até F, sejam conectados em estrela a um comutador Ethernet. Suponha que (i) B envia um quadro a E, \n(ii) E responde com um quadro a B, (iii) A envia um quadro a B, (iv) B responde com um quadro a A. A tabela \ndo comutador está inicialmente vazia. Mostre o estado da tabela do comutador antes e depois de cada evento. \nPara cada um dos eventos, identifique os enlaces em que o quadro transmitido será encaminhado e justifique \nsuas respostas em poucas palavras.\n\t\nP27.\t Neste problema, exploraremos o uso de pequenos pacotes de aplicações de voz sobre IP (VoIP). Uma \ndesvantagem de um pacote pequeno é que uma grande parte da largura de banda do enlace é consumida por \nbytes de cabeçalho. Portanto, suponha que o pacote é formado por P bytes e 5 bytes de cabeçalho.\na.\t Considere o envio direto de uma fonte de voz codificada digitalmente. Suponha que a fonte esteja codificada \na uma taxa constante de 128 Kbits/s. Considere que cada pacote esteja integralmente cheio antes de a fonte \nenviá-lo para a rede. O tempo exigido para encher um pacote é o atraso de empacotamento. Determine, \nem termos de L, o atraso de empacotamento em milissegundos.\nb.\t Os atrasos de empacotamento maiores do que 20 ms podem causar ecos perceptíveis e desagradáveis. \nDetermine o atraso de empacotamento para L = 1.500 bytes (correspondente, mais ou menos, a um pacote \nEthernet de tamanho máximo) e para L = 50 bytes (correspondente a um pacote ATM).\ncamada de enlace: enlaces, redes de acesso e redes locais  377 \nc.\t Calcule o atraso de armazenagem e repasse em um único comutador para uma taxa de enlace R = \n622 Mbits/s para L = 1.500 bytes e L = 50 bytes.\nd.\t Comente as vantagens de usar um pacote de tamanho pequeno.\n\t\nP28.\t Considere o único comutador VLAN da Figura 5.25, e suponha que um roteador externo está conectado à \nporta 1 do comutador. Atribua endereços IP aos hospedeiros EE e CS e à interface do roteador. Relacione as \netapas usadas em ambas as camadas, de rede e de enlace, para transferir o datagrama IP ao hospedeiro EE e \nao hospedeiro CS. (Dica: Leia novamente a discussão sobre a Figura 5.19 no texto.)\n\t\nP29.\t Considere a rede MPLS mostrada na Figura 5.29 e suponha que os roteadores R5 e R6 agora são habilitados \npara MPLS. Imagine que queremos executar engenharia de tráfego de modo que pacotes de R6 destinados \na A sejam comutados para A via R6­\n‑R4­\n‑R3­\n‑R1, e pacotes de R5 destinados a A sejam comutados via R5­\n‑R4­\n‑R2­\n‑R1. Mostre as tabelas MPLS em R5 e R6, bem como a tabela modificada em R4, que tornariam isso \npossível.\n\t\nP30.\t Considere a mesma situação do problema anterior, mas suponha que os pacotes de R6 destinados a D estão \ncomutados via R6-R4-R3, enquanto os pacotes de R5 destinados a D sejam comutados via R4-R2-R1-R3. \nApresente as tabelas MPLS em todos os roteadores que tornariam isso possível.\n\t\nP31.\t Neste problema, você juntará tudo o que aprendeu sobre protocolos de Internet. Suponha que você entre em \numa sala, conecte-se à Ethernet e queira fazer o download de uma página. Quais são as etapas de protocolo \nutilizadas, desde ligar o computador até receber a página? Suponha que não tenha nada no seu DNS ou nos \ncaches do seu navegador quando você ligar seu computador. (Dica: as etapas incluem o uso de protocolos da \nEthernet, DHCP, ARP, DNS, TCP e HTTP.) Indique explicitamente em suas etapas como obter os endereços \nIP e MAC de um roteador de borda.\n\t\nP32.\t Considere a rede do datacenter com topologia hierárquica da Figura 5.30. Suponha agora que haja 80 pares de \nfluxos, com dez fluxos entre a primeira e a nona estante, dez entre a segunda e a décima estante, e assim por \ndiante. Suponha ainda que todos os enlaces na rede seja de 10 Gbits/s, exceto os enlaces entre os hospedeiros \ne os comutadores TOR, que são de 1 Gbit/s.\na.\t Cada fluxo tem a mesma velocidade de dados; determine a velocidade máxima de um fluxo.\nb.\t Para o mesmo padrão de tráfego, determine a velocidade máxima de um fluxo para a topologia altamente \ninterconectada da Figura 5.31.\nc.\t Agora, suponha que haja um padrão de tráfego semelhante, mas envolvendo 20 fluxos em cada hospedeiro \ne 160 pares de fluxos. Determine as velocidades de fluxo máximas para as duas topologias.\n\t\nP.33\t Considere a rede hierárquica da Figura 5.30 e suponha que o datacenter precise suportar a distribuição \nde correio eletrônico e vídeo entre outras aplicações. Suponha que quatro estantes de servidores sejam \nreservadas para correio eletrônico e quatro para vídeo. Para cada aplicação, todas as quatro estantes precisam \nestar debaixo de um único comutador da camada 2, pois os enlaces entre a camada 2 e a camada 1 não têm \nlargura de banda suficiente para suportar o tráfego dentro da aplicação. Para a aplicação de correio eletrônico, \nsuponha que, durante 99,9% do tempo, só três estantes sejam usadas e que a aplicação de vídeo tenha padrões \nde uso idênticos.\na.\t Durante que fração do tempo a aplicação de correio eletrônico precisa usar uma quarta estante? E a \naplicação de vídeo?\nb.\t Supondo que o uso de correio eletrônico e o uso de vídeo sejam independentes, durante que fração de \ntempo (ou, de modo equivalente, qual é a probabilidade de que) as duas aplicações precisam de sua quarta \nestante?\nc.\t Suponha que seja aceitável para uma aplicação ter um servidor parado por 0,001% do tempo ou menos \n(causando raros períodos de degradação de desempenho para os usuários). Discuta como a topologia da \nFigura 5.31 pode ser usada de modo que somente sete estantes sejam coletivamente designadas para as \nduas aplicações (supondo que a topologia possa suportar todo o tráfego).\n   Redes de computadores e a Internet\n378\nWireshark Lab\nNo site de apoio deste livro você encontrará uma tarefa de laboratório Wireshark, em inglês, que examina \na operação do protocolo IEEE 802.3 e o formato do quadro Wireshark. Uma segunda tarefa de laboratório Wi-\nreshark examina a sequência dos pacotes usados numa situação de rede doméstica.\nSimon S. Lam\nSimon S. Lam é professor e regente da cadeira de ciência da computa-\nção na Universidade do Texas, em Austin. De 1971 a 1974 trabalhou no ARPA \n \nNetwork Measurement Center na UCLA, com comutação de pacotes por satélite e por \nrádio. Liderou um grupo de pesquisa que inventou os sockets seguros e construiu o \nprimeiro protótipo da camada de sockets seguros, em 1993, denominada Programa-\nção de Rede Segura (Secure Network Programming), pela qual ganhou o ACM Softwa-\nre System Award em 2004. Os pontos de interesse de sua pesquisa são o projeto e \na análise de protocolos de rede e serviços de segurança. Graduou-se em engenharia \nelétrica na Washington State University e é mestre e doutor pela UCLA. Foi eleito à \nAcademia Nacional de Engenharia em 2007.\nENTREVISTA\nO que o fez se decidir pela especialização em \nredes?\nQuando cheguei à UCLA, recém-formado, no outono \nde 1969, minha intenção era estudar teoria de controle. \nEntão assisti às aulas de Leonard Kleinrock sobre teoria \ndas filas e ele me impressionou muito. Durante algum \ntempo trabalhei em controle adaptativo de sistemas de \nfila como possível tópico de tese. No início de 1972, Lar-\nry Roberts iniciou o projeto ARPAnet Satellite System \n(mais tarde denominado Packet Satellite). O professor \nKleinrock perguntou se eu queria participar do proje-\nto. A primeira coisa que fiz foi introduzir um algoritmo \nde recuo (backoff) simples, mas realista, no protocolo \nslotted ALOHA. Pouco tempo depois, encontrei muitos \nproblemas interessantes para pesquisar, como o proble-\nma da instabilidade da ALOHA e a necessidade de re-\ncuo, que formariam o núcleo da minha tese.\nO senhor teve participação ativa nos primórdios \nda Internet na década de 1970, desde seus tem-\npos de estudante na UCLA. Como era o panora-\nma naquela época? As pessoas faziam alguma \nideia do que a Internet se tornaria?\nNa verdade, a atmosfera não era diferente da de ou-\ntros projetos de construção de sistemas que já vi na \nindústria e no ambiente acadêmico. A meta inicial de-\nterminada para a ARPAnet era bastante modesta, isto \né, prover acesso a computadores caros a partir de locais \nremotos, de modo que mais cientistas pudessem utilizá\n-los. Contudo, com o início do projeto Packet Satellite \nem 1972 e do projeto Packet Radio em 1973, a meta \nda ARPA foi ampliada substancialmente. Em 1973, a \nARPA estava montando, ao mesmo tempo, três redes \nde pacotes diferentes e tornou-se necessário que Vint \nCerf e Bob Kahn desenvolvessem uma estratégia de in-\nterconexão.\nNaquela época, todos esses desenvolvimentos pro-\ngressivos na área de redes eram vistos (acredito eu) \nmais como lógicos do que mágicos. Ninguém pode-\nria ter previsto a escalada da Internet e o poder dos \ncomputadores pessoais de hoje. Transcorreu uma dé-\ncada antes do aparecimento dos primeiros PCs. Para \nentender melhor as coisas, a maioria dos estudantes \napresentava seus programas de computador em car-\ntões perfurados, para processamento em lote (batch). \nSomente alguns tinham acesso direto a computadores, \nque em geral eram acomodados em áreas restritas. Os \nmodems eram lentos e ainda raros. No meu tempo de \nestudante, eu tinha apenas um telefone sobre minha \ncamada de enlace: enlaces, redes de acesso e redes locais  379 \nmesa e usava papel e lápis para fazer a maior parte do \nmeu trabalho.\nEm sua opinião, qual é o futuro do campo das \nredes e da Internet?\nNo passado, a simplicidade do protocolo IP da In-\nternet era sua maior força para vencer a concorrência \ne se tornar o padrão na prática do trabalho com redes. \nDiferentemente de seus concorrentes, como X-25 na \ndécada de 1980 e ATM na década de 1990, o IP pode \nrodar sobre qualquer tecnologia de rede da camada de \nenlace, porque oferece apenas um serviço de datagra-\nma de melhor esforço. Assim, qualquer rede de pacotes \npode se conectar com a Internet.\nHoje, a maior força do IP é na realidade uma deficiên­\ncia. O IP é como uma camisa de força que confina o \ndesenvolvimento da Internet a direções específicas. Nos \núltimos anos, muitos pesquisadores redirecionaram \nseus esforços apenas para as camadas de aplicação. Há \ntambém uma grande concentração de pesquisas em re-\ndes sem fio e redes ocasionais (ad hoc), redes de senso-\nres e redes por satélite. Essas redes podem ser conside-\nradas ou sistemas autônomos ou sistemas da camada de \nenlace, que podem se desenvolver porque estão fora da \ncamisa de força do IP.\nHá muita gente animada com a possibilidade da \nutilização de sistemas P2P como plataforma para no-\nvas aplicações da Internet. Todavia, a utilização de \nrecursos da Internet por sistemas P2P é altamente \nineficiente. Uma das minhas preocupações é se a ca-\npacidade de transmissão e comutação do núcleo da \nInternet continuará a crescer mais rapidamente do que \na demanda de tráfego na Internet, enquanto ela cresce \npara interconectar todos os tipos de equipamentos e \nsuportar futuras aplicações habilitadas para P2P. Sem \num superprovisionamento substancial de capacidade, \ngarantir a estabilidade da rede na presença de ataques \nmal-intencionados e de congestionamento seria um \ndesafio significativo.\nO crescimento fenomenal da Internet também requer \na alocação de novos endereços IP em uma velocidade \nrápida, para operadores e empresas de rede do mundo \ninteiro. Se continuarmos neste ritmo, o bloco de ende-\nreços IPv4 não alocados se esgotará em alguns anos. \nQuando isso acontecer, grandes blocos de espaços de \nendereços só poderão ser alocados de espaços de en-\ndereço IPv6. Já que a adoção do IPv6 não está fazendo \nmuito sucesso, pela falta de incentivo para os novos \nusuários, IPv4 e IPv6 provavelmente coexistirão por \nmuitos anos. A migração bem-sucedida de uma Inter-\nnet predominantemente IPv4 para uma IPv6 exigirá \num grande esforço global.\nQual parte de seu trabalho lhe apresenta mais \ndesafios?\nA parte mais desafiadora do meu trabalho como \nprofessor é ensinar e motivar todos os estudantes que \nassistem a minhas aulas e todos os estudantes de dou-\ntorado que supervisiono, e não apenas os mais desta-\ncados. Os muito inteligentes e motivados podem exigir \num pouco de supervisão, mas não muito mais do que \nisso. Muitas vezes aprendo mais com esses estudantes \ndo que eles aprendem comigo. Ensinar e motivar os \nque não se destacam muito é um grande desafio.\nQue impactos sobre o aprendizado o senhor acha \nque a tecnologia terá no futuro?\nCom o tempo, quase todo o conhecimento huma-\nno estará acessível pela Internet, algo que será a mais \npoderosa ferramenta de aprendizado. Essa vasta base \nde conhecimento terá o potencial de nivelar o terreno \npara estudantes em todo o mundo. Por exemplo, es-\ntudantes motivados de qualquer país poderão acessar \nos melhores sites de aulas, conferências multimídia e \nmaterial de ensino. Já foi dito que as bibliotecas digi-\ntais do IEEE e da ACM aceleraram o desenvolvimento \nde pesquisadores da ciência da computação na China. \nCom o tempo, a Internet transcenderá todas as barrei-\nras geográficas ao aprendizado.\nNo mundo da telefonia pode-se dizer que os quinze últimos anos foram os anos dourados da telefonia \ncelular. O número de assinantes de telefones móveis no mundo inteiro aumentou de 34 milhões em 1993 para \nquase 5,5 bilhões no final de 2011 e, agora, ultrapassa o número de linhas telefônicas convencionais. As muitas \nvantagens dos telefones celulares são evidentes para todos — em qualquer lugar, a qualquer hora, acesso de-\nsimpedido à rede global de telefonia por meio de um equipamento leve e totalmente portátil. Com o advento \nde notebooks, palmtops, smartphones e a promessa de acesso desimpedido à Internet global de qualquer lugar, a \nqualquer hora, será que estamos prestes a assistir a uma explosão semelhante da utilização de dispositivos sem \nfio para acesso à Internet?\nIndependentemente do crescimento futuro de equipamentos sem fio para Internet, já ficou claro que redes \nsem fio e os serviços móveis relacionados que elas possibilitam vieram para ficar. Do ponto de vista de rede, os \ndesafios propostos, em particular nas camadas de enlace e de rede, são tão diferentes dos desafios das redes de \ncomputadores cabeadas que é necessário um capítulo inteiro (este capítulo) devotado ao estudo de redes sem fio \ne redes móveis.\nIniciaremos este capítulo com uma discussão sobre usuários móveis, enlaces e redes sem fio e sua relação \ncom as redes maiores (normalmente cabeadas) às quais se conectam. Traçaremos uma distinção entre os desafios \npropostos pela natureza sem fio dos enlaces de comunicação nessas redes e pela mobilidade que os enlaces sem fio \nhabilitam. Fazer essa importante distinção — entre sem fio e mobilidade — nos permitirá isolar, identificar e domi-\nnar melhor os conceitos fundamentais em cada área. Note que, na realidade, há muitos ambientes de rede nos quais \nos nós são sem fio, mas não são móveis (por exemplo, redes residenciais sem fio ou redes de escritórios compostas \npor estações de trabalho estacionárias e monitores de grandes dimensões), e que existem formas limitadas de mo-\nbilidade que não requerem enlaces sem fio (por exemplo, um profissional que utiliza um notebook em casa, desliga \no equipamento e o leva para seu escritório, onde o liga à rede cabeada da empresa em que trabalha). É claro que \nmuitos dos ambientes sem fio mais interessantes são aqueles em que os usuários são sem fio e também móveis — por \nexemplo, um cenário no qual um usuário móvel (digamos, no banco traseiro de um carro) mantém uma chamada \nde voz sobre IP (VoIP) e várias conexões TCP ativas enquanto corre pela rodovia a 160 km/h. É nesse ponto, em que \no sem fio se cruza com a mobilidade, que encontraremos os desafios técnicos mais interessantes!\nComeçaremos por ilustrar, primeiro, o cenário no qual consideraremos comunicação e mobilidade sem \nfio — uma rede na qual usuários sem fio (e possivelmente móveis) estão conectados à infraestrutura da rede \nmaior por um enlace sem fio na borda de rede. Então, consideraremos as características desse enlace sem fio na \nSeção 6.2. Nessa seção, incluímos uma breve introdução ao acesso múltiplo por divisão de código (code division \nRedes sem fio e\nredes móveis\n1\n3\n8 9\n7\nc\n \na\n \np\n \ní\n \nt\n \nu\n \nl\n \no\n2\n4 6\n5\nRedes sem fio e redes móveis  381 \nmultiple access — CDMA), um protocolo de acesso ao meio compartilhado que é utilizado com frequência em \nredes sem fio. Na Seção 6.3, estudaremos com certa profundidade os aspectos da camada de enlace do padrão \nda LAN sem fio IEEE 802.11 (Wi-Fi); também falaremos um pouco sobre Bluetooth e outras redes pessoais sem \nfio. Na Seção 6.4 daremos uma visão geral do acesso à Internet por telefone celular, incluindo 3G e as tecnologias \ncelulares emergentes 4G, que fornecem acesso à Internet por voz e em alta velocidade. Na Seção 6.5, voltaremos \nnossa atenção à mobilidade, focalizando os problemas da localização de um usuário móvel, do roteamento até o \nusuário móvel e da transferência (hand-off) do usuário móvel que passa dinamicamente de um ponto de conexão \ncom a rede para outro. Estudaremos como esses serviços de mobilidade são executados no padrão IP móvel e \nem GSM nas seções 6.6 e 6.7, respectivamente. Por fim, na Seção 6.8 consideraremos o impacto dos enlaces e da \nmobilidade sem fio sobre protocolos de camada de transporte e aplicações em rede.\n6.1  Introdução\nA Figura 6.1 mostra o cenário no qual consideraremos os tópicos de comunicação de dados e mobilidade \nsem fio. Começaremos mantendo nossa discussão dentro de um contexto geral o suficiente para abranger uma \nampla faixa de redes, entre elas LANs sem fio (como a IEEE 802.11) e redes celulares (como uma rede 3G); em \noutras seções, passaremos então para uma discussão mais detalhada de arquiteturas sem fio específicas. Podemos \nidentificar os seguintes elementos em uma rede sem fio:\n• Hospedeiros sem fio. Como no caso de redes cabeadas (ou com fio), hospedeiros são os equipamentos de \nsistemas finais que executam aplicações. Um hospedeiro sem fio pode ser um notebook, um palmtop, \num smartphone ou um computador de mesa. Os hospedeiros em si podem ser móveis ou não.\n• Enlaces sem fio. Um hospedeiro se conecta a uma estação-base (definida mais adiante) ou a outro hos-\npedeiro sem fio por meio de um enlace de comunicação sem fio. Tecnologias diferentes de enlace sem \nfio têm taxas de transmissão diversas e podem transmitir a distâncias variadas. A Figura 6.2 mostra duas \ncaracterísticas fundamentais (área de cobertura e taxa de enlace) dos padrões de enlace sem fio mais \npopulares. (A figura serve apenas para dar uma ideia aproximada dessas características. Por exemplo, al-\nguns desses tipos de redes só estão sendo empregados agora, e algumas taxas de enlace podem aumentar \nou diminuir além dos valores mostrados, dependendo da distância, condições do canal e do número de \nusuários na rede sem fio.) Abordaremos esses padrões mais adiante, na primeira metade deste capítulo; \nconsideraremos também outras características de enlaces sem fio (como suas taxas de erros de bit e as \ncausas desses erros) na Seção 6.2.\n\t\nNa Figura 6.1, enlaces sem fio conectam hospedeiros localizados na borda da rede com a infraestrutura da \nrede de maior porte. Não podemos nos esquecer de acrescentar que enlaces sem fio às vezes também são uti-\nlizados dentro de uma rede para conectar roteadores, comutadores e outros equipamentos de rede. Contudo, \nneste capítulo, focalizaremos a utilização da comunicação sem fio nas bordas da rede, pois é aqui que estão \nocorrendo muitos dos desafios técnicos mais interessantes e a maior parte do crescimento.\n• Estação-base. A estação-base é uma parte fundamental da infraestrutura de rede sem fio. Diferentemente \ndos hospedeiros e enlaces sem fio, uma estação-base não tem nenhuma contraparte óbvia em uma rede \ncabeada. Uma estação-base é responsável pelo envio e recebimento de dados (por exemplo, pacotes) de e \npara um hospedeiro sem fio que está associado a ela. Uma estação-base frequentemente será responsável \npela coordenação da transmissão de vários hospedeiros sem fio com os quais está associada. Quando dize-\nmos que um hospedeiro sem fio está “associado” a uma estação-base, isso quer dizer que (1) o hospedeiro \nestá dentro do alcance de comunicação sem fio da estação-base e (2) o hospedeiro usa a estação-base para \nretransmitir dados entre ele (o hospedeiro) e a rede maior. Torres celulares em redes celulares e pontos \nde acesso em LANs sem fio 802.11 são exemplos de estações-base.\n\t\nNa Figura 6.1, a estação-base está conectada à rede maior (isto é, à Internet, à rede corporativa ou re-\nsidencial, ou à rede telefônica); portanto, ela funciona como uma retransmissora da camada de enlace \nentre o hospedeiro sem fio e o resto do mundo com o qual o hospedeiro se comunica.\n   Redes de computadores e a Internet\n382\n\t\nQuando hospedeiros estão associados com uma estação-base, em geral diz-se que estão operando em \nmodo de infraestrutura, já que todos os serviços tradicionais de rede (por exemplo, atribuição de endere-\nço e rotea­\nmento) são fornecidos pela rede com a qual estiverem conectados por meio da estação-base. Em \nredes ad hoc, hospedeiros sem fio não dispõem de qualquer infraestrutura desse tipo com a qual possam \nse conectar. Na ausência de tal infraestrutura, os próprios hospedeiros devem prover serviços como rotea-\nmento, atribuição de endereço, tradução de endereços semelhante ao DNS e outros.\n\t\nQuando um hospedeiro móvel se desloca para fora da faixa de alcance de uma estação­\n‑base e entra na \nfaixa de outra, ele muda seu ponto de conexão com a rede maior (isto é, muda a estação-base com a \nqual está associado) — um processo denominado transferência (handoff). Essa mobilidade dá origem a \nmuitas questões desafiadoras. Se um hospedeiro pode se mover, como descobrir sua localização atual na \nrede de modo que seja possível lhe encaminhar dados? Como é realizado o endereçamento, visto que um \nhospedeiro pode estar em um dentre muitos locais possíveis? Se o hospedeiro se movimentar durante \numa conexão TCP ou ligação telefônica, como os dados serão roteados para que a conexão continue sem \ninterrupção? Essas e muitas (mas muitas!) outras questões fazem das redes sem fio e móveis uma área de \npesquisa muito interessante sobre redes.\n• Infraestrutura de rede. É a rede maior com a qual um hospedeiro sem fio pode querer se comunicar.\nApós discutir sobre as “partes” da rede sem fio, observamos que essas partes podem ser combinadas de diver-\nsas maneiras diferentes para formar diferentes tipos de redes sem fio. Você pode achar uma taxonomia desses tipos \nde redes sem fio útil ao ler este capítulo, ou ler/aprender mais sobre redes sem fio além deste livro. No nível mais alto, \nAcesso público Wi-Fi: em breve, em um poste próximo de você?\nPontos de acesso Wi-Fi — locais públicos onde os \nusuários podem encontrar acesso sem fio 802.11 — \nestão se tornando cada vez mais comuns em hotéis, \naeroportos e cafés ao redor do mundo. A maioria dos \ncampi universitários oferece acesso sem fio espalha-\ndo por toda a parte, e é difícil encontrar um hotel que \nnão oferece acesso à Internet sem fio.\nDurante a última década, diversas cidades projeta-\nram, implantaram e operaram redes Wi-Fi municipais. \nA visão de oferecer acesso Wi-Fi por toda a parte para \na comunidade como um serviço público (semelhante \naos postes de luz) — ajudando a eliminar a exclusão \ndigital por meio do acesso à Internet para todos os \ncidadãos e a promover o desenvolvimento econômi-\nco — é tentadora. Muitas cidades do mundo inteiro, \nincluindo Filadélfia, Toronto, Hong Kong, Minneapo-\nlis, Londres e Auckland, anunciaram planos de prover \nesse acesso sem fio dentro de todo o município, ou \njá fizeram isso de formas variadas. O objetivo na Fi-\nladélfia foi “transformar a Filadélfia no maior ponto de \nacesso Wi-Fi do país e ajudar a melhorar a educação, \neliminar a exclusão digital, aprimorar o desenvolvi-\nmento da região e reduzir os custos do governo”. O \nambicioso programa — um acordo entre a cidade, a \nWireless Philadelphia (entidade sem fins lucrativos) e \no ISP Earthlink — construiu uma rede operacional de \npontos de acesso 802.11b nos braços de poste de \niluminação e semáforos, abrangendo 80% da cidade. \nPorém, questões financeiras e operacionais fizeram a \nrede ser vendida a um grupo de investidores privados \nem 2008, que mais tarde a revenderam para a cidade \nem 2010. Outros centros, como Minneapolis, Toronto, \nHong Kong e Auckland, tiveram sucesso com esfor-\nços em menor escala.\nO fato de que redes 802.11 operam no espectro não \nlicenciado (e por isso podem ser realizadas sem a com-\npra dos caríssimos direitos de uso do espectro) parece \ntorná-las financeiramente atraentes. Porém, os pontos \nde acesso 802.11 (ver Seção 6.3) possuem alcance \nmuito mais curto que as estações-base de celular 3G \n(ver Seção 6.4), exigindo um maior número de pontos \nde acesso para cobrir a mesma região geográfica. Por \noutro lado, as redes de dados por celular, que oferece \nacesso à Internet, operam no espectro licenciado. As \noperadoras de celular pagam bilhões de dólares pelos \ndireitos de acesso ao espectro para suas redes, tornan-\ndo as redes de dados por celular um negócio lucrativo, \nem vez de um empreendimento municipal.\nHistória\nRedes sem fio e redes móveis  383 \nFigura 6.2  Características de enlaces de padrões selecionados de rede sem fio\n802.11a,g\n802.11n\n802.11b\n802.15.1\n3G: UMTS/WCDMA, CDMA2000\n2G: IS-95, CDMA, GSM\nInterna\nExterna\nExterna de \nmeia distância\nExterna de \nlonga distância\n10–30m\n50–200m\n200m–4Km\n5Km–20Km\n54 Mbits/s\n4 Mbits/s\n5–11 Mbits/s\n200 Mbits/s\n1 Mbit/s\n384 Kbits/s\nKR 06 02\nEnhanced 3G: HSPA\n4G: LTE\n802.11a,g ponto-a-ponto\nFigura 6.1  Elementos de uma rede sem fio\nKR 06.01.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p0 Wide x 23p6 Deep\n11/17/11 rossi\nInfraestrutura \nde rede\nLegenda:\nPonto de acesso sem ﬁo\nÁrea de cobertura\nHospedeiro sem ﬁo\nHospedeiro sem ﬁo em movimento\npodemos classificar as redes sem fio de acordo com dois critérios: (i) se um pacote na rede sem fio atravessa exata-\nmente um salto único sem fio ou múltiplos saltos sem fio, e (ii) se há infraestrutura na rede, como uma estação-base:\n• Salto único, com infraestrutura. Essas redes têm uma estação-base conectada a uma rede cabeada maior \n(por exemplo, a Internet). Além disso, toda a comunicação é feita entre a estação-base e um hospedeiro \nsem fio através de um único salto sem fio. As redes 802.11 que você utiliza na sala de aula, na lanchonete \nou na biblioteca; e as redes de dados por celular 3G, que aprenderemos em breve, encaixam-se nesta \ncategoria.\n   Redes de computadores e a Internet\n384\n• Salto único, sem infraestrutura. Nessas redes, não existe estação-base conectada à rede sem fio. Entretan-\nto, como veremos, um dos nós nessa rede de salto único pode coordenar as transmissões dos outros nós. \nAs redes Bluetooth (que serão estudadas na Seção 6.3.6) e as redes 802.11 no modo ad hoc são redes de \nsalto único, sem infraestrutura.\n• Múltiplos saltos, com infraestrutura. Nessas redes, está presente uma estação-base cabeada para as redes \nmaiores. Entretanto, alguns nós sem fio podem ter que restabelecer sua comunicação através de outros \nnós sem fio para se comunicarem por meio de uma estação-base. Algumas redes de sensores sem fio e as \nchamadas redes em malha sem fio se encaixam nesta categoria.\n• Múltiplos saltos, sem infraestrutura. Não existe estação-base nessas redes, e os nós podem ter de restabe-\nlecer mensagens entre diversos outros nós para chegar a um destino. Os nós também podem ser móveis, \nocorrendo mudança de conectividade entre eles — uma categoria de redes conhecida como redes mó-\nveis ad hoc (MANETs). Se os nós móveis forem veículos, essa rede é denominada rede veicular ad hoc \n(VANET). Como você pode imaginar, o desenvolvimento de protocolos para essas redes é desafiador e \nconstitui o assunto de muita pesquisa em andamento.\nNeste capítulo, vamos nos limitar às redes de salto único e, depois, principalmente às redes baseadas em \ninfraestrutura.\nAgora vamos nos aprofundar um pouco mais nos desafios técnicos que surgem em redes sem fio e móveis. \nComeçaremos considerando, em primeiro lugar, o enlace sem fio individual, deixando nossa discussão sobre \nmobilidade para outra parte deste capítulo.\n6.2  Características de enlaces e redes sem fio\nVamos começar considerando uma rede simples cabeada, por exemplo, uma rede residencial, com hos-\npedeiros interconectados por um comutador Ethernet cabeado (ver Seção 5.4). Se substituíssemos a Ethernet \ncabeada por uma rede 802.11 sem fio, uma interface de rede sem fio substituiria a interface Ethernet cabeada nos \nhospedeiros e um ponto de acesso substituiria o comutador Ethernet, mas, na camada de rede ou acima dela, \npraticamente nenhuma mudança seria necessária. Isso sugere que concentremos nossa atenção na camada de en-\nlace ao procurarmos diferenças importantes entre redes com fio e sem fio. Realmente, podemos encontrar várias \ndiferenças importantes entre um enlace com fio e um enlace sem fio:\n• Redução da força do sinal. Radiações eletromagnéticas são atenuadas quando atravessam algum tipo de \nmatéria (por exemplo, um sinal de rádio ao atravessar uma parede). O sinal se dispersará mesmo ao ar \nlivre, resultando na redução de sua força (às vezes denominada atenuação de percurso) à medida que \naumenta a distância entre emissor e receptor.\n• Interferência de outras fontes. Várias fontes de rádio transmitindo na mesma banda de frequência sofre-\nrão interferência umas das outras. Por exemplo, telefones sem fio de 2,4 GHz e LANs sem fio 802.11b \ntransmitem na mesma banda de frequência. Assim, o usuário de uma LAN sem fio 802.11b que estiver \nse comunicando por um telefone sem fio de 2,4 GHz pode esperar que nem a rede nem o telefone fun-\ncionem particularmente bem. Além da interferência de fontes transmissoras, o ruído eletromagnético \npresente no ambiente (por exemplo, um motor ou um equipamento de micro-ondas próximo) pode \ncausar interferência.\n• Propagação multivias. A propagação multivias (ou multicaminhos) ocorre quando partes da onda ele-\ntromagnética se refletem em objetos e no solo e tomam caminhos de comprimentos diferentes entre um \nemissor e um receptor. Isso resulta no embaralhamento do sinal recebido no destinatário. Objetos que se \nmovimentam entre o emissor e o receptor podem fazer com que a propagação multivias mude ao longo \ndo tempo.\nRedes sem fio e redes móveis  385 \nPara obter uma discussão detalhada sobre as características, modelos e medidas do canal sem fio, consulte \nAnderson [1995].\nA discussão anterior sugere que erros de bit serão mais comuns em enlaces sem fio do que em enlaces \ncom fio. Por essa razão, talvez não seja nenhuma surpresa que protocolos de enlace sem fio (como o protocolo \n802.11 que examinaremos na seção seguinte) empreguem não só poderosos códigos de detecção de erros por \nCRC, mas também protocolos de transferência de dados confiável em nível de enlace, que retransmitem quadros \n \ncorrompidos.\nTendo considerado as falhas que podem ocorrer em um canal sem fio, vamos voltar nossa atenção para o \nhospedeiro que recebe o sinal sem fio. Esse hospedeiro recebe um sinal eletromagnético que é uma combinação \nde uma forma degradada do sinal original transmitido pelo remetente (degradada pelos efeitos da atenuação e da \npropagação multivias, discutidas acima, entre outros) e um ruído de fundo no ambiente. A relação sinal-ruído \n(SNR — signal-to-noise ratio) é uma medida relativa da potência do sinal recebido (ou seja, a informação sendo \ntransmitida) e o ruído. A SNR costuma ser calculada em unidades de decibéis (dB), uma unidade de medida que, \nsegundo alguns, é utilizada por engenheiros elétricos principalmente para confundir cientistas da computação. \nA SNR, medida em dB, é vinte vezes a razão do logaritmo de base 10 da amplitude do sinal recebido à amplitude \ndo ruído. Para nossos fins, precisamos saber apenas que uma SNR maior facilita ainda mais para o destinatário \nextrair o sinal transmitido de um ruído de fundo.\nA Figura 6.3 (adaptada de Holland [2001]) mostra a taxa de erro de bits (BER — bit error rate) — em termos \nsimples, a probabilidade de um bit transmitido ser recebido com erro no destinatário — versus a SNR para três \ntécnicas de modulação diferentes para codificar informações para a transmissão em um canal sem fio idealizado. \nA teoria da modulação e da codificação, bem como a extração do sinal e a BER, vai além do escopo deste livro \n(consulte Schwartz [1980] para obter uma discussão sobre esses assuntos). Não obstante, a Figura 6.3 ilustra di-\nversas características da camada física que são importantes para entender os protocolos de comunicação sem fio \nda camada superior:\n• Para um determinado esquema de modulação, quanto mais alta for a SNR, mais baixa será a BER. Visto \nque um remetente consegue aumentar a SNR elevando sua potência de transmissão, ele pode reduzir a \nprobabilidade de um quadro ser recebido com erro diminuindo tal potência. Observe, entretanto, que \nhá um pequeno ganho prático no aumento da potência além de certo patamar, digamos que para dimi-\nnuir a BER de 10-12 para 10-13. Existem também desvantagens associadas com o aumento da potência de \ntransmissão: mais energia deve ser gasta pelo remetente (uma consideração importante para usuários \nmóveis, que utilizam bateria), e as transmissões do remetente têm mais probabilidade de interferir nas \ntransmissões de outro remetente (consulte Figura 6.4(b)).\n• Para determinada SNR, uma técnica de modulação com uma taxa de transmissão de bit maior (com erro ou \nnão) terá uma BER maior. Por exemplo, na Figura 6.3, com uma SNR de 10 dB, a modulação BPSK com \numa taxa de transmissão de 1 Mbit/s possui uma BER menor do que 10-7, enquanto para a modulação \nQAM16 com uma taxa de transmissão de 4 Mbits/s, a BER é 10-1, longe de ser útil na prática. Entretanto, \ncom uma SNR de 20 dB, a modulação QAM16 possui uma taxa de transmissão de 4 Mbits/s e uma BER \nde 10-7, enquanto a modulação BPSK possui uma taxa de transmissão de apenas 1 Mbit/s e uma BER tão \nbaixa como estar (literalmente) “fora da parada”\n. Se é possível suportar uma BER de 10-7, a taxa de trans-\nmissão mais alta apresentada pela modulação QAM16 faria desta a técnica de modulação preferida nesta \nsituação. Tais considerações dão origem à característica final, descrita a seguir.\n• A seleção dinâmica da técnica de modulação da camada física pode ser usada para adaptar a técnica de \nmodulação para condições de canal. A SNR (e, portanto, a BER) pode mudar, como resultado da mobi-\nlidade ou em razão das mudanças no ambiente. A modulação adaptativa e a codificação são usadas em \nsistemas de dados celulares e nas redes de dados Wi-Fi 802.11 e celular 3G, que estudaremos nas Seções \n6.3 e 6.4. Isso permite, por exemplo, a seleção de uma técnica de modulação que ofereça a mais alta taxa \nde transmissão possível sujeita a uma limitação na BER, para as características de determinado canal.\n   Redes de computadores e a Internet\n386\nTaxas de erros de bits mais altas e que variam com o tempo não são as únicas diferenças entre um enlace com \nfio e um enlace sem fio. Lembre-se de que, no caso de enlaces de difusão cabeados, cada nó recebe as transmissões \nde todos os outros nós. No caso de enlaces sem fio, a situação não é tão simples, conforme mostra a Figura 6.4. \nSuponha que a estação A esteja transmitindo para a estação B. Suponha também que a estação C esteja transmitin-\ndo para a estação B. O denominado problema do terminal oculto, obstruções físicas presentes no ambiente (por \nexemplo, uma montanha ou um prédio), pode impedir que A e C escutem as transmissões um do outro, mesmo \nque as transmissões de A e C interfiram no destino, B. Isso é mostrado na Figura 6.4(a). Um segundo cenário que \nresulta em colisões que não são detectadas no receptor é causado pelo desvanecimento da força de um sinal à \nmedida que se propaga pelo meio sem fio. A Figura 6.4(b) ilustra o caso em que a localização de A e C é tal que \nas potências de seus sinais não são suficientes para que eles detectem as transmissões um do outro, mas, mesmo \nassim, são fortes o bastante para interferir uma com a outra na estação B. Como veremos na Seção 6.3, o problema \ndo terminal oculto e o desvanecimento tornam o acesso múltiplo em uma rede sem fio consideravelmente mais \ncomplexo do que em uma rede cabeada.\n10–7\n10–6\n10–5\n10–4\n10–3\n10–2\n10–1\n10\n20\n30\n40\n0\nSNR (dB)\nBER\nQAM16\n(4 Mbits/s)\nQAM256\n(8 Mbits/s)\nBPSK\n(1Mbit/s)\nKR 06.03.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n16p9 Wide x 16p2 Deep\n11/17/11 rossi\nFigura 6.3  Taxa de erro de bits, taxa de transmissão e SNR\nFigura 6.4  Problema do terminal oculto (a) e do desvanecimento (b)\nA\nA\nC\nB\nC\nLocalização\nb.\na.\n0\nPotência do sinal\nB\nRedes sem fio e redes móveis  387 \n6.2.1  CDMA\nLembre-se de que dissemos, no Capítulo 5, que, quando hospedeiros se comunicam por um meio comparti-\nlhado, é preciso um protocolo para que os sinais enviados por vários emissores não interfiram nos receptores. No \nmesmo capítulo descrevemos três classes de protocolos de acesso ao meio: de partição de canal, de acesso aleató-\nrio e de revezamento. O acesso múltiplo por divisão de código (code division multiple access — CDMA) pertence \nà família de protocolos de partição de canal. Ele predomina em tecnologias de LAN sem fio e celulares. Por ser \ntão importante no mundo sem fio, examinaremos o CDMA rapidamente agora, antes de passar para tecnologias \nespecíficas de acesso sem fio nas próximas seções.\nCom um protocolo CDMA, cada bit que está sendo enviado é codificado pela multiplicação do bit por um \nsinal (o código) que muda a uma velocidade muito maior (conhecida como taxa de chipping) do que a sequên-\ncia original de bits de dados. A Figura 6.5 mostra um cenário simples e idealizado de codificação/decodificação \nCDMA. Suponha que a velocidade com que bits de dados originais cheguem ao codificador CDMA defina a uni-\ndade de tempo; isto é, cada bit original de dados a ser transmitido requer um intervalo de tempo de um bit. Seja \ndi o valor do bit de dados para o i-ésimo intervalo de bit. Por conveniência do cálculo matemático, representamos \no bit de dados com valor 0 por –1. Cada intervalo de bit é ainda subdividido em M mini-intervalos. Na Figura \n6.5, M = 8, embora, na prática, M seja muito maior. O código CDMA usado pelo remetente consiste em uma \nsequência de M valores, cm, m = 1, . . ., M, cada um assumindo um valor de +1 ou –1. No exemplo da Figura 6.5, \no código CDMA de M bits que está sendo usado pelo remetente é (1, 1, 1, –1, 1, –1, –1, –1).\nPara ilustrar como o CDMA funciona, vamos focalizar o i-ésimo bit de dados, di. Para o m-ésimo mini-in-\ntervalo do tempo de transmissão de bits de di, a saída do codificador CDMA, Zi,m, é o valor de di multiplicado pelo \nm-ésimo bit do código CDMA escolhido, cm:\n\t\nZi, m = di ⋅ cm\t\n(6.1)\nSe o mundo fosse simples e não houvesse remetentes interferindo, o receptor receberia os bits \ncodificados, Zi,m, e recuperaria os bits de dados originais, di, calculando:\n\t\ndi = 1\nM \nM\nm=1\nZi, m\ncm\n\t\n(6.2)\nTalvez o leitor queira repassar os detalhes do exemplo da Figura 6.5 para verificar se os bits originais de dados \nsão, de fato, corretamente recuperados no receptor usando a Equação 6.2.\nNo entanto, o mundo está longe de ser ideal e, como mencionamos antes, o CDMA deve funcionar na pre-\nsença de remetentes que interferem e que estão codificando e transmitindo seus dados usando um código desig-\nnado diferente. Mas, como um receptor CDMA pode recuperar bits de dados originais de um remetente quando \nestes estão sendo embaralhados com bits que estão sendo transmitidos por outros remetentes? O CDMA trabalha \nna hipótese de que os sinais de bits interferentes sendo transmitidos são aditivos. Isso significa, por exemplo, que, \nse três remetentes enviam um valor 1 e um quarto envia um valor –1 durante o mesmo mini-intervalo, então o \nsinal recebido em todos os receptores durante o mini-intervalo é 2 (já que 1 + 1 + 1 − 1 = 2). Na presença de vários \nremetentes, s calcula suas transmissões codificadas, Zs\ni,m, exatamente como na Equação 6.1. O valor recebido no \nreceptor durante o m-ésimo mini-intervalo do i-ésimo intervalo de bit, contudo, é agora a soma dos bits transmi-\ntidos de todos os N remetentes durante o mini-intervalo:\nZ*\ni, m =\nN\ns =1\nZs\ni, m\nSurpreendentemente, se os códigos dos remetentes forem escolhidos com cuidado, cada receptor pode re-\ncuperar os dados enviados por um dado remetente a partir do sinal agregado apenas usando o código do reme-\ntente, como na Equação 6.2:\n   Redes de computadores e a Internet\n388\n\t\ndi = 1\nM\nM\nm=1\nZi, m\n*\ncm\n\t\n(6.3)\nA Figura 6.6 ilustra um exemplo de CDMA com dois remetentes. O código CDMA de M bits usado pelo \nremetente que está acima é (1, 1, 1, –1, 1, –1, –1, –1), ao passo que o código CDMA usado pelo que está embaixo \né (1, –1, 1, 1, 1,–1, 1, 1). A Figura 6.6 ilustra um receptor recuperando os bits de dados originais do remetente \nque está acima. Note que o receptor pode extrair os dados do remetente 1, a despeito da transmissão interferente \ndo remetente 2.\nVoltando à analogia do coquetel apresentada no Capítulo 5, um protocolo CDMA é semelhante à situação \nem que os convidados falam vários idiomas; nessa circunstância, os seres humanos até que são bons para manter \nconversações no idioma que entendem e, ao mesmo tempo, continuar filtrando (rejeitando) outras conversações. \nVemos aqui que o CDMA é um protocolo de partição, pois reparte o espaço de código (e não o tempo ou a fre-\nquência) e atribui a cada nó uma parcela dedicada do espaço de código.\nNossa discussão do código CDMA aqui é necessariamente breve; na prática, devem ser abordadas inúmeras \nquestões diferentes. Primeiro, para que receptores CDMA consigam extrair o sinal de um emissor qualquer, os \ncódigos CDMA devem ser escolhidos cuidadosamente. Segundo, nossa discussão considerou que as intensidades \ndos sinais recebidos de vários emissores são as mesmas; na realidade, isso pode ser difícil de conseguir. Existe \nmuita literatura abordando essas e outras questões relativas ao CDMA; veja Pickholtz [1982]; Viterbi [1995], se \nquiser mais detalhes.\nFigura 6.5  \u0007\nUm exemplo simples de CDMA: codificação no remetente, decodificação no \nreceptor\n1\n1 1 1\n-1\n-1 -1 -1\n1\n1 1 1\n-1\n-1 -1 -1\n1\n-1\n-1\n-1-1\n1 1 1\n1\n-1\n-1 -1 -1\n1 1 1\nIntervalo 1\nEntrada recebida\nIntervalo 0\nEntrada recebida\nCódigo\n1\n-1\n-1\n-1-1\n1 1 1\n1\n-1\n-1 -1 -1\n1 1 1\nBits de \ndados\nCódigo\n1\n1 1 1\n-1\n-1 -1 -1\n1\n1 1 1\n-1\n-1 -1 -1\nd1 = -1\nd0 = 1\nIntervalo 1\nRemetente\nSaída do canal Zi,m\nReceptor\nZi,m\ndi • cm\n=\nZi,m • cm\nd\nM\ni\nm=1\nM\n=\n∑\nIntervalo 1\nSaída do canal\nIntervalo 0\nSaída do canal\nIntervalo 0\nd1 = -1\nd0 = 1\nKR 06 05 eps\nRedes sem fio e redes móveis  389 \nFigura 6.6  Um exemplo de CDMA com dois remetentes\nReceptor 1\n1\n1 1 1\n-1\n-1 -1 -1\n1\n1 1 1\n-1\n-1 -1 -1\nIntervalo 1\nentrada recebida\nIntervalo 0\nentrada recebida\nBits de \ndados\nBits de \ndados\n1\n1 1 1\n-1\n-1 -1 -1\n1\n1 1 1\n-1\n-1 -1 -1\nCódigo\nRemetentes\n1 1 1\n-1\n1 1 1\n-1\n1\n-1\n-1\n1 1 1\n1 1\nCódigo\nCódigo\n+\n-2\n2\n2 2 2\n2\n-2\n2\n-2\n2\n2 2 2\n2\n-2\n2\nCanal, Zi,m\n*\nZi,m\ndi  • cm\n=\nZi,m • cm\nd\nM\ni\nm=1\nM\n=\n∑\nd1 = -1\nd0 = 1\nd1 = 1\n2\n1\n1\n*\n2\n2\n2\nZi,m\ndi  • cm\n=\n1\n1\n1\nd0 = 1\n2\n1\n1\nd1 = -1\nd0 = 1\n1\n1\nKR 06.06.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n33p3 Wide x 35p3 Deep\n11/17/11 rossi\n6.3  Wi-Fi: LANs sem fio 802.11\nPresentes no local de trabalho, em casa, em instituições educacionais, em cafés, aeroportos e esquinas, as \nLANs sem fio agora são uma das mais importantes tecnologias de rede de acesso na Internet de hoje. Embora \nmuitas tecnologias e padrões para LANs sem fio tenham sido desenvolvidos na década de 1990, uma classe \nparticular de padrões surgiu claramente como a vencedora: a LAN sem fio IEEE 802.11, também conhecida \ncomo Wi-Fi. Nesta seção estudaremos em mais detalhes as LANs sem fio 802.11, examinando a estrutura do \nquadro 802.11, o protocolo 802.11 de acesso ao meio e a interconexão de LANs 802.11 com LANs Ethernet \ncabeadas.\nHá diversos padrões 802.11 para tecnologia de LAN sem fio, entre eles 802.11b, 802.11a e 802.11g. A Tabela \n6.1 apresenta um resumo das principais características desses padrões. 802.11g é, de longe, a tecnologia mais \npopular. Estão também disponíveis diversos mecanismos de modos duplo (802.11a/g) e triplo (802.11a/b/g).\nOs três padrões 802.11 compartilham muitas características. Todos usam o mesmo protocolo de acesso ao meio, \nCSMA/CA, que discutiremos em breve. Os três também usam a mesma estrutura de quadro para seus quadros de \n   Redes de computadores e a Internet\n390\ncamada de enlace. Todos os três padrões têm a capacidade de reduzir sua taxa de transmissão para alcançar distâncias \nmaiores. E todos os três padrões permitem “modo de infraestrutura” e “modo ad hoc”\n, como discutiremos em breve. \nContudo, conforme mostra a Tabela 6.1, eles apresentam algumas diferenças importantes na camada física.\nA LAN sem fio 802.11b tem uma taxa de dados de 11 Mbits/s e opera na faixa de frequência não licenciada de \n2,4 a 2,485 GHz, competindo por espectro de frequência com telefones e fornos de micro-ondas de 2,4 GHz. LANs \nsem fio 802.11a podem funcionar a taxas de bits significativamente mais altas, porém em frequências mais altas. \nComo operam a uma frequência mais alta, a distância de transmissão dessas LANs é mais curta para determinado \nnível de potência e elas sofrem mais com a propagação multivias. LANs 802.11g, que operam na mesma faixa de \nfrequência mais baixa das LANs 802.11b e que são compatíveis com a 802.11b (para que se possa atualizar clientes \n802.11b de forma incremental), porém com as taxas de transmissão mais altas da 802.11a, devem permitir que os \nusuários tenham o melhor dos dois mundos.\nUm padrão Wi-Fi relativamente novo, 802.11n [IEEE 802.11n, 2012], utiliza antenas de entrada múltipla \ne saída múltipla (MIMO); ou seja, duas ou mais antenas no lado remetente e duas ou mais antenas no lado \ndestinatário que estão transmitindo/recebendo sinais diferentes [Diggavi, 2004]. Dependendo do esquema de \nmodulação utilizado, é possível alcançar taxas de transmissão de centenas de megabits por segundo com 802.11n.\nTabela 6.1  Resumo dos padrões IEEE 802.11\nPadrão\nFaixa de frequências (EUA)\nTaxa de dados\n802.11b\n2,4–2,485 GHz\naté 11 Mbits/s\n802.11a\n5,1–5,8 GHz\naté 54 Mbits/s\n802.11g\n2,4–2,485 GHz\naté 54 Mbits/s\n6.3.1  A arquitetura 802.11\nA Figura 6.7 ilustra os principais componentes da arquitetura de LAN sem fio 802.11. O bloco de construção \nfundamental da arquitetura 802.11 é o conjunto básico de serviço (basic service set — BSS). Um BSS contém uma ou \nmais estações sem fio e uma estação-base central, conhecida como um ponto de acesso (access point — AP) na termi-\nnologia 802.11. A Figura 6.7 mostra o AP em cada um dos dois BSSs conectando-se a um dispositivo de interconexão \n(tal como um comutador ou um roteador), que, por sua vez, leva à Internet. Em uma rede residencial típica, há apenas \num AP e um roteador (normalmente integrados como uma unidade) que conecta o BSS à Internet.\nComo acontece com dispositivos Ethernet, cada estação sem fio 802.11 tem um endereço MAC de 6 bytes \nque é armazenado no firmware do adaptador da estação (isto é, na placa de interface de rede 802.11). Cada AP \ntambém tem um endereço MAC para sua interface sem fio. Como na Ethernet, esses endereços MAC são admi-\nnistrados pelo IEEE e são (em teoria) globalmente exclusivos.\nComo observamos na Seção 6.1, LANs sem fio que disponibilizam APs em geral são denominadas LANs sem \nfio de infraestrutura e, nesse contexto, “infraestrutura” significa os APs junto com a infraestrutura de Ethernet \ncabeada que interconecta os APs e um roteador. A Figura 6.8 mostra que estações IEEE 802.11 também podem se \nagrupar e formar uma rede ad hoc — rede sem nenhum controle central e sem nenhuma conexão com o “mundo \nexterior”\n. Nesse caso, a rede é formada conforme a necessidade, por dispositivos móveis que, por acaso, estão pró-\nximos uns dos outros, têm necessidade de se comunicar e não dispõem de infraestrutura de rede no lugar em que \nse encontram. Uma rede ad hoc pode ser formada quando pessoas que portam notebooks se reúnem (por exemplo, \nem uma sala de conferências, um trem ou um carro) e querem trocar dados na ausência de um AP centralizado. As \nredes ad hoc estão despertando um interesse extraordinário com a contínua proliferação de equipamentos portáteis \nque podem se comunicar. Porém, nesta seção, concentraremos nossa atenção em LANs sem fio com infraestrutura.\nRedes sem fio e redes móveis  391 \nCanais e associação\nEm 802.11, cada estação sem fio precisa se associar com um AP antes de poder enviar ou receber dados da \ncamada de rede. Embora todos os padrões 802.11 usem associação, discutiremos esse tópico especificamente no \ncontexto da IEEE 802.11b/g.\nAo instalar um AP, um administrador de rede designa ao ponto de acesso um Identificador de Conjunto de \nServiços (Service Set Identifier — SSID) composto de uma ou duas palavras. (O comando “veja redes disponíveis” \nno Microsoft Windows XP, por exemplo, apresenta uma lista que mostra o SSID de todos os APs ordenado por fai-\nxa.) O administrador também deve designar um número de canal ao AP. Para entender números de canal, lembre­\n‑se de que as redes 802.11b operam na faixa de frequência de 2,4 GHz a 2,485 GHz. Dentro dessa faixa de 85 MHz, \no padrão 802.11 define 11 canais que se sobrepõem em parte. Não há sobreposição entre quaisquer dois canais se, \ne somente se, eles estiverem separados por quatro ou mais canais. Em particular, o conjunto dos canais 1, 6 e 11 é \no único de três canais não sobrepostos. Isso significa que um administrador poderia criar uma LAN sem fio com \numa taxa máxima de transmissão agregada de 33 Mbits/s instalando três APs 802.11b na mesma localização física, \ndesignando os canais 1, 6 e 11 aos APs e interconectando cada um desses APs com um comutador.\nFigura 6.7  A arquitetura de LAN IEEE 802.11\nKR 06.07.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n22p7 Wide x 17p0 Deep\n11/17/11 rossi\nInternet\nComutador \nou roteador\nAP\nBSS 1\nBSS 2\nAP\nFigura 6.8  Uma rede ad hoc IEEE 802.11\nBSS\nKR 06.08.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n11p0 Wide x 12p0 Deep\n11/17/11 rossi\n   Redes de computadores e a Internet\n392\nAgora que já entendemos o básico sobre canais 802.11, vamos descrever uma situação interessante (e que \nnão é completamente fora do comum) — uma selva de Wi-Fis. Uma selva de Wi-Fis (Wi-Fi jungle) é qualquer \nlocalização física na qual uma estação sem fio recebe um sinal suficientemente forte de dois ou mais APs. Por \nexemplo, em muitos cafés da cidade de Nova York, uma estação sem fio pode captar um sinal de diversos APs \npróximos. Um deles pode ser o AP gerenciado pelo café, enquanto os outros podem estar localizados em apar-\ntamentos vizinhos. Cada ponto de acesso provavelmente estaria localizado em uma sub-rede IP diferente e teria \nsido designado independentemente a um canal.\nAgora suponha que você entre nessa selva de Wi-Fis com seu computador portátil, em busca de acesso \nà Internet sem fio e de um cafezinho. Suponha que há cinco APs na selva de Wi-Fis. Para conseguir acesso à \nInternet, sua estação sem fio terá de se juntar a exatamente uma das sub-redes e, portanto, precisará se associar \ncom exatamente um dos APs. Associar significa que a estação sem fio cria um fio virtual entre ela mesma e o AP. \nDe modo específico, só o AP associado enviará quadros de dados (isto é, quadros contendo dados, tal como um \ndatagrama) a sua estação sem fio e esta enviará quadros de dados à Internet apenas por meio do AP associado. \nMas como sua estação sem fio se associa com um determinado AP? E, o que é mais fundamental, como sua \nestação sem fio sabe quais APs estão dentro da selva, se é que há algum?\nO padrão 802.11b requer que um AP envie periodicamente quadros de sinalização, cada qual incluindo o \nSSID e o endereço MAC do AP. Sua estação sem fio, sabendo que os APs estão enviando quadros de sinalização, \nfaz uma varredura dos 11 canais em busca de quadros de sinalização de quaisquer APs que possam estar por lá \n(alguns dos quais talvez estejam transmitindo no mesmo canal — afinal, estamos na selva!). Ao tomar conheci-\nmento dos APs disponíveis por meio dos quadros de sinalização, você (ou seu hospedeiro sem fio) seleciona um \ndesses pontos de acesso para se associar.\nO padrão 802.11 não especifica um algoritmo para selecionar com quais dos APs disponíveis se associar; \nesse algoritmo é de responsabilidade dos projetistas do firmware e do software 802.11 em seu hospedeiro sem \nfio. Em geral, o hospedeiro escolhe o AP cujo quadro de sinalização é recebido com a intensidade de sinal mais \nalta. Embora uma intensidade alta do sinal seja algo bom (veja, por exemplo, a Figura 6.3), esta não é a única \ncaracterística do AP que determinará o desempenho que um hospedeiro recebe. Em particular, é possível que o \nAP selecionado tenha um sinal forte, mas pode ser sobrecarregado com outros hospedeiros associados (que pre-\ncisarão compartilhar a largura de banda sem fio naquele AP), enquanto um AP não carregado não é selecionado \nem razão de um sinal levemente mais fraco. Diversas formas alternativas de escolher os APs foram propostas \nrecentemente [Vasudevan, 2005; Nicholson, 2006; Sudaresan, 2006]. Para obter uma discussão interessante e \nprática de como a intensidade do sinal é medida, consulte Bardwell [2004].\nO processo de varrer canais e ouvir quadros de sinalização é conhecido como varredura passiva (veja a \nFigura 6.9(a)). Um hospedeiro sem fio pode também realizar uma varredura ativa, transmitindo um quadro \nde investigação que será recebido por todos os APs dentro de uma faixa do hospedeiro sem fio, como mostrado \nna Figura 6.9(b). Os APs respondem ao quadro de requisição de investigação com um quadro de resposta de \ninvestigação. O hospedeiro sem fio pode, então, escolher o AP com o qual irá se associar dentre os APs que estão \nrespondendo.\nApós selecionar o AP ao qual se associará, o hospedeiro sem fio envia um quadro de solicitação de as-\nsociação ao AP, e este responde com um quadro de resposta de associação. Observe que essa segunda apre-\nsentação de solicitação/resposta é necessária com a varredura ativa, visto que um AP de resposta ao quadro \nde solicitação de investigação inicial não sabe quais dos (possivelmente muitos) APs de resposta o hospedeiro \nescolherá para se associar, do mesmo modo que um cliente DHCP pode escolher entre servidores múltiplos \nDHCP (veja a Figura 4.21). Uma vez associado ao AP, o hospedeiro desejará entrar na sub-rede (no sentido do \nendereçamento IP da Seção 4.4.2) à qual pertence o AP. Assim, o hospedeiro normalmente enviará uma men-\nsagem de descoberta DHCP (veja a Figura 4.21) à sub-rede por meio de um AP a fim de obter um endereço IP \nna sub-rede. Logo que o endereço é obtido, o resto do mundo, então, vê esse hospedeiro apenas como outro \nhospedeiro com um endereço IP naquela sub-rede.\nRedes sem fio e redes móveis  393 \nFigura 6.9  Varredura passiva e ativa para pontos de acesso\nKR 06.09.eps\nAW/Kurose and Ross\nComputer Networking 6/e\nsize:  33p0 wide  x  16p4 deep\n11/17/11 rossi\n1\n1\n3\n2\nH1\nAP 2\nAP 1\nBBS 1\na. Varredura passiva\n \n1. Quadros de sinalização enviados dos Aplicações\n \n2. Quadro de Solicitação de Associação enviado:\n \n \nH1 para AP selecionado\n \n3. Quadro de Resposta de Associação enviado:\n \n \nAP selecionado para H1\na. Varredura ativa\n \n1. Difusão do quadro de Solicitação de Investigação de H1\n \n2. Quadro de Resposta de Investigações enviado das Aplicações\n \n3. Quadro de Solicitação de Associação enviado:\n \n \nH1 para AP selecionado\n \n4. Quadro de Resposta de Associação enviado:\n \n \nAP selecionado para H1\n \n \nBBS 2\n2\n2\n4\n3\nH1\nAP 2\nAP 1\nBBS 1\nBBS 2\n1\nPara criar uma associação com um determinado AP, a estação sem fio talvez tenha de se autenticar peran-\nte o AP. LANs sem fio 802.11 dispõem de várias alternativas para autenticação e acesso. Uma abordagem, usada \npor muitas empresas, é permitir o acesso a uma rede sem fio com base no endereço MAC de uma estação. \nUma segunda abordagem, usada por muitos cafés Internet, emprega nomes de usuários e senhas. Em ambos \nos casos, o AP em geral se comunica com um servidor de autenticação usando um protocolo como o RADIUS \n[RFC 2865] ou o DIAMETER [RFC 3588]. Separar o servidor de autenticação do AP permite que um servidor \nde autenticação atenda a muitos APs, centralizando as decisões de autenticação e acesso (quase sempre deli-\ncadas) em um único servidor e mantendo baixos os custos e a complexidade do AP. Veremos, na Seção 8.8, \nque o novo protocolo IEEE 802.11i, que define aspectos de segurança da família de protocolos 802.11, adota \nexatamente essa técnica.\n6.3.2  O protocolo MAC 802.11\nUma vez associada com um AP, uma estação sem fio pode começar a enviar e receber quadros de dados \nde e para o ponto de acesso. Porém, como várias estações podem querer transmitir quadros de dados ao mesmo \ntempo sobre o mesmo canal, é preciso um protocolo de acesso múltiplo para coordenar as transmissões. Aqui, \nestação significa uma estação sem fio ou um AP. Como discutimos no Capítulo 5 e na Seção 6.2.1, em termos \ngerais, há três classes de protocolos de acesso múltiplo: partição de canal (incluindo CDMA), acesso aleatório e \nrevezamento. Inspirados pelo enorme sucesso da Ethernet e seu protocolo de acesso aleatório, os projetistas do \n802.11 escolheram um protocolo de acesso aleatório para as LANs sem fio 802.11. Esse protocolo de acesso alea-\ntório é denominado CSMA com prevenção de colisão ou, mais sucintamente, CSMA/CA. Do mesmo modo que \no CSMA/CD da Ethernet, o “CSMA” de CSMA/CA quer dizer “acesso múltiplo por detecção de portadora”\n, o que \nsignifica que cada estação sonda o canal antes de transmitir e abstém-se de transmitir quando percebe que o canal \nestá ocupado. Embora tanto a Ethernet quanto o 802.11 usem acesso aleatório por detecção de portadora, os dois \nprotocolos MAC apresentam diferenças importantes. Primeiro, em vez de usar detecção de colisão, o 802.11 usa \ntécnicas de prevenção de colisão. Segundo, por causa das taxas relativamente altas de erros de bits em canais sem \nfio, o 802.11 (ao contrário da Ethernet) usa um esquema de reconhecimento/retransmissão (ARQ) de camada de \nenlace. Mais adiante descreveremos os esquemas usados pelo 802.11 para prevenção de colisão e reconhecimento \nna camada de enlace.\n   Redes de computadores e a Internet\n394\nLembre-se de que, nas seções 5.3.2 e 5.4.2, dissemos que, com o algoritmo de detecção de colisão, uma \nestação Ethernet ouve o canal à medida que transmite. Se, enquanto estiver transmitindo, a estação detectar que \nalguma outra estação também está, ela abortará sua transmissão e tentará novamente após uma pequena unidade \nde tempo aleatória. Ao contrário do protocolo Ethernet 802.3, o protocolo MAC 802.11 não implementa detecção \nde colisão. Isso se deve a duas razões importantes:\n• A capacidade de detectar colisões exige as capacidades de enviar (o próprio sinal da estação) e de receber \n(para determinar se alguma outra estação está transmitindo) ao mesmo tempo. Como a potência do sinal \nrecebido em geral é muito pequena em comparação com a potência do sinal transmitido no adaptador \n802.11, é caro construir um hardware que possa detectar colisões.\n• Mais importante, mesmo que o adaptador pudesse transmitir e ouvir ao mesmo tempo (e, presumivel-\nmente, abortar transmissões quando percebesse um canal ocupado), ainda assim ele não seria capaz \nde detectar todas as colisões, devido ao problema do terminal escondido e do desvanecimento, como \ndiscutimos na Seção 6.2.\nComo LANs 802.11 sem fio não usam detecção de colisão, uma vez que uma estação comece a transmitir \num quadro, ela o transmite integralmente; isto é, tão logo uma estação inicie, não há volta. Como é de se esperar, \ntransmitir quadros inteiros (em particular os longos) quando existe grande possibilidade de colisão pode de-\ngradar significativamente o desempenho de um protocolo de acesso múltiplo. Para reduzir a probabilidade de \ncolisões, o 802.11 emprega diversas técnicas de prevenção de colisão, que discutiremos em breve.\nAntes de considerar prevenção de colisão, contudo, primeiro devemos examinar o esquema de reconheci-\nmento na camada de enlace do 802.11. Lembre-se de que dissemos, na Seção 6.2, que, quando uma estação em \numa LAN sem fio envia um quadro, este talvez não chegue intacto à estação de destino, por diversos motivos. \nPara lidar com essa probabilidade não desprezível de falha, o protocolo MAC 802.11 usa reconhecimentos de \ncamada de enlace. Como ilustrado na Figura 6.10, quando a estação de destino recebe um quadro que passou na \nverificação de CRC, ela espera um curto período de tempo, conhecido como Espaçamento Curto Interquadros \n(Short Inter-Frame Spacing — SIFS), e então devolve um quadro de reconhecimento. Se a estação transmissora \nnão receber um reconhecimento em dado período de tempo, ela admitirá que ocorreu um erro e retransmitirá o \nquadro usando de novo o protocolo CSMA/CA para acessar o canal. Se a estação transmissora não receber um \nreconhecimento após certo número fixo de retransmissões, desistirá e descartará o quadro.\nAgora que já discutimos como o 802.11 usa reconhecimentos da camada de enlace, estamos prontos para \ndescrever o protocolo CSMA/CA 802.11. Suponha que uma estação (pode ser uma estação sem fio ou um AP) \ntenha um quadro para transmitir.\n1.\t Se inicialmente a estação perceber que o canal está ocioso, ela transmitirá seu quadro após um curto pe-\nríodo de tempo conhecido como Espaçamento Interquadros Distribuído (Distributed Inter-Frame Space \n— DIFS); ver Figura 6.10.\n2.\t Caso contrário, a estação escolherá um valor aleatório de recuo usando o recuo exponencial binário (con-\nforme encontramos na Seção 5.3.2) e fará a contagem regressiva a partir desse valor quando perceber que \no canal está ocioso. Se a estação perceber que o canal está ocupado, o valor do contador permanecerá \ncongelado.\n3.\t Quando o contador chegar a zero (note que isso pode ocorrer somente quando a estação percebe que o \ncanal está ocioso), a estação transmitirá o quadro inteiro e então ficará esperando um reconhecimento.\n4.\t Se receber um reconhecimento, a estação transmissora saberá que o quadro foi corretamente recebido na \nestação de destino. Se a estação tiver outro quadro para transmitir, iniciará o protocolo CSMA/CA na eta-\npa 2. Se não receber um reconhecimento, a estação entrará de novo na fase de recuo na etapa 2 e escolherá \num valor aleatório em um intervalo maior.\nLembre-se de que, no protocolo de acesso múltiplo CSMA/CD (Seção 5.5.2), uma estação começa a trans-\nmitir tão logo percebe que o canal está ocioso. Com o CSMA/CA, entretanto, a estação priva-se de transmitir \nRedes sem fio e redes móveis  395 \nenquanto realiza a contagem regressiva, mesmo quando percebe que o canal está ocioso. Por que o CSMA/CD e \no CDMA/CA adotam essas abordagens diferentes aqui?\nPara responder a essa pergunta, vamos considerar um cenário com duas estações em que cada uma tem um \nquadro a transmitir, mas nenhuma transmite imediatamente porque percebe que uma terceira estação já está trans-\nmitindo. Com o CSMA/CD da Ethernet, cada uma das duas estações transmitiria tão logo detectasse que a terceira \nestação terminou de transmitir. Isso causaria uma colisão, o que não é um problema sério em CSMA/CD, já que \nambas as estações abortariam suas transmissões e assim evitariam a transmissão inútil do restante dos seus quadros. \nEntretanto, com 802.11 a situação é bem diferente. Como o 802.11 não detecta uma colisão nem aborta transmis-\nsão, um quadro que sofra uma colisão será transmitido integralmente. Assim, a meta do 802.11 é evitar colisões \nsempre que possível. Com esse protocolo, se duas estações perceberem que o canal está ocupado, ambas entrarão \nimediatamente em backoff aleatório e, esperamos, escolherão valores diferentes de backoff. Se esses valores forem, \nde fato, diferentes, assim que o canal ficar ocioso, uma das duas começará a transmitir antes da outra e (se as duas \nnão estiverem ocultas uma da outra) a “estação perdedora” ouvirá o sinal da “estação vencedora”\n, interromperá seu \ncontador e não transmitirá até que a estação vencedora tenha concluído sua transmissão. Desse modo é evitada uma \ncolisão dispendiosa. É claro que ainda podem ocorrer colisões com 802.11 nesse cenário: as duas estações podem \nestar ocultas uma da outra ou podem escolher valores de backoff aleatório próximos o bastante para que a transmis-\nsão da estação que se inicia primeiro tenha ainda de atingir a segunda. Lembre-se de que já vimos esse problema \nantes em nossa discussão sobre algoritmos de acesso aleatório no contexto da Figura 5.12.\nTratando de terminais ocultos: RTS e CTS\nO protocolo 802.11 MAC também inclui um esquema de reserva inteligente (mas opcional) que ajuda a evitar \ncolisões mesmo na presença de terminais ocultos. Vamos estudar esse esquema no contexto da Figura 6.11, que mos-\ntra duas estações sem fio e um ponto de acesso. Ambas as estações estão dentro da faixa do AP (cuja área de cobertura \né representada por um círculo sombreado) e ambas se associaram com o AP\n. Contudo, pelo desvanecimento, as faixas \nFigura 6.10  802.11 usa reconhecimentos da camada de enlace\nDestino\nDIFS\nSIFS\ndados\nack\nOrigem\nKR 06.10.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n14p5 Wide x 23p2 Deep\n11/17/11 rossi\n   Redes de computadores e a Internet\n396\nFigura 6.11  Exemplo de terminal oculto: H1 está oculto de H2, e vice-versa\nKR 06.11.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n22p5 Wide x 12p10 Deep\n11/17/11 rossi\nAP\nH1\nH2\nde sinal de estações sem fio estão limitadas ao interior dos círculos sombreados mostrados na Figura 6.11. Assim, \ncada uma das estações está oculta da outra, embora nenhuma esteja oculta do AP\n.\nAgora vamos considerar por que terminais ocultos podem ser problemáticos. Suponha que a estação H1 \nesteja transmitindo um quadro e, a meio caminho da transmissão, a estação H2 queira enviar um quadro para o \nAP. O H2, que não está ouvindo a transmissão de H1, primeiro esperará um intervalo DIFS para, então, transmi-\ntir o quadro, resultando em uma colisão. Por conseguinte, o canal será desperdiçado durante todo o período da \ntransmissão de H1, bem como durante a transmissão de H2.\nPara evitar esse problema, o protocolo IEEE 802.11 permite que uma estação utilize um quadro de controle \nRTS (Request to Send — solicitação de envio) curto e um quadro de controle CTS (Clear to Send — pronto para \nenvio) curto para reservar acesso ao canal. Quando um remetente quer enviar um quadro DATA, ele pode enviar \nprimeiro um quadro RTS ao AP indicando o tempo total requerido para transmitir o quadro DATA e o quadro \nde reconhecimento (ACK). Quando o AP recebe o quadro RTS, responde fazendo a transmissão por difusão de \num quadro CTS. Esse quadro CTS tem duas finalidades: dá ao remetente uma permissão explícita para enviar e \ntambém instrui as outras estações a não enviar durante o tempo reservado.\nAssim, na Figura 6.12, antes de transmitir um quadro DATA, H1 primeiro faz uma transmissão por difusão \nde um quadro RTS, que é ouvida por todas as estações que estiverem dentro do seu círculo de alcance, incluindo o \nAP. O AP então responde com um quadro CTS, que é ouvido por todas as estações dentro de sua faixa de alcance, \nincluindo H1 e H2. Como ouviu o CTS, a estação H2 deixa de transmitir durante o tempo especificado no quadro \nCTS. Os quadros RTS, CTS, DATA e ACK são mostrados na Figura 6.12.\nA utilização dos quadros RTS e CTS pode melhorar o desempenho de dois modos ­\nimportantes:\n• O problema da estação oculta é atenuado, visto que um quadro DATA longo é transmitido apenas após \no canal ter sido reservado.\n• Como os quadros RTS e CTS são curtos, uma colisão que envolva um quadro RTS ou CTS terá apenas a \nduração dos quadros RTS ou CTS curtos. Desde que os quadros RTS e CTS sejam corretamente transmi-\ntidos, os quadros DATA e ACK subsequentes deverão ser transmitidos sem colisões.\nAconselhamos o leitor a verificar o applet 802.11 no site deste livro. Esse applet interativo ilustra o protocolo \nCSMA/CA, incluindo a sequência de troca RTS/CTS.\nEmbora a troca RTS/CTS ajude a reduzir colisões, também introduz atraso e consome recursos do canal. \nPor essa razão, a troca RTS/CTS é utilizada (quando utilizada) apenas para reservar o canal para a transmissão de \num quadro DATA longo. Na prática, cada estação sem fio pode estabelecer um patamar RTS tal que a sequência \nRTS/CTS seja utilizada somente quando o quadro for mais longo do que o patamar. Para muitas estações sem \nfio, o valor default do patamar RTS é maior do que o comprimento máximo do quadro, de modo que a sequência \nRTS/CTS é omitida para todos os quadros DATA enviados.\nRedes sem fio e redes móveis  397 \nUsando o 802.11 como enlace ponto a ponto\nAté aqui nossa discussão focalizou a utilização do 802.11 em um cenário de múltiplo acesso. Devemos \nmencionar que, se dois nós tiverem, cada um, uma antena direcional, eles poderão dirigir suas antenas um para o \noutro e executar o protocolo 802.11 sobre o que é, essencialmente, um enlace ponto-a-ponto. Dado o baixo custo \ncomercial do hardware 802.11, a utilização de antenas direcionais e uma maior potência de transmissão permi-\ntem que o 802.11 seja utilizado como um meio barato de prover conexões sem fio ponto-a-ponto por dezenas de \nquilômetros. Raman [2007] descreve uma dessas redes sem fio multissaltos que funciona nas planícies rurais do \nrio Ganges, na Índia, e que contém enlaces 802.11 ponto a ponto.\n6.3.3  O quadro IEEE 802.11\nEmbora o quadro 802.11 tenha muitas semelhanças com um quadro Ethernet, ele também contém vários \ncampos que são específicos para sua utilização para enlaces sem fio. O quadro 802.11 é mostrado na Figura 6.13. \nFigura 6.12  Prevenção de colisão usando os quadros RTS e CTS\nDestino\nTodos os outros nós\nAdiar acesso\nOrigem\nDIFS\nACK\nSIFS\nSIFS\nSIFS\nDADOS\nCTS\nCTS\nACK\nRTS\nKR 06.12.eps\nKurose and Ross\nComputer Networking 6/e\n23p6 Wide x 32p6 Deep\n11/17/11 rossi\n   Redes de computadores e a Internet\n398\nOs números acima de cada campo no quadro representam os comprimentos dos campos em bytes; os números \nacima de cada subcampo no campo de controle do quadro representam os comprimentos dos subcampos em bits. \nAgora vamos examinar os campos no quadro, bem como alguns dos subcampos mais importantes no campo de \ncontrole do quadro.\nCampos de carga útil e de CRC\nNo coração do quadro está a carga útil, que consiste, tipicamente, em um datagrama IP ou em um pacote \nARP. Embora o comprimento permitido do campo seja 2.312 bytes, em geral ele é menor do que 1.500 bytes, con-\ntendo um datagrama IP ou um pacote ARP. Como um quadro Ethernet, um quadro 802.11 inclui uma verificação \nde redundância cíclica (CRC), de modo que o receptor possa detectar erros de bits no quadro recebido. Como já \nvimos, erros de bits são muito mais comuns em LANs sem fio do que em LANs cabeadas, portanto, aqui, CRC é \nainda mais útil.\nCampos de endereço\nTalvez a diferença mais marcante no quadro 802.11 é que ele tem quatro campos de endereço e cada um \npode conter um endereço MAC de 6 bytes. Mas por que quatro campos de endereço? Um campo de origem MAC \ne um campo de destino MAC não são suficientes como são na Ethernet? Acontece que aqueles três campos de \nendereço são necessários para finalidades de interconexão em rede — especificamente, para mover o datagrama \nde camada de enlace de uma estação sem fio, passando por um AP, até uma interface de roteador. O quarto campo \nde endereço é usado quando APs encaminham quadros uns aos outros em modo ad hoc. Visto que estamos consi-\nderando apenas redes de infraestrutura, vamos concentrar nossa atenção nos três primeiros campos de endereço. \nO padrão 802.11 define esses campos da seguinte forma:\n• Endereço 2 é o endereço MAC da estação que transmite o quadro. Assim, se uma estação sem fio trans-\nmitir o quadro, o endereço MAC daquela estação será inserido no campo de endereço 2. De modo se-\nmelhante, se um AP transmitir o quadro, o endereço MAC do AP será inserido no campo de endereço 2.\n• Endereço 1 é o endereço MAC da estação sem fio que deve receber o quadro. Assim, se uma estação móvel \nsem fio transmitir o quadro, o endereço 1 conterá o endereço MAC do AP de destino. De modo semelhante, \nse um AP transmitir o quadro, o endereço 1 conterá o endereço MAC da estação sem fio de destino.\n• Para entender o endereço 3, lembre-se de que o BSS (que consiste no AP e estações sem fio) faz parte de \numa sub-rede, e que esta se conecta com outras sub-redes, por meio de alguma interface de roteador. O \nendereço 3 contém o endereço MAC dessa interface de roteador.\nPara compreender melhor a finalidade do endereço 3, vamos examinar um exemplo de interconexão em \nrede no contexto da Figura 6.14. Nessa figura há dois APs, cada um responsável por certo número de estações \nsem fio. Cada AP tem uma conexão direta com um rotea­\ndor que, por sua vez, se liga com a Internet global. \nFigura 6.13  O quadro 802.11\nKR 06.13.eps\nKurose and Ross\nComputer Networking 6/e\n32p0 Wide x 9p5 Deep\n11/17/11 rossi\nControle \nde quadro\n2\n2\n2\n4\n1\n1\n1\n1\n1\n1\n1\n1\n2\n6\n6\n6\n2\n6\n0-2312\n4\nQuadro (os números indicam o comprimento do campo em bytes):\nEndereço\n1\nDuração\nCarga útil\nCRC\nVersão do \nprotocolo\nPara o\nAP\nDo\nAP\nMais\nfrag\nGer. de \nenergia\nMais \ndados\nEndereço\n2\nEndereço\n3\nEndereço\n4\nControle de \nsequência\nTipo\nSubtipo\nNova \ntentativa\nWEP\nReser-\nvado\nDetalhamento do campo de controle do quadro (os números indicam o comprimento do campo em bits):\nRedes sem fio e redes móveis  399 \nDevemos ter sempre em mente que um AP é um dispositivo da camada de enlace e, portanto, não “fala” IP nem \nentende endereços IP. Agora, considere mover um datagrama da interface de roteador R1 até a estação sem fio \nH1. O roteador não está ciente de que há um AP entre ele e H1; do ponto de vista do roteador, H1 é apenas um \nhospedeiro em uma das sub-redes às quais ele (o roteador) está conectado.\n• O roteador, que conhece o endereço IP de H1 (pelo endereço de destino do datagrama), utiliza ARP para \ndeterminar o endereço MAC de H1, exatamente como aconteceria em uma LAN Ethernet comum. Após \nobter o endereço MAC de H1, a interface do roteador R1 encapsula o datagrama em um quadro Ethernet. \nO campo de endereço de origem desse quadro contém o endereço MAC de R1 e o campo de endereço de \ndestino contém o endereço MAC de H1.\n• Quando o quadro Ethernet chega ao AP, este converte o quadro Ethernet 802.3 para um quadro 802.11 antes \nde transmiti-lo para o canal sem fio. O AP preenche o endereço 1 e o endereço 2 com o endereço MAC de H1 \ne seu próprio endereço MAC, ­\nrespectivamente, como descrito. Para o endereço 3, o AP insere o endereço \nMAC de R1. Dessa maneira, H1 pode determinar (a partir do endereço 3) o endereço MAC da interface \n \nde roteador que enviou o datagrama para a sub-rede.\nAgora considere o que acontece quando a estação sem fio H1 responde movendo um datagrama de H1 \npara R1.\n• H1 cria um quadro 802.11, preenchendo os campos de endereço 1 e 2 com o endereço MAC do AP e \ncom o endereço MAC de H1, respectivamente, como descrito. Para o endereço 3, H1 insere o endereço \nMAC de R1.\n• Quando recebe o quadro 802.11, o AP o converte para um quadro Ethernet. O campo de endereço de \norigem para esse quadro é o endereço MAC de H1 e o campo de endereço de destino é o endereço MAC \nde R1. Assim, o endereço 3 permite que o AP determine o endereço de destino MAC apropriado ao \nconstruir o quadro Ethernet.\nEm resumo, o endereço 3 desempenha um papel crucial na interconexão do BSS com uma LAN cabeada.\nCampos de número de sequência, duração e controle de quadro\nLembre-se de que, em 802.11, sempre que uma estação recebe corretamente um quadro de outra, devolve \num reconhecimento. Como reconhecimentos podem ser perdidos, a estação emissora pode enviar várias cópias \nFigura 6.14  \u0007\nA utilização de campos de endereço em quadros 802.11: movendo um quadro entre \nH1 e R1\nKR 06.14.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n22p7 Wide x 17p0 Deep\n11/17/11 rossi\nInternet\nRoteador\nAP\nH1\nR1\nBSS 1\nBSS 2\nAP\n   Redes de computadores e a Internet\n400\nde determinado quadro. Como vimos em nossa discussão sobre o protocolo rdt2.1 (Seção 3.4.1), a utilização de \nnúmeros de sequência permite que o receptor distinga entre um quadro recém-transmitido e a retransmissão \nde um quadro anterior. Assim, o campo de número de sequência no quadro 802.11 cumpre aqui, na camada de \nenlace, exatamente a mesma finalidade que cumpria na camada de transporte do Capítulo 3.\nNão se esqueça de que o protocolo 802.11 permite que uma estação transmissora reserve o canal durante \num período que inclui o tempo para transmitir seu quadro de dados e o tempo para transmitir um reconheci-\nmento. Esse valor de duração é incluído no campo de duração do quadro (tanto para quadros de dados quanto \npara os quadros RTS e CTS).\nComo mostrado na Figura 6.13, o campo de controle de quadro inclui muitos subcampos. Diremos ape-\nnas umas poucas palavras sobre alguns dos mais importantes; se o leitor quiser uma discussão mais completa, \naconselhamos que consulte a especificação 802.11 [Held, 2001; Crow, 1997; IEEE 802.11, 1999]. Os campos tipo e \nsubtipo são usados para distinguir os quadros de associação, RTS, CTS, ACK e de dados. Os campos de e para são \nusados para definir os significados dos diferentes campos de endereço. (Esses significados mudam dependendo \nda utilização dos modos ad hoc ou de infraestrutura e, no caso do modo de infraestrutura, mudam dependendo \nde o emissor do quadro ser uma estação sem fio ou um AP.) Finalmente, o campo WEP (Wireless Equivalent Pri-\nvacy) indica se está sendo ou não utilizada criptografia. (A WEP é discutida no Capítulo 8.)\n6.3.4  Mobilidade na mesma sub-rede IP\nPara ampliar a faixa física de uma LAN sem fio, empresas e universidades frequentemente distribuirão \nvários BSSs dentro da mesma sub-rede IP. Isso, claro, levanta a questão da mobilidade entre os BSSs — como \nestações sem fio passam imperceptivelmente de um BSS para outro enquanto mantêm sessões TCP em curso? \nComo veremos nesta subseção, a mobilidade pode ser manipulada de uma maneira relativamente direta quando \nos BSSs são parte de uma sub-rede. Quando estações se movimentam entre sub-redes, são necessários protocolos \nde gerenciamento de mobilidade mais sofisticados, tais como os que estudaremos nas seções 6.5 e 6.6.\nAgora vamos examinar um exemplo específico de mobilidade entre BSSs na mesma sub­\n‑rede. A Figura 6.15 \nmostra dois BSSs interconectados e um hospedeiro, H1, que se move entre BSS1 e BSS2. Como nesse exemplo \no dispositivo de interconexão entre os dois BSSs não é um roteador, todas as estações nos dois BSSs, incluindo \nos APs, pertencem à mesma sub-rede IP. Assim, quando H1 se move de BSS1 para BSS2, ele pode manter seu \nendereço IP e todas as suas conexões TCP em curso. Se o dispositivo de interconexão fosse um roteador, então \nH1 teria de obter um novo endereço IP na sub-rede na qual estava percorrendo. Essa mudança de endereço inter-\nromperia (e, em consequência, finalizaria) qualquer conexão TCP em curso em H1. Na Seção 6.6, veremos como \num protocolo de mobilidade da camada de rede, como o IP móvel, pode ser usado para evitar esse problema.\nMas o que acontece especificamente quando H1 passa de BSS1 para BSS2? À medida que se afasta de AP1, \no H1 detecta um enfraquecimento do sinal de AP1 e começa a fazer uma varredura em busca de um sinal mais \nforte. H1 recebe quadros de sinalização de AP2 (que em muitos ambientes empresariais e universitários terão o \nmesmo SSID do AP1). Então, H1 se desassocia de AP1 e se associa com AP2, mantendo, ao mesmo tempo, seu \nendereço IP e suas sessões TCP em curso.\nIsso resolve o problema de transferência do ponto de vista do hospedeiro e do AP. Mas e quanto ao comu-\ntador na Figura 6.15? Como é possível saber que o hospedeiro se locomoveu de um AP a outro? O leitor talvez se \nlembre de que, no Capítulo 5, dissemos que comutadores são “autodidatas” e constroem automaticamente suas \ntabelas de repasse. Essa característica de autoaprendizagem funciona bem para movimentações ocasionais (por \nexemplo, quando um profissional é transferido de um departamento para outro); contudo, comutadores não são \nprojetados para suportar usuários com alto grau de mobilidade, que querem manter conexões TCP enquanto se \nmovimentam entre BSSs. Para avaliar este problema aqui, lembre-se de que, antes da movimentação, o comuta-\ndor tem um registro em sua tabela de repasse que vincula o endereço MAC de H1 com a interface de saída do \ncomutador por meio da qual o H1 pode ser alcançado. Se H1 estiver inicialmente em BSS1, então um datagrama \nRedes sem fio e redes móveis  401 \ndestinado a H1 será direcionado a ele via AP1. Contudo, tão logo H1 se associe com BSS2, seus quadros deverão \nser direcionados para AP2. Uma solução (na verdade um tanto forçada) é o AP2 enviar ao comutador um quadro \nEthernet de difusão com o endereço de origem de H1 logo após a nova associação. Quando o comutador recebe \no quadro, atualiza sua tabela de repasse, permitindo que H1 seja alcançado via AP2. O grupo de padrões 802.11f \nestá desenvolvendo um protocolo entre APs para cuidar dessas e outras questões associadas.\n6.3.5  Recursos avançados em 802.11\nFinalizaremos nossa abordagem sobre 802.11 com uma breve discussão sobre capacidades avançadas en-\ncontradas nas redes 802.11. Como veremos, essas capacidades não são completamente especificadas no padrão \n802.11, mas, em vez disso, são habilitadas por mecanismos especificados no padrão. Isso permite que forne-\ncedores diferentes implementem essas capacidades usando suas próprias abordagens, aparentemente trazendo \nvantagens sobre a concorrência.\nAdaptação da taxa 802.11\nVimos antes, na Figura 6.3, que as diferentes técnicas de modulação (com as diferentes taxas de transmissão \nque elas fornecem) são adequadas para diferentes cenários de SNR. Considere, por exemplo, um usuário 802.11 \nmóvel que está, inicialmente, a 20 metros de distância da estação-base, com uma alta relação sinal-ruído. Dada a \nalta SNR, o usuário pode se comunicar com essa estação usando uma técnica de modulação da camada física que \noferece altas taxas de transmissão enquanto mantém uma BER baixa. Esse usuário é um felizardo! Suponha agora \nque o usuário se torne móvel, se distanciando da estação-base, e que a SNR diminui à medida que a distância \nda estação-base aumenta. Neste caso, se a técnica de modulação usada no protocolo 802.11 que está operando \nentre a estação-base e o usuário não mudar, a BER será inaceitavelmente alta à medida que a SNR diminui e, por \nconseguinte, nenhum quadro transmitido será recebido corretamente.\nPor essa razão, algumas execuções de 802.11 possuem uma capacidade de adaptação de taxa que seleciona, de \nmaneira adaptável, a técnica de modulação da camada física sobreposta a ser usada com base em características \natuais ou recentes do canal. Se um nó enviar dois quadros seguidos sem receber confirmação (uma indicação implí-\ncita de erros de bit no canal), a taxa de transmissão cai para a próxima taxa mais baixa. Se dez quadros seguidos forem \n \nconfirmados, ou se um temporizador (que registra o tempo desde o último fallback) expirar, a taxa de transmis-\nsão aumenta para a próxima taxa mais alta. Esse mecanismo de adaptação da taxa compartilha a mesma filosofia \nde “investigação” (refletida por recebimentos de ACK); quando algo “ruim” acontece, a taxa de transmissão é \nreduzida. A adaptação da taxa 802.11 e o controle de congestionamento TCP são, desse modo, semelhantes à \ncriança: está sempre exigindo mais e mais de seus pais (digamos, mais doces, para uma criança, e chegar mais \nFigura 6.15  Mobilidade na mesma sub-rede\nKR 06.15.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n16p7 Wide x 11p1 Deep\n11/17/11 rossi\nBSS 1\nBSS 2\nH1\nComutador\nAP 1\nAP 2\n   Redes de computadores e a Internet\n402\ntarde em casa, para adolescentes) até eles por fim dizerem “Chega!” e a criança desistir (para tentar novamente \napós a situação melhorar!). Diversos métodos também foram propostos para aperfeiçoar esse esquema básico de \najuste automático de taxa [Kamerman, 1997; Holland, 2001; Lacage, 2004].\nGerenciamento de energia\nA energia é uma fonte preciosa em aparelhos móveis e, assim, o padrão 802.11 provê capacidades de geren-\nciamento de energia, permitindo que os nós 802.11 minimizem o tempo de suas funções de percepção, transmis-\nsão e recebimento, e outros circuitos necessários para “funcionar”\n. O gerenciamento de energia 802.11 opera da \nseguinte maneira. Um nó é capaz de alternar claramente entre os estados “dormir” e “acordar” (como um aluno \ncom sono em sala de aula!). Um nó indica ao ponto de acesso que entrará no modo de dormir ajustando o bit de \ngerenciamento de energia no cabeçalho de um quadro 802.11 para 1. Um temporizador localizado no nó é, então, \najustado para acordar o nó antes de um AP ser programado para enviar seu quadro de sinalização (lembre-se de \nque, em geral, um AP envia um quadro de sinalização a cada 100 ms). Uma vez que o AP descobre, pelo bit de \ntransmissão de energia, que o nó vai dormir, ele (o AP) sabe que não deve enviar nenhum quadro àquele nó, e \narmazenará qualquer quadro destinado ao hospedeiro que está dormindo para transmissão posterior.\nUm nó acordará logo após o AP enviar um quadro de sinalização, e logo entrará no modo ativo (ao contrário \ndo aluno sonolento, esse despertar leva apenas 250 µs [Kamerman, 1997]!). Os quadros de sinalização enviados \npelo AP contêm uma relação de nós cujos quadros foram mantidos em buffer no AP. Se não houver quadros man-\ntidos em buffer para o nó, ele pode voltar a dormir. Caso contrário, o nó pode solicitar explicitamente o envio dos \nquadros armazenados, enviando uma mensagem de polling ao AP. Com um tempo de 100 ms entre as sinalizações, \num despertar de 250 μs e um tempo semelhantemente pequeno para receber um quadro de sinalização e verificar \nque não haja quadros em buffer, um nó que não possui quadros para enviar ou receber pode dormir 99% do tempo, \nresultando em uma economia de energia significativa.\n6.3.6  Redes pessoais: Bluetooth e Zigbee\nComo ilustrado na Figura 6.2, o padrão Wi-Fi IEEE 802.11 é voltado para a comunicação entre aparelhos \nseparados por até 100 m (exceto quando 802.11 é usado em configuração ponto a ponto com antena direcional). \nDois outros protocolos IEEE 802 — Bluetooth e Zigbee (definidos nos padrões IEEE 802.15.1 e IEEE 802.15.4 \n[IEEE 802.15, 2012]) e o WiMAX (definido no padrão IEEE 802.16 [IEEE 802.16d, 2004; IEEE 802.16e, 2005]) \n— são padrões para se comunicar a distâncias mais curtas e mais longas, nessa ordem. Examinaremos o WiMAX \nrapidamente quando discutirmos sobre redes de dados celulares, na Seção 6.4; aqui, vamos nos concentrar em \nredes para distâncias mais curtas.\nBluetooth\nUma rede IEEE 802.15.1 opera sobre uma curta faixa, a baixa potência, e a um custo baixo. Esta é, basi-\ncamente, uma tecnologia de “substituição de cabos” de baixa velocidade, curto alcance e baixa potência para \ninterconectar notebooks, dispositivos periféricos, telefones celulares e smartphones, enquanto 802.11 é uma tec-\nnologia de “acesso” de velocidade mais alta, alcance médio e potência mais alta. Por essa razão, as redes 802.15.1, \nàs vezes, são denominadas redes pessoais sem fio (WPAN — Wireless Personal Area Networks). As camadas de \nenlace e física do 802.15.1 são baseadas na especificação do Bluetooth anterior para redes pessoais [Held, 2001; \nBisdikian, 2001]. Redes 802.15.1 operam na faixa de rádio não ­\nlicenciada de 2,4 GHz em modo TDM, com inter-\nvalos de tempo (time slots) de 625 μs. Durante cada intervalo de tempo, um emissor transmite por um entre 79 \ncanais, sendo que, de intervalo para intervalo, o canal muda de uma maneira conhecida, porém pseudoaleatória. \nEssa forma de saltar de canal em canal, conhecida como espectro espalhado com salto de frequência (FHSS \nRedes sem fio e redes móveis  403 \n— Frequency-Hopping Spread Spectrum), espalha transmissões sobre o espectro de frequência pelo tempo. O \n802.15.1 pode oferecer velocidades de dados de até 4 Mbits/s.\nRedes 802.15.1 são redes ad hoc: não é preciso que haja uma infraestrutura de rede (por exemplo, um ponto \nde acesso) para interconectar dispositivos 802.15.1. Assim, esses dispositivos devem se organizar por si sós. Dis-\npositivos 802.15.1 são primeiro organizados em uma picorrede (piconet: pequena rede) de até oito dispositivos \nativos, como ilustra a Figura 6.16. Um desses dispositivos é designado como o mestre, e os outros agem como \nescravos. Na verdade, o nó mestre comanda a picorrede como um rei — seu relógio determina o tempo na picor-\nrede, ele pode transmitir em cada intervalo de tempo de número ímpar e um escravo pode transmitir somente \napós o mestre ter se comunicado com ele no intervalo de tempo anterior e, mesmo assim, só pode transmitir para \no mestre. Além dos dispositivos escravos, a rede também pode conter até 255 dispositivos estacionados. Esses \ndispositivos não podem se comunicar até que o nó mestre tenha mudado seus estados de estacionado para ativo.\nSe o leitor quiser obter mais informações sobre WPANs 802.15.1, pode consultar as referências do Bluetoo-\nth [Held, 2001; Bisdikian, 2001] ou o site oficial do IEEE 802.15 [IEEE 802.15, 2012].\nZigbee\nUma segunda rede pessoal padronizada pelo IEEE é o padrão 802.14.5 [IEEE 802.15, 2012], conhecido como \nZigbee. Enquanto as redes Bluetooth oferecem uma taxa de dados de “substituição de cabo” de mais de um Mbit por \nsegundo, Zigbee é voltada para aplicações de menos potência, menor taxa de dados e menor ciclo de trabalho do que \nBluetooth. Embora sejamos tentados a pensar que “maior e mais rápido é melhor”\n, nem todas as aplicações de rede \nprecisam de muita largura de banda e dos custos mais altos decorrentes disso (custos tanto econômicos quanto de \nenergia). Por exemplo, sensores domésticos de temperatura e iluminação, dispositivos de segurança e interruptores \nde parede são todos dispositivos muito simples, de baixa potência, baixo ciclo de trabalho e baixo custo. Portanto, \nZigbee é ideal para esses dispositivos. Zigbee define taxas de canal de 20, 40, 100 e 250 Kbits/s, dependendo da fre-\nquência do canal.\nOs nós em uma rede Zigbee podem ser de dois tipos. Os chamados “dispositivos de função reduzida” operam \ncomo escravos controlados por um único “dispositivo de função completa”\n, assim como dispositivos Bluetooth \nescravos. Um dispositivo de função completa pode operar como um dispositivo mestre no Bluetooth controlando \nvários dispositivos escravos, e múltiplos dispositivos de função completa podem, além disso, ser configurados \nem uma rede de malha, na qual direcionam quadros entre si. Zigbee compartilha muitos mecanismos de pro-\ntocolo que já encontramos em outros protocolos da camada de enlace: quadros de sinalização e confirmações \nda camada de enlace (semelhantes ao 802.11), protocolos de acesso aleatório de detecção de portadora com \nFigura 6.16  Uma picorrede Bluetooth\nKR 06.16.eps\nAW/Kurose and Ross\nRaio de \ncobertura\nDispositivo mestre\nDispositivo escravo\nDispositivo estacionado\nLegenda:\nM\nM\nS\nS\nS\nS\nP\nP\nP\nP\nP\n   Redes de computadores e a Internet\n404\nrecuo exponencial binário (semelhante ao 802.11 e Ethernet) e alocação fixa, garantida, de intervalos de tempo \n(semelhante ao DOCSIS).\nRedes Zigbee podem ser configuradas de muitas maneiras diferentes. Vamos considerar o caso simples de \num único dispositivo de função completa controlando vários dispositivos de função reduzida com intervalos de \ntempo e usando quadros de sinalização. A Figura 6.17 mostra um caso em que a rede Zigbee divide o tempo em \nsuperquadros recorrentes, cada qual começando com um quadro de sinalização. Cada quadro de sinalização di-\nvide o superquadro em um período ativo (durante o qual os dispositivos podem transmitir) e um período inativo \n(durante o qual todos os dispositivos, inclusive o controlador, podem dormir e, portanto, economizar energia). \nO período ativo consiste em 16 intervalos de tempo, alguns usados pelos dispositivos em uma forma de acesso \naleatório CSMA/CA, e alguns são alocados pelo controlador para dispositivos específicos, oferecendo assim o \nacesso garantido ao canal para esses dispositivos. Outros detalhes sobre redes Zigbee poderão ser encontrados \nem Baronti [2007], IEEE 802.15.4 [2012].\nFigura 6.17  Estrutura do superquadro Zigbee 802.14.4\nKR 06.17.eps\nAW/Kurose and Ross\nComputer Networking 6/e\nsize:  28p0 wide  x  8p3 deep\n11/16/11, 11/23/11 rossi\nSinalizador\nIntervalos garantidos\nIntervalos de disputa\nPeríodo inativo\nSuperquadro\n6.4  Acesso celular à Internet\nNa seção anterior vimos como um hospedeiro da Internet pode acessá-la quando estiver dentro de um \nhotspot Wi-Fi, isto é, quando estiver nas vizinhanças de um ponto de acesso 802.11. Mas a área de cobertura da \nmaioria dos hotspots Wi-Fi é pequena, entre 10 e 100 m de diâmetro. O que fazer, então, quando precisamos de-\nsesperadamente de um acesso sem fio à Internet e não podemos acessar um hotspot Wi-Fi?\nDado que a telefonia celular agora está presente por toda parte, em muitas áreas no mundo inteiro, uma \nestratégia natural é estender redes celulares de modo que suportem não apenas a telefonia de voz, mas também o \nacesso sem fio à Internet. Idealmente, esse acesso teria uma velocidade razoavelmente alta e ofereceria mobilida-\nde imperceptível, permitindo que usuários mantivessem suas sessões TCP enquanto viajassem, por exemplo, em \num ônibus ou em um trem. Com taxas de bits altas o bastante entre a origem e o usuário e vice-versa, o usuário \npoderia até mesmo manter sessões de videoconferência enquanto estivesse em trânsito. Esse cenário não é assim \ntão implausível. Em 2012, muitas operadoras de telefonia celular ofereciam a seus assinantes um serviço de acesso \ncelular à Internet por menos de 50 dólares mensais com taxas de bits entre a operadora e o usuário e vice-versa \nna faixa de algumas centenas de kilobits por segundo. Taxas de dados de vários megabits por segundo estão se \ntornando acessíveis à medida que os serviços de dados de banda larga, como aqueles que veremos aqui, são cada \nvez mais empregados.\nNesta seção, damos uma breve visão geral das tecnologias de acesso celular à Internet já existentes e emer-\ngentes. Mais uma vez, aqui focalizaremos o primeiro salto sem fio, bem como a rede que conecta o primeiro salto \nsem fio à rede telefônica maior e/ou à Internet; na Seção 6.7 consideraremos como as chamadas são roteadas para \num usuário que está se movimentando entre estações-base. Nossa breve discussão oferecerá, necessariamente, \napenas uma descrição simplificada e de alto nível das tecnologias celulares. É claro que a questão das comunica-\nções celulares modernas é de grande amplitude e profundidade, e muitas universidades oferecem diversos cursos \nsobre o assunto. O leitor que desejar um conhecimento mais profundo da questão deve consultar Goodman \n[1997]; Kaaranen [2001]; Lin [2001]; Korhonen [2003]; Schiller [2003]; Scourias [2012]; Turner [2012]; Akyildiz \n[2010], bem como as referências particularmente excelentes e completas em Mouly [1992].\nRedes sem fio e redes móveis  405 \n6.4.1  Visão geral da arquitetura de rede celular\nEm nossa discussão sobre a arquitetura da rede celular nesta seção, adotaremos a terminologia dos padrões \ndo Sistema Global para Comunicações Móveis (GSM — Global System for Mobile Communications). (Para os \namantes de história, a sigla GSM, na origem, veio de Groupe Spécial Mobile, até a forma anglicizada ser adotada, \nmantendo as letras iniciais originais.) Nos anos 1980, os europeus reconheceram a necessidade de um sistema \npan-europeu de telefonia celular digital que substituiria os diversos sistemas analógicos de telefonia celular in-\ncompatíveis, levando ao padrão GSM [Mouly, 1992]. Os europeus implantaram a tecnologia GSM com um gran-\nde sucesso no início dos anos 1990 e, desde então, o GSM cresceu e se tornou um gigante do mundo da telefonia \ncelular, com mais de 80% de assinantes no mundo utilizando essa tecnologia.\nQuando as pessoas falam sobre tecnologia celular, em geral a classificam como pertencendo a uma das \ndiversas “gerações”\n. As primeiras gerações foram, em essência, projetadas para o tráfego de voz. Os sistemas \nde primeira geração (1G) eram sistemas FDMA analógicos, desenvolvidos especialmente para a comunicação \napenas por voz. Esses sistemas 1G quase não existem mais, tendo sido substituídos pelos sistemas digitais 2G. \nOs sistemas originais 2G também foram projetados para voz, sendo estendidos mais tarde (2,5G) para suportar \ndados (por exemplo, a Internet), bem como o serviço de voz. Os sistemas 3G, que estão sendo executados hoje, \ntambém suportam voz e dados, mas com uma ênfase cada vez maior nas capacidades de dados e enlaces de acesso \nvia rádio com maior velocidade.\nCelular móvel 3G versus LANs sem fio\nMuitas operadoras de telefonia celular móvel estão \ndisponibilizando sistemas celulares móveis 3G, com \ntaxas de dados de 2 Mbits/s em ambiente fechado e \n384 kbits/s, ou mais, ao ar livre. Esses sistemas 3G es-\ntão sendo disponibilizados em faixas de radiofre­\nquência \nlicenciadas, e o preço que algumas operadoras pagam \naos governos pela licença é uma quantia considerável. \nOs sistemas 3G permitirão que os usuários acessem a \nInternet a partir de locais externos remotos enquanto \nestão em trânsito, de modo semelhante ao acesso por \ntelefone celular existente hoje. Por exemplo, a tecno-\nlogia 3G permite que um usuário acesse informações \nsobre mapas de estradas enquanto dirige seu carro, ou \ninformações sobre a programação de cinema enquan-\nto estiver tomando sol na praia. Não obstante, pode-\nse questionar a extensão à qual os sistemas 3G serão \nusados, dados o seu custo e o fato de que os usuários \nem geral possuem acesso simultâneo a LANs sem fio \ne 3G:\n• \nA infraestrutura emergente de LAN sem fio logo es-\ntará presente em quase toda parte. As LANs sem \nfio IEEE 802.11, que operam a taxas de 54 Mbits/s, \nestão sendo amplamente disponibilizadas. Quase \ntodos os computadores portáteis e smartphones \njá vêm da fábrica equipados com capacidades de \nLAN 802.11. Além disso, as novidades em equi-\npamentos de Internet — como câmeras sem fio e \nmolduras para fotografias — também terão capaci-\ndades de LAN sem fio de baixa potência.\n• Estações-base de LANs sem fio também podem \nmanipular equipamentos de telefonia móvel. Mui-\ntos telefones já podem se conectar com a rede de \ntelefonia celular ou a uma rede IP de forma nativa \nou usando o serviço de voz sobre IP semelhante \nao Skype, ultrapassando, assim, os serviços de \ndados 3G e de voz por celular da operadora.\nÉ claro que muitos outros especialistas acham que \na tecnologia 3G não só será o maior sucesso, mas \ntambém revolucionará drasticamente o modo como \ntrabalhamos e vivemos. É claro que ambas, Wi-Fi e \n3G, podem se tornar tecnologias sem fio dominantes, \ncom equipamentos sem fio em trânsito que selecio-\nnam automaticamente a tecnologia de acesso que \nofereça o melhor serviço em seu local físico atual.\nHistória\n   Redes de computadores e a Internet\n406\nArquitetura de rede celular, 2G: conexões por voz à rede telefônica\nO termo celular refere-se ao fato de que uma área geográfica é dividida em várias áreas de cobertura geo-\ngráfica, conhecidas como células, ilustradas como hexágonos à esquerda da Figura 6.18. Assim como estudamos \no padrão Wi-Fi 802.11 na Seção 6.3.1, o GSM possui sua nomenclatura específica. Cada célula contém uma \nestação-base de transceptor (BTS) que transmite e recebe sinais de estações móveis dentro de sua célula. A área \nde cobertura de uma célula depende de muitos fatores, incluindo potência da BTS, potência de transmissão de \naparelhos do usuário, obstáculos na célula, como prédios, e altura das antenas da estação-base. Embora a Figura \n6.18 mostre cada célula com uma estação-base de transceptor posicionada no seu meio, hoje, muitos sistemas \nposicionam a BTS em pontos de interseção de três células, de modo que uma única BTS com antenas direcionais \npossa atender a três células.\nO padrão GSM para sistemas celulares 2G utiliza FDM/TDM (rádio) combinados para a interface ar. Lem-\nbre-se de que vimos no Capítulo 1 que, com FDM puro, o canal é dividido em uma série de bandas de frequência, \ne cada banda se dedica a uma chamada. Também no Capítulo 1, vimos que, com FDM puro, o tempo é dividido \nem quadros, cada quadro é dividido em intervalos e cada chamada é destinada ao uso de um intervalo específico \nno quadro rotativo. Nos sistemas combinados FDM/TDM, o canal é dividido em uma série de sub-bandas de \nfrequência; dentro de cada sub-banda, o tempo é dividido em quadros e intervalos. Desse modo, para um siste-\nma combinado FDM/TDM, se o canal for dividido em F sub-bandas e o tempo em T intervalos, então o canal \npoderá suportar F ∙ T chamadas simultâneas. Lembre-se de que vimos, na Seção 5.3.4, que as redes de acesso a \ncabo também usam uma técnica combinada FDM/TDM. Os sistemas GSM consistem em bandas de frequência \nde 200-kHz e cada banda suporta oito chamadas TDM. O GSM codifica a voz a 13 kbits/s e 12,2 kbits/s.\nUm controlador de estação-base (BSC) da rede GSM normalmente prestará serviço a dezenas de estações\n-base do transceptor. O papel da BSC é alocar os canais de rádio da BTS a assinantes móveis, realizar paginação \n(encontrar a célula na qual reside um usuário móvel) e realizar transferência de usuários móveis — um assunto \nque abordaremos, em poucas palavras, na Seção 6.7.2. O controlador de estação-base e suas estações-base de \ntransceptor, coletivamente, constituem um sistema de estação-base (BSS) GSM.\nComo veremos na Seção 6.7, a central de comutação móvel (MSC) desempenha o papel central na contabi-\nlidade e autorização do usuário (por exemplo, determinar se um aparelho móvel tem permissão para se conectar \nà rede celular), estabelecimento e interrupção de chamada, e transferências. Em geral, uma única MSC conterá \naté cinco BSCs, resultando em cerca de 200K assinantes por MSC. A rede de um provedor de celular terá diversas \nMSCs, com MSCs especiais conhecidas como roteadores de borda das MSCs, conectando a rede celular do pro-\nvedor à rede telefônica pública mais ampla.\n6.4.2  \u0007\nRedes de dados celulares 3G: estendendo a Internet para \nassinantes de celular\nNossa discussão na Seção 6.4.1 foi concentrada na conexão de usuários de voz celular à rede telefônica \npública. Porém, claro, quando estamos nos movimentando, também queremos ler e-mails, acessar a Web, obter \nserviços dependentes do local (por exemplo, mapas e recomendações de restaurantes) e talvez ainda assistir a \nvídeos. Para isso, nosso smartphone precisará executar uma pilha de protocolos TCP/IP completa (incluindo as \ncamadas física, de enlace, rede, transporte e aplicação) e conectar-se à Internet por meio da rede de dados celular. \nO assunto de redes de dados celulares é uma coleção um tanto confusa de padrões concorrentes e em evolução, \nà medida que uma geração (e meia geração) substitui a anterior e introduz novas tecnologias e serviços, com \nnovos acrônimos. Para piorar as coisas ainda mais, não há um único órgão oficial que defina os requisitos para as \ntecnologias 2,5G, 3G, 3,5G ou 4G, tornando mais difícil apontar as diferenças entre os padrões concorrentes. Em \nnossa discussão a seguir, vamos nos concentrar nos padrões 3G do UMTS (Universal Mobile Telecommunications \nRedes sem fio e redes móveis  407 \nService), desenvolvidos pelo Projeto de Parceria da 3a Geração (3GPP — 3rd Generation Partnership Project) \n[3GPP 2012], uma tecnologia 3G bastante difundida.\nVamos fazer uma análise de cima para baixo da arquitetura de rede de dados celular 3G mostrada na \nFigura 6.19.\nO núcleo da rede 3G\nO núcleo da rede de dados celular 3G conecta as redes de acesso por rádio à Internet pública. O núcleo \nda rede opera em conjunto com os componentes da rede celular de voz existente (particularmente, o MSC), \nque encontramos anteriormente na Figura 6.18. Dada a quantidade considerável de infraestrutura existente \n(e serviços lucrativos!) na rede celular de voz, o método utilizado pelos projetistas dos serviços de dados 3G é \nevidente: deixe o núcleo da rede celular de voz GSM intacto, acrescentando funcionalidade de dados por celular \nem paralelo à rede de voz existente. A alternativa — integrar novos serviços de dados diretamente no núcleo da \nrede celular de voz — teria gerado os mesmos desafios encontrados na Seção 4.4.4, na qual discutimos sobre a \nintegração das tecnologias IPv6 (nova) e IPv4 (legada) na Internet.\nExistem dois tipos de nós no núcleo da rede 3G: Servidor de Nó de Suporte GPRS (SGSN – Serving \nGPRS Support Nodes) e Roteador de borda de suporte GPRS (GGSN — Gateway GPRS Support Nodes). (O \nacrônimo GPRS significa Generalized Packet Radio Service — serviço de rádio por pacotes generalizado —, um \nantigo serviço celular de dados em redes 2G; aqui, discutimos a versão aperfeiçoada do GPRS nas redes 3G.) \nUm SGSN é responsável por entregar datagramas de e para os nós móveis na rede de acesso por rádio à qual \no SGSN está ligado. O SGSN interage com o MSC da rede celular de voz para essa área, oferecendo autoriza-\nção do usuário e transferência, mantendo informações de local (célula) sobre nós móveis ativos e realizando \nrepasse de datagramas entre os nós móveis na rede de acesso por rádio e um GGSN. O GGSN atua como um \nroteador de borda (gateway), conectando vários SGSNs à Internet maior. Um GGSN, portanto, é a última parte \nda infraestrutura 3G que um datagrama originado do nó móvel encontra antes de entrar na Internet. Para o \nFigura 6.18  Componentes da arquitetura de rede celular 2G GSM\nBSC\nBSC\nMSC\nLegenda:\nEstação-base do transceptor \n(BTS)\nControlador da estação-base \n(BSC)\nCentral de comutação de unidade móvel \n(MSC)\nAssinantes móveis\nRoteador de borda \ndo MSC\nSistema da estação-base \n(BSS)\nSistema da estação-base (BSS)\nKR 06.18.eps\nKUROSE/ROSS\nComputer Networking 6/e\n36p0 Wide x 22p0 Deep\n11/17/11 ROSSI ILLUSTRATION\nRede telefônica \npública\nG\n   Redes de computadores e a Internet\n408\nmundo exterior, o GGSN é semelhante a qualquer outro roteador de borda; a mobilidade dos nós 3G dentro \nda rede do GGSN fica oculta do mundo exterior, por trás do GGSN.\nA rede de acesso por rádio 3G: a borda sem fio\nA rede de acesso por rádio 3G é a rede do primeiro salto sem fio que vemos como usuá­\nrios do 3G. O con-\ntrolador da rede de rádio (RNC) em geral controla várias estações-base transceptoras da célula, semelhantes às \nestações-base que encontramos nos sistemas 2G (mas, no linguajar do UMTS 3G, conhecida oficialmente como \n“Node Bs” — um nome nada descritivo!). O enlace sem fio de cada célula opera entre os nós móveis e uma esta-\nção-base transceptora, assim como nas redes 2G. O RNC se conecta à rede de voz do celular por comutação de \ncircuitos via um MSC, e à Internet por comutação de pacotes via um SGSN. Assim, embora os serviços de voz \ne dados por celular 3G utilizem núcleos de rede diferentes, eles compartilham uma rede comum de acesso por \nrádio do primeiro e último salto.\nUma mudança significativa no UMTS 3G em relação às redes 2G é que, em vez de usar o método FDMA/\nTDMA do GSM, o UMTS emprega uma técnica CDMA denominada Direct Sequence Wideband CDMA (CDMA \nbanda larga de sequência direta) (DS-WCDMA) [Dahlman, 1998] dentro de intervalos TDMA; estes, por sua \nvez, são acessíveis em frequências múltiplas — uma utilização interessante de todos os três métodos de compar-\ntilhamento de canal dedicado que identificamos no Capítulo 5 e semelhante à técnica usada nas redes de acesso a \ncabo (veja Seção 5.3.4). Essa mudança exige uma nova rede celular 3G de acesso sem fio operando paralelamente \nFigura 6.19  Arquitetura do sistema 3G\nRoteador de \nborda do \nMSC\nG\nLegenda:\nServidor de nó \nde suporte GPRS \n(SGSN)\nRoteador de borda \nde suporte GPRS \n(GGSN)\nControlador da rede \nde rádio  (RNC)\nKR 06.19.eps\nKUROSE/ROSS\nComputer Networking 6/e\n34p0 Wide x 29p7 Deep\n11/16/11, 11/23/11 ROSSI ILLUSTRATION\nGGSN\nSGSN\nG\nG\nMSC\nRede telefônica \npública\nInterface de rádio \n(WCDMA, HSPA)\nRede de acesso por rádio\nRede de acesso por rádio \nterrestre universal (UTRAN)\nNúcleo da rede\nServiço geral de rádio por pacotes \n(GPRS) Core Network\nInternet \npública\nInternet \npública\nRedes sem fio e redes móveis  409 \ncom a rede de rádio BSS 2G mostrada na Figura 6.18. O serviço de dados associado à especificação do WCDMA é \nconhecido como Acesso a Pacote em Alta Velocidade (HSP — High Speed Packet Access) e promete taxas de dados \nde até 14 Mbits/s. Mais detalhes referentes às redes 3G podem ser encontrados no site do Projeto de Parceria da \n3a Geração (3GPP) [3GPP, 2012].\n6.4.3  No caminho para o 4G: LTE\nCom os sistemas 3G agora sendo utilizados no mundo inteiro, será que os sistemas 4G estão bem atrasa-\ndos? Decerto não! Na realidade, projeto, teste e implantação inicial dos sistemas 4G já estão sendo realizados. O \npadrão 4G Long-Term Evolution (LTE) apresentado pelo 3GPP tem duas inovações importantes em relação aos \nsistemas 3G:\n• Núcleo de pacote desenvolvido (EPC — Evolved Packet Core) [3GPP Network Architecture, 2012]. O \nEPC é uma rede de núcleo simplificado em IP, que unifica a rede celular de voz comutada por circuitos \nseparada e a rede celular de dados comutada por pacotes mostrada na Figura 6.19. Esta é uma rede \n“toda em IP”\n, pois voz e dados serão transportados em datagramas IP. Como vimos no Capítulo 4 e \nestudaremos com mais detalhes no Capítulo 7, o modelo de serviço de “melhor esforço” do IP não é \ninerentemente bem adequado para os requisitos de desempenho exigentes do tráfego VoIP (voz sobre \nIP), a menos que os recursos da rede sejam controlados com cuidado para evitar (em vez de reagira a) \ncongestionamento. Assim, uma tarefa importante do EPC é controlar os recursos da rede para oferecer \nessa alta qualidade de serviço. O EPC também faz uma separação nítida entre os planos de controle da \nrede e dados do usuário, com muitos dos recursos de suporte a mobilidade que estudaremos na Seção \n6.7 sendo executados no plano de controle. O EPC permite que vários tipos de redes de acesso por \nrádio, incluindo redes de acesso 2G e 3G legadas, se conectem ao núcleo da rede. Duas introduções \nbastante claras ao EPC são [Motorola, 2007; Alcatel-Lucent, 2009].\n• Rede de acesso por rádio LTE. O padrão LTE usa uma combinação de multiplexação por divisão \nde frequência e multiplexação por divisão de tempo no canal descendente, conhecida como mul-\ntiplexação por divisão de frequência ortogonal (OFDM) [Rohde, 2008; Ericsson, 2011]. (O termo \n“ortogonal” vem do fato de que os sinais enviados em diferentes canais de frequência são criados de \nmodo que interfiram muito pouco uns nos outros, mesmo quando as frequências de canal são pouco \nespaçadas.) No LTE, cada nó móvel ativo recebe um ou mais intervalos de tempo de 0,5 ms em uma \nou mais das frequências do canal. A Figura 6.20 mostra a alocação de oito intervalos de tempo sobre \nquatro frequências. Recebendo cada vez mais intervalos de tempo (seja na mesma frequência ou em \nfrequências diferentes), um nó móvel pode alcançar velocidades de transmissão cada vez mais altas. \nA (re)alocação de intervalo entre os nós móveis pode ser realizada até mesmo a cada milissegundo. \nDiferentes esquemas de modulação também podem ser usados para alterar a taxa de transmissão; \nveja nossa discussão anterior da Figura 6.3 e a ­\nseleção dinâmica de esquemas de modulação em redes \nWi-Fi. Outra inovação na rede de rádio LTE é o uso de sofisticadas antenas de entrada múltipla, saída \nmúltipla (MIMO). A taxa de dados máxima para um usuário LTE é 100 Mbits/s na direção descen-\ndente e 50 Mbits/s na direção ascendente, quando estiver usando 20 MHz do espectro sem fio.\nA alocação em particular de intervalos de tempo a nós móveis não é exigida pelo padrão LTE. Em vez \ndisso, a decisão de quais nós móveis terão permissão para transmitir em dado intervalo de tempo em dada fre-\nquência é determinada pelos algoritmos de escalonamento fornecidos pelo fornecedor de equipamento LTE e/\nou operador da rede. Com o escalonamento oportunista [Bender, 2000; Kolding, 2003; Kulkarni, 2005], com-\nbinando o protocolo da camada física e as condições do canal entre remetente e destinatário e escolhendo os \ndestinatários aos quais os pacotes serão enviados com base nas condições do canal, permite que o controla-\ndor da rede de rádio faça o melhor uso do meio sem fio. Além disso, as prioridades do usuário e os níveis de \n   Redes de computadores e a Internet\n410\nserviço contratados (por exemplo, prata, ouro ou platina) podem ser usados no escalonamento das transmis-\nsões descendentes de pacotes. Além das capacidades do LTE descritas aqui, LTE-Advanced permite larguras \n \nde banda descendentes de centenas de Mbits/s, com a alocação de canais agregados a um nó móvel [Akyildiz, \n2010].\nOutra tecnologia 4G sem fio — WiMAX (World Interoperability for Microwave Access) — é uma família \nde padrões IEEE 802.16 que diferem bastante do LTE. Ainda não sabemos se a tecnologia 4G preferida será LTE \nou WiMAX, mas no momento (2012), LTE parece ter um impulso mais significativo. Uma discussão detalhada \ndo WiMAX poderá ser encontrada no site deste livro.\n6.5  Gerenciamento da mobilidade: princípios\nApós estudarmos a natureza sem fio dos enlaces de comunicação em uma rede sem fio, é hora de voltarmos \nnossa atenção à mobilidade que esses enlaces sem fio possibilitam. No sentido mais amplo, um nó móvel é aque-\nle que muda seu ponto de conexão com a rede ao longo do tempo. Como o termo mobilidade adquiriu muitos \nsignificados nos campos da computação e da telefonia, será proveitoso, antes de tudo, considerarmos diversas \ndimensões da mobilidade com algum detalhe.\n• Do ponto de vista da camada de rede, até que ponto um usuário é móvel? Um usuário fisicamente móvel \napresentará à camada de rede conjuntos de desafios muito diferentes dependendo de como ele se movi-\nmenta entre pontos de conexão com a rede. Em uma extremidade do espectro na Figura 6.21, um usuário \npode carregar consigo um notebook equipado com uma placa de interface de rede sem fio dentro de um \nprédio. Como vimos na Seção 6.3.4, esse usuário não é móvel do ponto de vista da camada de rede. Além \ndo mais, se ele se associar com o mesmo ponto de acesso independentemente do local, então não será \nmóvel nem mesmo do ponto de vista da camada de enlace.\n• Na outra extremidade do espectro, considere o usuário que está dentro de um BMW, correndo pela \nestrada a 150 km/h, passando por várias redes de acesso sem fio e querendo manter uma conexão TCP \nininterrupta com uma aplicação remota durante a viagem. Esse usuário é definitivamente móvel! Entre \nesses extremos está um usuário que leva seu notebook de um local (por exemplo, escritório ou quarto \nde dormir) a outro (por exemplo, lanchonete, biblioteca) e quer se conectar à rede no novo local. Ele \ntambém é móvel (embora menos do que o motorista da BMW!), mas não precisa manter uma conexão \nFigura 6.20  \u0007\nVinte intervalos de 0,5 milissegundos organizados em quadros de 10 milissegundos \nem cada frequência. A região sombreada indica uma alocação de oito intervalos\nf1\nKR 06.20.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n22p3 Wide x 15p7 Deep\n11/16/11 rossi\nf2\nf3\nf4\nf5\nf6\n0\n0,5\n1,0\n1,5\n2,0\n2,5\n9,0\n9,5\n10,0\nRedes sem fio e redes móveis  411 \nativa enquanto se movimenta entre pontos de conexão com a rede. A Figura 6.21 ilustra esse espectro de \nmobilidade do usuário do ponto de vista da camada de rede.\n• Qual é a importância de o endereço do nó móvel permanecer sempre o mesmo? Com a telefonia móvel, o \nnúmero de seu telefone — basicamente, o endereço de camada de rede do seu aparelho — permanece \no mesmo quando você transita entre uma operadora de rede de telefonia móvel e outra. Um notebook \ntambém terá de manter o mesmo endereço IP enquanto se movimenta entre redes IP?\n\t\nA resposta a essa pergunta dependerá muito da aplicação que está sendo executada. Para o motorista da \nBMW que quer manter uma conexão TCP ininterrupta com uma aplicação remota enquanto voa pela \nestrada, seria conveniente manter o mesmo endereço IP. Lembre-se de que dissemos, no Capítulo 3, que \numa aplicação de Internet precisa conhecer o endereço IP e o número de porta da entidade remota com \na qual está se comunicando. Se uma entidade móvel puder manter seu endereço IP enquanto estiver em \ntrânsito, a mobilidade torna-se invisível do ponto de vista da aplicação. Essa transparência é de grande \nvalor — uma aplicação não precisa se preocupar com uma potencial mudança de endereço IP e o mesmo \ncódigo de aplicação servirá igualmente a conexões móveis e não móveis. Na próxima seção veremos que \no IP móvel oferece essa transparência, permitindo que um nó móvel mantenha seu endereço IP perma-\nnente enquanto se movimenta entre redes.\n\t\nPor outro lado, um usuário móvel menos sofisticado poderia querer apenas desligar seu notebook no \nescritório, levá-lo para casa, ligá-lo de novo e continuar trabalhando. Se o notebook funciona principal-\nmente como um cliente em aplicações cliente-servidor (por exemplo, enviar/receber e-mails, navegar \npela web, usar Telnet com um hospedeiro remoto), o endereço IP particular utilizado pelo notebook \nnão é tão importante. Em particular, é possível passar muito bem com um endereço temporariamente \nalocado ao notebook pelo ISP que atende a residência. Na Seção 4.4, vimos que o DHCP já oferece essa \nfuncionalidade.\n• Qual é a infraestrutura cabeada de suporte disponível? Em todos os quatro cenários descritos, admitimos \nde modo implícito haver uma infraestrutura fixa à qual o usuário móvel pode se conectar — por exem-\nplo, a rede do ISP que atende a residência, a rede de acesso sem fio no local de trabalho ou as redes de \nacesso sem fio na rodovia. E se tal infraestrutura não existir? Se dois usuários estiverem a uma distância \nque permita comunicação, eles poderão estabelecer uma conexão de rede na ausência de qualquer outra \ninfraestrutura de camada de rede? As redes ad hoc oferecem exatamente essas capacidades. Essa área, que \nestá em rápido desenvolvimento e representa a pesquisa de ponta em redes móveis, está fora do escopo \ndeste livro. Perkins [2000] e as páginas Web do grupo de trabalho Mobile Ad hoc Network (manet) do \nIETF apresentam um tratamento aprofundado do assunto.\nPara ilustrar as questões que envolvem permitir que um usuário mantenha conexões em curso enquanto \nse movimenta entre redes, vamos fazer uma analogia com seres humanos. Um adulto de 20 e poucos anos que \nsai da casa dos pais torna-se móvel, pois passa a morar em uma série de quartos e/ou apartamentos e está sem-\npre mudando de endereço. Se uma velha amiga quiser entrar em contato com ele, como conseguirá o endereço \nde seu amigo móvel? Uma maneira comum de fazer isso é entrar em contato com a família, já que um jovem \nFigura 6.21  Vários graus de mobilidade do ponto de vista da camada de rede\nKR 06.21.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p Wide x 8p1 Deep\n11/17/11 rossi\nUsuário se movimenta \napenas dentro da mesma \nrede de acesso sem ﬁo\nNenhuma mobilidade\nAlta mobilidade\nUsuário se movimenta \nentre redes de acesso, \nencerrando conexões \nenquanto se movimenta \nentre redes\nUsuário se movimenta \nentre redes de acesso \nmantendo conexões em curso\n   Redes de computadores e a Internet\n412\nmóvel costuma informar seus novos endereços (nem que seja só para que os pais possam lhe enviar dinheiro \npara ajudar a pagar o aluguel!). A residência da família, com seu endereço permanente, torna-se o único lugar \na que outros podem se dirigir como uma primeira etapa para estabelecer comunicação com o jovem móvel. As \ncomunicações posteriores da velha amiga podem ser indiretas (por exemplo, pelo envio de uma carta primeiro \nà casa dos pais, que a encaminharão ao jovem móvel) ou diretas (por exemplo, utilizar o endereço informado \npelos pais e enviar uma carta diretamente ao amigo móvel).\nEm um ambiente de rede, a residência permanente de um nó móvel (tal como um notebook ou um \nsmartphone) é conhecida como rede nativa (home network) e a entidade dentro dessa rede que executa o geren-\nciamento de funções de mobilidade em nome do nó móvel é conhecida como agente nativo (home agent). A rede \nna qual o nó móvel está residindo é conhecida como rede externa (foreign network) ou rede visitada (visited \nnetwork), e a entidade dentro da rede externa que auxilia o nó móvel no gerenciamento das funções de mobilida-\nde discutidas adiante é conhecida como agente externo (foreign agent). No caso de profissionais móveis, suas re-\ndes nativas possivelmente seriam as redes das empresas em que trabalham, enquanto a rede visitada poderia ser a \nrede de um colega que estão visitando. Um correspondente é a entidade que quer se comunicar com o nó móvel. \nA Figura 6.22 ilustra esses conceitos, bem como conceitos de endereçamento considerados mais adiante. Observe \nque, na Figura 6.22, os agentes são colocados junto dos roteadores (por exemplo, como processos que rodam em \nroteadores), mas, como alternativa, poderiam estar rodando em outros hospedeiros ou servidores na rede.\nFigura 6.22  Elementos iniciais de uma arquitetura de rede móvel\nKR 06.22.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n33p4 Wide x 23p4 Deep\n11/16/11, 11/23/11, 11/29/11 rossi\nAgente nativo\nRede nativa:\n128.119.40/24\nRede visitada:\n79.129.13/24\nNó móvel\nEndereço permanente:\n128.119.40.186\nEndereço permanente:\n128.119.40.186\nAgente externo\nEndereço aos cuidados (COA) \n(ou endereço administrado):\n79.129.13.2\nCorrespondente\nRede de longa \ndistância\n6.5.1  Endereçamento\nJá observamos que, para a mobilidade do usuário ser transparente para aplicações de rede, é desejável que \num nó móvel mantenha seu endereço quando transita de uma rede para outra. Quando um nó móvel residir em \numa rede externa, todo o tráfego enviado ao endereço permanente do nó agora precisará ser roteado para a rede \nRedes sem fio e redes móveis  413 \nexterna. Como isso pode ser feito? Uma opção é a rede externa anunciar para todas as outras que o nó móvel \nagora reside em sua rede, o que poderia ser feito mediante a costumeira troca de informações de roteamento in-\nterdomínio e intradomínio, exigindo poucas mudanças na infraestrutura de roteamento. A rede externa poderia \napenas anunciar a seus vizinhos que tem uma rota altamente específica para o endereço permanente do nó móvel \n(isto é, em essência, informar a outras redes que ela tem o caminho correto para rotear datagramas para o en-\ndereço permanente do nó móvel; ver Seção 4.4). Esses vizinhos então propagariam a informação de roteamento \npor toda a rede como parte do procedimento normal de atualização de informações de roteamento e tabelas de \nrepasse. Quando o nó móvel saísse de uma rede externa e se juntasse a outra, a nova rede externa anunciaria uma \nnova rota, altamente específica, até o nó móvel e a antiga rede externa retiraria suas informações de roteamento \nreferentes ao nó móvel.\nEsse procedimento resolve dois problemas de uma vez só e o faz sem promover mudanças significativas na \ninfraestrutura da camada de rede. Outras redes conhecem a localização do nó móvel e é fácil rotear datagramas \npara o nó móvel, visto que as tabelas de repasse dirigirão datagramas à rede externa. Uma desvantagem signifi-\ncativa, contudo, é a da facilidade de expansão. Se a responsabilidade pelo gerenciamento da mobilidade tivesse \nde recair sobre os roteadores da rede, eles teriam de manter registros em tabelas de repasse para potencialmente \nmilhões de nós móveis e atualizar esses registros à medida que os nós se movimentassem. Algumas desvantagens \nadicionais serão exploradas nos problemas ao final deste capítulo.\nUm método alternativo (que tem sido adotado na prática) é passar a funcionalidade de mobilidade do \nnúcleo da rede para a borda — um tema recorrente em nosso estudo da arquitetura da Internet. Um modo na-\ntural de fazer isso é por meio da rede nativa do nó móvel. De maneira muito semelhante ao modo como os pais \ndaquele jovem de 20 e poucos anos monitoram a localização do filho, o agente nativo na rede nativa do nó móvel \npode monitorar a rede externa na qual o nó móvel reside. Decerto será preciso um protocolo (ou um agente \nexterno representando o nó móvel) entre o nó móvel e o agente nativo para atualizar a localização do nó móvel.\nAgora vamos considerar o agente externo com mais detalhes. A técnica conceitualmente mais simples, \nmostrada na Figura 6.22, é localizar agentes externos nos roteadores de borda na rede externa. Um dos papéis do \nagente externo é criar o denominado endereço aos cuidados (care-of-address — COA) ou endereço administra-\ndo para o nó móvel, sendo que a parte da rede do endereço COA combinaria com a parte da rede do endereço \nda rede externa. Assim, há dois endereços associados a um nó móvel, seu endereço permanente (semelhante ao \nendereço da família do nosso jovem móvel) e seu endereço COA, às vezes denominado endereço externo (seme-\nlhante ao endereço da casa onde nosso jovem móvel estiver residindo). No exemplo da Figura 6.22, o endereço \npermanente do nó móvel é 128.119.40.186. Quando está visitando a rede 79.129.13/24, o nó móvel tem um COA \n79.129.13.2. Um segundo papel desempenhado pelo agente externo é informar ao agente nativo que o nó móvel \nestá residindo em sua rede (a rede do agente externo) e tem o endereço COA informado. Veremos, em breve, que \no COA pode ser utilizado para redirecionar datagramas para o nó móvel através de seu agente externo.\nEmbora tenhamos separado a funcionalidade do nó móvel e do agente externo, vale a pena observar que o \nnó móvel também pode assumir as responsabilidades do agente externo. Por exemplo, o nó móvel poderia obter \num COA na rede externa (por exemplo, utilizando um protocolo como o DHCP) e ele mesmo informar seu COA \nao agente nativo.\n6.5.2  Roteamento para um nó móvel\nAgora já vimos como um nó móvel obtém um COA e como é possível informar esse endereço ao agente \nnativo. Mas conseguir que o agente nativo conheça o COA resolve apenas parte do problema. Como datagramas \ndevem ser endereçados e repassados para o nó móvel? Visto que só o agente nativo (e não os roteadores no âmbi-\nto da rede) conhece a localização do nó móvel, já não será mais suficiente apenas endereçar um datagrama para o \nendereço permanente do nó móvel e enviá-lo para a infraestrutura da camada de rede. É preciso fazer algo mais. \nDuas técnicas podem ser identificadas, as quais denominaremos roteamento indireto e roteamento direto.\n   Redes de computadores e a Internet\n414\nRoteamento indireto para um nó móvel\nVamos considerar primeiro um correspondente que quer enviar um datagrama a um nó móvel. Na abor-\ndagem de roteamento indireto o correspondente apenas endereça o datagrama ao endereço permanente do nó \nmóvel, envia o datagrama para a rede e nem precisa saber se o nó móvel reside em sua rede nativa ou está visitando \numa rede externa; assim, a mobilidade é completamente transparente para o correspondente. Esses datagramas são \nprimeiro rotea­\ndos, como sempre, para a rede local do nó móvel. Isso é ilustrado na etapa 1 da Figura 6.23.\nAgora vamos voltar nossa atenção ao agente nativo. Além de ser responsável por interagir com um agente \nexterno para monitorar o COA do nó móvel, ele tem outra função muito importante. Sua segunda tarefa é ficar à \nespreita de datagramas que chegam e são endereçados a nós cuja rede nativa é a rede do agente nativo, mas que es-\ntão residindo no momento em uma rede externa. O agente nativo intercepta esses datagramas e então os repassa a \num nó móvel por um processo de duas etapas. Primeiro, o datagrama é repassado para o agente externo usando o \nCOA do nó móvel (etapa 2 na Figura 6.23), e depois do agente externo para o nó móvel (etapa 3 na Figura 6.23).\nÉ instrutivo considerar esse redirecionamento com mais detalhes. O agente nativo precisará endereçar o \ndatagrama usando o COA do nó móvel, de modo que a camada de rede roteará o datagrama para a rede externa. \nPor outro lado, é desejável deixar intacto o datagrama do correspondente, pois a aplicação que recebe o datagra-\nma deve desconhecer que este foi repassado por meio do agente nativo. Ambas as metas podem ser cumpridas \nfazendo o agente nativo encapsular o datagrama original completo do correspondente dentro de um novo (e \nmaior) datagrama. Este é endereçado e entregue ao COA do nó móvel. O agente externo “proprietário” do COA \nreceberá e desencapsulará o datagrama — isto é, removerá o datagrama original do correspondente de dentro \ndaquele datagrama maior de encapsulamento e repassará o datagrama original (etapa 3 na Figura 6.23) para o \nnó móvel. A Figura 6.24 mostra um datagrama original de um correspondente sendo enviado para a rede nativa, \num datagrama encapsulado sendo enviado ao agente externo e o datagrama original sendo entregue ao nó móvel. \nO leitor atento notará que o encapsulamento/desencapsulamento descrito aqui é idêntico à noção de implemen-\ntação de túnel discutida no Capítulo 4, no contexto do IP de transmissão para um grupo (multicast) e do IPv6.\nFigura 6.23  Roteamento indireto para um nó móvel\nKR 06.23.eps\nAgente \nnativo\nRede nativa:\n128.119.40/24\nRede visitada:\n79.129.13/24\nNó móvel\nEndereço permanente:\n128.119.40.186\nEndereço permanente:\n128.119.40.186\nAgente \nexterno\nEndereço aos \ncuidados (COA):\n79.129.13.2\nRede de longa \ndistância\nCorrespondente\n1\n2\n4\n3\nRedes sem fio e redes móveis  415 \nA seguir, vamos considerar como um nó móvel envia datagramas a um correspondente. Isso é muito sim-\nples, pois o nó móvel pode endereçar seu datagrama diretamente ao correspondente (usando o próprio endereço \npermanente como o de origem e o do correspondente como o endereço de destino). Visto que o nó móvel conhe-\nce o endereço do correspondente, não há necessidade de rotear o datagrama de volta por meio do agente nativo. \nIsso é mostrado como etapa 4 na Figura 6.23.\nVamos resumir o que discutimos sobre roteamento indireto relacionando as novas funcionalidades da ca-\nmada de rede exigidas para dar suporte à mobilidade.\n• Um protocolo de nó móvel ao agente externo. O nó móvel fará seu registro no agente externo ao se conec-\ntar à rede externa. De modo semelhante, um nó móvel cancelará seu registro no agente externo quando \nsair da rede externa.\n• Um protocolo de registro do agente externo ao agente nativo. O agente externo registrará o COA do nó \nmóvel no agente nativo. Um agente externo não precisa cancelar explicitamente um COA quando um \nnó móvel sai da rede porque o registro subsequente de um novo COA se encarregará disso quando o nó \nmóvel passar para uma nova rede.\n• Um protocolo de encapsulamento de datagrama para o agente nativo. Encapsulamento e repasse do data-\ngrama original do correspondente dentro de um datagrama endereçado ao COA.\n• Um protocolo de desencapsulamento para o agente externo. Extração do datagrama original do correspon-\ndente de dentro daquele que o encapsulou e repasse do datagrama original ao nó móvel. \nA discussão anterior provê todas as peças — agentes externos, agente nativo e repasse indireto — de que um \nnó móvel necessita para manter uma conexão em curso enquanto transita entre redes. Para ilustrar como essas \npeças se encaixam, suponha que o nó móvel está ligado à rede externa A, registrou um COA no agente local na \nrede A e está recebendo datagramas que estão sendo roteados indiretamente por meio de seu agente nativo. O nó \nmóvel agora passa para a rede externa B e se registra no agente externo na rede B, que informa ao agente nativo \no novo COA do nó móvel. Desse ponto em diante, o agente nativo redirecionará datagramas para a rede externa \nB. No que diz respeito ao correspondente, a mobilidade é transparente — datagramas são roteados por meio do \nmesmo agente nativo antes e depois de o nó móvel mudar de rede. Quanto ao agente nativo, não há qualquer \nruptura no fluxo de datagramas — datagramas que chegam são repassados primeiro para a rede externa A; após a \nFigura 6.24  Encapsulamento e desencapsulamento\nKR 06.24.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n35p3 Wide x 20p5 Deep\n11/16/11,  11/23/11,  11/29/11rossi\nAgente \nnativo\nEndereço permanente:\n128.119.40.186\nEndereço permanente:\n128.119.40.186\nAgente \nexterno\nCorrespondente\ndestino: 128.119.40.186\ndestino: 79.129.13.2\ndestino: 128.119.40.186\ndestino: 128.119.40.186\nEndereço aos cuidados:\n79.129.13.2\n   Redes de computadores e a Internet\n416\nmudança no COA, são repassados para a rede externa B. Mas o nó móvel verá um fluxo de datagramas interrom-\npido ao se movimentar entre redes? Contanto que seja pequeno o tempo transcorrido entre o desligamento do nó \nmóvel da rede A (ponto em que ele não pode mais receber datagramas via A) e sua conexão com a rede B (ponto \nem que registrará um novo COA no agente nativo da rede), poucos datagramas serão perdidos. Lembre-se de que \ndissemos, no Capítulo 3, que conexões fim a fim podem sofrer perda de datagramas por causa do congestiona-\nmento na rede. Por conseguinte, a perda ocasional de datagramas dentro de uma conexão quando um nó se move \nentre redes não é, de maneira alguma, um problema catastrófico. Se for preciso uma comunicação livre de perdas, \nmecanismos de camadas superiores recuperarão a perda de dados, quer elas resultem do congestionamento da \nrede ou da mobilidade do usuário.\nUma abordagem indireta de roteamento é utilizada no padrão IP móvel [RFC 5944], como discutiremos na \nSeção 6.6.\nRoteamento direto para um nó móvel\nA abordagem do roteamento indireto ilustrada na Figura 6.23 sofre de uma ineficiência conhecida como \nproblema do roteamento triangular — datagramas endereçados ao nó móvel devem ser roteados primeiro para \no agente nativo e em seguida para a rede externa, mesmo quando existir uma rota muito mais eficiente entre o \ncorrespondente e o nó móvel. No pior caso, imagine um usuário móvel que está visitando a rede externa de um \ncolega. Os dois estão sentados lado a lado e trocando dados pela rede. Datagramas do correspondente (nesse caso, \ndo colega do visitante) são roteados para o agente nativo do usuário móvel e, então, novamente de volta para a \nrede externa!\nO roteamento direto supera a ineficiência do roteamento triangular, mas o faz à custa de complexidade \nadicional. Na abordagem do roteamento direto, um agente correspondente na rede do correspondente primei-\nro aprende o COA do nó móvel. Isso pode ser realizado fazendo o agente correspondente consultar o agente \nnativo, admitindo que (como é o caso no roteamento indireto) o nó móvel tem um valor atualizado para seu \nCOA registrado no seu agente nativo. Também é possível que o próprio correspondente execute a função do \nagente correspondente, assim como um nó móvel poderia executar a função do agente externo. Tal situação \né ilustrada como as etapas 1 e 2 na Figura 6.25. O agente correspondente, então, executa um túnel para os \ndatagramas diretamente até o COA do nó móvel, de modo semelhante ao túnel executado pelo agente nativo, \netapas 3 e 4 da Figura 6.25.\nEmbora supere o problema do roteamento triangular, o roteamento direto introduz dois importantes desa-\nfios adicionais:\n• É preciso um protocolo de localização de usuário móvel para o agente correspondente consultar o agente \nnativo de modo a obter o COA do nó móvel (etapas 1 e 2 da Figura 6.25).\n• Quando o nó móvel passa de uma rede externa para outra, como os dados são repassados, agora, para \na nova rede? No caso do roteamento indireto, esse problema era facilmente resolvido atualizando-se o \nCOA mantido pelo agente nativo. Todavia, com roteamento direto, o agente correspondente consulta \no COA junto ao agente nativo apenas uma vez, no início da sessão. Assim, atualizar o COA do agente \nnativo, embora necessário, não será suficiente para resolver o problema do roteamento de dados para a \nnova rede externa do nó móvel.\nUma solução seria criar um novo protocolo para notificar a mudança de COA ao correspondente. Uma \nsolução alternativa, que, como veremos, é adotada na prática em redes GSM, funciona da seguinte maneira: supo-\nnha que estão sendo repassados dados correntemente para o nó móvel na rede externa onde ele estava localizado \nquando a sessão foi iniciada (etapa 1 na Figura 6.26). O agente externo naquela rede externa onde o nó móvel foi \nencontrado pela primeira vez será denominado agente externo âncora. Quando o nó móvel passar para uma nova \nrede externa (etapa 2 na Figura 6.26), ele se registrará com o novo agente externo (etapa 3), o qual fornecerá ao \nagente externo âncora o novo COA do nó móvel (etapa 4). Quando o agente externo âncora receber um datagra-\nma encapsulado para um nó móvel que já deixou a rede, ele então poderá reencapsular o datagrama e repassá-lo \nRedes sem fio e redes móveis  417 \nFigura 6.25  Roteamento direto para um usuário móvel\nKR 06.25.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n33p8 Wide x 25p2 Deep\n11/16/11, 11/23/11,  11/29/11 rossi\nAgente \nnativo\nRede nativa:\n128.119.40/24\nRede visitada:\n79.129.13/24\nNó móvel\nEndereço permanente:\n128.119.40.186\nLegenda:\nEndereço permanente:\n128.119.40.186\nAgente \nexterno\nEndereço aos cuidados (COA):\n79.129.13.2\nRede de longa \ndistância\nCorrespondente\nMensagens de controle\nAgente \ncorrespondente\n1\n2\n3\nFluxo de dados\n4\nFigura 6.26  Transferência móvel entre redes com roteamento direto\nKR 06.26.eps\nAgente \nnativo\nRede nativa:\nRede externa \nvisitada no \ninício da sessão:\nNova rede \nexterna:\nAgente \nexterno âncora\nNovo agente externo\nRede de longa \ndistância\nCorrespondente\nAgente \ncorrespondente\n1\n4\n2\n3\n5\npara o nó móvel (etapa 5) usando o novo COA. Se, mais tarde, o nó móvel passar para mais alguma outra rede \nexterna, o agente externo nessa nova rede visitada então contatará o agente externo âncora para estabelecer re-\npasse para essa nova rede externa.\n   Redes de computadores e a Internet\n418\n6.6  IP móvel\nA arquitetura e os protocolos da Internet para suporte de mobilidade, conhecidos como IP móvel, estão \ndefinidos principalmente no RFC 5944 para IPv4. O IP móvel é um protocolo flexível que suporta muitos modos \nde operação diferentes (por exemplo, operação com ou sem um agente externo), várias maneiras para agentes e \nnós móveis ­\ndescobrirem uns aos outros, utilização de um único COA ou de vários COAs e diversas formas de \nencapsulamento. Por isso, o IP móvel é um protocolo complexo, cuja descrição detalhada exigiria um livro intei-\nro; um desses livros é Perkins [1998b]. Aqui, nossa modesta meta é oferecer uma visão geral dos aspectos mais \nimportantes do IP móvel e ilustrar sua utilização em alguns cenários comuns.\nA arquitetura do IP móvel contém muitos dos elementos que acabamos de considerar, incluindo os con-\nceitos de agentes nativos, agentes externos, endereços administrados e encapsulamento/desencapsulamento. O \npadrão atual [RFC 5944] especifica a utilização de rotea­\nmento indireto para o nó móvel.\nO padrão IP móvel consiste em três partes principais:\n• Descoberta de agente. O IP móvel define os protocolos utilizados por um agente nativo ou por um agente \nexterno para anunciar seus serviços a nós móveis e protocolos para que os nós móveis solicitem os servi-\nços de um agente externo ou nativo.\n• Registro no agente nativo. O IP móvel define os protocolos usados pelo nó móvel e/ou agente externo \npara registrar e anular os registros de COAs no agente local de um nó móvel.\n• Roteamento indireto de datagramas. O padrão também define a maneira pela qual datagramas são re-\npassados para nós móveis por um agente nativo, incluindo regras para repassar datagramas, regras para \nmanipular condições de erro e diversas formas de encapsulamento [RFC, 2003; RFC, 2004].\nConsiderações de segurança têm destaque em todo o padrão IP móvel. Por exemplo, a autenticação de \num nó móvel é claramente necessária para impedir que um usuário mal­\n‑intencionado registre no agente na-\ntivo um falso endereço administrado, o que poderia fazer datagramas endereçados a um endereço IP serem \ndirecionados ao usuário mal-intencionado. O IP móvel consegue segurança usando muitos dos mecanismos \nque examinaremos no Capítulo 8, portanto, não consideraremos a questão da segurança de endereços na \ndiscussão a seguir.\nDescoberta de agente\nUm nó IP móvel que está chegando a uma nova rede, esteja ele se conectando a uma rede externa ou \nretornando à sua rede nativa, tem de aprender a identidade do correspondente externo ou do agente nativo. \nDe fato, é a descoberta de um novo agente externo, com um novo endereço de rede, que permite à camada de \nrede em um nó móvel descobrir que ele passou para uma nova rede externa. Esse processo é conhecido como \ndescoberta de agente. A descoberta de agente pode ser realizada de duas maneiras: via anúncio de agente ou \nvia solicitação de agente\nNo caso do anúncio de agente, um agente externo ou nativo anuncia seus serviços usando uma extensão \ndo protocolo de descoberta de roteador existente [RFC 1256]. O agente transmite periodicamente por difusão \numa mensagem ICMP tendo 9 no campo de tipo (descoberta de roteador) em todos os enlaces aos quais está \nconectado. A mensagem de descoberta de roteador contém o endereço IP do roteador (isto é, o agente), e isso \npermite que um nó móvel descubra o endereço IP do agente. A mensagem de descoberta de roteador também \ncontém uma extensão de anúncio de agente de mobilidade que guarda informações adicionais de que o nó móvel \nnecessita. Entre os campos mais importantes na extensão estão os seguintes:\n• Bit do agente nativo (H). Indica que o agente é um agente nativo para a rede na qual reside.\n• Bit de agente externo (F). Indica que o agente é um agente externo para a rede na qual reside.\n• Bit de registro obrigatório (R). Indica que um usuário móvel nessa rede deverá se registrar em um agente \nexterno. Em particular, um usuário móvel não pode obter um endereço administrado na rede externa \nRedes sem fio e redes móveis  419 \n(por exemplo, usando DHCP) e assumir a funcionalidade de agente externo para si mesmo sem se regis-\ntrar no agente externo.\n• Bits de encapsulamento M, G. Indicam se será utilizada uma forma de encapsulamento que não seja o \nencapsulamento IP em IP.\n• Campos de endereços aos cuidados (COA). Uma lista de um ou mais endereços aos cuidados, fornecida \npelo agente externo. No exemplo logo a seguir, o COA estará associado com o agente externo, que re-\nceberá datagramas enviados ao COA e então os repassará para o nó móvel adequado. O usuário móvel \nselecionará um desses endereços como seu COA ao se registrar no seu agente nativo.\nA Figura 6.27 ilustra alguns dos principais campos na mensagem de anúncio de agente.\nCom solicitação de agente, um nó móvel que quer conhecer agentes sem esperar para receber um anúncio \nde agente pode transmitir uma mensagem de solicitação em difusão, que é nada mais que uma mensagem ICMP \ncujo valor de tipo é 10. Ao receber a solicitação, um agente transmite um anúncio individual de agente diretamen-\nte ao nó móvel, que, então, procederá como se tivesse recebido um anúncio não solicitado.\nRegistro no agente nativo\nTão logo um nó móvel IP tenha recebido um COA, o endereço deve ser registrado no agente nativo, o que \npode ser feito por meio do agente externo (que, então, registra o COA no agente nativo) ou diretamente pelo \npróprio nó móvel IP. A seguir, consideramos o primeiro caso. Há quatro etapas envolvidas.\n1.\t Após o recebimento de um anúncio de agente externo, um nó móvel envia uma mensagem de registro IP \nmóvel ao agente externo. A mensagem de registro é transportada dentro de um datagrama UDP e enviada à \nporta 434. A mensagem de registro leva um COA anunciado pelo agente externo, o endereço do agente na-\ntivo (HA), o endereço permanente do nó móvel (MA), o tempo de vida de registro requerido e uma identifi-\ncação de registro de 64 bits. O tempo de vida de registro requerido é o número de segundos durante os quais \no registro será válido. Se o registro não for renovado no agente nativo dentro do tempo de vida especificado, \nele se tornará inválido. O identificador de registro age como um número de sequência e serve para combinar \numa resposta de registro recebida com uma solicitação de registro, como discutiremos mais adiante.\nFigura 6.27  \u0007\nMensagem ICMP de descoberta de roteador com extensão de anúncio de agente \nde mobilidade\nKR 06.27.eps\nAW/Kurose and Ross\nTipo = 9\nCódigo = 0\nTipo = 16\nComprimento\nNúmero de sequência\nTempo de vida de registro\nReservado\nBits \nRBHFMGrT\nSoma de veriﬁcação\nCampos \nICMP padrão\n0\n8\n16\n24\nEndereço do roteador\n0 ou mais endereços aos cuidados\nExtensão de \nanúncio de \nagente de mobilidade\n   Redes de computadores e a Internet\n420\n2.\t O agente externo recebe a mensagem de registro e registra o endereço IP permanente do nó móvel. Agora \no agente externo sabe que deve procurar datagramas que contenham um datagrama encapsulado cujo en-\ndereço de destino combine com o endereço permanente do nó móvel. O agente externo, então, envia uma \nmensagem de registro do IP móvel (novamente, dentro de um datagrama UDP) à porta 434 do agente \nnativo. A mensagem contém o COA, o HA, o MA, o formato de encapsulamento requisitado, o tempo de \nvida de registro requisitado e a identificação do registro.\n3.\t O agente nativo recebe a requisição de registro e verifica sua autenticidade e exatidão. O agente nativo \nvincula o endereço IP permanente do nó móvel ao COA; no futuro, datagramas que chegarem ao agente \nnativo e endereçados ao nó móvel serão encapsulados e enviados por túnel até o COA. O agente nativo \nenvia uma resposta de registro IP móvel contendo o HA, o MA, o tempo de vida de registro vigente e a \nidentificação de registro da solicitação que está sendo atendida com essa resposta.\n4.\t O agente externo recebe a resposta de registro e então a repassa ao nó móvel.\nNesse ponto o registro está concluído e o nó móvel pode receber datagramas enviados a seu endereço per-\nmanente. A Figura 6.28 ilustra essas etapas. Note que o agente nativo especifica um tempo de vida menor do que \naquele requisitado pelo nó móvel.\nFigura 6.28  Anúncio de agente e registro IP móvel\nKR\nAgente nativo\nHA: 128.119.40.7\nAgente móvel\nMA: 128.119.40.186\nRede visitada:\n79.129.13/24\nAnúncio de agente ICMP\nCOA: 79.129.13.2\n. . .\nCOA: 79.129.13.2\nHA:128.119.40.7\nMA: 128.119.40.186\nTempo de vida: 9999\nidentiﬁcação: 714\n. . .\nRequisição de registro\nCOA: 79.129.13.2\nHA:128.119.40.7\nMA: 128.119.40.186\nTempo de vida: 9999\nidentiﬁcação: 714\nformato de encapsulamento\n. . .\nRequisição de registro\nTempo\nTempo\nTempo\nHA: 128.119.40.7\nMA: 128.119.40.186\nTempo de vida: 4999\nidentiﬁcação: 714\nformato de encapsulamento\n. . .\nResposta de registro\nHA: 128.119.40.7\nMA: 128.119.40.186\nTempo de vida: 4999\nidentiﬁcação: 714\n. . .\nResposta de registro\nAgente externo\nCOA: 79.129.13.2\nRedes sem fio e redes móveis  421 \nUm agente externo não precisa anular explicitamente um registro de COA quando um nó móvel sai da rede. \nIsso ocorrerá automaticamente quando o nó móvel passar para uma nova rede (seja outra rede externa, seja sua \nrede nativa) e registrar um novo COA.\nO padrão IP móvel permite muitos cenários e capacidades adicionais além das que acabamos de descrever. \nO leitor interessado deve consultar Perkins [1998b]; RFC [5944].\n6.7  \u0007\nGerenciamento de mobilidade em redes celulares\nAgora que acabamos de examinar como a mobilidade é gerenciada em redes IP, vamos voltar nossa \natenção para redes cujo histórico de suporte à mobilidade é ainda mais longo — redes de telefonia celular. \nEnquanto na Seção 6.4 focalizamos o enlace sem fio do primeiro salto em redes celulares, aqui focalizaremos \na mobilidade utilizando a arquitetura de rede celular GSM [Goodman, 1997; Mouly, 1992; Scourias, 2012; \nKaaranen, 2001; Korhonen, 2003; Turner, 2012] como objeto de nosso estudo, visto que é uma tecnologia \nmadura e amplamente disponibilizada. Como no caso do IP móvel, veremos que vários dos princípios funda-\nmentais que identificamos da Seção 6.5 estão incorporados à arquitetura de rede do GSM.\nDo mesmo modo que o IP móvel, o GSM adota uma abordagem de roteamento indireto (veja Seção 6.5.2), \nprimeiro roteando a chamada do correspondente para a rede nativa do nó móvel e daí para a rede visitada. Em \nterminologia GSM, a rede nativa do nó móvel é denominada rede pública terrestre móvel nativa (PLMN na-\ntiva). Visto que o acrônimo PLMN é um pouco complicado, e sempre firmes na nossa decisão de evitar uma \nsopa de letrinhas, denominaremos a rede PLMN nativa GSM apenas rede nativa. A rede nativa é a operadora de \ncelular da qual o usuário móvel é assinante (isto é, a operadora que cobra mensalmente do usuário pelo serviço \ncelular). A PLMN visitada, que denominaremos rede visitada, é a rede na qual o nó móvel estiver residindo.\nComo no caso do IP móvel, as responsabilidades das redes nativas e visitadas são bastante diferentes.\n• A rede nativa mantém um banco de dados conhecido como registro nativo de localização (home loca-\ntion register — HLR), que contém o número permanente do telefone celular e as informações do perfil do \nassinante para cada assinante. O importante é que o HLR também contém informações sobre as locali-\nzações atuais desses assinantes. Isto é, se um usuário móvel estiver em trânsito pela rede celular de outra \noperadora, o HLR conterá informações suficientes para obter (por meio de um processo que descreve-\nremos em breve) um endereço na rede visitada para o qual deverá ser roteada uma chamada ao usuário \nmóvel. Como veremos, quando é feita uma chamada para um usuário móvel, um comutador especial na \nrede nativa, conhecido como Central de Comutação para Portal de Serviços Móveis (Gateway Mobile \nServices Switching Center — GMSC), é contatado por um correspondente. Mais uma vez, conforme nossa \ndecisão de evitar a sopa de letrinhas, denominaremos a GMSC por um termo mais descritivo, uma MSC \nnativa.\n• A rede visitada mantém um banco de dados conhecido como registro de localização de visitantes (visi-\ntor location register — VLR). O VLR contém um registro para cada usuário móvel que está atualmente na \nparte da rede atendida pelo VLR. Assim, os registros do VLR vêm e vão à medida que usuários entram \ne saem da rede. Um VLR normalmente está localizado juntamente com a central de comutação móvel \n(MSC) que coordena o estabelecimento de uma chamada de e para a rede visitada.\nNa prática, a rede celular de uma operadora servirá como uma rede nativa para seus assinantes e como uma rede \nvisitada para usuários móveis que são assinantes de outras operadoras de serviços celulares.\n6.7.1  Roteando chamadas para um usuário móvel\nAgora estamos prontos para descrever como é estabelecida uma chamada para um usuário GSM em uma \nrede visitada. A seguir, consideraremos um exemplo simples; cenários mais complexos são descritos em Mouly \n[1992]. As etapas, como ilustradas na Figura 6.29, são as seguintes:\n   Redes de computadores e a Internet\n422\n1.\t O correspondente disca o número do telefone do usuário móvel. Esse número, em si, não se refere a uma \ndeterminada linha telefônica ou localização (afinal, o número do telefone é fixo, mas o usuário é móvel!). \nOs dígitos iniciais do número são suficientes para identificar globalmente a rede nativa do usuário móvel. \nA chamada é roteada desde o correspondente, passa através do PSTN e chega até a MSC na rede nativa \ndo usuário móvel. Esse é o primeiro trecho da chamada.\n2.\t A MSC nativa recebe a chamada e interroga o HLR para determinar a localização do usuário móvel. No \ncaso mais simples, o HLR retorna o número roaming da estação móvel (mobile station roaming number \n— MSRN), que denominaremos número de roaming. Note que esse número é diferente do número per-\nmanente do telefone móvel, que é associado com a rede nativa do usuário móvel. O número de roaming \né efêmero: é designado temporariamente a um usuário móvel quando ele entra em uma rede visitada. O \nnúmero de roaming desempenha um papel semelhante ao do endereço COA no IP móvel e, da mesma \nforma, é invisível para o correspondente e para o usuário móvel. Se o HLR não tiver o número de roaming, \nele retornará ao endereço do VLR na rede visitada. Nesse caso (que não é mostrado na Figura 6.29), a \nMSC nativa precisará consultar o VLR para obter o número de roaming do nó móvel. Mas, antes de tudo, \ncomo o HLR obtém o número de roaming ou o endereço VLR? O que acontece a esses valores quando o \nusuário móvel passa para outra rede visitada? Em breve consideraremos essas perguntas importantes.\n3.\t Dado o número de roaming, a MSC nativa estabelece o segundo trecho da chamada através da rede até a \nMSC na rede visitada. A chamada está concluída — foi roteada do correspondente até a MSC nativa e daí \npara a MSC visitada, e desta até a estação-base que atende ao usuário móvel.\nUma questão não resolvida na etapa 2 é como o HLR obtém informação sobre a localização do usuário \nmóvel. Quando um telefone móvel é ligado ou entra em uma parte de uma rede visitada que é coberta por um \nnovo VLR, ele deve se registrar na rede visitada. Isso é feito por meio da troca de mensagens de sinalização entre \no usuário móvel e o VLR. O VLR visitado, por sua vez, envia uma mensagem de requisição de atualização de \nlocalização ao HLR do usuário móvel. Essa mensagem informa ao HLR o número de roaming no qual o usuário \nmóvel pode ser contatado ou o endereço do VLR (que então pode ser consultado mais tarde para obter esse nú-\nmero). Como parte dessa troca, o VLR também obtém do HLR informações sobre o assinante do usuário móvel \ne determina quais serviços (se houver algum) devem ser prestados a ele pela rede visitada.\nFigura 6.29  Estabelecendo uma chamada para um usuário móvel: roteamento indireto\nUsuário \nmóvel\nSistema \nvisitado\nRede \nnativa\nRede telefônica \npública comutada\n1\n3\nKR 06.29.eps\nKUROSE/ROSS\nComputer Networking 6/e\nCorrespondente\nVLR\nHLR\n2\nRedes sem fio e redes móveis  423 \n6.7.2  Transferências (handoffs) em GSM\nUma transferência (handoff) ocorre quando uma estação móvel muda sua associação de uma estação-base \npara outra durante uma chamada. Como ilustra a Figura 6.30, uma chamada de telefone móvel é roteada de início \n(antes da transferência) para o usuário móvel por meio de uma estação-base (a qual denominaremos antiga esta-\nção-base) e, após, por meio de outra estação-base (a qual denominaremos nova estação-base). Note que uma trans-\nferência entre estações-base resulta não apenas em transferência/recepção de/para um telefone móvel e uma nova \nestação-base, mas também no redirecionamento da chamada em curso de um ponto de comutação dentro da rede \npara a nova estação-base. Vamos admitir inicialmente que as estações­\n‑base antiga e nova compartilham a mesma \nMSC e que o redirecionamento ocorre nessa MSC.\nHá diversas razões possíveis para ocorrer transferência, incluindo (1) o sinal entre a estação-base corrente e \no usuário móvel pode ter-se deteriorado a tal ponto que a chamada corre perigo de “cair” e (2) uma célula pode \nter ficado sobrecarregada, manipulando grande número de chamadas. Esse congestionamento pode ser aliviado \ntransferindo usuários móveis para células próximas, menos congestionadas.\nEnquanto está associado com uma estação-base, um usuário móvel mede periodicamente a potência de \num sinal de sinalização emitido por sua estação-base corrente, bem como de sinais de sinalização emitidos por \nestações-base próximas que ele pode “ouvir”\n. Essas medições são passadas uma ou duas vezes por segundo para \na estação-base corrente do usuário móvel. A transferência em GSM é iniciada pela estação-base antiga com base \nnessas medições, nas cargas correntes de usuários móveis em células próximas e outros fatores [Mouly, 1992]. O \npadrão GSM não determina o algoritmo específico a ser utilizado por uma estação-base para decidir se realiza \nou não uma transferência.\nA Figura 6.31 ilustra as etapas envolvidas quando uma estação-base decide transferir um usuário móvel:\n1.\t A antiga estação-base (BS) informa à MSC visitada que deve ser feita uma transferência e a BS (ou possível \nconjunto de BSs) para a qual o usuário móvel deve ser transferido.\n2.\t A MSC visitada inicia o estabelecimento do caminho até a nova BS, alocando os recursos necessários para \ncarregar a chamada redirecionada e sinalizando à nova BS que uma transferência está prestes a ocorrer.\n3.\t A nova BS reserva e ativa um canal de rádio para ser utilizado pelo usuário móvel.\n4.\t A nova BS devolve um sinal à MSC visitada e à antiga BS, indicando que foi estabelecido um caminho \nentre a MSC visitada e a nova BS e que o usuário móvel deve ser informado da transferência iminente. A \nnova BS provê todas as informações de que o usuário móvel necessitará para se associar com a nova BS.\n5.\t O usuário móvel é informado de que deve realizar uma transferência. Note que, até esse ponto, ele está \ntotalmente desavisado de que a rede estava preparando o terreno para uma transferência (por exemplo, \nreservando um canal na nova BS e alocando um caminho entre a MSC visitada e a nova BS).\n6.\t\nO usuário móvel e a nova BS trocam uma ou mais mensagens para ativar totalmente o novo canal na nova BS.\nFigura 6.30  Cenário de transferência entre estações-base que têm uma MSC em comum\nAntiga estação-base \n(BS)\nNova estação-base \n(BS)\nAntigo \nroteamento\nNovo \nroteamento\nKR 06.30.eps\nKUROSE/ROSS\nComputer Networking 6/e\n15p7 Wide x 10p5 Deep\nVLR\n   Redes de computadores e a Internet\n424\n7.\t O móvel envia à nova BS uma mensagem de conclusão de transferência que é repassada para a MSC visi-\ntada. Então, a MSC visitada redireciona a chamada em curso para o usuário móvel, por meio da nova BS.\n8.\t Os recursos reservados ao longo do caminho até a antiga BS são liberados.\nVamos concluir nossa discussão sobre transferência considerando o que acontece quando o usuário móvel \npassa para uma BS associada com uma MSC diferente da antiga BS e o que acontece quando essa transferência \nentre MSCs ocorre mais de uma vez. Como ilustra a Figura 6.32, o GSM define a noção de uma MSC âncora. \nA MSC âncora é a MSC visitada pelo usuário móvel logo no início de uma chamada; assim, ela não muda duran-\nte a chamada. Por toda a duração da chamada e independentemente do número de transferências entre MSCs \nrealizadas pelo usuário móvel, a chamada é roteada desde a MSC nativa até a âncora e, então, desta até a MSC \nvisitada, onde o usuário móvel está localizado no momento. Quando um usuário móvel passa da área de cobertu-\nra de uma MSC para a área de cobertura de outra, a chamada em curso é redirecionada desde a MSC âncora até \na nova MSC visitada, que contém a nova estação-base. Assim, durante o tempo todo há, no máximo, três MSCs \nFigura 6.31  \u0007\nEtapas da execução de uma transferência entre estações-base que têm uma MSC \nem comum\n1\n5\n7\n8\n2\n3\n6\nKR 06.31.eps\nKUROSE/ROSS\nComputer Networking 6/e\n16p0 Wide x 12p0 Deep\n11/17/11 ROSSI ILLUSTRATION\n4\nVLR\nAntiga\nestação-base \n(BS)\nNova\nestação-base \n(BS)\nFigura 6.32  Redirecionamento via a MSC âncora\nRede nativa\nCorrespondente\na.  Antes da transferência\nMSC \nâncora\nPSTN\nKR 06.32.eps\nAW/Kurose and Ross\nComputer Networking 6/e\nb.  Após a transferência\nCorrespondente\nMSC \nâncora\nPSTN\nRede nativa\nRedes sem fio e redes móveis  425 \n(a nativa, a âncora e a visitada) entre o correspondente e o usuário móvel. A Figura 6.32 ilustra o roteamento de \numa chamada entre as MSCs visitadas por um usuário móvel.\nEm vez de manter um único salto desde a MSC âncora até a MSC corrente, uma técnica alternativa seria encadear \nas MSCs visitadas pelo móvel, fazendo uma MSC antiga transmitir a chamada em curso até a nova MSC cada vez que o \nmóvel passa para uma nova MSC. Esse encadeamento de MSC pode de fato ocorrer em redes celulares IS-41, com uma \netapa opcional de minimização de caminho para remover MSCs entre a âncora e a nova visitada [Lin, 2001].\nVamos encerrar nossa discussão sobre o gerenciamento da mobilidade GSM fazendo uma comparação en-\ntre gerenciamento de mobilidade em GSM e em IP Móvel. A comparação apresentada na Tabela 6.2 indica que, \nembora redes IP e celulares sejam em essência diferentes em muitos aspectos, compartilham um número surpre-\nendente de elementos funcionais comuns e abordagens gerais para o tratamento da mobilidade.\n6.8  \u0007\nSem fio e mobilidade: impacto sobre protocolos de \ncamadas superiores\nNeste capítulo, vimos que redes sem fio são significativamente diferentes de suas contrapartes cabeadas \ntanto na camada de enlace (como resultado de características de canais sem fio como desvanecimento, propaga-\nção multivias e terminais ocultos) quanto na camada de rede (como resultado de usuários móveis que mudam \nseus pontos de conexão com a rede). Mas há diferenças importantes nas camadas de transporte e de aplicação? \nÉ tentador pensar que essas diferenças seriam pequenas, visto que a camada de rede provê o mesmo modelo de \nserviço de entrega de melhor esforço às camadas superiores tanto em redes cabeadas quanto em redes sem fio. \nDe modo semelhante, se protocolos como TCP ou UDP são usados para oferecer serviços da camada de trans-\nporte a aplicações tanto em redes cabeadas como em redes sem fio, então a camada de aplicação também deve \npermanecer inalterada. Nossa intuição está certa em um sentido — TCP e UDP podem operar, e de fato operam, \nem redes com enlaces sem fio. Por outro lado, protocolos de transporte em geral e o TCP em particular às vezes \npodem ter desempenhos muito diferentes em redes cabeadas e em redes sem fio, e é neste particular, em termos \nde desempenho, que as diferenças se manifestam. Vejamos por quê.\nTabela 6.2  Semelhanças entre a mobilidade em IP móvel e em GSM\nElemento do GSM\nComentário sobre o elemento do GSM\nElemento do IP móvel\nSistema nativo\nRede à qual pertence o número de telefone permanente \ndo usuário.\nRede nativa\nCentral de comutação de unidade \nmóvel ou simplesmente MSC nativa, \nRegistro nativo de localização (HLR)\nMSC nativa: ponto de contato para obter endereço \nroteável de usuário móvel.\nHLR: banco de dados no sistema nativo que contém \nnúmero de telefone permanente, informações de perfil, \nlocalização corrente de usuário móvel, informações de \nassinatura.\nAgente nativo\nSistema visitado\nRede, exceto o sistema nativo, onde o usuário móvel está \nresidindo.\nRede visitada\nCentral de serviços de comutação de \nunidade móvel visitada, Registro de \nLocalização de Visitante (VLR)\nMSC visitada: responsável por estabelecer chamadas de/\npara nós móveis em células associadas com MSC.\nVLR: registro temporário em banco de dados em sistema \nvisitado, contendo informações de assinatura para cada \nusuário móvel visitado.\nAgente externo\nNúmero de roaming de estação móvel \n(MSRN) ou simplesmente número de \nroaming\nEndereço roteável para segmento de chamada telefônica \nentre MSC nativa e MSC visitada, que não é visível nem \npara o usuário móvel nem para o correspondente.\nEndereço administrado\n   Redes de computadores e a Internet\n426\nLembre-se de que o TCP retransmite um segmento que é perdido ou corrompido no caminho entre \nremetente e destinatário. No caso de usuários móveis, a perda pode resultar de congestionamento de rede (es-\ntouro de buffer de roteador) ou de transferência (por exemplo, de atrasos no redirecionamento de segmentos \npara um novo ponto de conexão do usuário à rede). Em todos os casos, o ACK do destinatário ao remetente do \nTCP indica apenas que um segmento não foi recebido intacto; o remetente não sabe se o segmento foi perdido \npor congestionamento, durante a transferência, ou por erros de bits detectados. Em todos os casos, a resposta \ndo remetente é a mesma — retransmitir o segmento. A resposta do controle de congestionamento do TCP tam-\nbém é a mesma em todos os casos — o TCP reduz sua janela de congestionamento, como discutimos na Seção \n3.7. Reduzindo de modo incondicional sua janela de congestionamento, o TCP admite implicitamente que a \nperda de segmento resulta de congestionamento e não de corrupção ou transferência. Vimos na Seção 6.2 que \nerros de bits são muito mais comuns em redes sem fio do que nas cabeadas. Quando ocorrem esses erros de \nbits ou quando há perda na transferência, na realidade não há razão alguma para que o remetente TCP reduza \nsua janela de congestionamento (reduzindo, assim, sua taxa de envio). Na verdade, é bem possível que os \nbuffers de roteador estejam vazios e que pacotes estejam fluindo ao longo de caminhos fim a fim desimpedidos, \nsem congestionamento.\nEntre o início e meados da década de 1990, pesquisadores perceberam que, dadas as altas taxas de erros de \nbits em enlaces sem fio e a possibilidade de perdas pela transferência de usuá­\nrios, a resposta do controle de con-\ngestionamento do TCP poderia ser problemática em um ambiente sem fio. Há duas classes gerais de abordagens \npossíveis para tratar esse problema:\n• Recuperação local. Os protocolos de recuperação local recuperam erros de bits quando e onde (por exem-\nplo, no enlace sem fio) eles ocorrem (por exemplo, o protocolo ARQ 802.11, que estudamos na Seção 6.3, \nou técnicas mais sofisticadas que utilizam ARQ e também FEC [Ayanoglu, 1995]).\n• Remetente TCP ciente de enlaces sem fio. Em técnicas de recuperação locais, o remetente TCP fica com-\npletamente desavisado de que seus segmentos estão atravessando um enlace sem fio. Uma técnica alter-\nnativa é o remetente e o destinatário ficarem cientes da existência de um enlace sem fio, para distinguir \nentre perdas por congestionamento na rede cabeada e corrupção/perdas no enlace sem fio, e invocar o \ncontrole de congestionamento somente em resposta a perdas por congestionamento na rede cabeada. \nBalakrishnan [1997] investiga vários tipos de TCP, supondo que sistemas finais possam fazer essa distin-\nção. Liu [2003] investiga técnicas para distinguir entre perdas nos segmentos cabeado e sem fio de um \ncaminho fim a fim.\n• Técnicas de conexão dividida. Nesta técnica de conexão dividida [Bakre, 1995], a conexão fim a fim entre \no usuário móvel e o outro ponto terminal é dividida em duas conexões da camada de transporte: uma \ndo hospedeiro móvel ao ponto de acesso sem fio, e uma do ponto de acesso sem fio ao outro ponto ter-\nminal de comunicação (admitiremos, aqui, um usuário cabeado). A conexão fim a fim é, então, formada \npor uma concatenação de uma parte sem fio e uma parte cabeada. A camada de transporte sobre um \nsegmento sem fio pode ser uma conexão-padrão TCP [Bakre, 1995], ou principalmente um protocolo \nde recuperação de erro personalizado em cima do UDP. Yavatkar [1994] analisa o uso de um protocolo \nde repetição seletiva da camada de transporte por uma conexão sem fio. As medidas relatadas em Wei \n[2006] indicam que conexões TCP divididas são bastante usadas em redes de dados celulares, e que aper-\nfeiçoamentos significativos podem ser feitos com o uso dessas conexões.\nAqui, nosso tratamento do TCP em enlaces sem fio foi necessariamente breve. Estudos mais aprofundados \ndos desafios e soluções do TCP nessas redes podem ser encontrados em Hanabali [2005]; Leung [2006]. Aconse-\nlhamos o leitor a consultar as referências se quiser mais detalhes sobre essa área de pesquisa em curso.\nAgora que já consideramos protocolos de camada de transporte, vamos analisar em seguida o efeito \ndo sem fio e da mobilidade sobre protocolos da camada de aplicação. Uma consideração importante aqui é \nque enlaces sem fio muitas vezes têm larguras de banda relativamente baixas, como vimos na Figura 6.2. Por \nconseguinte, aplicações que operam por enlaces sem fio, em particular por enlaces celulares sem fio, devem \nRedes sem fio e redes móveis  427 \ntratar a largura de banda como uma mercadoria escassa. Por exemplo, um servidor Web que serve conteúdo \na um navegador Web que está rodando em um telefone 3G talvez não consiga prover o mesmo conteúdo rico \nem imagens que oferece a um navegador que está rodando sobre uma conexão cabeada. Embora enlaces sem \nfio proponham desafios na camada de aplicação, a mobilidade que eles criam também torna possível um rico \nconjunto de aplicações cientes de localização e de contexto [Chen, 2000; Baldauf, 2007]. Em termos mais ge-\nrais, redes sem fio e redes móveis desempenharão um papel fundamental na concretização dos ambientes de \ncompu­\ntação onipresentes do futuro [Weiser, 1991]. É justo dizer que vimos somente a ponta do iceberg quan-\ndo se trata do impacto de redes sem fio e móveis sobre aplicações em rede e seus protocolos!\n6.9  Resumo\nRedes sem fio e móveis revolucionaram a telefonia e também estão causando um impacto cada vez mais \nprofundo no mundo das redes de computadores. Com o acesso à infraestrutura da rede global que oferecem — \ndesimpedido, a qualquer hora, em qualquer lugar — estão não só aumentando a onipresença do acesso a redes, \nmas também habilitando um novo conjunto muito interessante de serviços dependentes de localização. Dada a \ncrescente importância das redes sem fio e móveis, este capítulo focalizou os princípios, as tecnologias de enlace e \nas arquiteturas de rede para suportar comunicações sem fio e móveis.\nIniciamos o capítulo com uma introdução às redes sem fio e móveis, traçando uma importante distinção en-\ntre os desafios propostos pela natureza sem fio dos enlaces de comunicação desse tipo de rede, e pela mobilidade \nque permitem. Isso nos possibilitou isolar, identificar e dominar melhor os conceitos fundamentais em cada área. \nFocalizamos primeiro a comunicação sem fio, considerando as características de um enlace sem fio na Seção 6.2. \nNas seções 6.3 e 6.4 examinamos os aspectos de camada de enlace do padrão IEEE 802.11 (Wi-Fi) para LANs sem \nfio, duas redes pessoais IEEE 802.15 (Bluetooth e Zigbee) e acesso à Internet pela rede celular 3G e 4G. Depois, \nvoltamos nossa atenção para a questão da mobilidade. Na Seção 6.5, identificamos diversas formas de mobilidade \ne verificamos que há pontos nesse espectro que propõem desafios diferentes e admitem soluções diferentes. Con-\nsideramos os problemas de localização e roteamento para um usuário móvel, bem como técnicas para transferir \no ­\nusuário móvel que passa dinamicamente de um ponto de conexão com a rede para outro. Examinamos como \nessas questões foram abordadas no padrão IP móvel e em GSM nas seções 6.6 e 6.7, respectivamente. Por fim, na \nSeção 6.8, consideramos o impacto causado por enlaces sem fio e pela mobilidade sobre protocolos de camada \nde transporte e aplicações em rede.\nEmbora tenhamos dedicado um capítulo inteiro ao estudo de redes sem fio e redes móveis, seria preciso \ntodo um livro (ou mais) para explorar completamente esse campo tão animador e que está se expandindo tão \ndepressa. Aconselhamos o leitor a se aprofundar mais nesse campo consultando as muitas referências fornecidas \nneste capítulo.\nExercícios\nde fixação e perguntas\nQuestões de revisão do Capítulo 6\nSEÇÃO 6.1\n\t\nR1.\t O que significa para uma rede sem fio estar operando no “modo de infraestrutura”? Se a rede não estiver nesse \nmodo, em qual modo ela está e qual é a diferença entre esse modo de operação e o de infraestrutura?\n\t\nR2.\t Quais são os quatro tipos de redes sem fio identificadas em nossa taxonomia na Seção 6.1? Quais desses tipos \nde rede sem fio você usou?\n   Redes de computadores e a Internet\n428\nSEÇÃO 6.2\n\t\nR3.\t Quais são as diferenças entre os seguintes tipos de falhas no canal sem fio: atenuação de percurso, propagação \nmultivias, interferência de outras fontes?\n\t\nR4.\t Um nó móvel se distancia cada vez mais de uma estação-base. Quais são as duas atitudes que uma estação­\n‑base poderia tomar para garantir que a probabilidade de perda de um quadro transmitido não aumente?\nSEÇÕES 6.3-6.4\n\t\nR5.\t Descreva o papel dos quadros de sinalização em 802.11.\n\t\nR6.\t Verdadeiro ou falso: antes de uma estação 802.11 transmitir um quadro de dados, ela deve primeiro enviar \num quadro RTS e receber um quadro CTS correspondente.\n\t\nR7.\t Por que são usados reconhecimentos em 802.11, mas não em Ethernet cabeada?\n\t\nR8.\t Verdadeiro ou falso: Ethernet e 802.11 usam a mesma estrutura de quadro.\n\t\nR9.\t Descreva como funciona o patamar RTS.\n\t\nR10.\t Suponha que os quadros RTS e CTS IEEE 802.11 fossem tão longos quanto os padronizados DATA e ACK. \nHaveria alguma vantagem em usar os quadros CTS e RTS? Por quê?\n\t\nR11.\t A Seção 6.3.4 discute mobilidade 802.11, na qual uma estação sem fio passa de um BSS para outro dentro da \nmesma sub-rede. Quando os APs estão interconectados com um comutador, um AP pode precisar enviar um \nquadro com um endereço MAC fingido para fazer o comutador transmitir quadros adequadamente. Por quê?\n\t\nR12.\t Quais são as diferenças entre o dispositivo mestre em uma rede Bluetooth e uma estação­\n‑base em uma rede \n802.11?\n\t\nR13.\t O que significa um superquadro no padrão Zigbee 802.15.4?\n\t\nR14.\t Qual é a função do “núcleo da rede” na arquitetura de dados celular 3G?\n\t\nR15.\t Qual é a função do RNC na arquitetura da rede de dados celular 3G? Que função o RNC desempenha na rede \ncelular de voz?\nSEÇÕES 6.5-6.6\n\t\nR16.\t Se um nó tem uma conexão sem fio à Internet, ele precisa ser móvel? Explique. Suponha que um usuário \nportando um notebook ande com ele pela casa, e sempre acesse a Internet por meio do mesmo ponto de \nacesso. Em relação à rede, este é usuário móvel? Explique.\n\t\nR17.\t Qual é a diferença entre um endereço permanente e um endereço aos cuidados (COA)? Quem determina o \nendereço aos cuidados?\n\t\nR18.\t Considere uma conexão TCP através de um IP móvel. Falso ou verdadeiro: a fase da conexão TCP entre o \ncorrespondente e o hospedeiro móvel percorre a rede doméstica móvel, mas a fase de transferência de dados \nestá diretamente entre o correspondente e o hospedeiro móvel, pulando a rede doméstica.\nSEÇÃO 6.7\n\t\nR19.\t Quais são os objetivos do HLR e VLR nas redes GSM? Quais elementos de IP móvel são semelhantes ao HLR \ne ao VLR?\n\t\nR20.\t Qual é o papel da MSC âncora em redes GSM?\nSEÇÃO 6.8\n\t\nR21.\t Quais são os três métodos que podem ser realizados para evitar que um único enlace sem fio reduza o \ndesempenho de uma conexão TCP fim a fim da camada de transporte?\nRedes sem fio e redes móveis  429 \nproblemas\n\t\nP1.\t Considere o exemplo do remetente CDMA único na Figura 6.5. Qual seria a saída do remetente (para os 2 bits \nde dados mostrados) se o código do remetente CDMA fosse (1, –1, 1, –1, 1, –1, 1, –1)?\n\t\nP2.\t Considere o remetente 2 na Figura 6.6. Qual é a saída do remetente para o canal (antes de ser adicionada ao \nsinal vindo do remetente 1), Z2\ni,m?\n\t\nP3.\t Suponha que o receptor na Figura 6.6 queira receber os dados que estão sendo enviados pelo remetente 2. \nMostre (por cálculo) que o receptor pode, na verdade, recuperar dados do remetente 2 do sinal agregado do \ncanal usando o código do remetente 2.\n\t\nP4.\t Para o exemplo sobre dois remetentes, dois destinatários, dê um exemplo de dois códigos CDMA contendo \nos valores 1 e –1, que não permitem que dois destinatários extraiam os bits originais transmitidos por dois \nremetentes CDMA.\n\t\nP5.\t Suponha que dois ISPs fornecem acesso Wi-Fi em um determinado local, e que cada um deles opera seu \npróprio AP e tem seu próprio bloco de endereços IP.\na.\t Suponha ainda mais, que, por acidente, cada ISP configurou seu AP para operar no canal 11. O protocolo \n802.11 falhará totalmente nessa situação? Discuta o que acontece quando duas estações, cada uma \nassociada com um ISP diferente, tentam transmitir ao mesmo tempo.\nb.\t Agora suponha que um AP opera no canal 1 e outro no canal 11. Como você mudaria suas respostas?\n\t\nP6.\t Na etapa 4 do protocolo CSMA/CA, uma estação que transmite um quadro com sucesso inicia o protocolo \nCSMA/CA para um segundo quadro na etapa 2, e não na 1. Quais seriam as razões que os projetistas do \nCSMA/CA provavelmente tinham em mente para fazer essa estação não transmitir o segundo quadro de \nimediato (se o canal fosse percebido como ocioso)?\n\t\nP7.\t Suponha que uma estação 802.11b seja configurada para sempre reservar o canal com a sequência RTS/CTS. \nImagine que essa estação de repente queira transmitir 1.000 bytes de dados e que todas as outras estações estão \nociosas nesse momento. Calcule o tempo requerido para transmitir o quadro e receber o reconhecimento \ncomo uma função de SIFS e DIFS, ignorando atraso de propagação e admitindo que não haja erros de bits.\n\t\nP8.\t Considere o cenário mostrado na Figura 6.33, no qual existem quatro nós sem fios, A, B, C e D. A cobertura \nde rádio dos quatro nós é mostrada pelas formas ovais mais escuras; todos os nós compartilham a mesma \nfrequência. Quando A transmite, ele pode ser ouvido/recebido por B; quando B transmite, ele só pode ser \nouvido/recebido por A e C; quando C transmite, B e D podem ouvir/receber de C; quando D transmite, \nsomente C pode ouvir/receber de D.\nFigura 6.33  Cenário para o problema P8\nKR 06.33.eps\nAW/Kurose and Ross\nComputer Networking 6/e\nsize:  28p0 wide  x  7p6 deep\n11/17/11 rossi\nA\nB\nC\nD\n\t\n\t Agora suponha que cada nó possua um estoque infinito de mensagens que ele queira enviar para os outros \nnós. Se o destinatário da mensagem não for um vizinho imediato, então a mensagem deve ser retransmitida. \nPor exemplo, se A quer enviar para D, uma mensagem de A deve ser primeiro enviada a B, que, então, envia a \nmensagem a C, e este a D. O tempo é dividido em intervalos, com um tempo de transmissão de mensagem de \n   Redes de computadores e a Internet\n430\nexatamente um intervalo de tempo, como em um slotted Aloha, por exemplo. Durante um intervalo, um nó \npode fazer uma das seguintes opções: (i) enviar uma mensagem; (ii) receber uma mensagem (se, exatamente, \numa mensagem estiver sendo enviada a ele), (iii) permanecer silencioso. Como sempre, se um nó ouvir duas \nou mais transmissões simultâneas, ocorrerá uma colisão e nenhuma das mensagens transmitidas é recebida \ncom sucesso. Você pode admitir aqui que não existem erros de bits e, dessa forma, se uma mensagem for \nenviada, ela será recebida corretamente pelos que estão dentro do raio de transmissão do emissor.\na.\t Suponha que um controlador onisciente (ou seja, que sabe o estado de cada nó na rede) possa comandar \ncada nó a fazer o que ele (o controlador onisciente) quiser, isto é, enviar uma mensagem, receber uma \nmensagem, ou permanecer silencioso. Dado esse controlador onisciente, qual é a taxa máxima à qual uma \nmensagem de dados pode ser transferida de C para A, sabendo que não existem outras mensagens entre \nnenhuma outra dupla remetente/destinatária?\nb.\t Suponha que A envie uma mensagem a B, e D envie uma mensagem a C. Qual é a taxa máxima combinada \nà qual as mensagens de dados podem fluir de A a B e de D a C?\nc.\t Considere agora que A envie uma mensagem a B, e C envie uma mensagem a D. Qual é a taxa máxima \ncombinada à qual as mensagens de dados podem fluir de A a B e de C a D?\nd.\t Suponha agora que os enlaces sem fio sejam substituídos por enlaces cabeados. Repita as questões de “a” \na “c” neste cenário cabeado.\ne.\t Agora imagine que estamos de novo em um cenário sem fio e que para cada mensagem de dados enviada \ndo remetente ao destinatário, este envie de volta uma mensagem ACK para o remetente (como no TCP, \npor exemplo). Suponha também que cada mensagem ACK possua um intervalo. Repita as questões de “a” \na “c” para este cenário.\n\t\nP9.\t Descreva o formato do quadro Bluetooth 802.15.1. Você precisará de uma leitura complementar para \nencontrar essa informação. Existe algo no formato do quadro que basicamente limite o número de nós ativos \npara oito em uma rede 802.15.1? Explique.\n\t\nP10.\t Considere o seguinte cenário WiMAX ideal. O subquadro de descendente (veja Figura 6.17) é dividido em \nintervalos de tempo, com N intervalos descendentes por subquadro, e todos os intervalos de tempo têm o \nmesmo comprimento. Existem quatro nós, A, B, C e D, alcançáveis da estação-base a taxas de 10 Mbits/s, \n5 Mbits/s, 2,5 Mbits/s e 1 Mbit/s, respectivamente no canal descendente. A estação-base possui infinitos \ndados para enviar a cada nó e pode enviar para qualquer um dos quatro nós durante qualquer intervalo de \ntempo no subquadro descendente.\na.\t Qual é a taxa máxima à qual a estação-base pode enviar aos nós, admitindo que ela pode enviar a qualquer \nnó de sua escolha durante cada intervalo de tempo? Sua solução é justa? Explique e defina o que você quis \ndizer com “justo”\n.\nb.\t Se há requisito de equidade que todos os nós devem receber uma quantidade igual de dados durante cada \nquadro de downstream, qual é a taxa média de transmissão pela estação-base (para todos os nós) durante \no subquadro de downstream? Explique como você chegou a essa resposta.\nc.\t Suponha que, como critério de equidade, qualquer nó possa receber, no máximo, duas vezes tantos dados \nquanto qualquer outro nó durante o subquadro. Qual é a taxa média de transmissão pela estação-base \n(para todos os nós) durante o subquadro de downstream? Explique como você chegou a esta resposta.\n\t\nP11.\t Na Seção 6.5, uma solução proposta que permitia que usuários móveis mantivessem seu endereço IP à \nmedida que transitavam entre redes externas era fazer uma rede externa anunciar ao usuário móvel uma rota \naltamente específica e usar a infraestrutura de roteamento existente para propagar essa informação por toda a \nrede. Uma das preocupações que identificamos foi a escalabilidade. Suponha que, quando um usuário móvel \npassa de uma rede para outra, a nova rede externa anuncie uma rota específica para o usuário móvel e a antiga \nrede externa retire sua rota. Considere como informações de roteamento se propagam em um algoritmo vetor \nde distâncias (em particular para o caso de roteamento interdomínios entre redes que abrangem o globo \nterrestre).\nRedes sem fio e redes móveis  431 \na.\t Outros roteadores conseguirão rotear datagramas imediatamente para a nova rede externa tão logo essa \nrede comece a anunciar sua rota?\nb.\t É possível que roteadores diferentes acreditem que redes externas diferentes contenham o usuário móvel?\nc.\t Discuta a escala temporal segundo a qual outros roteadores na rede finalmente aprenderão o caminho até \nos usuários móveis.\n\t\nP12.\t Suponha que o correspondente na Figura 6.22 fosse móvel. Faça um desenho esquemático da infraestrutura \nadicional de camada de rede que seria necessária para rotear o datagrama do usuário móvel original até o \ncorrespondente (que agora é móvel). Mostre a estrutura do(s) datagrama(s) entre o usuário móvel original e \no correspondente (agora móvel), como na Figura 6.23.\n\t\nP13.\t Em IP móvel, que efeito terá a mobilidade sobre atrasos fim a fim de datagramas entre a fonte e o destino?\n\t\nP14.\t Considere o exemplo de encadeamento discutido no final da Seção 6.7.2. Suponha que um usuário móvel \nvisite as redes externas A, B e C, e que um correspondente inicie uma conexão com o usuário móvel enquanto \neste reside na rede externa A. Relacione a sequência de mensagens entre agentes externos e entre agentes \nexternos e o agente nativo, enquanto o usuário passa da rede A para a rede B e para a rede C. Em seguida, \nsuponha que não é executado encadeamento e que as mudanças no endereço administrado do usuário móvel \ndevem ser notificadas explicitamente ao correspondente (bem como ao agente nativo). Relacione a sequência \nde mensagens que seria necessário trocar nesse segundo cenário.\n\t\nP15.\t Considere dois nós móveis em uma rede externa que tem um agente externo. É possível que esses nós utilizem \no mesmo endereço aos cuidados em IP móvel? Explique sua resposta.\n\t\nP16.\t Quando discutimos como o VLR atualizava o HLR com informações sobre a localização corrente de usuários \nmóveis, quais eram as vantagens e as desvantagens de fornecer ao HLR o MSRN em vez do endereço do VLR?\nWireshark Lab\nNo site de apoio do livro você encontrará um Wireshark Lab, em inglês, para este capítulo, que captura e \nestuda os quadros 802.11 trocados entre um notebook sem fio e um ponto de acesso.\nDeborah Estrin\nDeborah Estrin é professora de ciência da computação na UCLA, Jon Postel Chair in \nComputer Networks, diretora do Center for Embedded Networked Sensing (CENS), e co-\nfundadora da organização sem fins lucrativos openmhealth.org. Ela recebeu doutorado \n(1985) em ciência da computação pelo MIT e bacharelado (1980) pela Universidade da \nCalifórnia em Berkeley. O estudo inicial de Estrin foi voltado para o projeto de protocolos \nde rede, incluindo roteamento de transmissão para um grupo (multicast) e interdomínio. Em \n2002, fundou o NSF-funded Science and Technology Center, CENS (http://cens.ucla.edu), \npara desenvolver e explorar tecnologias e aplicações de monitoração ambiental. Hoje, Es-\ntrin e seus colaboradores estão desenvolvendo sistemas de sensores participativos, aproveitando a programabilida-\nde, proximidade e difusão dos telefones móveis; os principais contextos de implementação são saúde móvel (http://\nopenmhealth.org), coleta de dados da comunidade e educação STEM (http://mobilizingcs.org). A professora Estrin \né membro eleito da Ame­\nrican Academy of Arts and Sciences (2007) e da National Academy of Engineering (2009). \nÉ fellow do IEEE, ACM e AAAS. Foi selecionada como a primeira ACM-W Athena Lecturer (2006), recebeu o prêmio \nWomen of Vision Award for Innovation (2007) pelo Anita Borg Institute, entrou para o hall da fama do WITI (2008) e \nrecebeu o prêmio Doctor Honoris Causa do EPFL (2008) e da Uppsala University (2011). \nENTREVISTA\n   Redes de computadores e a Internet\n432\nPor favor, descreva alguns dos projetos mais \ninteressantes em que trabalhou durante sua \ncarreira. Quais foram os maiores desafios?\nEm meados da década de 1990 na USC e ISI, tive a \nsorte de trabalhar com pessoas como Steve Deering, \nMark Handley e Van Jacobson no projeto de protocolos \nde roteamento para transmissão para grupos (em par-\nticular, PIM). Tentei fazer muitas das lições de projeto \narquitetônico desde a transmissão para um grupo no \nprojeto de vetores de monitoramento ecológico, onde, \npela primeira vez, comecei a levar a sério as aplicações \ne a pesquisa multidisciplinar. Esse interesse em reunir \na inovação no espaço social e tecnológico é o que me \ninteressa mais sobre minha última área de pesquisa, a \nsaúde móvel. Os desafios nesses projetos foram tão di-\nversificados quanto os domínios de problema, mas o \nque todos eles tinham em comum foi a necessidade de \nmanter os olhos abertos quanto a se tínhamos a defini-\nção correta do problema enquanto íamos e vínhamos \nentre projeto e desenvolvimento, protótipo e piloto. \nNenhum desses foram problemas que poderiam ser so-\nlucionados analiticamente, com simulação ou mesmo \nnas experiências construídas em laboratório. Tudo isso \ndesafiou nossa capacidade de reter arquiteturas limpas \nna presença de problemas e contextos confusos, e exi-\ngiu uma extensa colaboração.\nQue mudanças e inovações você prevê que \naconteçam nas redes sem fio e na mobilidade no \nfuturo?\nNunca coloquei muita fé na previsão do ­\nfuturo, mas \ndiria que poderemos ver o fim dos telefones comuns \n(isto é, os que não são programáveis e são usados ape-\nnas para voz e mensagens de texto) à medida que smar-\ntphones se tornam mais e mais poderosos e são o pon-\nto principal de acesso à Internet para muitos. Também \nacho que veremos a proliferação continuada de SIMs \nembutidas pelas quais todos os tipos de dispositivos \ntêm a capacidade de se comunicar por meio da rede \ncelular com baixas taxas de dados.\nQue futuro você vê para as redes e a Internet?\nOs esforços em dados nomeados e redes definidas \npor software surgirão para criar uma infraestrutura \nmais controlável, expansível e rica, representando de \nmodo mais geral a mudança do papel da arquitetura \npara mais alto na pilha. Nos primórdios da Internet, a \narquitetura ia até a camada 4, com as aplicações sendo \nmais monolíticas, no topo. Agora, dados e análise do-\nminam o transporte.\nQue pessoas a inspiraram profissionalmente?\nTrês pessoas me vêm à mente. Primeiro, Dave Clark, \no tempero secreto e herói desconhecido da comuni-\ndade da Internet. Tive a sorte de estar por perto nos \nprimeiros dias para vê-lo atuar como o “princípio orga-\nnizador” do IAB e da governança na Internet; o sacer-\ndote do consenso primitivo e do código em execução. \nSegundo, Scott Shenker, por seu brilhantismo inte-\nlectual, integridade e persistência. Procuro obter (mas \nraramente consigo) sua clareza na definição de proble-\nmas e soluções. Ele sempre é a primeira pessoa a quem \nenvio e-mail pedindo conselho sobre coisas grandes e \npequenas. Terceiro, minha irmã Judy Estrin, que teve \na criatividade e coragem de passar toda a sua carreira \nlevando ideias e conceitos ao mercado. Sem as Judys do \nmundo, as tecnologias da Internet nunca teriam trans-\nformado nossas vidas.\nQuais são suas recomendações para alunos que \ndesejam seguir carreira em ciência da computação \ne redes?\nPrimeiro, crie um alicerce forte em seu trabalho aca-\ndêmico, equilibrado com toda e qualquer experiência \nde trabalho do mundo real que possa conseguir. Ao \nprocurar um ambiente de trabalho, busque oportuni-\ndades nas áreas de problema com que você realmente \nse importa e com pessoas com quem possa aprender.\nPessoas em todas as partes do mundo estão usando a Internet para assistir vídeos e shows de televisão por \ndemanda. Empresas de distribuição de filmes e televisão, como Netflix e Hulu na América do Norte e Youku e \nKankan na China praticamente se tornaram nomes corriqueiros. Mas as pessoas não estão apenas assistindo ví-\ndeos pela Internet, estão usando sites como YouTube para fazer upload e distribuir seu conteúdo gerado pelo pró-\nprio usuário, tornando-se produtores de vídeo e também consumidores. Além do mais, aplicações de rede como \nSkype, Google Talk e QQ (tremendamente populares na China) permitem que os usuários não apenas façam \n“ligações telefônicas” pela Internet, mas também as melhorem com vídeo e conferência entre várias pessoas. De \nfato, podemos com segurança prever que, ao final desta década, quase toda a distribuição de vídeo e interações \nde voz será feita de ponta a ponta pela Internet, muitas vezes para dispositivos sem fio conectados à Internet por \nmeio de redes de acesso 4G e Wi-Fi.\nComeçaremos este capítulo com uma taxonomia das aplicações multimídia na Seção 7.1. Veremos que uma \naplicação de multimídia pode ser classificada como fluxo de áudio/vídeo armazenado, voz interativa/voz-sobre-IP \ne fluxo áudio/vídeo ao vivo. Veremos que cada uma dessas classes de aplicações tem seus próprios requisitos de \nserviço exclusivos, que diferem significativamente daqueles das aplicações tradicionais, como e-mail, navegação \nWeb e login remoto. Na Seção 7.2 examinaremos o fluxo de vídeo com mais atenção. Exploraremos muitos dos \nprincípios básicos por trás do fluxo de vídeo, incluindo buffer do cliente, pré-busca e adaptação da qualidade do \nvídeo à largura de banda disponível. Também investigaremos as redes de distribuição de conteúdo (CDNs), que \nsão bastante usadas hoje pelos principais sistemas de fluxo de vídeo. Depois examinamos os sistemas YouTube, \nNetflix e Kankan como estudos de caso para o fluxo de vídeo. Na Seção 7.3, analisaremos a voz e o vídeo colo-\nquiais, que, diferentemente das aplicações elásticas, são altamente sensíveis ao atraso de fim a fim, mas podem \ntolerar uma perda de dados ocasional. Aqui, examinaremos como técnicas do tipo reprodução adaptativa, corre-\nção de erro de repasse e ocultação de erro podem mitigar perda de pacotes e atrasos induzidos pela rede. Também \nexaminaremos o Skype como um estudo de caso. Na Seção 7.4, estudaremos RTP e SIP, dois protocolos populares \npara aplicações de voz e vídeo interativas em tempo real. Na Seção 7.5, investigaremos os mecanismos dentro da \nrede que podem ser usados para diferenciar uma classificação de tráfego (por exemplo, aplicações que toleram \natrasos, como a voz interativa) de outras (por exemplo, aplicações elásticas, como a navegação de páginas Web), \ne fornecer um serviço diferenciado entre várias classificações de tráfego.\nRedes\nmultimídia\n1\n3\n6\n8 9\nc\n \na\n \np\n \ní\n \nt\n \nu\n \nl\n \no\n2\n4 5 7\n   Redes de computadores e a Internet\n434\n7.1  Aplicações de rede multimídia\nDefinimos uma aplicação de rede multimídia como qualquer aplicação de rede que empregue áudio ou \nvídeo. Nesta seção, oferecemos uma taxonomia das aplicações multimídia. Veremos que cada classe de aplicações \ntem seu próprio conjunto exclusivo de requisitos de serviço e questões de projeto. Porém, antes de nos aprofun-\ndarmos em uma discussão sobre aplicações multimídia da Internet, é útil considerar as características intrínsecas \ndas próprias mídias de áudio e vídeo.\n7.1.1  Propriedades de vídeo\nTalvez a característica mais destacada do vídeo seja sua alta taxa de bits. O vídeo distribuído pela Internet \ncostuma variar de 100 kbits/s para videoconferências de baixa qualidade até mais de 3 Mbits/s para os filmes de \nfluxo de vídeo com alta definição. Para ter uma ideia de como as demandas de largura de banda de vídeo são \ncomparadas com aquelas de outras aplicações da Internet, vamos considerar de modo breve três usuários, cada \num usando uma aplicação diferente na Internet. Nosso primeiro usuário, Frank, está vendo rapidamente as fotos \npostadas nas páginas do Facebook de seus amigos. Suponhamos que Frank esteja vendo uma nova foto a cada \n10 segundos, e que as fotos tenham um tamanho médio de 200 Kbytes. (Como sempre, nesta discussão vamos \nsimplificar e supor que 1 Kbyte = 8.000 bits.) Nosso segundo usuário, Marta, está baixando música da Internet \n(“a nuvem”) para o seu smartphone. Vamos imaginar que Marta esteja escutando muitas canções em MP3, uma \napós a outra, cada uma codificada a uma taxa de 128 kbits/s. O terceiro usuário, Vítor, está assistindo vídeo que foi \ncodificado a 2 Mbits/s. Por fim, vamos supor que o tamanho da sessão para todos os três usuários seja 4.000 se-\ngundos (cerca de 67 minutos). A Tabela 7.1 compara as taxas de bits e o total de bytes transferidos para esses três \nusuários. Vemos que o vídeo de fluxo contínuo consome, de longe, a maior largura de banda, tendo uma taxa de \nbits mais de dez vezes maior que a das aplicações de Facebook e fluxo de música. Portanto, ao projetar aplicações \nde vídeo em rede, a primeira coisa que precisamos ter em mente são os altos requisitos de taxa de bits do vídeo. \nDada a popularidade do vídeo e sua alta taxa de bits, talvez não seja surpresa que a Cisco preveja [Cisco, 2011] \nque o vídeo de fluxo contínuo e armazenado será mais ou menos 90% do tráfego da Internet para o consumidor \nglobal em 2015.\nOutra característica importante do vídeo é que ele pode ser compactado, compensando assim a qualidade \ncom a taxa de bits. Um vídeo é uma sequência de imagens, em geral exibidas a uma velocidade constante, por \nexemplo, 24 ou 30 imagens por segundo. Uma imagem não compactada, codificada digitalmente, consiste em \numa matriz de pixels, com cada pixel codificado em uma série de bits para representar luminosidade e cor. \nExistem dois tipos de redundância no vídeo, e ambos podem ser explorados pela compactação de vídeo. A \nredundância espacial é a que ocorre dentro de determinada imagem. De modo intuitivo, uma imagem que con-\nsiste principalmente em espaço em branco tem alto grau de redundância e pode ser compactada de maneira \neficiente sem sacrificar a qualidade da imagem significativamente. A redundância temporal reflete a repetição \nde uma imagem para a seguinte. Por exemplo, se uma imagem e a seguinte forem idênticas, não há razão para \ncodificar de novo a imagem seguinte; em vez disso, é mais eficiente apenas indicar, durante a codificação, que \nela é exatamente a mesma. Os algoritmos de compactação de hoje, prontos para uso, podem compactar um \nvídeo basicamente para qualquer taxa de bits desejada. É claro que, quanto mais alta a taxa de bits, melhor a \nqualidade da imagem e melhor a experiência de exibição geral do usuário.\nTabela 7.1  Comparação dos requisitos de taxa de bits de três aplicações na Internet\nTaxa de bits\nBytes transferidos em 67 min\nFacebook de Frank\n160 kbits/s\n80 Mbytes\nMúsica de Marta\n128 kbits/s\n64 Mbytes\nVídeo de Vítor\n2 Mbits/s\n1 Gbyte\nredes multimídia  435 \nTambém podemos usar a compactação para criar múltiplas versões do mesmo vídeo, cada uma em um \nnível de qualidade diferente. Por exemplo, podemos usar a compactação para criar, digamos, três versões do mes-\nmo vídeo, nas taxas de 300 kbits/s, 1 Mbit/s e 3 Mbits/s. Os usuários podem decidir qual versão eles querem ver \ncomo uma função da largura de banda disponível. Os usuários com conexões de alta velocidade com a Internet \nescolheriam a de 3 Mbits/s; os que assistem ao vídeo por 3G com um smartphone poderiam escolher a versão de \n300 kbits/s. De modo semelhante, o vídeo em uma aplicação de videoconferência pode ser compactado “no ato” \npara oferecer a melhor qualidade dada a largura de banda de fim a fim disponível entre usuários conversando.\n7.1.2  Propriedades de áudio\nO áudio digital (incluindo a fala e a música digitalizadas) tem requisitos de largura de banda muito menores \ndo que o vídeo. O áudio digital, porém, tem suas propriedades exclusivas, que devem ser consideradas quando se \nprojetam aplicações de redes multimídia. Para entender essas propriedades, vamos primeiro considerar como o \náudio analógico (que os humanos e os instrumentos musicais geram) é convertido para um sinal digital:\n• O sinal analógico de áudio é primeiro amostrado a alguma taxa fixa; por exemplo, 8 mil amostras por \nsegundo. O valor de cada amostra é um número real qualquer.\n• Cada amostra é então arredondada para um valor entre um número finito de valores. Essa operação é \ndenominada quantização. O número de valores finitos — denominados valores de quantização — em \ngeral é uma potência de 2, por exemplo, 256 valores de quantização.\n• Cada um dos valores de quantização é representado por um número fixo de bits. Por exemplo, se houver \n256 valores de quantização, então cada valor — e, portanto, cada amostra de áudio — será representado por \n1 byte. As representações por bits de todas as amostras são, então, concatenadas em conjunto para formar \na representação digitalizada do sinal. Como exemplo, se um sinal de áudio for amostrado a uma taxa de \n8 mil amostras por segundo e cada amostra for quantizada e representada por 8 bits, então o sinal digital \nresultante terá uma taxa de 64 mil bits por segundo. Esse sinal digital pode então ser reconvertido — isto é, \ndecodificado — em um sinal analógico. Contudo, o sinal analógico decodificado é apenas uma aproxima-\nção do sinal de áudio original, e a qualidade do som pode ser nitidamente degradada (por exemplo, sons \nde alta frequência podem estar faltando no sinal decodificado). Aumentando a taxa de amostragem e o nú-\nmero de valores de quantização, o sinal decodificado pode se aproximar melhor do sinal analógico original. \nAssim, há uma clara permuta (como no vídeo) entre a qualidade do sinal decodificado e os requisitos de \narmazenamento e taxa de bits do sinal digital.\nA técnica básica de codificação que acabamos de descrever é denominada modulação por codificação \nde pulso (pulse code modulation — PCM). A codificação de voz frequentemente usa PCM, com uma taxa de \namostragem de 8 mil amostras por segundo e 8 bits por amostra, o que resulta uma taxa de 64 kbits/s. O disco \ncompacto de áudio (CD) também usa PCM, com taxa de amostragem de 44.100 amostras por segundo e 16 bits \npor amostra; isso dá uma taxa de 705,6 kbits/s para mono e 1,411 Mbits/s para estéreo.\nEntretanto, a voz e a música codificadas com PCM raramente são usadas na Internet. Em vez disso, como no \nvídeo, são usadas técnicas de compactação para reduzir as taxas de bits do fluxo. A voz humana pode ser compactada \npara menos de 10 kbits/s e ainda ser inteligível. Uma técnica de compactação popular para a música estéreo com \nqualidade quase de CD é MPEG 1 layer 3, mais conhecida como MP3. Codificadores MP3 podem compactar para \nmuitas taxas diferentes; 128 kbits/s é a taxa de codificação mais comum, produzindo muito pouca degradação de \nsom. Um padrão relacionado é o Advanced Audio Coding (AAC), que foi popularizado pela Apple. Assim como o \nvídeo, diversas versões de um fluxo de áudio pré-gravado podem ser criadas, cada uma em uma taxa de bits diferente.\nEmbora as taxas de bit de áudio sejam em geral muito menores do que as de vídeo, os usuários costumam \nser muito mais sensíveis a pequenas falha de áudio do que de vídeo. Considere, por exemplo, uma conferência de \nvídeo ocorrendo pela Internet. Se, de vez em quando, o sinal de vídeo for perdido por alguns segundos, a confe-\nrência de vídeo provavelmente poderá prosseguir sem muita frustração do usuário. Porém, se o sinal de áudio for \nperdido com frequência, os usuários poderão ter que terminar a sessão.\n   Redes de computadores e a Internet\n436\n7.1.3  Tipos de aplicações de redes multimídia\nA Internet pode comportar uma grande variedade de aplicações de multimídia úteis e divertidas. Nesta sub-\nseção, classificaremos as aplicações de multimídia em três categorias abrangentes: (i) áudio e vídeo de fluxo contínuo \narmazenados, (ii) voz e vídeo-sobre-IP interativos e (iii) áudio e vídeo de fluxo contínuo ao vivo. Como veremos em bre-\nve, cada uma dessas categorias de aplicação tem seu próprio conjunto de requisitos de serviço e questões de projeto.\nÁudio e vídeo de fluxo contínuo armazenados\nPara concretizar nossa discussão, focalizamos aqui o vídeo de fluxo contínuo armazenado, que em geral com-\nbina componentes de vídeo e áudio. O áudio de fluxo contínuo armazenado (como a música em fluxo contínuo) é \nmuito semelhante ao vídeo de fluxo contínuo armazenado, embora as taxas de bits costumem ser muito menores.\nNesta classe de aplicações, o meio subjacente é o vídeo pré-gravado, como um filme, um show de TV, um \nevento esportivo pré-gravado ou um vídeo pré-gravado gerado pelo usuário (como aqueles vistos no YouTube). \nEsses vídeos são colocados em servidores e os usuários enviam solicitações aos servidores para verem os vídeos \npor demanda. Muitas empresas de Internet oferecem hoje vídeo de fluxo contínuo, incluindo YouTube (Google), \nNetflix e Hulu. Por algumas estimativas, o vídeo de fluxo contínuo armazenado contribui com mais de 50% do \ntráfego descendente nas redes de acesso à Internet atualmente [Cisco, 2011]. O vídeo de fluxo contínuo armaze-\nnado tem três características distintas importantes:\n• Fluxo contínuo. Em uma aplicação de vídeo de fluxo contínuo armazenado, normalmente o cliente inicia \na reprodução alguns segundos após começar a receber o vídeo do servidor. Isso significa que o cliente \nreproduz de uma parte do vídeo ao mesmo tempo em que recebe do servidor partes do arquivo que estão \nmais à frente. Essa técnica, conhecida como fluxo contínuo (streaming), evita ter de descarregar o arqui-\nvo inteiro (e incorrer em atraso potencialmente longo) antes de começar a reproduzi-lo.\n• Interatividade. Como a mídia é pré-gravada, o usuário pode interromper, reposicionar para a frente, \nreposicionar para trás, avançar rapidamente, e assim por diante, pelo conteúdo do vídeo. O tempo desde \nquando o usuário faz essa solicitação até que a ação se manifeste no cliente deverá ser menor que alguns \nsegundos para que haja reação aceitável.\n• Reprodução contínua. Assim que se inicia a reprodução do vídeo, ela deve prosseguir de acordo com a \ntemporização original da gravação. Portanto, os dados devem ser recebidos do servidor a tempo de ser \nreproduzidos no cliente; caso contrário, os usuários podem receber quadros de vídeo congelando (quan-\ndo o cliente espera pelos quadros atrasados) ou saltando (quando o cliente pula os quadros atrasados).\nDe longe, a medida de desempenho mais importante para o vídeo de fluxo contínuo é a vazão média. Para \noferecer reprodução contínua, a rede precisa oferecer uma vazão média à aplicação de fluxo contínuo que seja \npelo menos tão grande quanto a taxa de bits do próprio vídeo. Conforme veremos na Seção 7.2, com o uso de \nbuffers e pré-busca, é possível oferecer reprodução contínua mesmo quando a vazão flutua, desde que a vazão \nmédia (medida durante 5 a 10 segundos) permaneça acima da taxa do vídeo [Wang, 2008].\nPara muitas aplicações de vídeo de fluxo contínuo, o vídeo pré-gravado é armazenado e enviado por CDN, \nem vez de um data center único. Há também muitas aplicações de vídeo de fluxo contínuo P2P, para as quais o \nvídeo é armazenado nos hospedeiros (pares) dos usuários, com diferentes pedaços do vídeo chegando de diversos \npares, que podem estar espalhados por todo o globo. Dada a proeminência do vídeo de fluxo contínuo na Inter-\nnet, exploraremos esse tema com mais detalhes na Seção 7.2, prestando atenção particular ao buffer do cliente, \npré-busca, adaptação da qualidade à disponibilidade de largura de banda e distribuição CDN.\nVoz e vídeo sobre IP interativos\nA voz interativa em tempo real pela Internet é chamada de telefonia da Internet, visto que, do ponto de \nvista do usuário, é semelhante ao serviço telefônico tradicional, por comutação de circuitos. Isso em geral é cha-\nredes multimídia  437 \nmado de Voz-sobre-IP (VoIP — Voice-Over-IP). O vídeo interativo é semelhante, exceto que inclui o vídeo dos \nparticipantes, bem como suas vozes. A maioria dos sistemas interativos por voz e vídeo permite que os usuários \ncriem conferências com três ou mais participantes. Voz e vídeo interativos são muito usados na Internet hoje, \ncom as empresas Skype, QQ e Google Talk atraindo centenas de milhões de usuários diários.\nEm nossa discussão sobre requisitos de serviço de aplicação no Capítulo 2 (Figura 2.4), identificamos algu-\nmas linhas pelas quais esses requisitos de aplicação podem ser classificados. Duas delas— considerações quanto \nà temporização e à tolerância à perda de dados — são de particular importância para aplicações interativas de \nvoz e vídeo. Considerações de temporização são importantes porque muitas aplicações interativas de voz e vídeo \nsão altamente sensíveis a atraso. Para uma interação com dois ou mais interlocutores, o atraso desde quando um \nusuário fala ou se move até que a ação seja manifestada na outra ponta deverá ser menor do que algumas centenas \nde milissegundos. Para a voz, atrasos menores que 150 ms não são percebidos por um ouvinte humano, atrasos \nentre 150 e 400 ms podem ser aceitáveis, e atrasos que ultrapassam 400 ms podem resultar em conversas de voz \nfrustrantes, ou totalmente ininteligíveis.\nPor outro lado, aplicações de multimídia interativas são tolerantes à perda — perdas ocasionais causam \napenas pequenas perturbações na recepção de áudio e vídeo, e podem ser parcial ou totalmente encobertas. Essas \ncaracterísticas de sensibilidade a atraso e de tolerância à perda são claramente diferentes daquelas das aplicações \nelásticas, como a navegação Web, correio eletrônico, redes sociais e login remoto. Para aplicações elásticas, atra-\nsos longos são incômodos, mas não particularmente prejudiciais; porém, a completude e a integridade dos dados \ntransferidos são de suma importância. Exploraremos a voz e o vídeo interativos com mais detalhes na Seção 7.3, \nprestando especial atenção ao modo como a reprodução adaptativa, a correção de erro de repasse e a ocultação \nde erros podem aliviar a perda de pacotes e o atraso induzidos pela rede.\nÁudio e vídeo de fluxo contínuo ao vivo\nEsta terceira classe de aplicação é semelhante à transmissão tradicional de rádio e televisão, exceto que \né realizada pela Internet. Essas aplicações permitem que um usuário receba uma transmissão de rádio ou \ntelevisão ao vivo de qualquer parte do mundo (por exemplo, um evento esportivo ao vivo ou um evento de \nnotícias contínuas). Milhares de estações de rádio e televisão do mundo inteiro estão transmitindo conteúdo \npela Internet.\nAplicações ao vivo, do tipo de difusão, normalmente possuem muitos usuários que recebem o mesmo pro-\ngrama de áudio/vídeo ao mesmo tempo. Embora a distribuição de áudio/vídeo ao vivo para muitos receptores \npossa ser realizada com eficiência utilizando as técnicas de transmissão IP para um grupo, conforme estuda-\nmos na Seção 4.7, a distribuição para um grupo é mais comumente feita por meio de aplicações em camadas \n(usando redes P2P ou CDN), ou de múltiplos fluxos individuais separados. Como acontece com a multimídia \narmazenada de fluxo contínuo, a rede precisa oferecer a cada fluxo de multimídia ao vivo uma vazão média que \nseja maior que a taxa de consumo de vídeo. Como o evento é ao vivo, o atraso também pode ser um problema, \nembora as limitações de temporização sejam menos severas do que para aplicações interativas de voz. Atrasos \nde até dez segundos ou mais desde o instante em que o usuário requisita a entrega/reprodução de uma trans-\nmissão ao vivo até o início da reprodução podem ser tolerados. Não estudaremos a mídia de fluxo contínuo \nao vivo neste livro, pois muitas das técnicas usadas para a mídia de fluxo contínuo ao vivo — atraso de buffer \ninicial, uso de largura de banda adaptativa e distribuição CDN — são semelhantes àquelas para a mídia de fluxo \ncontínuo armazenado.\n7.2  Vídeo de fluxo contínuo armazenado\nPara aplicações de vídeo de fluxo contínuo, os vídeos pré-gravados são armazenados em servidores aos \nquais os usuários enviam solicitações para verem os vídeos por demanda. O usuário pode assistir o vídeo do \ninício ao fim, sem interrupção, pode parar de assistir o vídeo antes que ele termine ou interagir com o vídeo \n   Redes de computadores e a Internet\n438\ninterrompendo ou reposicionando para uma cena futura ou passada. Os sistemas de vídeo de fluxo contínuo \npodem ser classificados em três categorias: UDP de fluxo contínuo, HTTP de fluxo contínuo e HTTP de fluxo \ncontínuo adaptativo. Embora todos os três tipos de sistemas sejam usados na prática, a maioria dos sistemas de \nhoje emprega o HTTP de fluxo contínuo e o HTTP de fluxo contínuo adaptativo.\nUma característica comum de todas as três formas de vídeo de fluxo contínuo é o uso extenso de buffer \nde aplicação no lado do cliente para aliviar os efeitos de variar os atrasos de fim a fim e variar as quantidades de \nlargura de banda disponível entre servidor e cliente. Para o vídeo de fluxo contínuo (tanto armazenado quanto ao \nvivo), os usuários podem tolerar um pequeno atraso inicial de alguns segundos entre o momento em que o clien-\nte solicita um vídeo e quando a reprodução inicia no cliente. Por conseguinte, quando o vídeo começa a chegar no \ncliente, um cliente não precisa iniciar a reprodução imediatamente, mas pode acumular alguma reserva de vídeo \nem um buffer de aplicação. Quando o cliente tiver acumulado uma reserva de alguns segundos de vídeo manti-\ndo em buffer, mas não reproduzido, poderá iniciar a reprodução do vídeo. Existem duas vantagens importantes \nfornecidas por tal buffer de cliente. Primeiro, o buffer no lado do cliente pode absorver variações no atraso entre \nservidor e cliente. Se um trecho de vídeo se atrasar, desde que ele chegue antes que a reserva de vídeo recebido \nmas não reproduzido se esgote, esse atraso longo não será observado. Segundo, se a largura de banda do servidor \nao cliente cair depressa para menos do que a taxa de consumo de vídeo, um usuário pode continuar a aproveitar \na reprodução contínua, de novo desde que o buffer da aplicação cliente não seja completamente esgotado.\nA Figura 7.1 ilustra o buffer no lado do cliente. Neste exemplo simples, suponha que o vídeo seja codifica-\ndo a uma taxa de bits fixa, e assim cada bloco de vídeo contenha quadros de vídeo que devem ser reproduzidos, \nmas sobre a mesma quantidade de tempo, D. Um servidor transmite o primeiro bloco de vídeo em t0, o segundo \nbloco em t0 + D, o terceiro bloco em t0 + 2D, e assim por diante. Quando o cliente inicia a reprodução, cada bloco \ndeve ser reproduzido D unidades de tempo após o bloco anterior, a fim de reproduzir a temporização do vídeo \ngravado original. Por causa dos atrasos variáveis da rede de fim a fim, diferentes blocos de vídeo experimentam \ndiferentes atrasos. O primeiro bloco de vídeo chega ao cliente em t1 e o segundo bloco chega em t2. O atraso da \nrede para o i-ésimo bloco é a distância horizontal entre o momento em que o bloco foi transmitido pelo servidor e \no momento em que ele é recebido no cliente; observe que o atraso da rede varia de um bloco de vídeo para outro. \nNeste exemplo, se o cliente tivesse de começar a reprodução assim que o primeiro bloco chegasse em t1, então o \nsegundo bloco não teria chegado a tempo para ser reproduzido em t1 + D. Nesse caso, a reprodução teria que ser \nadiada (esperando até que o bloco 1 chegasse) ou o bloco 1 poderia ser pulado — ambos resultando em prejuízos \nindesejáveis na reprodução. Em vez disso, se o cliente tivesse de adiar o início da reprodução até t3, quando todos \nos blocos de 1 a 6 tivessem chegado, a reprodução periódica poderia prosseguir com todos os blocos tendo sido \nrecebidos antes do seu tempo de reprodução.\nFigura 7.1  Atraso de reprodução do cliente no vídeo de fluxo contínuo\nKR 07.01.eps\nAW/Kurose and Ross\nComp ter Net orking 6/e\nAtraso \nde rede \nvariável\nAtraso na \nreprodução \ndo cliente\nTransmissão de \nvídeo com taxa \nde bits constante \npelo servidor\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nReprodução \nde vídeo com \ntaxa de bits \nconstante pelo cliente\nTempo\nNúmero do bloco de vídeo\nt0\nt1\nt2\nt3\nt0+2Δ\nt0+Δ\nt1+Δ\nt3+∆\nRecepção \nde vídeo \nno cliente\nredes multimídia  439 \n7.2.1  UDP de fluxo contínuo\nAté aqui, só falamos rapidamente sobre o UDP de fluxo contínuo, levando o leitor a procurar discussões \nmais profundas dos protocolos por trás desses sistemas, quando for o caso. Com o UDP de fluxo contínuo, o \nservidor transmite vídeo a uma taxa que corresponde à taxa de consumo de vídeo do cliente, com uma tempori-\nzação dos trechos de vídeo sobre UDP a uma taxa constante.­\n Por exemplo, se a taxa de consumo for 2 Mbits/s e \ncada pacote UDP transportar 8.000 bits de vídeo, o servidor transmitiria um pacote UDP em seu socket  a cada \n(8.000 bits)/(2 Mbits/s) = 4 ms. Conforme aprendemos no Capítulo 3, como o UDP não emprega um mecanismo \nde controle de congestionamento, o servidor pode empurrar pacotes na rede na taxa de consumo do vídeo sem \nas restrições de controle de taxa do TCP. O UDP de fluxo contínuo normalmente usa um pequeno buffer no lado \ndo cliente, grande o suficiente para manter menos de um segundo de vídeo.\nAntes de passar os trechos de vídeo ao UDP, o servidor encapsulará os trechos de vídeo dentro de pacotes \nde transporte projetados especialmente para transportar áudio e vídeo, usando o Real-Time Transport Protocol \n(RTP) [RFC 3550] ou um esquema semelhante (possivelmente, patenteado). Deixaremos nossa explicação sobre \nRTP para a Seção 7.3, na qual discutimos o RTP no contexto dos sistemas interativos de voz e vídeo.\nOutra propriedade distinta do UDP de fluxo contínuo é que, além do fluxo de vídeo do servidor ao cliente, \ncliente e servidor também mantêm, em paralelo, uma conexão de controle separada sobre a qual o cliente envia \ncomandos referentes a mudanças de estado de sessão (como pausar, retomar, reposicionar e assim por diante). \nEssa conexão de controle é, de várias maneiras, semelhante à conexão de controle FTP que estudamos no Ca-\npítulo 2. O Real-Time Streaming Protocol (RTSP) [RFC 2326], explicado com detalhes no site deste livro, é um \nprotocolo aberto popular para esse tipo de conexão de controle.\nEmbora o UDP de fluxo contínuo tenha sido empregado em muitos sistemas de fonte aberta e produtos patentea­\ndos, ele possui três desvantagens significativas. Primeiro, pela quantidade imprevisível e variável de largura de banda \ndisponível entre servidor e cliente, o UDP de fluxo contínuo com taxa constante pode deixar de oferecer reprodução \ncontínua. Por exemplo, considere o cenário onde a taxa de consumo de vídeo é de 1 Mbit/s e a largura de banda dis-\nponível do servidor ao cliente normalmente é maior que 1 Mbit/s, mas a cada minuto ou mais a largura de banda \ndisponível cai para menos de 1 Mbit/s por vários segundos. Nesse cenário, um sistema de UDP de fluxo contínuo \nque transmite vídeo a uma taxa constante de 1 Mbit/s por RTP/UDP provavelmente não agradaria ao usuário, com \nquadros congelando ou pulando logo depois que a largura de banda disponível caísse para menos de 1 Mbit/s. A \nsegunda desvantagem é que ele exige um servidor de controle de mídia, como um servidor RTSP, para processar \nsolicitações de interatividade cliente-servidor e acompanhar o estado do cliente (por exemplo, o ponto de repro-\ndução do cliente no vídeo, esteja o vídeo sendo interrompido ou reproduzido, e assim por diante) para cada sessão \ndo cliente em andamento. Isso aumenta o custo geral e a complexidade da implantação de um sistema de vídeo por \ndemanda em grande escala. A terceira desvantagem é que muitos firewalls são configurados para bloquear o tráfego \nUDP, impedindo que os usuários por trás desses firewalls recebam o vídeo UDP.\n7.2.2  HTTP de fluxo contínuo\nNo HTTP de fluxo contínuo, o vídeo é apenas armazenado em um servidor HTTP como um arquivo co-\nmum com uma URL específica. Quando um usuário quer assistir a este vídeo, ele estabelece uma conexão TCP \ncom o servidor e realiza um comando HTTP GET para aquele URL. O servidor envia então o arquivo de vídeo, \ndentro de uma mensagem de resposta HTTP, o mais rápido possível, isto é, tão depressa quanto o controle de \ncongestionamento TCP e o controle de fluxo permitirem. No cliente, os bytes são armazenados em um buffer \nde aplicação cliente. Uma vez que o número de bytes neste buffer exceder um limite predeterminado, a aplicação \ncliente inicia uma reprodução — mais especificamente, ela captura quadros do vídeo do buffer da aplicação clien-\nte, descompacta os quadros e os apresenta na tela do usuário.\nAprendemos no Capítulo 3 que, quando transferimos um arquivo usando TCP, a taxa de transmissão do \nservidor para o cliente pode variar significativamente por causa do mecanismo de controle de congestionamen-\n   Redes de computadores e a Internet\n440\nto do TCP. Mais especificamente, não é raro para a taxa de transmissão variar no formato “dente de serra” (por \nexemplo, Figura 3.53) associado com o controle de congestionamento do TCP. Além disso, os pacotes podem \ntambém sofrer atraso significativo, pelo mecanismo de retransmissão do TCP. Por causa destas características do \nTCP, o consenso geral era que fluxo contínuo de vídeo nunca funcionaria bem sobre TCP. Ao longo do tempo, \nporém, projetistas de sistemas de fluxo contínuo de vídeo aprenderam que o controle de congestionamento do \nTCP e os mecanismos de transferência de dados confiáveis não necessariamente impedem a emissão quando o \nbuffer do cliente e pré-busca (discutido na próxima seção) são usados.\nO uso do HTTP sobre TCP também permite ao vídeo atravessar firewalls e NATs mais facilmente (os quais \nsão em geral configurados para bloquear a maior parte do tráfego UDP, mas permitir o tráfego HTTP). Vídeos de \nfluxo contínuo sobre HTTP também deixam clara a necessidade de um servidor de controle de mídia, tal como \num servidor RTSP, de modo a reduzir o custo de um desenvolvimento em larga escala pela Internet. Por todas \nessas vantagens, a maior parte dos aplicativos de fluxo contínuo de vídeo — incluindo YouTube e Netflix — usam \nHTTP de fluxo contínuo (sobre TCP) como seu protocolo de fluxo contínuo subjacente.\nPré-busca de vídeo\nComo acabamos de aprender, o uso de buffers no lado do cliente pode ser usado para mitigar os efeitos \nde vários atrasos fim a fim e variadas larguras de banda disponíveis. Em nosso último exemplo na Figura 7.1, o \nservidor transmite vídeo a uma taxa na qual deverá ser exibido. Porém, para vídeos de fluxo contínuo armazena-\ndo, o cliente pode tentar baixar o vídeo em uma taxa maior que a de consumo, e assim fazer uma pré-busca dos \nquadros desse vídeo que serão exibidos no futuro. Esse vídeo previamente baixado é armazenado no buffer de \naplicação do cliente. A pré-busca ocorre naturalmente com o TCP de fluxo contínuo, uma vez que o mecanismo \nque impede o congestionamento tentará usar toda a largura de banda disponível entre o cliente e o servidor.\nPara entendermos melhor a pré-busca, vamos observar um exemplo simples. Suponha que a taxa de consumo \ndo vídeo seja 1 Mbit/s mas a rede é capaz de transmitir o vídeo do servidor para o cliente em uma taxa constante de \n1,5 Mbits/s. Então o cliente será capaz, não só de exibir o vídeo com um atraso de reprodução muito pequeno, como \ntambém conseguirá aumentar a quantidade de dados do vídeo armazenado no buffer a uma taxa de 500 Kbits a \ncada segundo. Desta maneira, se no futuro o cliente receber dados a uma taxa menor que 1 Mbit/s, por um pequeno \nperíodo de tempo, ao cliente será possível continuar a exibir sem interrupções graças à reserva em seu buffer. Wang \n[2008] nos mostra que, quando a taxa de transferência disponível do TCP for aproximadamente o dobro da taxa \nde bits da mídia, o fluxo contínuo sobre TCP resultará em uma mínima inanição e baixos atrasos no uso do buffer.\nBuffer de aplicação do cliente e buffers TCP\nA Figura 7.2 ilustra a interação entre o cliente e o servidor para HTTP de fluxo contínuo. No lado do ser-\nvidor, a parte do arquivo de vídeo em branco já foi enviada dentro do socket do servidor, enquanto a parte mais \nescura é o que falta ser enviado. Depois de “passar pela porta do socket”\n, os bytes são colocados no buffer de envio \ndo TCP antes de serem transmitidos na Internet, como descrito no Capítulo 3. Na Figura 7.2, como o buffer de \nenvio do TCP é apresentado como cheio, o servidor momentaneamente é impedido de enviar mais bytes do \narquivo de vídeo para o socket. No lado do cliente, a aplicação cliente (o tocador de mídia) lê os bytes que estão \nno buffer de recepção do TCP (através do socket do cliente) e coloca os bytes no buffer da aplicação cliente. Ao \nmesmo tempo, a aplicação cliente periodicamente retém quadros de vídeo do buffer da aplicação cliente, des-\ncompacta os quadros e os apresenta na tela do usuário. Note que, se o buffer da aplicação cliente for maior que o \narquivo do vídeo, então o processo completo de movimentação dos bytes do armazenamento no servidor para \no buffer da aplicação cliente é equivalente a baixar um arquivo comum usando HTTP — o cliente apenas obtém o \nvídeo do servidor tão rápido quanto o TCP permitir!\nConsidere agora o que acontece quando o usuário pausa o vídeo durante o processo de fluxo contínuo. No pe-\nríodo de pausa, bits não são removidos do buffer da aplicação cliente, embora continuem a entrar no buffer, vindos \nredes multimídia  441 \ndo servidor. Se o buffer da aplicação cliente for finito, ele pode eventualmente ficar cheio, o que causaria “pressão \ncontrária” por todo o caminho de volta até o servidor. Mais especificamente, uma vez que o buffer da aplicação clien-\nte fique cheio, bytes não poderão mais ser removidos do buffer de recepção do TCP no cliente, então ele também \nficará cheio. Desde que o buffer de recepção do TCP no cliente se encha, bytes não poderão mais ser removidos do \nbuffer de envio do TCP no cliente, então ele também ficará cheio. E uma vez que o buffer de envio do TCP no cliente \nfique cheio, o servidor não poderá mais enviar byte algum para o socket. Sendo assim, se o usuário pausar o vídeo, o \nservidor poderá ser forçado a parar de transmitir, e neste caso será bloqueado até que o usuário volte a exibir o vídeo.\nDe fato, mesmo durante uma reprodução regular (isto é, sem pausa), se o buffer da aplicação cliente ficar \ncheio, uma pressão contrária fará os buffers do TCP ficarem cheios, o que forçará o servidor a reduzir sua taxa de \ntransmissão. Para determinar a taxa resultante, note que, quando a aplicação cliente remove f bits, ela cria espaço \npara f bits no buffer da aplicação cliente, que por sua vez permitirá ao servidor enviar f bits adicionais. Sendo \nassim, a taxa de envio do servidor não pode ser maior que a taxa de consumo do vídeo no cliente. Portanto, um \nbuffer da aplicação cliente cheio indiretamente impõe um limite sobre a taxa que o vídeo poderá ser enviado do \nservidor para o cliente quando estiver usando fluxo contínuo sobre HTTP.\nAnálise do vídeo de fluxo contínuo\nAlguns modelos simples oferecerão um melhor entendimento sobre o atraso inicial na reprodução e o con-\ngelamento devido ao esgotamento do buffer da aplicação. Como mostrado na Figura 7.3, B representa o tamanho \n(em bits) do buffer da aplicação cliente, e Q indica o número de bits que têm de ser armazenados no buffer antes \nque a aplicação cliente comece a exibir o vídeo. (Q < B, é claro.) A letra r representa a taxa de consumo do vídeo — a \ntaxa na qual o cliente retira bits do buffer da aplicação cliente durante a reprodução. Então, por exemplo, se a taxa \nde quadros do vídeo é 30 quadros/segundo, e cada quadro (compactado) tem 100 mil bits, então r = 3 Mbits/s. Para \nver a floresta por entre as árvores, iremos ignorar os buffers do TCP de recepção e de envio.\nVamos supor que o servidor envia bits a uma taxa constante x sempre que o buffer do cliente não estiver cheio. \n(Esta é uma simplificação grosseira, pois a taxa de envio do TCP varia por causa do controle de congestionamento; \nvamos examinar de forma mais realista as taxas como função do tempo x(t) nos problemas do final deste capítulo.) \nSuponha que, no tempo t = 0, o buffer da aplicação está vazio e o vídeo começa a chegar no buffer da aplicação clien-\nte. Agora perguntamos: em que tempo t = tp o vídeo começará a ser exibido? E durante a reprodução, em que tempo \nt = tf o buffer da aplicação cliente ficará cheio?\nPrimeiro, vamos determinar tp, o tempo que leva para que Q bits tenham entrado no buffer da aplicação \ncliente e a reprodução comece. Lembre que os bits chegam ao buffer da aplicação cliente a uma taxa x e nenhum \nFigura 7.2  Vídeo de fluxo contínuo armazenado com HTTP/TCP\nKR 07.02.eps\nKurose and Ross\nComputer Networking 6/e\n34p0 Wide x 14p7 Deep\n11/18/11, 11/23/11, 11/29/11 rossi\nArquivo de vídeo\nServidor Web\nCliente\nBuffer \nde envio \ndo TCP\nBuffer de \nrecepção \ndo TCP\nBuffer da \naplicação TCP\nQuadros lidos \nperiodicamente \ndo buffer, \ndescompactados \ne exibidos na tela\n   Redes de computadores e a Internet\n442\nbit é removido antes de a reprodução começar. Então, a quantidade de tempo necessária para acumular Q bits (o \natraso inicial do buffer) é tp = Q/x.\nAgora vamos determinar tf , o momento no tempo quando o buffer da aplicação cliente fica cheio. Primei-\nro devemos observar que, se x < r (ou seja, se a taxa de envio do servidor for menor que a taxa de consumo do \nvídeo), então o buffer do cliente nunca ficará cheio! De fato, iniciando no tempo tp, o buffer estará esgotado em \numa taxa r e estará preenchido em uma taxa x < r. Eventualmente o buffer do cliente se esvaziará completamente, \nno momento em que o vídeo congelar na tela enquanto o buffer do cliente espera outros tp segundos para acu-\nmular Q bits de vídeo. Então, quando a taxa disponível na rede for menor que a taxa do vídeo, a reprodução irá \nalternar entre períodos de reprodução contínua e períodos de congelamento. Em um dever de casa, você será \nsolicitado a determinar o tamanho de cada período de reprodução contínua e de congelamento, como função \nde Q, r e x. Agora vamos determinar tf , quando x > r. Neste caso, começando no tempo tp, o buffer aumenta de \n \nQ até B a uma taxa x – r desde que os bits sejam retirados a uma taxa r mas cheguem a uma taxa x, como mos-\ntrado na Figura 7.3. Com as dicas apresentadas, será solicitado, em um problema de dever de casa, determinar \ntf , o tempo que o buffer do cliente leva para ficar cheio. Observe que, quando a taxa disponível na rede for maior \nque a taxa do vídeo, após o atraso inicial do buffer, o usuário irá desfrutar de uma reprodução contínua até que \na reprodução do vídeo termine.\nTérmino antecipado e reposicionamento do vídeo\nSistemas de HTTP de fluxo contínuo fazem uso com frequência do cabeçalho do intervalo de bytes HTTP \nna mensagem da requisição do HTTP GET, o qual determina a faixa específica de bytes que o cliente no momento \nquer buscar do vídeo desejado. Isto é útil em particular quando o usuário quer reposicionar (ou seja, saltar) para \num ponto adiante no vídeo. Quando o usuário reposiciona para uma nova posição, o cliente envia uma nova \nrequisição HTTP, indicando no cabeçalho do intervalo de bytes a partir de qual byte no arquivo o servidor deve \nenviar dados. Quando o servidor recebe uma nova requisição HTTP, pode esquecer qualquer requisição anterior \ne, em vez disso, enviar bytes começando daquele indicado na requisição de intervalo de bytes.\nEnquanto estamos tratando do assunto reposicionamento, mencionamos por alto o fato de que, quando um \nusuário reposiciona para um ponto adiante no vídeo ou termina o vídeo antecipadamente, alguns dados “pré-busca-\ndos mas ainda não visualizados”\n, transmitidos pelo servidor, permanecerão sem ser assistidos — um desperdício de \nlargura de banda da rede e de recursos do servidor. Por exemplo, suponha que o buffer do cliente esteja cheio com B \nbits em algum tempo t0 no vídeo, e neste tempo o usuário reposiciona em algum instante t > t0 + B/r, e então assiste \nao vídeo até o fim a partir dali. Neste caso, todos os B bits no buffer não serão mais assistidos, assim como a largura de \nbanda e os recursos do servidor que foram usados para transmitir esses B bits foram completamente desperdiçados. \nExiste um significativo desperdício de largura de banda na Internet por causa de términos antecipados de reprodução, \nFigura 7.3  Análise do uso de buffer no lado do cliente, para vídeo de fluxo contínuo\nKR 07.03.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p Wide x 12p4 Deep\n11/19/11 rossi\nTaxa de \npreenchimento = x\nTaxa de esgotamento  = r\nServidor \nde vídeo\nInternet\nQ\nB\nBuffer da aplicação cliente\nredes multimídia  443 \no que pode ser realmente custoso, em particular para conexões sem fio [Ihm, 2011]. Por esta razão, muitos sistemas de \nfluxo contínuo definem um tamanho apenas moderado para o buffer da aplicação cliente, ou limitarão a quantidade \nde vídeos pré-buscados usando o cabeçalho de intervalo de bytes nas requisições HTTP [Rao, 2011].\nReposicionamento e término antecipado podem ser comparados a cozinhar um grande bife, comer só uma \nparte dele e jogar o resto fora, logo, desperdiçando comida. Então, da próxima vez que seus pais o criticarem por \ndesperdiçar alimento porque não comeu todo o seu jantar, você pode rapidamente responder dizendo que eles \ntambém estão dissipando largura de banda e recursos do servidor quando reposicionam enquanto assistem a \nfilmes pela Internet! Mas, é claro, dois erros não fazem um acerto — tanto comida quanto largura de banda não \ndevem ser desperdiçadas!\n7.2.3  Fluxo contínuo adaptativo e DASH\nApesar de o HTTP de fluxo contínuo, como descrito nas subseções anteriores, estar sendo amplamente \nutilizado na prática (por exemplo, pelo YouTube desde o seu início), ele possui uma grande deficiência: todos \nos clientes recebem a mesma codificação do vídeo, apesar das grandes variações na quantidade de largura de \nbanda disponível para o cliente, tanto através de diferentes clientes quanto ao longo do tempo para um mesmo \ncliente. Isto levou ao desenvolvimento de um novo tipo de fluxo contínuo baseado em HTTP, comumente \nreferido como Fluxo Contínuo Adaptativo Dinamicamente sobre HTTP (DASH). Pelo DASH, o vídeo é \ncodificado em muitas versões diferentes, cada qual com uma taxa de bits e um diferente nível de qualidade. O \ncliente requisita dinamicamente, dessas diferentes versões, trechos de alguns segundos de segmentos do vídeo. \nQuando a quantidade de largura de banda disponível é alta, o cliente em geral seleciona trechos de uma versão \nque possui uma taxa alta; e quando a largura de banda disponível é baixa, ele naturalmente seleciona de uma \nversão cuja taxa é baixa. O cliente seleciona diferentes trechos um de cada vez com mensagens de requisição \nHTTP GET [Akhshabi, 2011].\nPor um lado, DASH permite aos clientes com diferentes taxas de acesso à Internet fluir em um vídeo por di-\nferentes taxas codificadas. Clientes com conexões 3G lentas podem receber uma versão com baixa taxa de bits (e \nbaixa qualidade), e clientes com conexões por fibra ótica podem receber versões de alta qualidade. Por outro lado, \no uso de DASH permite a um cliente se adaptar à largura de banda disponível, se a largura de banda fim a fim \nmudar durante a sessão. Esta funcionalidade é particularmente importante para usuários de dispositivos móveis, \nque com frequência experimentam flutuações na largura de banda disponível, conforme se movimentam de uma \nestação de base para outra. Por exemplo, a Comcast tem implantado um sistema de fluxo contínuo adaptativo \nno qual cada arquivo fonte de vídeo é codificado em 8 a 10 diferentes formatos MPEG-4, permitindo assim que \no formato de vídeo de maior qualidade possível seja acessado pelo cliente, sendo a adaptação feita em resposta a \nmudanças na rede e nas condições dos dispositivos.\nCom o DASH, cada versão do vídeo é armazenada em um servidor HTTP, cada um com uma diferente \nURL. O servidor HTTP também possui um arquivo de manifesto, que provê uma URL para cada versão junto \ncom a sua taxa de bits. Primeiro, o cliente requisita o arquivo de manifesto e identifica as várias versões. O cliente, \nentão, seleciona um trecho de cada vez, especificando uma URL e um intervalo de bytes em uma mensagem de \nrequisição HTTP GET, para cada trecho. Enquanto estiver baixando trechos, o cliente também mede a largura \nde banda de recepção e executa um algoritmo de determinação de taxa para selecionar o próximo trecho a ser \nrequisitado. Naturalmente, se o cliente possui muito vídeo em seu buffer e se a largura de banda de recepção que \nfoi medida for alta, ele escolherá um trecho de uma versão com taxa alta associada. E, claro, se o cliente possui \npouco vídeo em seu buffer e a sua largura de banda de recepção for baixa, escolherá um trecho de uma versão de \ntaxa baixa. Portanto, DASH permite ao cliente alternar livremente em diferentes níveis de qualidade. Uma vez \nque uma súbita queda na taxa de bits, por selecionar outra versão, pode acarretar uma degradação na qualidade \nvisual facilmente perceptível, a redução na taxa de bits pode ser realizada usando múltiplas versões intermediá-\nrias e assim possibilitar uma transição mais suave para uma taxa em que a taxa de consumo do cliente diminua \nabaixo de sua largura de banda disponível. Quando as condições da rede melhorarem, o cliente pode então depois \nescolher trechos de versões com maior taxa de bits.\n   Redes de computadores e a Internet\n444\nPelo monitoramento dinâmico da largura de banda disponível e do nível do buffer do cliente, e ajustando a \ntaxa de transmissão pela troca de versão, o uso de DASH geralmente pode efetuar uma reprodução contínua no \nmelhor nível de qualidade possível, sem o problema de congelamento ou salto de quadros. Além disso, uma vez \nque o cliente (ao invés do servidor) mantenha a inteligência para determinar qual trecho deve ser enviado em \nseguida, este esquema também aumenta a escalabilidade no lado do servidor. Outro benefício dessa abordagem \né o cliente poder usar a requisição de faixa de bytes do HTTP para controlar com precisão a quantidade de vídeo \nobtido por pré-busca, o qual é armazenado localmente.\nConcluímos nossa breve discussão sobre DASH mencionando que, para muitas execuções, o servidor não \napenas armazena diversas versões do vídeo, como também armazena em separado muitas versões do áudio. \nCada versão do áudio possui seu próprio nível de qualidade e taxa de bits, assim como sua própria URL. Nessas \nexecuções, o cliente dinamicamente seleciona trechos tanto do vídeo como do áudio, e localmente sincroniza a \nreprodução do áudio e vídeo.\n7.2.4  Redes de distribuição de conteúdo\nMuitas companhias de vídeo na Internet têm distribuído sob demanda fluxos contínuos multi-Mbits/s para \nmilhões de usuários diariamente. YouTube, por exemplo, com uma biblioteca de centenas de milhões de vídeos, \ndistribui centenas de milhões de vídeos de fluxo contínuo para usuários ao redor do mundo inteiro, todos os dias \n[Ding, 2011]. Controlar o fluxo contínuo de todo esse tráfego para locais ao redor do mundo inteiro, enquanto \nprovê reprodução contínua e grande interatividade constitui-se claramente uma tarefa desafiadora.\nPara uma empresa de vídeos na Internet, talvez a mais franca abordagem para prover serviços de vídeo de \nfluxo contínuo seja construir um único e sólido datacenter, armazenar ali todos os vídeos e realizar o fluxo contí-\nnuo dos vídeos diretamente do datacenter para os clientes ao redor do mundo. No entanto, existem três grandes \nproblemas com essa abordagem. Primeiro, se o cliente estiver muito longe do datacenter, os pacotes que vão do \nservidor para o cliente atravessarão muitos enlaces de comunicação, e também possivelmente passarão por mui-\ntos ISPs com alguns destes ISPs talvez localizados em diferentes continentes. Se um dos enlaces fornecer uma \ntaxa de transferência menor que a de consumo do vídeo, a taxa de transferência fim a fim será também abaixo da \nde consumo, resultando para o usuário em atrasos incômodos por congelamento. (Lembre-se, do Capítulo 1, que \numa taxa de transferência fim a fim de um fluxo contínuo é controlada pela taxa de transferência de seu enlace de \ngargalo.) A probabilidade de isso acontecer cresce na mesma proporção em que o número de enlaces no caminho \nfim a fim aumenta. Uma segunda desvantagem é que um vídeo popular será possivelmente enviado muitas vezes \npelos mesmos enlaces de comunicação. Isto não apenas é um desperdício de largura de banda de rede, mas a \nprópria companhia de vídeo pela Internet estará pagando pelo seu ISP (conectado com o datacenter) por enviar \nos mesmos bytes pela Internet muitas e muitas vezes. Um terceiro problema com essa solução é que um único \ndatacenter representa um único ponto de falha — se o datacenter ou seus enlaces para a Internet cairem, ele não \nserá capaz de distribuir nenhum vídeo de fluxo de dados.\nPara enfrentar o desafio de distribuição de uma quantidade maciça de dados de vídeo para usuários espa-\nlhados ao redor do mundo, quase todas as maiores companhias de vídeo de fluxo contínuo fazem uso de Redes \nde Distribuição de Conteúdo (CDNs). Uma CDN gerencia servidores em múltiplas localidades distribuídas \ngeograficamente, armazena cópias dos vídeos (e outros tipos de conteúdos da rede, incluindo documentos, \nimagens e áudio) em seus servidores, e tenta direcionar cada requisição do usuário para uma localidade CDN \nque proporcionará a melhor experiência para o usuário. A CDN pode ser uma CDN privada, isto é, que per-\ntence ao próprio provedor de conteúdo; por exemplo, a CDN da Google distribui vídeos do YouTube e outros \ntipos de conteúdo. A CDN também pode ser uma CDN de terceiros, que distribui conteúdo em nome de \nmúltiplos provedores de conteúdo; a da Akamai, por exemplo, é uma CDN de terceiros que distribui conteúdos \ndo Netflix e da Hulu, dentre outros. Uma visão geral das modernas CDNs, que é muito lida, pode ser vista em \nLeighton [2009].\nAs CDNs adotam em geral uma entre as duas filosofias de instalação de servidores [Huang, 2008]:\nredes multimídia  445 \n• Enter deep. Uma filosofia, que começou a ser praticada pela Akamai, é entrar profundamente dentro das \nredes de acesso dos Provedores de Serviço de Internet pela implantação de clusters de servidores no aces-\nso de ISPs por todo o mundo. (Redes de acesso são descritas na Seção 1.3.) A Akamai usa esta abordagem \ncom clusters de servidores em cerca de 1.700 locais. O objetivo é conseguir proximidade com os usuários \nfinais, melhorando assim o atraso percebido pelo usuário e a taxa de transferência pela diminuição do \nnúmero de enlaces e roteadores entre o usuário final e o agrupamento da CDN da qual ele recebe con-\nteúdo. Por causa desse projeto altamente distribuído, a tarefa de manter e gerenciar os agrupamentos se \ntorna desafiadora.\n• Bring home. Uma segunda filosofia de projeto, adotada pela Limelight e por muitas outras compa-\nnhias de CDN é trazer para dentro de casa os ISPs, construindo clusters enormes, mas em um número \nmenor (por exemplo, dezenas) de lugares-chave e conectando-os em uma rede privada de alta velo-\ncidade. Em vez de entrar nos ISPs de acesso, estas CDNs normalmente colocam cada cluster em uma \nlocalidade que seja ao mesmo tempo próxima aos PoPs (veja a Seção 1.3) de muitos ISPs da camada \n1, por exemplo, a algumas milhas tanto da AT&T quanto aos PoPs da Verizon em uma cidade maior. \nComparado com a filosofia de projeto enter deep, o projeto bring home em geral resulta em menos \ndesperdício com gerenciamento e manutenção, mas com um maior atraso e menores taxas de trans-\nferência para os usuários finais.\nInfraestrutura de rede da Google\nPara suportar sua vasta lista de serviços na nuvem \n— incluindo busca, gmail, calendário, vídeos do You-\nTube, mapas, documentos e redes sociais — a Google \nimplementou uma rede privada extensiva de infraes-\ntrutura de CDN. A infraestrutura de CDN da Google \npossui três camadas de agrupamentos de servidores:\n• Oito “mega datacenters”, com seis localizados \nnos Estados Unidos e dois na Europa [Google \nLocations, 2012], com cada datacenter contendo \nalgo na ordem de 100 mil servidores. Esses mega \ndatacenters são responsáveis por servir conteúdo \ndinâmico (e muitas vezes personalizado), incluindo \nresultados de busca e mensagens de gmail.\n• Cerca de 30 clusters bring home (veja discussão \nna Seção 7.2.4), com cada cluster consistindo algo \nna ordem de 100-500 servidores [Adhikari, 2011a]. \nAs localizações dos agrupamentos são distribuí-\ndas ao redor do mundo, com cada localização nor-\nmalmente próxima a múltiplos pontos de presença \nde ISP da camada 1. Esses agrupamentos são \nresponsáveis por servir conteúdo estático, incluin-\ndo vídeos do YouTube [Adhikari, 2011a].\n• Muitas centenas de clusters enter deep (veja Se-\nção 7.2.4), com cada cluster localizado dentro de \num ISP de acesso. Este cluster consiste em de-\nzenas de servidores dentro de uma única estan-\nte. Os servidores enter deep realizam a divisão de \nTCP (veja a Seção 3.7) e servem conteúdo estático \n[Chen, 2011], incluindo as partes estáticas das pá-\nginas da Internet que incluem resultados de busca.\nTodos esses datacenters e localizações de clus-\nters de servidores estão dispostos juntos em rede \ncom a rede privada da Google, como parte de um \nenorme AS (AS 15169). Quando um usuário efetua um \npedido de busca, geralmente esse pedido é enviado \nprimeiro do ISP local para o cache de um servidor en-\nter deep próximo de onde o conteúdo estático é lido; \nembora fornecendo o conteúdo estático ao cliente, \neste cache próximo também encaminha a consulta \npela rede privada da Google para um dos mega da-\ntacenters, de onde o resultado da busca personaliza-\nda é obtido. Para um vídeo do YouTube, o vídeo em \nsi pode vir de um dos caches bring home, enquanto \npartes da página Web ao redor do vídeo podem vir do \ncache enter deep nas vizinhanças, e os anúncios ao \nredor do vídeo podem vir dos datacenters. Resumin-\ndo, exceto para os ISPs locais, os serviços na nuvem \nda Google são, em grande parte, fornecidos por uma \ninfraestrutura de rede que é independente da Internet \npública.\nestudo de caso\n   Redes de computadores e a Internet\n446\nUma vez que os seus clusters estejam operando, a CDN replica conteúdo através dos seus clusters. A CDN \npode não querer pôr uma cópia de todos os vídeos em cada cluster, já que alguns vídeos são vistos raramente \nou são populares só em alguns países. De fato, muitas CDNs não empurram vídeos para seus clusters, mas, em \nvez disso, usam uma estratégia simples de puxar o conteúdo: Se um cliente requisita um vídeo de um cluster que \nnão o está armazenando, o cluster recupera o vídeo (de um repositório central ou de outro cluster) e armazena \numa cópia localmente enquanto envia o fluxo contínuo para o cliente ao mesmo tempo. Similar aos caches de \nInternet (veja no Capítulo 2), quando a memória do cluster fica cheia, ele remove vídeos que não são frequen-\ntemente requisitados.\nOperação da CDN\nTendo identificado as duas principais técnicas de implantação de uma CDN, vamos agora nos aprofundar \nem como uma CDN opera. Quando um navegador em um hospedeiro do usuário recebe a instrução para recu-\nperar um vídeo específico (identificado por uma URL), a CDN tem de interceptar a requisição a fim de: (1) deter-\nminar um cluster de servidor de CDN apropriado para o cliente naquele momento, e (2) redirecionar a requisição \ndo cliente para um servidor naquele cluster. Falaremos, em linhas gerais, como uma CDN pode determinar um \ncluster apropriado. Antes, no entanto, vamos examinar os mecanismos que são usados para interceptar e redire-\ncionar uma requisição.\nA maioria das CDNs utiliza o DNS para interceptar e redirecionar requisições; uma discussão interessante \nsobre esse uso do DNS pode ser vista em Vixie [2009]. Consideremos um simples exemplo para ilustrar como o \nDNS normalmente é envolvido. Suponha que um provedor de conteúdo, NetCinema, utiliza outra companhia de \nCDN, KingCDN, para distribuir seus vídeos para os seus consumidores. Nas páginas de Internet do NetCinema, \ncada vídeo está associado a uma URL que inclui a palavra “vídeo” e um identificador único para o vídeo em si; \npor exemplo Tranformers 7 pode ser associado a <http://video.netcinema.com/6Y7B23V>. Então, acontecem \nseis passos, como mostra a Figura 7.4:\n1.\t O usuário visita a página da Internet no NetCinema.\n2.\t Quando o usuário clica no link <http://video.netcinema.com/6Y7B23V>, o hospedeiro do usuário envia \numa consulta de DNS para video.netcinema.com.\n3.\t O Servidor de DNS local (LDNS) do usuário retransmite a consulta de DNS a um servidor DNS autori-\ntativo pelo NetCinema, o qual encontra a palavra “video” no nome do hospedeiro video.netcinema.com. \nPara “entregar” a consulta de DNS à KingCDN, em vez de um endereço IP, o servidor de DNS autoritativo \nFigura 7.4  DNS redireciona uma requisição do usuário para um servidor de CDN\nKR 07.04.eps\nKurose and Ross\nComputer Networking 6/e\nServidor de \nDNS local\nServidor de DNS \nautoritativo da NetCinema\nwww.NetCinema.com\nServidor autoritativo \nda KingCDN\nServidor de distribuição \nde conteúdo KingCDN\n2\n5\n6\n3\n1\n4\nredes multimídia  447 \ndo NetCinema retorna ao LDNS um nome de hospedeiro no domínio da KingCDN, por exemplo, a1105.\nkingcdn.com.\n4.\t Deste ponto em diante, a consulta de DNS entra na infraestrutura da DNS privada da KingCDN. O LDNS \ndo usuário, então, envia uma segunda consulta, agora para a1105.kingcdn.com, e o sistema de DNS da \nKingCDN por fim retorna os endereços IP de um servidor de conteúdo da KingCDN para o LDNS. As-\nsim, é aqui, dentro do sistema de DNS da KingCDN, que é especificado o servidor de CDN de onde o \ncliente receberá o seu conteúdo.\n5.\t O LDNS encaminha o endereço IP do nó de conteúdo/serviço da CDN para o hospedeiro do usuário.\n6.\t Uma vez que o cliente obtém o endereço IP de um servidor de conteúdo da KingCDN, ele estabelece uma \nconexão TCP direta com o servidor que se encontra nesse endereço IP e executa uma requisição HTTP \nGET para obter o vídeo. Se utilizar DASH, o servidor primeiro enviará ao cliente um arquivo de manifesto \ncom uma lista de URLs, uma para cada versão do vídeo, e o cliente irá dinamicamente selecionar trechos \ndas diferentes versões.\nEstratégias de seleção de cluster\nNo centro de qualquer distribuição de uma CDN está a estratégia de seleção de cluster, isto é, um meca-\nnismo para direcionamento dinâmico de clientes para um cluster de servidor ou uma central de dados dentro da \nCDN. Como acabamos de ver, a CDN descobre qual o endereço IP do servidor LDNS do cliente consultando o \nDNS do cliente. Após descobrir esse endereço IP, a CDN precisa selecionar um cluster apropriado baseado nesse \nendereço IP. As CDNs geralmente empregam estratégias próprias de seleção de cluster. Examinaremos agora \nalgumas abordagens naturais, cada uma com suas vantagens e desvantagens.\nUma estratégia simples é associar o cliente ao cluster que está geograficamente mais próximo. Usando bases \nde dados de geolocalização comerciais (Quova [2012] e Max-Mind [2012], por exemplo), cada endereço IP de \nLDNS é mapeado para uma localização geográfica. Quando uma requisição DNS é recebida de um determinado \nLDNS, a CDN escolhe o cluster mais próximo geograficamente, isto é, o cluster que está a uma distância menor, \nem quilômetros, do LDNS. Esta solução pode funcionar razoavelmente bem para uma boa parte dos clientes \n[Agarwal, 2009]. No entanto, para alguns clientes, esta solução pode ter um desempenho ruim, uma vez que o \ncluster geograficamente mais próximo pode não ser o mais próximo se considerarmos o caminho percorrido pela \nrede. Mais ainda, existe um problema inerente a todas as abordagens baseadas em DNS, que consiste no fato de \nque alguns usuários finais são configurados para utilizar LDNSs remotas [Shaikh, 2001; Mao, 2002]. Nesses ca-\nsos, a localização do LDNS pode ser muito longe da localização do cliente. Além disso, essa estratégia elementar \nignora a variação no atraso e a largura de banda disponível dos caminhos da Internet, porque sempre associa o \nmesmo cluster a determinado cliente.\nPara determinar o melhor cluster para um cliente baseado nas atuais condições de tráfego, as CDNs podem, \ncomo alternativa, realizar medições em tempo real do atraso e problemas de baixo desempenho entre seus clusters \ne clientes. Por exemplo, uma CDN pode ter cada um de seus clusters periodicamente enviando mensagens de ve-\nrificação (por exemplo, mensagens ping ou consultas de DNS) para todos os LDNSs do mundo inteiro. Um obstá-\nculo a essa técnica é o fato de que muitos LDNSs estão configurados para não responder a esse tipo de mensagem.\nUma alternativa para medir as propriedades dos caminhos, sem precisar trafegar essas mensagens estra-\nnhas, é usar as características do tráfego mais recente entre os clientes e os servidores da CDN. Por exemplo, o \natraso entre um cliente e um cluster pode ser estimado examinando o intervalo entre um SYNACK do servidor \npara o cliente e o ACK do cliente para o servidor, durante a apresentação de três vias do TCP. Tais soluções, po-\nrém, requerem, de tempos em tempos, o redirecionamento de clientes para (possivelmente) clusters que não são \nos melhores, a fim de medir as propriedades dos caminhos para esses clusters. Embora seja necessário apenas um \npequeno número de requisições para servir de mensagens de verificação, os clientes selecionados podem sofrer \numa degradação de desempenho significativo quando receberem conteúdo (vídeo ou qualquer outro) [Andrews, \n2002; Krishnan, 2009]. Uma alternativa para verificação do caminho cluster-cliente é utilizar o tráfego de consultas \n \n   Redes de computadores e a Internet\n448\nDNS para medir o atraso entre clientes e clusters. Em especial, durante a fase de DNS (veja o Passo 4 da Figura \n7.4), o LDNS do cliente pode ser ocasionalmente direcionado para servidores de autorização de DNS instalados \nnas várias localizações dos clusters, produzindo um tráfego de DNS que pode então ser medido entre o LDNS \ne tais localizações. Nesse método, os servidores de DNS continuam a retornar o cluster ideal para o cliente, de \nmodo que a entrega de vídeos e outros objetos da Internet não seja afetada [Huang, 2010].\nUma abordagem muito diferente para combinar clientes com servidores CDN é utilizar o IP para qualquer \nmembro do grupo[RFC 1546]. A ideia por trás do IP para qualquer membro do grupo é colocar os roteadores \nna rota da Internet dos pacotes do cliente para o cluster “mais próximo”\n, como determinado pelo BGP. Especi-\nficamente, como mostrado na Figura 7.5, durante o estágio de configuração do IP para qualquer membro do \ngrupo, a companhia de CDN associa o mesmo endereço IP a cada um dos seus clusters, e utiliza BGP padrão para \ninformar esse endereço IP de cada uma das diferentes localizações de clusters. Quando um roteador BGP recebe \nmúltiplos anúncios de rotas do mesmo endereço IP, trata esses anúncios como diferentes caminhos disponíveis \npara a mesma localização física (quando, na verdade, os anúncios são para diferentes caminhos para diferentes \nlocalizações físicas). Seguindo procedimentos padronizados de operação, o roteador BGP selecionará a “melhor” \n(por exemplo, o mais próximo, como determinado pelos contadores de salto de AS) rota para o endereço IP, de \nacordo com o seu mecanismo de seleção de rota local. Por exemplo, se um roteador BGP (correspondente a uma \nlocalização) está a apenas um salto de AS do roteador e todos os demais roteadores BGP (correspondentes às \noutras localizações) estão a dois ou mais saltos de AS, então o roteador BGP normalmente escolherá rotear pa-\ncotes para a localidade que atravessar apenas um AS (veja Seção 4.6). Depois dessa fase inicial de configuração, a \nCDN pode fazer o seu trabalho principal, que é distribuir conteúdo. Quando algum cliente quer assistir a algum \nvídeo, o DNS das CDNs retorna o endereço para qualquer membro do grupo, não importa onde o cliente esteja \nlocalizado. Quando o cliente envia um pacote para esse endereço IP, o pacote é roteado para o cluster “mais pró-\nximo”\n, conforme determinado pelas tabelas de repasse pré-configuradas pelo BGP, como acabamos de descrever. \nEssa abordagem possui a vantagem de encontrar o cluster que é mais próximo do cliente, em vez do cluster que é \nFigura 7.5  Usando o anycast IP para rotear clientes para o cluster da CDN mais próximo\nKR 07.05.eps\nAS1\nAS3\n3b\n3c\n3a\n1a\n1c\n1b\n1d\nAS2\nAS4\n2a\n2c\n4a\n4c\n4b\nAnúncio\n212.21.21.21\nServidor CDN B\nServidor CDN A\nAnúncio\n212.21.21.21\nRecebe anúncios \nBGP para 212.21.21.21 \nde AS1 e de AS4. \nRepassa para Servidor B, \npois está mais próximo.\n2b\nredes multimídia  449 \nmais próximo do LDNS do cliente. No entanto, a estratégia do anycast IP novamente não leva em consideração a \nnatureza dinâmica da Internet sobre escalas de tempo pequenas [Ballani, 2006].\nAlém das considerações relacionadas à rede, tais como atraso, perda e desempenho da largura de banda, \nexistem muitos fatores adicionais importantes, que tratam da forma de projetar a estratégia de seleção de cluster. \nA carga nos clusters é um desses fatores — clientes não devem ser direcionados para clusters sobrecarregados. O \ncusto de distribuição do ISP é outro fator — os clusters podem ser escolhidos de modo que esses ISPs específicos \nsejam usados para transportar o tráfego da CDN para o cliente, levando em conta as diferentes estruturas de custo \nnos relacionamentos contratuais entre os ISPs e os operadores dos clusters.\n7.2.5  Estudos de caso: Netflix, YouTube e Kankan\nConcluímos nossa discussão sobre vídeo de fluxo contínuo armazenado observando três implementações \nem larga escala de grande sucesso: Netflix, YouTube e Kankan. Veremos que todos estes sistemas utilizam dife-\nrentes métodos, empregando muitos dos princípios destacados e discutidos nesta seção.\nNetflix\nGerando quase 30% de todo o tráfego de descarga de fluxo contínuo da Internet nos Estados Unidos em \n2011, a Netflix tornou-se, nesse país, o provedor de serviço líder no segmento de filmes on-line e shows de TV \n[Sandvine, 2011]. A fim de executar rapidamente seus serviços de larga escala, tem feito uso extensivo de \nCDNs e serviços em nuvens de terceiros. De fato, a Netflix é um exemplo interessante de uma companhia reali-\nzando um serviço on-line em larga escala por meio de aluguel de servidores, largura de banda, armazenamento \ne serviços de bancos de dados de terceiros, enquanto utiliza  pouca infraestrutura própria. A seguinte discus-\nsão foi adaptada de um estudo de medição — muito bem descrito — da arquitetura da Netflix [Adhikari, 2012]. \nComo veremos, a companhia emprega muitas das técnicas tratadas antes nesta seção, incluindo distribuição de \nvídeo usando CDN (na verdade, múltiplas CDNs) e HTTP de fluxo contínuo adaptativo.\nA Figura 7.6 mostra a arquitetura básica da plataforma de vídeo de fluxo contínuo da Netflix. Ela possui \nquatro componentes principais: os servidores de registro e pagamento, a nuvem da Amazon, múltiplos provedo-\nres CDN e os clientes. Em sua infraestrutura de hardware própria, a Netflix mantém os servidores de registro e \nFigura 7.6  Plataforma de vídeo de fluxo contínuo da Netflix\nKR 07.06.eps\nAW/Kurose and Ross\nNuvem da Amazon\nServidor de CDN\nServidor de CDN\nEnvio de \nversões \npara CDNs\nServidores \nde registro e \npagamento da Netﬂix\nServidor \nde CDN\nCliente\nArquivo de \nmanifesto\nRegistro e \npagamento\nTrechos \nde vídeo \n(DASH)\n   Redes de computadores e a Internet\n450\npagamento, os quais permitem o registro de novas contas e capturam informações de pagamento por cartão de \ncrédito. Com exceção destas funções básicas, a Netflix realiza seus serviços on-line implementando máquinas (ou \nmáquinas virtuais) na nuvem da Amazon. A seguir, algumas das funcionalidades que estão na nuvem da Amazon:\n• Obtenção de conteúdo. Antes de a Netflix poder distribuir um filme para seus clientes, é necessário obter \ne processar o filme. A Netflix recebe as versões principais de estúdio e as carrega para os hospedeiros na \nnuvem da Amazon.\n• Processamento de conteúdo. As máquinas na nuvem da Amazon criam muitos formatos diferentes para \ncada filme, de acordo com uma série de tocadores de vídeo do cliente, rodando em computadores de \nmesa, smartphones e consoles de jogos conectados a aparelhos de televisão. Uma versão diferente é cria-\nda para cada formato e as múltiplas taxas de bits, permitindo assim que se utilize HHTP de vídeo de fluxo \ncontínuo, usando DASH.\n• Descarregamento de versões para as CDNs. Uma vez que todas as versões de um filme foram criadas, os \nhospedeiros na nuvem da Amazon descarregam as versões para as CDNs.\nPara entregar filmes a seus consumidores por demanda, a Netflix faz uso extensivo da tecnologia CDN. De \nfato, como foi escrito em 2012, a empresa utiliza não apenas um, mas três companhias de CDN ao mesmo tempo \n— Akamai, Limelight e Level-3.\nTendo descrito os componentes da arquitetura da Netflix, vamos analisar mais de perto a interação entre \no cliente e os vários servidores envolvidos na entrega do filme. As páginas de Internet para se navegar pela bi-\nblioteca de vídeos da Netflix usam os servidores da nuvem da Amazon. Quando o usuário seleciona um filme \npara “Reproduzir agora”\n, o cliente do usuário obtém um arquivo de manifesto, proveniente também na nuvem da \nAmazon. Tal arquivo contém uma série de informações, incluindo uma lista ordenada de CDNs e das URLs para \nas diferentes versões do filme, a qual é usada para a reprodução DASH. A lista ordenada das CDNs é determinada \npela Netflix, e pode mudar de uma sessão de fluxo contínuo para outra. Em geral, o cliente selecionará a CDN \nque está mais bem posicionada no arquivo de manifesto. Após o cliente selecionar a CDN, esta aproveita o DNS \npara redirecionar o cliente a um servidor específico da CDN, conforme descrito na Seção 7.2.4. O cliente e aquele \nservidor da CDN então interagem usando DASH. Mais especificamente, como foi descrito na Seção 7.3.2, o clien-\nte utiliza o cabeçalho do intervalo de bytes nas mensagens de requisição HTTP GET para requisitar trechos das \ndiferentes versões do filme. A Netflix usa trechos de cerca de 4 s [Adhikari, 2012]. Enquanto os trechos vão sendo \nbaixados, o cliente mede a vazão recebida e executa um algoritmo para determinação da taxa, a fim de definir a \nqualidade do próximo trecho da requisição seguinte.\nA Netflix incorpora muitos dos princípios-chave discutidos antes nesta seção, incluindo fluxo continuo \nadaptativo e distribuição da CDN. A empresa também gentilmente ilustra como o principal serviço de Internet, \ngerando quase 30% do tráfego de Internet, pode funcionar quase inteiramente em uma nuvem de terceiros e em \ninfraestruturas de CDN de terceiros, utilizando uma infraestrutura própria tão pequena!\nYouTube\nCom cerca de meio bilhão de vídeos em suas bibliotecas e meio bilhão de exibições de vídeos por dia [Ding, \n2011], o YouTube é indiscutivelmente o maior site da Internet no mundo para compartilhamento de vídeos. O You-\nTube começou seus serviços em abril de 2005 e foi adquirido pela Google em novembro de 2006. Embora o projeto \ne os protocolos do Google/YouTube sejam proprietários, através de muitos esforços de medição independentes, \npodemos ter um entendimento básico sobre como o YouTube opera [Zink, 2009; Torres, 2011; Adhikari, 2011a].\nAssim como a Netflix, o YouTube faz uso extensivo da tecnologia CDN para distribuir seus vídeos [Torres, \n2011]. Ao contrário da Netflix, porém, a Google não emprega CDNs de terceiros, mas, em vez disso, utiliza sua \nprópria CDN privada para distribuir os vídeos do YouTube. A Google tem instalado clusters de servidores em \ncentenas de locais diferentes. De um subconjunto de cerca de 50 desses locais, a Google distribui os vídeos do \nYouTube [Adhikari 2011a]. A Google usa DNS para redirecionar uma requisição do consumidor para um cluster es-\npecífico, conforme descrito na Seção 7.2.4. Na maior parte do tempo, a estratégia de seleção de cluster da Google \nredes multimídia  451 \ndireciona o cliente ao cluster para o qual o RTT entre o cliente e o cluster seja o menor; porém, a fim de balancear \na carga através dos clusters, algumas vezes o cliente é direcionado (via DNS) a um cluster mais distante [Torres, \n2011]. Além disso, se não possui o vídeo solicitado, em vez de buscá-lo de algum outro lugar e retransmiti-lo \npara o cliente, o cluster pode retornar uma mensagem de redirecionamento HTTP, desse modo redirecionando o \ncliente para outro cluster [Torres, 2011].\nO YouTube emprega HTTP de fluxo contínuo, como foi descrito na Seção 7.2.2. Muitas vezes disponibiliza \numa pequena quantidade de versões diferentes do vídeo, cada uma com uma taxa de bits diferente e correspon-\ndente ao nível de qualidade. Assim como ocorreu em 2011, o YouTube não utiliza fluxo contínuo adaptativo \n(como o DASH), mas em vez disso, requer que o usuário selecione manualmente uma versão. No intuito de \neconomizar tanto largura de banda quanto recursos do servidor, que poderiam ser desperdiçados por reposicio-\nnamento ou por término precoce, o YouTube usa a requisição HTTP de intervalo de bytes, de modo a limitar o \nfluxo de dados transmitidos após uma quantidade do vídeo ter sido obtida pela pré-busca.\nAlguns milhões de vídeos são enviados ao YouTube diariamente. Não apenas vídeos do YouTube são en-\nviados do servidor ao cliente por HTTP, mas os alimentadores também enviam seus vídeos do cliente ao servi-\ndor por HTTP. O YouTube processa cada vídeo que recebe, convertendo-o para um formato de vídeo próprio \ne criando várias versões em diferentes taxas de bits. Esse processamento ocorre todo dentro dos datacenters da \nGoogle. Assim, ao contrário do Netflix, que executa seu serviço quase totalmente em infraestruturas de tercei-\nros, a Google executa o serviço YouTube inteiro dentro de sua própria vasta infraestrutura de datacenters, CDN \nprivada e rede global interconectando seus centros de dados e clusters de CDN. (Veja o estudo de caso sobre a \ninfraestrutura de rede da Google, na Seção 7.2.4.)\nKankan\nAcabamos de ver que, tanto nos serviços do Netflix quanto nos do YouTube, os servidores operados pelas \nCDNs (tanto CDNs privadas quanto de terceiros) fazem o fluxo contínuo de vídeo para seus clientes. O Netflix \ne o YouTube não apenas precisam pagar pelo hardware do servidor (tanto diretamente por mantê-lo, quanto \nindiretamente através do aluguel), mas também pela largura de banda que os servidores utilizam para distribuir \nos seus vídeos. Considerando a grande quantidade de serviços e de largura de banda que eles consumem, estas \nimplementações “cliente-servidor” são extremamente custosas.\nConcluímos esta seção descrevendo uma abordagem inteiramente diferente de fornecer vídeo por de-\nmanda, em larga escala, através da Internet — uma maneira que permite ao provedor de serviços reduzir \nsignificativamente sua infraestrutura e custos de largura de banda. Como você deve suspeitar, esta abordagem \nutiliza entrega via P2P em vez de entregas cliente-servidor (via CDNs). Entrega de vídeo P2P é usada com grande \nsucesso através de muitas companhias na China, incluindo a Kankan (pertence e é operada pela Xunlei), PPTV \n(antes era PPLive), e PPs (antes era PPstream). A Kankan, que é a líder do setor de fornecimento de vídeo por \ndemanda P2P na China, possui mais de 20 milhões de usuários únicos visualizando seus vídeos todo mês.\nEm uma análise de alto nível, o vídeo por demanda via P2P é muito parecido com a descarga de arquivos \nBitTorrent (discutida no Capítulo 2). Quando um par quer assistir um vídeo, entra em contato com um rastrea­\ndor (que pode estar centralizado ou baseado em um par usando um DHT) para descobrir outros pares dentro \ndo sistema que tenham uma cópia daquele vídeo. Este par então requisita trechos do arquivo de vídeo, e isto \nparalelamente aos outros pares que possuem o arquivo. Diferente do modo que o BitTorrent opera, porém, as \nrequisições são preferencialmente feitas para trechos que estão para ser reproduzidos em um futuro próximo a \nfim de garantir uma reprodução contínua.\nO projeto da Kankan utiliza um rastreador e seu próprio DHT para rastrear conteúdos. As dimensões de \nmultidão para o conteúdo mais popular envolvem dezenas de milhares de pares, normalmente maior que as \nmaiores multidões no BitTorrent [Dhungel, 2012]. Os protocolos da Kankan — para comunicação entre par e \nrastreador, entre par e DHT e entre pares — todos são proprietários. Curiosamente, para distribuir trechos de \nvídeos entre pares, Kankan utiliza UDP sempre que possível, levando a uma enorme quantidade de tráfego UDP \ndentro da Internet da China [Zhang M., 2010].\n   Redes de computadores e a Internet\n452\n7.3  Voice-over-IP\nÁudio interativo em tempo real pela Internet é frequentemente denominado telefone por Internet, já que, \nda perspectiva do usuário, é semelhante ao tradicional serviço telefônico por comutação de circuitos. Também é \nchamado comumente por Voice-over-IP (VoIP). Nesta seção descrevemos os princípios e protocolos fundamen-\ntais do VoIP. Vídeo interativo é similar em muitos aspectos ao VoIP, exceto pelo fato de que ele inclui o vídeo dos \nparticipantes assim como as suas vozes. Para que a discussão seja mantida dentro do foco e de modo concreto, \nenfatizaremos somente a voz nesta seção, em vez de abordar voz e vídeo combinados.\n7.3.1  As limitações de um serviço IP de melhor esforço\nO protocolo de camada de rede da Internet, IP, provê um serviço de melhor esforço. Em outras palavras, a \nInternet faz seu melhor esforço para transportar cada datagrama do remetente ao receptor o mais rapidamente \npossível, mas não faz nenhuma promessa sequer sobre o atraso fim a fim para um pacote individual, ou sobre \no limite de perdas de pacotes. A falta dessas garantias impõe desafios significativos ao projeto de aplicações de \náudio interativo em tempo real, que são bastante sensíveis ao atraso de pacotes, variação de atraso e perdas.\nNesta seção, abordaremos vários meios de o desempenho do VoIP em uma rede de melhor esforço ser apri-\nmorado. Nosso foco será em técnicas da camada de aplicações da Internet, isto é, abordagens que não exijam \nquaisquer mudanças na parte principal da rede ou mesmo na camada de transporte no lado dos hospedeiros. Para \nmanter a discussão em termos concretos, discutiremos as limitações de um serviço de IP de melhor esforço no \ncontexto de um exemplo específico de VoIP. O emitente gera bytes a uma taxa de 8.000 bytes por segundo; a cada \n20 ms o emitente agrupa esses bytes em um bloco. Um bloco e um cabeçalho especial (discutiremos sobre isso em \nseguida) são encapsulados em um segmento UDP, por meio de uma chamada a interface do socket. Sendo assim, o \nnúmero de bytes do bloco é (20 ms) ∙ (8.000 bytes/s) = 160 bytes, e um segmento de UDP é enviado a cada 20 ms.\nSe cada pacote chega ao receptor com um atraso fim a fim constante, então os pacotes chegam no receptor \na cada 20 ms. Nessas condições ideais, o receptor pode simplesmente reproduzir cada parte assim que ele chega. \nInfelizmente alguns pacotes talvez se percam e a maioria deles não possuirá o mesmo atraso fim a fim, ainda que \na Internet esteja ligeiramente congestionada. Por este motivo, o receptor deve se empenhar em determinar (1) \nquando reproduzir uma parte e (2) o que fazer com uma parte perdida.\nPerda de pacotes\nConsidere um dos segmentos UDPs, gerado por nossa aplicação VoIP. O segmento UDP é encapsulado por \num datagrama IP. Enquanto o datagrama vagueia pela rede, ele passa por buffers (isto é, por filas) nos roteado-\nres, aguardando para ser transmitido em enlaces de saída. É possível que um ou mais dos buffers na rota entre o \nremetente e o destinatário estejam lotados e não possam aceitar o datagrama IP. Nesse caso, o datagrama IP será \ndescartado e nunca chegará à aplicação receptora.\nA perda pode ser eliminada enviando os pacotes por TCP (que proporciona transferência de dados con-\nfiável), em vez de por UDP. Contudo, os mecanismos de retransmissão frequentemente são considerados ina-\nceitáveis para aplicações interativas de áudio em tempo real, como o VoIP, porque aumentam o atraso fim a fim \nBolot [1996]. Além disso, por causa do controle de congestionamento do TCP, após a perda de pacote a taxa de \ntransmissão no remetente pode ser reduzida a uma taxa mais baixa do que a de reprodução no receptor, possivel-\nmente ocasionando inanição no buffer. Isto pode causar um forte impacto sobre a integridade da voz no receptor. \nPor essas razões, quase todas as aplicações de VoIP existentes executam em UDP. Baset [2006] relata que o UDP \né usado pelo Skype a não ser que o usuário esteja atrás de um NAT ou de um firewall que bloqueia os segmentos \nUDP (nesse caso TCP é usado).\nMas perder pacotes talvez não seja tão desastroso quanto se possa imaginar. Na verdade, taxas de perda de \npacotes entre 1% e 20% podem ser toleradas, dependendo de como a voz é codificada e transmitida e de como \na perda é ocultada no receptor. Por exemplo, o mecanismo de correção de erros de repasse (forward error \nredes multimídia  453 \ncorrection — FEC) pode ajudar a ocultar a perda de pacotes. Veremos mais adiante que, com a FEC, a informação \nredundante é transmitida junto com a informação original, de modo que parte dos dados originais perdidos pode \nser recuperada da informação redundante. Não obstante, se um ou mais dos enlaces entre remetente e receptor \nestiverem fortemente congestionados e a perda de pacotes exceder 10–20% (por exemplo, em um enlace sem fio), \nentão, na realidade, nada poderá ser feito para conseguir uma qualidade de som aceitável. Assim, fica claro que o \nserviço de melhor esforço tem suas limitações.\nAtraso de fim a fim\nAtraso de fim a fim é o acúmulo de atrasos de processamento, de transmissão e de formação de filas nos rote-\nadores; atrasos de propagação nos enlaces e atrasos de processamento em sistemas finais. Para as aplicações de áudio \naltamente interativas em tempo real, como oVoIP, atrasos fim a fim menores do que 150 ms não são percebidos pelo \nouvido humano; entre 150 e 400 ms podem ser aceitáveis, mas não são o ideal e os que excedem 400 ms podem \natrapalhar seriamente a interatividade em conversações por voz. O lado receptor de uma aplicação de telefone por \nInternet em geral desconsiderará quaisquer pacotes cujos atrasos ultrapassarem determinado patamar, por exemplo, \nmais do que 400 ms. Assim, os pacotes cujos atrasos ultrapassem o patamar são efetivamente perdidos.\nVariação de atraso do pacote\nUm componente crucial do atraso fim a fim são os atrasos variáveis de fila que os pacotes sofrem nos roteado-\nres. Por causa de tais atrasos, o tempo decorrido entre o momento em que um pacote é gerado na fonte e o momento \nem que é recebido no destinatário pode variar de pacote para pacote, como mostra a Figura 7.1. Esse fenômeno \né denominado variação de atraso. Como exemplo, considere dois pacotes consecutivos em nossa aplicação de \ntelefone por Internet. O remetente envia o segundo pacote 20 ms após ter enviado o primeiro. Mas, no receptor, o \nespaçamento entre eles pode ficar maior do que 20 ms. Para entender, suponha que o primeiro pacote chegue a uma \nfila de roteador quase vazia, mas que, exatamente antes de o segundo pacote chegar, um grande número de pacotes \nvindos de outras fontes chegue à mesma fila. Como o primeiro pacote sofre um pequeno atraso de fila e o segundo \npacote sofre um grande atraso de fila nesse roteador, o primeiro e o segundo pacotes ficam espaçados em mais de \n20 ms. O espaçamento entre pacotes consecutivos também pode ficar menor do que 20 ms. Para entender, considere \nnovamente dois pacotes consecutivos. Suponha que o primeiro se junte ao final de uma fila com um grande número \nde pacotes e que o segundo chegue à fila antes dos pacotes de outras fontes. Nesse caso, nossos dois pacotes se en-\ncontram um exatamente atrás do outro na fila. Se o tempo para transmitir um pacote no enlace de saída do roteador \nfor menor do que 20 ms, então o primeiro e o segundo pacotes ficarão espaçados por menos de 20 ms.\nA situação é análoga a dirigir carros em rodovias. Suponha que você e um amigo seu, cada um dirigindo \nseu próprio carro, estejam viajando de San Diego a Phoenix. Suponha que você e seu amigo tenham um estilo \nsemelhante de dirigir e mantenham a velocidade de 100 km/h, se o tráfego permitir. Por fim, imagine que seu \namigo parta uma hora antes de você. Então, dependendo do tráfego, você pode chegar a Phoenix mais ou menos \numa hora depois de seu amigo.\nSe o receptor ignorar a presença de variação de atraso e reproduzir as porções assim que elas chegam, então \na qualidade de áudio resultante poderá facilmente se tornar ininteligível no receptor. Felizmente, a variação de \natraso quase sempre pode ser eliminada com a utilização de números de sequência, marcas de tempo e atraso \nde reprodução, como discutido a seguir.\n7.3.2  Eliminação da variação de atraso no receptor para áudio\nPara uma aplicação de VoIP, em que pacotes vão sendo gerados periodicamente, o receptor deve tentar \nprover reprodução sincronizada de porções de voz na presença de variação de atraso aleatório. Isto normalmente \né feito combinando os dois mecanismos seguintes:\n   Redes de computadores e a Internet\n454\n• Preceder cada parte com uma marca de tempo. O remetente marca cada parte com o instante em que ela \nfoi gerada.\n• Atrasar a reprodução de porções no receptor. Como vimos em nossa discussão anterior da Figura 7.1, o \natraso da reprodução das porções de áudio recebidas deve ser longo o suficiente para que a maioria dos \npacotes seja recebida antes de seus tempos de reprodução programados. O atraso da reprodução pode \nser fixado para todo o período de duração da sessão de áudio ou pode variar adaptativamente durante o \nperíodo útil da sessão.\nDiscutiremos agora como esses três mecanismos, quando combinados, podem atenuar ou até eliminar os \nefeitos da variação de atraso. Examinaremos duas estratégias de reprodução: atraso de reprodução fixo e atraso \nde reprodução adaptativo.\nAtraso por reprodução fixa\nCom a estratégia do atraso fixo, o receptor tenta reproduzir cada parte exatamente q milissegundos após a \nparte ter sido gerada. Assim, se a marca de tempo de uma parte for t, o receptor reproduz a parte no tempo t + q, \nadmitindo-se que a parte já tenha chegado naquele instante. Os pacotes que chegam após seus tempos de repro-\ndução programados são descartados e considerados perdidos.\nQual é uma boa escolha para q? VoIP pode suportar atrasos de até cerca de 400 ms, embora com valores \nmenores que q a experiência interativa resultante seja mais satisfatória. Por outro lado, caso se escolha um q \nmuito menor do que 400 ms, muitos pacotes podem perder seus tempos de reprodução programados por causa \nda variação de atraso induzida pela rede. Em termos gerais, se grandes variações no atraso fim a fim forem carac-\nterísticas, será preferível usar um q grande; por outro lado, se o atraso for pequeno e as variações também forem \npequenas, será preferível usar um q pequeno, talvez menor do que 150 ms.\nO compromisso entre o atraso de reprodução e a perda de pacotes é ilustrado na Figura 7.7. A figura mostra \nos tempos em que os pacotes são gerados e reproduzidos para uma única rajada de voz. Dois atrasos de reprodu-\nção iniciais distintos são considerados. Como mostram os degraus mais à esquerda do gráfico, o remetente gera \npacotes a intervalos regulares — digamos, a cada 20 ms. O primeiro pacote dessa rajada de voz é recebido no \ntempo r. Como mostra a figura, as chegadas dos pacotes subsequentes não são espaçadas regularmente por causa \nda variação de atraso da rede.\nPara o primeiro esquema de reprodução, o atraso inicial está estabelecido em p – r. Com esse esquema, o \nquarto pacote não chega dentro de seu atraso de reprodução programado e o receptor o considera perdido. \nFigura 7.7  Perda de pacotes para diferentes atrasos por reprodução fixa\nKR 07.07.eps\nAW/Kurose and Ross\nPacotes \ngerados\nTempo\nPacotes\nr p\np'\nEsquema de \nreprodução\np–r\nEsquema de \nreprodução\np'–r\nPacotes \nrecebidos\nReprodução \nperdida\nredes multimídia  455 \nPelo segundo esquema, o atraso inicial de reprodução fixo está estabelecido em p’ – r. Aqui, todos os pacotes \nchegam antes de seu tempo de reprodução e, portanto, não há perda.\nAtraso por reprodução adaptativa\nO exemplo anterior demonstra um importante compromisso entre atraso e perda que surge ao se projetarem \nesquemas de reprodução com atrasos por reprodução fixa. Ao se decidir por um atraso inicial de reprodução grande, \na maioria dos pacotes cumprirá suas programações e, portanto, a perda será insignificante; contudo, para serviços \ninterativos como VoIP, atrasos longos podem se tornar incômodos, se não intoleráveis. Idealmente, gostaríamos que \no atraso de reprodução fosse minimizado mas que mantivesse a limitação de perda abaixo de uns poucos por cento.\nA maneira natural de tratar esse compromisso é estimar o atraso de rede e sua variância e ajustar o atraso de \nreprodução de acordo com o resultado, no início de cada rajada de voz. Esse ajuste adaptativo de atrasos de repro-\ndução no início das rajadas de voz fará que os períodos de silêncio no receptor sejam comprimidos e alongados; \ncontudo, compressão e alongamento de silêncio em pequenas quantidades não são percebidos durante a fala.\nTomando Ramjee [1994] como base, escrevemos agora um algoritmo genérico que o receptor pode usar \npara ajustar seus atrasos de reprodução adaptativamente. Para tal, consideremos:\n\u0007\nti = marca de tempo do i-ésimo pacote = o instante em que o pacote foi gerado pelo remetente\nri = o instante em que o pacote i é recebido pelo receptor\npi = o instante em que o pacote i é reproduzido no receptor\nO atraso de rede fim a fim do i-ésimo pacote é ri – ti. Por causa da variação de atraso da rede, esse atraso \nvariará de pacote a pacote. Seja di a estimativa do atraso de rede médio para a recepção do i-ésimo pacote. Essa \nestimativa é obtida das marcas de tempo como segue:\ndi = (1 – u) di–1 + u (ri – ti)\nsendo u uma constante fixa (por exemplo, u = 0,01). Assim, di é uma média ajustada dos atrasos de rede observa-\ndos r1 – t1,…, ri – ti. A estimativa atribui maior peso aos atrasos de rede verificados mais recentemente do que aos \natrasos de rede ocorridos no passado distante. Esse modelo de estimativa não deve ser tão fora do comum; uma \nideia semelhante é usada para estimar os tempos de viagem de ida e volta no TCP, como discutido no Capítulo 3. \nSeja vi uma estimativa do desvio médio do atraso em relação ao atraso médio estimado. A estimativa também é \nobtida com base nas marcas de tempo:\nvi = (1 – u) vi–1 + u | ri – ti – di |\nAs estimativas di e vi são calculadas para todos os pacotes recebidos, embora somente sejam usadas para deter-\nminar o ponto de reprodução para o primeiro pacote em qualquer rajada de voz.\nUma vez calculadas essas estimativas, o receptor emprega o seguinte algoritmo para a reprodução de paco-\ntes. Se o pacote i for o primeiro de uma rajada de voz, seu tempo de reprodução pi será computado como:\npi = ti + di + Kvi\nsendo K uma constante positiva (por exemplo, K = 4). A finalidade do termo Kvi é estabelecer o tempo de re-\nprodução em um futuro longínquo o suficiente, de modo que apenas uma pequena fração dos pacotes que estão \nchegando na rajada de voz seja perdida devido às chegadas das atrasadas. O ponto de reprodução para qualquer \npacote subsequente em uma rajada de voz é computado como um deslocamento em relação ao ponto no tempo \nem que o primeiro pacote foi reproduzido. Em particular, seja\nqi = pi – ti\na duração do tempo decorrido desde a geração do primeiro pacote da rajada de voz até sua reprodução. Se o pa-\ncote j também pertencer a essa rajada de voz, ele será reproduzido no tempo\npj = tj + qi\n   Redes de computadores e a Internet\n456\nO algoritmo que acabamos de descrever tem perfeito sentido admitindo-se que o receptor possa dizer se um \npacote é o primeiro na rajada de voz. Isto pode ser feito examinando o sinal de energia em cada pacote recebido.\n7.3.3  Recuperação de perda de pacotes\nJá discutimos com algum detalhe como uma aplicação de VoIP pode lidar com a variação de atraso de pa-\ncote. Agora, descreveremos de modo breve diversos esquemas que tentam preservar uma qualidade aceitável de \náudio na presença da perda de pacotes. Eles são denominados esquemas de recuperação de perdas. Definimos \naqui a perda de pacotes em um sentido amplo: um pacote será considerado perdido se nunca chegar ao receptor \nou se chegar após o tempo de reprodução programado. Nosso exemplo do VoIP servirá outra vez de contexto \npara a descrição de esquemas de recuperação de perdas.\nComo mencionamos no início desta seção, em geral a retransmissão de pacotes perdidos não é apropriada \npara aplicações interativas em tempo real como VoIP. Na verdade, a retransmissão de um pacote que perdeu seu \nprazo de reprodução não serve para absolutamente nada. E a retransmissão de um pacote que transbordou de \numa fila de roteador em geral não pode ser feita com a necessária rapidez. Por essas considerações, aplicações \nde VoIP frequentemente usam algum tipo de esquema de prevenção de perda. Dois desses esquemas são a FEC \ne a intercalação.\nForward Error Correction (FEC)\nA ideia básica da FEC é adicionar informações redundantes ao fluxo de pacotes original. Ao custo de au-\nmentar marginalmente a taxa de transmissão do áudio de fluxo, a informação redundante pode ser usada para \nreconstruir aproximações ou versões exatas de alguns pacotes perdidos. Esboçamos, agora, dois mecanismos \nde FEC segundo Bolot [1996] e Perkins [1998]. O primeiro mecanismo envia uma parte redundante codifica-\nda após cada n porções. A parte redundante é obtida por XOR as n porções originais [Shacham, 1990]. Desse \nmodo, se qualquer pacote do grupo de n + 1 pacotes for perdido, o receptor poderá reconstruí-lo integralmen-\nte. Mas, se dois ou mais pacotes de um grupo forem perdidos, o receptor não poderá reconstruir os pacotes \nperdidos. Mantendo n + 1 (o tamanho do grupo) pequeno, uma grande fração dos pacotes perdidos pode ser \nrecuperada, quando a perda não for excessiva. Contudo, quanto menor o tamanho do grupo, maior será o \naumento relativo da taxa de transmissão do fluxo de áudio. Em particular, a taxa de transmissão aumenta de \num fator de 1/n; por exemplo, se n = 3, então a taxa de transmissão aumenta 33%. Além disso, esse esquema \nsimples aumenta o atraso de reprodução, pois o receptor tem que esperar o término da recepção do grupo de \npacotes inteiro antes de poder começar a reproduzir. Se o leitor quiser mais detalhes práticos sobre como a \nFEC funciona para transporte de multimídia, consulte [RFC 5109].\nO segundo mecanismo de FEC é enviar um fluxo de áudio de resolução mais baixo como informação \nredundante. Por exemplo, o emitente pode criar um fluxo de áudio nominal e um fluxo de áudio corres-\npondente de baixa resolução e baixa taxa de bits. (O fluxo nominal poderia ser uma codificação PCM de \n64 kbits/s e o fluxo de qualidade mais baixa, uma codificação GSM de 13 kbits/s.) O fluxo de baixa taxa \n \nde bits é denominado fluxo redundante. Conforme mostra a Figura 7.8, o remetente constrói o enésimo \npacote tomando a enésima parte do fluxo nominal e anexando a ele a parte de ordem (n – 1) do fluxo redun-\ndante. Dessa maneira, sempre que houver perda de pacote não consecutiva, o receptor pode ocultar a perda \nreproduzindo a parte codificada de baixa taxa de bits que chegar com o pacote subsequente. É evidente que \nas porções de baixa taxa de bits oferecem qualidade mais baixa do que as porções nominais. Contudo, um \nfluxo no qual a maioria das porções é de alta qualidade, as partes de baixa qualidade são ocasionais e nenhu-\nma parte perdida oferece boa qualidade geral de áudio. Note que, nesse esquema, o receptor tem de receber \nsomente dois pacotes antes da reprodução, de modo que o aumento no atraso de reprodução é pequeno. \nAlém disso, se a codificação de baixa taxa de bits for muito menor do que a codificação nominal, o aumento \nmarginal da taxa de transmissão será pequeno.\nredes multimídia  457 \nPara enfrentar perdas consecutivas, podemos utilizar uma variação simples. Em vez de anexar apenas a \nparte de baixa taxa de bits de ordem (n – 1) à enésima parte nominal, o remetente pode anexar as porções de \nbaixa taxa de bits de ordens (n – 1) e (n – 2) ou anexar as porções de baixa taxa de bits de ordens (n – 1) e (n – 3) \ne assim por diante. Anexando mais porções de baixa taxa de bits a cada parte nominal, a qualidade do áudio no \nreceptor fica aceitável para uma variedade mais ampla de ambientes de melhor esforço adversos. Por outro lado, \nas porções adicionais aumentam a largura de banda de transmissão e o atraso de reprodução.\nIntercalação\nComo uma alternativa à transmissão redundante, uma aplicação de VoIP pode enviar áudio intercalado. \nComo mostra a Figura 7.9, o remetente rearranja a sequência das unidades de dados de áudio antes da transmis-\nsão, para que unidades originalmente adjacentes fiquem separadas por alguma distância no fluxo transmitido. \nFigura 7.8  Dando carona à informação redundante de qualidade mais baixa\nKR 07.08.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n26p2 Wide x 14p9 Deep\n11/23/11 rossi\n1\n1\n1\n1\n1\n2\n2\n2\n2\n3\n3\nperda\n3\n4\n3\n4\n1\n2\n3\n4\n4\nRedundância\nFluxo \nrecebido\nFluxo \noriginal\nFluxo \nreconstruído\nFigura 7.9  Envio de áudio intercalado\nKR 07.09.eps\nAW/Kurose and Ross\nFluxo \noriginal\nFluxo \nintercalado\nFluxo \nrecebido\nFluxo \nreconstruído\n5\n9\n13\n1\n2\n4\n1\n2\n3\n4\n1\n5\n9\n13\n1\n2\n10 14\n6\n5\n8\n6\n5\n7\n8\n6\n2\n10 14\nperda\n6\n7\n11 15\n3\n10\n12\n9\n10 11 12\n9\n4\n12\n16\n8\n13\n16\n14\n13\n15\n16\n14\n4\n12\n16\n8\n   Redes de computadores e a Internet\n458\nA intercalação pode atenuar o efeito da perda de pacotes. Se, por exemplo, as unidades têm comprimento de 5 ms \ne as porções são de 20 ms (isto é, quatro unidades por parte), então a primeira parte pode conter unidades 1, 5, 9, \n13; a segunda parte pode conter unidades 2, 6, 10, 14, e assim por diante. A Figura 7.9 mostra que a perda de um \núnico pacote de um fluxo intercalado resulta em várias pequenas lacunas no fluxo reconstruído, em vez de em \numa única lacuna grande que ocorreria com um sistema não intercalado.\nA intercalação pode melhorar significativamente a qualidade percebida de um fluxo de áudio [Perkins, 1998]. \nAlém disso, a sobrecarga é baixa. A desvantagem óbvia da intercalação é que ela aumenta a latência, o que limita seu \nuso em aplicações interativas como o VoIP, embora possa funcionar bem para fluxo contínuo de áudio armazenado. \nUma importante vantagem da intercalação é que ela não aumenta as exigências de largura de banda de um fluxo.\nOcultação de erro\nEsquemas de recuperação baseados no receptor tentam produzir um substituto para um pacote perdido \nsemelhante ao original. Como discutido em Perkins [1998], isso é possível desde que os sinais de áudio, em es-\npecial os de voz, exibam grandes índices de semelhança entre si dentro de períodos de tempo reduzidos. Assim, \nessas técnicas funcionam para taxas de perda relativamente pequenas (menos de 15%) e para pacotes pequenos \n(4–40 ms). Quando o comprimento da perda se aproxima do comprimento de um fonema (5–100 ms), essas \ntécnicas causam pane, já que fonemas inteiros podem ser perdidos pelo ouvinte.\nTalvez o modo mais simples de recuperação baseada no receptor seja a repetição de pacotes. Ela substitui pa-\ncotes perdidos por cópias de pacotes que chegaram imediatamente antes da perda. É de baixa complexidade compu-\ntacional e funciona bastante bem. Outro modo de recuperação baseada no receptor é a interpolação, que usa áudio \nantes e após a perda para interpolar um pacote adequado para cobrir a perda. Ela funciona um pouco melhor do \nque a repetição de pacotes, mas é significativamente mais trabalhosa para processamento [Perkins, 1998].\n7.3.4  Estudo de caso: VoIP com Skype\nSkype é uma aplicação extremamente popular, com mais de 50 milhões de contas ativas. Além de fornecer \nserviço de VoIP, o Skype oferece serviços de chamadas de hospedeiro para telefone, de telefone para hospedeiro e \nserviços de videoconferência de hospedeiro para hospedeiro com múltiplos participantes. (Um hospedeiro, neste \ncontexto, é qualquer dispositivo IP conectado à Internet.) O Skype foi adquirida pela Microsoft em 2011 por mais \nde 8 bilhões de dólares.\nComo o protocolo do Skype é proprietário, e todos os controles e pacotes do Skype são criptografados, fica \ndifícil determinar exatamente como o Skype opera. Apesar disso, a partir do site da Internet do Skype e de muitos \nestudos sobre métricas, pesquisadores têm compreendido cada vez melhor como, em geral, o Skype funciona \n[Baset, 2006; Guha, 2006; Chen, 2006; Suh, 2006; Ren, 2006; Zhang X, 2012]. Tanto para voz quanto para vídeo, \nos clientes têm à disposição muitos diferentes codecs, os quais são capazes de codificar a mídia em um gama de \ntaxas e qualidades diferentes. Por exemplo, as taxas de vídeo para Skype medidas variam de 30 kbits/s para uma \nsessão de baixa qualidade até 1 Mbit/s para uma sessão de alta qualidade [Zhang X, 2012]. Normalmente a qua-\nlidade do áudio do Skype é melhor que a dos serviços de telefonia antigos (“POTS” — Plain Old Telephone Ser-\nvice), fornecida pelo sistema de telefonia convencional por fio (os codecs do Skype fazem a amostragem da voz \nnuma velocidade de 16.000 amostras/s ou mais, o que oferece tons de mais qualidade que o POTS, que amostra a \nvoz a 8.000 amostras/s). O Skype padronizou o uso de UDP para o envio de pacotes de áudio e vídeo. No entanto, \nos pacotes de controle são enviados via TCP, e pacotes de mídia são também enviados via TCP quando os firewalls \nbloqueiam fluxo contínuo de UDP. O Skype usa FEC para recuperação de perdas de fluxo contínuo tanto de voz \nquanto de vídeo. A aplicação cliente também adapta os fluxos contínuos de voz e vídeo que ela envia às condições \natuais da rede, através de mudanças na qualidade do vídeo e overhead de FEC [Zhang X, 2012].\nSkype usa técnicas P2P de muitas formas inovadoras, ilustrando de uma maneira bem interessante como P2P \npode ser usado em aplicações que usam distribuição de conteúdo e compartilhamento de arquivos. Assim como \nredes multimídia  459 \nmensagens instantâneas, telefonia por Internet de hospedeiro para hospedeiro é, em sua natureza, uma técnica \nP2P, já que, na essência da aplicação, pares de usuários se comunicam entre si em tempo real. A Skype, porém, \ntambém emprega técnicas P2P para duas outras funções importantes: localização de usuário e travessia de NAT.\nComo apresentado na Figura 7.10, os pares (hospedeiros) na Skype são organizados em uma rede de sobre-\nposição hierárquica, com cada par classificado como um superpar ou um par comum. Skype mantém um índice \nque mapeia os nomes de usuários do Skype com seus endereços IP atuais (e números de porta). Esse índice é dis-\ntribuído para todos os superpares. Quando Alice quer chamar Bob, seu aplicativo cliente Skype procura o índice \ndistribuído para determinar o atual endereço IP de Bob. Como os protocolos Skype são proprietários, ainda não \né conhecido como os mapas de índice são organizados através dos superpares, embora seja muito provável que se \nuse alguma forma de organização de DHT.\nTécnicas de P2P também são usadas nos repasses (relays) do Skype, onde são úteis para estabelecer cha-\nmadas entre hospedeiros em redes domésticas. Muitas configurações de redes domésticas fornecem acesso à \nInternet através de NATs, como foi discutido no Capítulo 4. Lembre-se que um NAT impede que um hospedeiro \nde fora da rede doméstica inicie uma conexão com um hospedeiro dentro da rede doméstica. Mas se os dois \nque desejam se comunicar usarem NATs, então existe um problema — nenhum deles poderá aceitar chamadas \niniciadas pelo outro, o que tornaria a chamada impossível de ser realizada. O uso inteligente de superpares e \nretransmissões pode muito bem resolver este problema. Suponha que, quando Alice se conecta, ela é associada \na um superpar que não está usando NAT e inicia uma sessão com esse superpar. (Como Alice está iniciando a \nsessão, o seu NAT permite essa sessão.) Essa sessão possibilita que Alice e o seu superpar troquem mensagens de \ncontrole. O mesmo acontece para Bob quando ele se conecta. Então, quando Alice quiser chamar Bob, ela infor-\nma o superpar dela, que por sua vez informa o superpar de Bob, que então informa Bob da chamada de Alice para \nele. Se Bob aceitar, os dois superpares selecionam um terceiro superpar que não esteja usando NAT — o par de \nretransmissão — cujo trabalho será retransmitir dados entre Alice e Bob. Os superpares de Alice e Bob então são \norientados a iniciar uma sessão com o retransmissor. Como mostrado na Figura 7.10, Alice então envia pacotes \nde voz para o retransmissor pela conexão estabelecida entre eles (a qual foi iniciada por Alice) e o retransmissor \nentão encaminha esses pacotes pela conexão do retransmissor com Bob (o qual foi iniciado por Bob); pacotes de \nFigura 7.10  Pares no Skype\nKR 07.10.eps\nKurose and Ross\nComputer Networking 6/e\nPar \nchamado\nPar \nchamador\nPar de \nrepasse\nSuperpar\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\n   Redes de computadores e a Internet\n460\nBob para Alice fluem por estes mesmos retransmissores ao contrário. E voilà! — Bob e Alice estabelecem uma \nconexão fim a fim, embora nenhum dos dois possa acessar uma sessão originada de fora da rede doméstica.\nAté agora nossa discussão sobre o Skype tem sido focada nas chamadas envolvendo duas pessoas. Agora, \nvamos examinar chamadas de audioconferência com mais de um participante. Com N > 2 participantes, se \ncada usuário enviar uma cópia do seu fluxo contínuo de áudio para cada um dos outros N – 1 usuários, então \num total de N(N – 1) fluxos contínuos de áudio precisará ser enviado pela rede para suportar a audiocon-\nferência. A fim de reduzir o uso de sua largura de banda, Skype emprega uma técnica de distribuição mais \ninteligente. Especificamente, cada usuário envia seu fluxo continuo de áudio para o promotor da conferência. \nO promotor então combina os fluxos contínuos de áudio em um fluxo contínuo (basicamente adicionando \ntodos os sinais de áudio juntos) e então envia uma cópia de cada fluxo contínuo combinado para cada um dos \noutros N – 1 participantes. Desta maneira, o número de fluxos contínuos é reduzido para 2(N – 1). Para con-\nversações simples de vídeo, envolvendo apenas duas pessoas, o Skype envia a chamada par a par, a menos que \na travessia do NAT seja requerida, e neste caso a chamada é retransmitida através do par que não utiliza NAT, \ncomo descrito antes. Para uma chamada de videoconferência envolvendo N > 2 participantes, pela natureza \ndo meio em que é transmitido o vídeo, o Skype não compõe a chamada em um fluxo contínuo em uma loca-\nlidade e então redistribui o fluxo contínuo para todos os participantes, como faz para chamadas de voz. Em \nvez disso, cada participante do fluxo contínuo de vídeo é direcionado para um cluster de servidor (localizado \nna Estônia, desde 2011), o qual por sua vez redireciona para cada participante os N – 1 fluxos contínuos dos \noutros N – 1 participantes [Zhang X, 2012]. Você pode estar se perguntando por que cada participante envia \numa cópia para o servidor em vez de direcionar uma cópia do seu fluxo contínuo para cada um dos outros \nN – 1 participantes. De fato, para ambas as abordagens, N(N – 1) fluxos contínuos de vídeo estão sendo co-\nletivamente recebidos pelos N participantes na conferência. A razão é a seguinte: como as larguras de banda \ndos enlaces upstream são significativamente menores que as larguras de banda dos enlaces de downstream na \nmaioria dos enlaces de acesso, os enlaces de upstream podem não conseguir suportar os N – 1 fluxos contí-\nnuos com a abordagem P2P.\nSistemas VoIP como Skype, QQ e Google Talk introduzem novas preocupações referentes à privacidade. \nEspecificamente, quando Alice e Bob se comunicam via VoIP, Alice pode descobrir o endereço IP de Bob e então \nusar serviços de geolocalização [MaxMind, 2012; Quova, 2012] para determinar o local atual de Bob e seu ISP (por \nexemplo, seu ISP no trabalho ou em casa). Na verdade, com Skype é possível para Alice bloquear a transmissão \nde determinados pacotes durante o estabelecimento da chamada, e assim ela obtém o endereço IP atual de Bob, \ndigamos, a cada hora, sem que ele saiba que está sendo monitorado e sem estar na lista de contatos de Bob. Além \ndisso, o endereço IP descoberto pelo Skype pode ser correlacionado com o endereço IP encontrado no BitTorrrent, \ne então Alice pode acessar os arquivos que Bob está baixando [LeBlond, 2011]. Mais ainda, é possível decodificar \nem parte uma chamada Skype, fazendo uma análise do tamanho dos pacotes em um fluxo contínuo [White, 2011].\n7.4  \u0007\nProtocolos para aplicações interativas em tempo real\nAplicações interativas em tempo real, incluindo VoIP e videoconferência, são atraentes e muito populares. \nPortanto, não é surpresa que órgãos de padrões, tais como IETF e ITU, tenham se ocupado durante tantos anos (e \nainda se ocupem!) com a produção de padrões para essa classe de aplicações. Tendo à mão padrões apropriados \npara aplicações interativas em tempo real, empresas independentes poderão criar novos produtos atraentes que \ninteragem uns com os outros. Nesta seção estudamos RTP e SIP para aplicações interativas em tempo real. Todos \nos três conjuntos de padrões estão sendo implementados amplamente em produtos do setor.\n7.4.1  Protocolo de tempo real (RTP)\nNa seção anterior, aprendemos que o lado remetente de uma aplicação de VoIP anexa campos de cabeçalho \naos trechos de áudio/vídeo antes de passá-los à camada de transporte. Esses campos contêm números de sequência \nredes multimídia  461 \ne marcas de tempo. Já que a maioria das aplicações de rede multimídia pode usar números de sequência e marcas \nde tempo, é conveniente ter uma estrutura de pacote padronizada que inclua campos para dados de áudio/vídeo, \nnúmeros de sequência e marcas de tempo, bem como outros campos potencialmente úteis. O RTP, definido no RFC \n3550, é um padrão desse tipo. Ele pode ser usado para transportar formatos comuns como PCM, ACC e MP3 para \nsom e MPEG e H.263 para vídeo. O protocolo também pode ser usado para transportar formatos proprietários de \nsom e de vídeo. Hoje, o RTP é amplamente implementado em muitos produtos e protótipos de pesquisa. Também é \ncomplementar a outros importantes protocolos interativos de tempo real, entre eles SIP.\nNesta seção, faremos uma introdução ao RTP. Também aconselhamos o leitor a visitar o site de Henning \nSchulzrinne [Schulzrinne-RTP, 2012], que trata do RTP e traz uma profusão de informações sobre o assunto. O \nleitor também poderá visitar o site da RAT [RAT, 2012], que documenta uma aplicação de VoIP que usa RTP.\nFundamentos de RTP\nO RTP comumente roda sobre UDP. O lado remetente encapsula uma parte de mídia dentro de um pacote \nRTP, em seguida encapsula o pacote em um segmento UDP, e então passa o segmento para o IP. O lado receptor \nextrai o pacote RTP do segmento UDP, em seguida extrai a parte de mídia do pacote RTP e então passa a parte \npara o transdutor para decodificação e apresentação.\nComo exemplo, considere a utilização do RTP para transportar voz. Suponha que a origem de voz esteja \ncodificada (isto é, amostrada, quantizada e digitalizada) em PCM a 64 kbits/s. Suponha também que a aplica-\nção colete os dados codificados em trechos de 20 milissegundos, isto é, 160 bytes por trecho. O lado remetente \nprecede cada parte dos dados de áudio com um cabeçalho RTP que contém o tipo de codificação de áudio, um \nnúmero de sequência e uma marca de tempo. O tamanho do cabeçalho RTP é normalmente 12 bytes. A parte de \náudio, junto com o cabeçalho RTP, forma o pacote RTP. O pacote RTP é, então, enviado para dentro da interface \nde socket UDP. No lado receptor, a aplicação recebe o pacote RTP da interface do seu socket. A aplicação extrai \na parte de áudio do pacote RTP e usa os campos de cabeçalho do pacote RTP para decodificar e reproduzir ade-\nquadamente a parte de áudio.\nSe uma aplicação incorporar o RTP — em vez de um esquema proprietário para fornecer tipo de carga útil, \nnúmero de sequência ou marcas de tempo —, ela interagirá mais facilmente com as outras aplicações de rede \nmultimídia. Por exemplo, se duas empresas diferentes desenvolvem um software de VoIP e ambas incorporam o \nRTP a seu produto, pode haver alguma chance de um usuário que estiver usando um desses produtos conseguir \nse comunicar com alguém que estiver usando o outro produto de VoIP. Na Seção 7.4.2, veremos que o RTP é \nmuito utilizado em conjunto com os padrões de telefonia por Internet.\nDevemos enfatizar que o RTP não fornece nenhum mecanismo que assegure a entrega de dados a tempo \nnem fornece outras garantias de qualidade de serviço (QoS); ele não garante nem mesmo a entrega dos pacotes \nnem impede a entrega de pacotes fora de ordem. Na verdade, o encapsulamento realizado pelo RTP é visto so-\nmente nos sistemas finais. Os roteadores não distinguem datagramas IP que carregam pacotes RTP de datagra-\nmas IP que não o fazem.\nO RTP permite que seja atribuído a cada origem (por exemplo, uma câmera ou um microfone) seu próprio \nfluxo independente de pacotes RTP. Por exemplo, para uma videoconferência entre dois participantes, quatro \nfluxos RTP podem ser abertos — dois para transmitir o áudio (um em cada direção) e dois para transmitir o ví-\ndeo (um em cada direção). Contudo, muitas técnicas de codificação populares — incluindo MPEG1 e o MPEG2 \n— conjugam áudio e vídeo em um único fluxo durante o processo de codificação. Quando áudio e vídeo são \nconjugados pelo codificador, somente um fluxo RTP é gerado em cada direção.\nPacotes RTP não são limitados às aplicações individuais. Eles podem também ser enviados por árvores para \num grupo um-para-muitos e muitos-para-muitos. Para uma sessão para um grupo muitos-para-muitos, todos \nos remetentes e origens da sessão em geral usam o mesmo grupo para enviar seus fluxos RTP. Fluxos para um \ngrupo RTP que formam um conjunto, como fluxos de áudio e vídeo que emanam de vários remetentes em uma \naplicação de videoconferência, pertencem a uma sessão RTP.\n   Redes de computadores e a Internet\n462\nCampos do cabeçalho do pacote RTP\nComo mostra a Figura 7.11, os quatro principais campos de cabeçalho do pacote RTP são os campos de tipo \nda carga útil, número de sequência, marca de tempo e os campos identificadores da origem.\nO campo de tipo de carga útil do pacote RTP tem 7 bits de comprimento. Para um fluxo de áudio, o cam-\npo de tipo de carga útil serve para indicar o tipo de codificação de áudio (por exemplo, PCM, modulação delta \nadaptativa, codificação por previsão linear) que está sendo usado. Se um remetente decidir mudar a codificação \nno meio de uma sessão, ele poderá informar a mudança ao receptor por meio desse campo de tipo de carga útil. \nO remetente pode querer mudar o código para aumentar a qualidade do áudio ou para reduzir a taxa de bits do \nfluxo RTP. A Tabela 7.2 apresenta alguns dos tipos de carga útil de áudio suportados pelo RTP.\nPara um fluxo de vídeo, o tipo de carga útil é usado para indicar o tipo de codificação de vídeo (por exemplo, \nMotion JPEG, MPEG1, MPEG2, H.261). Novamente, o remetente pode mudar a codificação de vídeo durante a sessão. \nA Tabela 7.3 apresenta alguns tipos de carga útil de vídeo suportados pelo RTP\n. Os outros campos importantes são:\n• Campo do número de sequência. O campo do número de sequência tem comprimento de 16 bits. O nú-\nmero de sequência é incrementado de uma unidade a cada pacote RTP enviado e pode ser usado pelo \nreceptor para detectar perda de pacotes e restaurar a sequência de pacotes. Por exemplo, se o lado recep-\ntor da aplicação receber um fluxo de pacotes RTP com uma lacuna entre os números de sequência 86 e \n89, então o receptor saberá que os pacotes 87 e 88 estão faltando. O receptor poderá, então, tentar ocultar \nos dados perdidos.\n• Campo de marca de tempo. O campo de marca de tempo tem 32 bits de comprimento. Ele reflete o instan-\nte da amostragem do primeiro byte no pacote RTP. Como vimos na seção anterior, o receptor pode usar \nmarcas de tempo para eliminar a variação de atraso dos pacotes introduzida na rede e fornecer recepção \nTabela 7.2  Tipos de carga útil de áudio suportados pelo RTP\nNúmero do tipo de carga útil\nFormato de áudio\nTaxa de amostragem\nVazão\n0\nPCM µ-law\n8 kHz\n64 kbits/s\n1\n1016\n8 kHz\n4,8 kbits/s\n3\nGSM\n8 kHz\n13 kbits/s\n7\nLPC\n8 kHz\n2,4 kbits/s\n9\nG.722\n16 kHz\n48-64 kbits/s\n14\nÁudio MPEG\n90 kHz\n—\n15\nG.728\n8 kHz\n16 kbits/s\nTabela 7.3  Alguns tipos de carga útil de vídeo suportados pelo RTP\nNúmero do tipo de carga útil\nFormato de vídeo\n26\nMotion JPEG\n31\nH.261\n32\nVídeo MPEG 1\n33\nVídeo MPEG 2\nFigura 7.11  Pares no Skype\nKR 07.11.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p Wide x 2p1 Deep\n11/23/11 rossi\nTipo de \ncarga útil\nNúmero de \nsequência\nIdentiﬁcador de \nsincronização da origem\nCampos variados\nMarca de \ntempo\nredes multimídia  463 \nsincronizada no receptor. A marca de tempo é derivada de um relógio de amostragem no remetente. \nComo exemplo, para áudio, o relógio de marca de tempo é incrementado de uma unidade para cada \nperíodo de amostragem (por exemplo, a cada 125 µs para um relógio de amostragem de 8 kHz); se a \naplicação de áudio gerar porções consistindo em 160 amostras codificadas, a marca de tempo aumentará \nem 160 para cada pacote RTP enquanto a origem estiver ativa. O relógio de marca de tempo continua a \naumentar a uma taxa constante, mesmo que a origem esteja inativa.\n• Identificador de sincronização da origem (synchronization source identifier — SSRC). O campo SSRC tem \n32 bits de comprimento e identifica a origem do fluxo RTP. Em geral, cada origem de uma sessão RTP \ntem um SSRC distinto. O SSRC não é o endereço IP do remetente, mas um número atribuído aleatoria-\nmente pela origem quando um novo fluxo é iniciado. A probabilidade de que seja atribuído o mesmo \nSSRC a dois fluxos é muito pequena. Se isso acontecer, as duas origens escolherão novos valores SSRC.\n7.4.2  SIP\nO Protocolo de Inicialização de Sessão (Session Initiation Protocol — SIP), definido em [RFC 3261; RFC \n5411], é um protocolo aberto e simples, que faz o seguinte:\n• Provê mecanismos para estabelecer chamadas entre dois interlocutores por uma rede IP. Permite que \nquem chama avise ao que é chamado que quer iniciar uma chamada. Permite que os participantes con-\ncordem com a codificação da mídia. E também permite que encerrem as chamadas.\n• Provê mecanismos que permitem a quem chama determinar o endereço IP atual de quem é chamado. Os \nusuários não têm um endereço IP único, fixo, porque podem receber endereços dinamicamente (usando \nDHCP) e porque podem ter vários equipamentos IP, cada um com um endereço IP diferente.\n• Provê mecanismos para gerenciamento de chamadas, tais como adicionar novos fluxos de mídia, mudar a \ncodificação, convidar outros participantes, tudo durante a chamada, e ainda transferir e segurar chamadas.\nEstabelecendo uma chamada para um endereço IP conhecido\nPara entender a essência do SIP, é melhor examinar um exemplo concreto. Nesse exemplo, Alice está tra-\nbalhando em seu computador e quer chamar Bob, que também está trabalhando no computador dele. Ambos \nos PCs estão equipados com software baseado em SIP para fazer e receber chamadas telefônicas. Nesse exemplo \ninicial, admitiremos que Alice conhece o endereço IP do PC de Bob. A Figura 7.12 ilustra o processo de estabe-\nlecimento de chamada do SIP.\nNa Figura 7.12, vemos que uma sessão SIP começa quando Alice envia a Bob uma mensagem INVITE, que \né parecida com uma mensagem de requisição HTTP. Essa mensagem é enviada por UDP à porta bem conhecida \n5060, que é a porta do SIP. (Mensagens SIP também podem ser enviadas por TCP.) A mensagem INVITE inclui \num identificador para Bob (bob@193.64.210.89), uma indicação do endereço IP atual de Alice, uma indicação \nde que Alice deseja receber áudio, o qual deve ser codificado em formato AVP 0 (PCM codificado com lei µ) e \nencapsulado em RTP e uma indicação de que ela quer receber os pacotes RTP na porta 38060. Após receber a \nmensagem INVITE de Alice, Bob envia uma mensagem de resposta SIP, que é parecida com uma mensagem de \nresposta HTTP. A mensagem de resposta SIP também é enviada à porta SIP 5060. A resposta de Bob inclui um \n200 OK, bem como uma indicação de seu endereço IP, o código e o empacotamento que deseja para recepção e \nseu número de porta para a qual os pacotes de áudio devem ser enviados. Note que, neste exemplo, Alice e Bob \nvão usar mecanismos diferentes de codificação de áudio: Alice deve codificar seu áudio com GSM, ao passo \nque Bob deve fazê-lo com lei µ do PCM. Após receber a resposta de Bob, Alice lhe envia uma mensagem SIP de \nreconhecimento (ACK). Depois dessa transação SIP, Bob e Alice podem conversar. (Para melhor visualização, a \nFigura 7.12 mostra Alice falando depois de Bob, mas, na verdade, eles falariam ao mesmo tempo.) Bob codificará \n   Redes de computadores e a Internet\n464\ne empacotará o áudio como solicitado e enviará os pacotes de áudio para a porta número 38060 no endereço IP \n167.180.112.24. Alice também codificará e empacotará o áudio como solicitado e enviará os pacotes de áudio \npara a porta número 48753 no endereço IP 193.64.210.89.\nCom esse exemplo simples, aprendemos várias características fundamentais do SIP. Primeiro, o SIP é um \nprotocolo “fora da banda”: suas mensagens são enviadas e recebidas em portas diferentes das utilizadas para \nenviar e receber os dados da mídia. Segundo, as próprias mensagens SIP podem ser lidas em ASCII e são pare-\ncidas com mensagens HTTP. Terceiro, o SIP requer que todas as mensagens sejam reconhecidas, portanto, pode \nexecutar sobre UDP ou TCP.\nNeste exemplo, consideremos o que aconteceria se Bob não tivesse um codec PCM µ-law para codificar áu-\ndio. Nesse caso, em vez de responder com 200 OK, Bob responderia com um 600 Not Acceptable e apresentaria \nna mensagem uma lista com todos os codecs que ele pode usar. Então, Alice escolheria um dos codecs da lista \ne enviaria outra mensagem INVITE, agora anunciando o codec escolhido. Bob também poderia apenas rejeitar \na chamada enviando um dos muitos códigos de rejeição de resposta possíveis. (Há muitos desses códigos, entre \neles: “ocupado” (busy), “encerrado” (gone), “pagamento solicitado” (payment required) e “proibido” (forbidden).)\nEndereços SIP\nNo exemplo anterior, o endereço SIP de Bob é sip:bob@193.64.210.89. Contudo, esperamos que muitos \nendereços SIP — se não a maioria — sejam parecidos com endereços de e-mail. Por exemplo, o endereço de Bob \nFigura 7.12  Estabelecimento de chamada SIP quando Alice conhece o endereço IP de Bob\nTempo\nTempo\n167.180.112.24\nINVITE bob@193.64.210.89\nc=IN IP4 167.180.112.24\nm=audio 38060 RTP/AVP 0\n200 OK\nc=In IP4 193.64.210.89\nm=audio 48753 RTP/AVP 3\nCampainha \ndo terminal \nde Bob soa\n193.64.210.89\nÁudio com lei µ\nporta 5060\nporta 5060\nporta 38060\nAlice\nBob\nporta 5060\nporta 48753\nACK\nGSM\nKR 07.12.eps\nAW/Kurose and Ross\nComputer Networking 6/e\nsize:  28p2  x   27p0\n11/23/11 ROSSI ILLUSTRATION\nredes multimídia  465 \npoderia ser sip:bob@domain.com. Quando o dispositivo SIP de Alice enviasse uma mensagem INVITE, esta \nincluiria esse endereço semelhante a um endereço de e-mail; a infraestrutura SIP então rotearia a mensagem ao \ndispositivo IP que Bob está usando no momento (como discutiremos mais adiante). Outras formas possíveis para \no endereço SIP seriam o número de telefone legado de Bob ou simplesmente seu nome próprio, ou seu primeiro \nou segundo sobrenome (admitindo-se que sejam únicos).\nUma característica interessante de endereços SIP é que eles podem ser incluídos em páginas Web, exata-\nmente como endereços de e-mail são incluídos nessas páginas junto com o URL “mail to”\n. Por exemplo, suponha \nque Bob tem uma home page pessoal e quer fornecer um meio para que os visitantes da página entrem e o con-\ntatem. Ele poderia, então, simplesmente incluir o URL sip:bob@domain.com. Quando o visitante clicar sobre o \nURL, a aplicação SIP no dispositivo do visitante será iniciada e uma mensagem INVITE será enviada a Bob.\nMensagens SIP\nNesta curta introdução ao SIP, não abordaremos todos os tipos e cabeçalhos de mensagens. Em lugar disso, \nexaminaremos brevemente a mensagem SIP INVITE, junto com algumas linhas de cabeçalho comuns. Mais uma \nvez, vamos supor que Alice queira iniciar uma chamada VoIP com Bob e, agora, ela conhece só o endereço SIP de \nBob, bob@domain.com, e não o endereço IP do dispositivo que Bob está usando. Então, sua mensagem poderia \nser algo parecido com:\nINVITE sip:bob@domain.com SIP/2.0\nVia: SIP/2.0/UDP 167.180.112.24\nFrom: sip:alice@hereway.com\nTo: sip:bob@domain.com\nCall-ID: a2e3a@pigeon.hereway.com\nContent-Type: application/sdp\nContent-Length: 885\nc=IN IP4 167.180.112.24  \nm=audio 38060 RTP/AVP 0\nA linha INVITE inclui a versão do SIP, assim como uma mensagem de requisição HTTP. Sempre que uma \nmensagem SIP passa por um dispositivo SIP (incluindo o que origina a mensagem), ele anexa um cabeçalho Via, \nque indica o endereço IP do dispositivo. (Veremos, em breve, que uma mensagem INVITE típica passa por mui-\ntos dispositivos SIP antes de chegar à aplicação SIP do usuário que foi chamado.) Semelhante a uma mensagem \nde e-mail, a mensagem SIP inclui uma linha de cabeçalho From e uma linha de cabeçalho To. Inclui também \num Call-ID (identificador de chamadas), que identifica a chamada exclusivamente (semelhante à mensagem-ID \nno e-mail). Inclui uma linha de cabeçalho Content-Type (tipo de conteúdo), que define o formato usado para \ndescrever o conteúdo da mensagem SIP. Inclui também uma linha de cabeçalho Content-Lenght (tamanho de \nconteúdo), que indica o comprimento em bytes do conteúdo na mensagem. Por fim, após um carriage return e \num line feed, a mensagem contém o corpo de informação. Nesse caso, o conteúdo fornece informações sobre o \nendereço IP de Alice e como ela quer receber o áudio.\nTradução de nome e localização de usuário\nNo exemplo da Figura 7.12, admitimos que o dispositivo SIP de Alice conhecia o endereço IP onde Bob \npodia ser contatado. Mas essa premissa é pouco realista, não só porque muitas vezes os endereços IP são \natribuídos dinamicamente com DHCP, mas também porque Bob pode ter vários dispositivos IP (por exem-\nplo, dispositivos diferentes em casa, no trabalho e no carro). Portanto, agora vamos supor que Alice conhece \nsomente o endereço de e-mail de Bob, bob@domain.com, e que esse mesmo endereço é usado para chamadas \nSIP. Nesse caso, Alice precisa obter o endereço IP do dispositivo que o usuário bob@domain.com está usando \nno momento. Para descobri-lo, Alice cria uma mensagem INVITE que começa com INVITE bob@domain.\n   Redes de computadores e a Internet\n466\ncom SIP/2.0, e a envia a um proxy SIP. O proxy responderá com uma resposta SIP que poderá incluir o en-\ndereço IP do dispositivo que bob@domain.com está usando naquele momento. Como alternativa, a resposta \npoderia incluir o endereço IP da caixa postal de voz de Bob, ou poderia incluir um URL de uma página Web \n(que diz “Bob está dormindo. Não perturbe!”). Além disso, o resultado devolvido pelo proxy poderia depen-\nder de quem chama: se a chamada vier da sogra de Bob, ele poderia responder com o URL que aponta para a \npágina do “estou dormindo”!\nAgora, você provavelmente está imaginando como o servidor proxy pode determinar o endereço IP atual \npara bob@domain.com. Para responder a essa pergunta, é preciso dizer algumas palavras sobre um outro dispo-\nsitivo SIP, a entidade registradora SIP. Cada usuário SIP tem uma entidade registradora associada. Sempre que \num usuário lança uma aplicação SIP em um dispositivo, ela envia uma mensagem de registro SIP à entidade re-\ngistradora, informando seu endereço IP atual. Por exemplo, quando Bob lançasse sua aplicação SIP em seu PDA, \na aplicação enviaria uma mensagem semelhante a esta:\nREGISTER sip:domain.com SIP/2.0\nVia: SIP/2.0/UDP 193.64.210.89\nFrom: sip:bob@domain.com\nTo: sip:bob@domain.com\nExpires: 3600\nA entidade registradora monitora o endereço IP atual de Bob. Sempre que ele mudar para um novo \ndispositivo SIP, o novo dispositivo enviará uma mensagem de registro, indicando o novo endereço IP. Além \ndisso, se Bob permanecer no mesmo dispositivo durante um longo período de tempo, o dispositivo enviará \nmensagens de confirmação de registro, indicando que o endereço IP mais recentemente enviado ainda é vá-\nlido. (No exemplo anterior, é preciso enviar mensagens de confirmação a cada 3.600 segundos para manter \no endereço no servidor da entidade registradora.) É importante observar que a entidade registradora é se-\nmelhante a um servidor de nomes DNS com autoridade. O servidor DNS traduz nomes de hospedeiros fixos \npara endereços IP fixos; a entidade registradora SIP traduz identificadores fixos de pessoas (por exemplo, \nbob@domain.com) para endereços IP dinâmicos. Muitas vezes, as entidades registradoras SIP e os proxies SIP \nsão executados no mesmo hospedeiro.\nAgora vamos examinar como o servidor proxy SIP de Alice obtém o endereço IP atual de Bob. Vimos, na \ndiscussão anterior, que o servidor proxy precisa apenas transmitir a mensagem INVITE de Alice à entidade re-\ngistradora/proxy de Bob. Então, a registradora/proxy poderia transmitir a mensagem ao dispositivo SIP atual de \nBob. Por fim, como agora Bob já recebeu a mensagem INVITE de Alice, ele pode enviar a ela uma resposta SIP.\nComo exemplo, considere a Figura 7.13, na qual jim@umass.edu, que está trabalhando em 217.123.56.89, \nquer iniciar uma sessão de voz sobre IP (VoIP) com keith@upenn.edu, trabalhando em 197.87.54.21. São obe-\ndecidas as seguintes etapas: (1) Jim envia uma mensagem INVITE ao proxy SIP de umass. (2) O proxy faz uma \nconsulta DNS para a entidade registradora SIP de upenn.edu (que não é mostrada no diagrama) e então envia a \nmensagem ao servidor da registradora. (3) Como keith@upenn.edu não está mais registrado na entidade regis-\ntradora de upenn, esta envia uma resposta de redirecionamento, indicando que é preciso tentar keith@eurecom.\nfr. (4) O proxy de umass envia uma mensagem INVITE à registradora SIP de eurecom. (5) A registradora de eu-\nrecom conhece o endereço IP de keith@eurecom.fr e repassa a mensagem INVITE ao hospedeiro 197.87.54.21, \nque está executando o cliente SIP de Keith. (6–8) Uma resposta SIP é devolvida por meio de registradoras/proxies \nao cliente SIP em 217.123.56.89. (9) A mídia é enviada diretamente entre os dois clientes. (Também há uma men-\nsagem de reconhecimento SIP, que não é mostrada.)\nNossa discussão sobre o SIP focalizou a inicialização de chamadas para chamadas de voz. Como o SIP é \num protocolo de sinalização para inicializar e encerrar chamadas em geral, ele pode ser usado para chamadas de \nvideoconferência, bem como para sessões de texto. Na verdade, o SIP tornou-se um componente fundamental \nem muitas aplicações de mensagem instantânea. Os leitores que quiserem saber mais sobre esse protocolo devem \nvisitar o site de Henning Schulzrinne [Schulzrinne-SIP, 2012]. Em particular, nesse site você encontrará software \nde código-fonte aberto para clientes e servidores SIP [SIP software, 2012].\nredes multimídia  467 \n7.5  Suporte de rede para multimídia\nNas seções de 7.2 a 7.4, aprendemos como os mecanismos em nível de aplicação, como buffer do cliente, \npré-busca, adaptação da qualidade da mídia à largura de banda disponível, reprodução adaptativa e técnicas de \nmitigação de perda, podem ser usados pelas aplicações de multimídia para melhorar o desempenho de uma apli-\ncação de multimídia. Também aprendemos como as redes de distribuição de conteúdo e as redes de sobreposição \nP2P podem ser usadas para fornecer uma técnica em nível de sistema para entregar conteúdo de multimídia. \nEssas técnicas e abordagens são todas criadas para uso na Internet de melhor esforço de hoje. Na verdade, elas \nestão em uso exatamente porque a Internet oferece apenas uma única classe de serviço de melhor esforço. Mas, \ncomo projetistas de redes de computadores, não podemos evitar a pergunta sobre se a rede (em vez das aplicações \nou apenas da infraestrutura em nível de aplicação) poderia oferecer mecanismos para dar suporte à entrega de \nconteúdo de multimídia. Como veremos em breve, a resposta é, naturalmente, “sim”! Mas também vimos que \ndiversos desses novos mecanismos de rede ainda precisam ser colocados em prática. Talvez isso se deva a sua \ncomplexidade e ao fato de que as técnicas em nível de aplicação, junto com o serviço de melhor esforço e recursos \nde rede corretamente dimensionados (por exemplo, largura de banda), podem de fato oferecer um serviço de \nentrega de multimídia de fim a fim “bom o bastante” (mesmo que não seja perfeito).\nA Tabela 7.4 resume três técnicas gerais para oferecer suporte em nível de rede para aplicações de multimídia.\n• Obtendo o melhor do serviço de melhor esforço. Os mecanismos em nível de aplicação e infraestrutura que \nestudamos nas seções de 7.2 a 7.4 podem ser usados com sucesso em uma rede bem dimensionada, onde \na perda de pacotes e o atraso excessivo de fim a fim raramente ocorrem. Quando são previstos aumentos \nde demanda, os ISPs empregam largura de banda e capacidade de comutação adicionais para continuar \na garantir um desempenho satisfatório com atraso e perda de pacotes [Huang, 2005]. Discutiremos esse \ndimensionamento de rede melhor na Seção 7.5.1.\n• Serviço diferenciado. Desde os primeiros dias da Internet, tem sido previsto que diferentes tipos de trá-\nfego (por exemplo, conforme indicado no campo de tipo de serviço do cabeçalho de pacote IPv4) po-\nderiam ser fornecidos com classes de serviço diversas, em vez de um único serviço de melhor esforço \nFigura 7.13  Inicialização de sessão envolvendo proxies e entidades registradoras SIP\nKR 07.13.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n19p1 Wide x 21p8 Deep\n11/23/11 rossi\n9\n5\n6\n4\n7\n2\n3\n1\n8\nEntidade registradora \nSIP upenn.edu\nProxy SIP \numass.edu\nCliente SIP\n217.123.56.89\nCliente SIP\n197.87.54.21\nEntidade \nregistradora SIP\n   Redes de computadores e a Internet\n468\nabrangente. Com o serviço diferenciado, um tipo de tráfego poderia receber prioridade estrita sobre \noutra classe de tráfego quando os dois tipos forem enfileirados em um roteador. Por exemplo, pacotes \npertencentes a uma aplicação interativa em tempo real poderiam ter prioridade sobre outros pacotes, por \ncausa de restrições de atraso mais rigorosas. A introdução do serviço diferenciado na rede exigirá novos \nmecanismos para marcação de pacotes (indicando a classe de serviço de um pacote), programação de \npacotes e outros. Veremos o serviço diferenciado, e novos mecanismos de rede necessários para executar \nesse serviço, na Seção 7.5.2.\n• Garantias de qualidade de serviço (QoS) por conexão. Com garantias de QoS por conexão, cada instância \nde uma aplicação reserva explicitamente largura de banda de fim a fim e, assim, tem um desempenho \nde fim a fim garantido. Uma garantia rígida significa que a aplicação receberá sua qualidade de serviço \n(QoS) requisitada com certeza. Uma garantia flexível significa que a aplicação receberá sua qualidade de \nserviço requisitada com alta probabilidade. Por exemplo, se um usuário quiser fazer uma chamada VoIP \ndo Hospedeiro A para o Hospedeiro B, a aplicação VoIP do usuário reserva largura de banda explicita-\nmente em cada enlace ao longo de uma rota entre os dois hospedeiros. Porém, permitir que as aplicações \nfaçam reservas e exigir que a rede cumpra as reservas requer algumas grandes mudanças. Primeiro, pre-\ncisamos de um protocolo que, em favor das aplicações, reserve largura de banda do enlace nos caminhos \ndos remetentes aos destinatários. Segundo, precisaremos de novas políticas de escalonamento nas filas \ndo roteador, para que as reservas de largura de banda por conexão possam ser cumpridas. Por fim, para \nfazer uma reserva, as aplicações precisam dar à rede uma descrição do tráfego que elas pretendem enviar, \ne a rede precisará regular o tráfego de cada aplicação para ter certeza de que obedecerá a essa descrição. \nTais mecanismos, quando combinados, exigem software novo e complexo nos hospedeiros e roteadores. \nComo o serviço garantido de QoS por conexão não teve implementação significativa, veremos esses me-\ncanismos apenas rapidamente na Seção 7.5.3.\n7.5.1  Dimensionando redes de melhor esforço\nFundamentalmente, a dificuldade no suporte de aplicações de multimídia surge de seus requisitos de de-\nsempenho rigorosos — pouco atraso, variação de atraso e perda de pacotes de fim a fim — e do fato de que atraso, \nvariação do atraso e perda de pacotes ocorrem sempre que a rede fica congestionada. Uma primeira técnica para \nmelhorar a qualidade das aplicações de multimídia — e que muitas vezes pode ser usada para resolver quase \nqualquer problema onde os recursos são restritos — é apenas “gastar dinheiro no problema” e, desse modo, \n \nTabela 7.4  Três técnicas em nível de rede para dar suporte a aplicações de multimídia\nTécnica\nGranularidade\nGarantia\nMecanismos\nComplexidade\nImplementação no \nmomento\nObtendo o melhor do \nserviço de melhor esforço\ntodo o tráfego tratado \nigualmente\nnenhuma ou flexível\nsuporte da camada \nde aplicação, CDNs, \nsobreposições, provisão \nde recurso em nível \nde rede\nmínima\nem toda a parte\nServiço diferenciado\ndiferentes classes de \ntráfego tratadas de \nformas diferentes\nnenhuma ou flexível\nmarcação, regulação \ne programação de \npacotes\nmédia\nalguma\nGarantias de qualidade \nde serviço (QoS) por \nconexão\ncada fluxo de origem-\ndestino tratado de \nforma diferente\nflexível ou rígida, \numa vez admitido \no fluxo\nmarcação, regulação \ne programação de \npacotes; admissão \ne sinalização de \nchamadas\nleve\npouca\nredes multimídia  469 \nevitar disputa por recursos. No caso da multimídia em rede, isso significa oferecer capacidade de enlace suficiente \natravés da rede para que nunca (ou raramente) ocorra congestionamento, além de consequentes atrasos e perdas \nde pacotes. Com capacidade de enlace suficiente, os pacotes poderiam correr a Internet de hoje sem atraso ou \nperda nas filas. Por muitos pontos de vista, essa é uma situação ideal — aplicações de multimídia funcionariam \nperfeitamente, os usuários estariam satisfeitos e tudo isso poderia ser obtido sem mudanças na arquitetura de \nmelhor esforço da Internet.\nA questão, logicamente, é quanta capacidade é “suficiente” para se conseguir esse paraíso, e se os custos para \nfornecer largura de banda “suficiente” são práticos do ponto de vista comercial para os ISPs. A questão de quanta \ncapacidade oferecer nos enlaces de rede em determinada topologia para alcançar determinado nível de desem-\npenho costuma ser conhecida como provisão de largura de banda. A questão ainda mais complicada de como \nprojetar uma topologia de rede (onde posicionar roteadores, como interconectar roteadores com enlaces e que \ncapacidade atribuir aos enlaces) para alcançar determinado nível de desempenho de fim a fim é um problema \nde projeto de redes que muitas vezes é chamado de dimensionamento de redes. Tanto a provisão de largura \nde banda quanto o dimensionamento de rede são assuntos complexos, bem além do escopo deste livro. Obser-\nvamos aqui, porém, que as seguintes questões precisam ser resolvidas para que se possa prever o desempenho \nem nível de aplicação entre duas extremidades da rede, oferecendo assim capacidade suficiente para atender \naos requisitos de desempenho de uma aplicação.\n• Modelos de demanda de tráfego entre as extremidades da rede. Os modelos podem precisar ser especifi-\ncados no nível de chamada (por exemplo, os usuários “chegando” à rede e executando aplicações) e no \nnível de pacote (por exemplo, pacotes sendo gerados pelas aplicações em curso). Observe que a carga de \ntrabalho pode variar com o tempo.\n• Requisitos de desempenho bem definidos. Por exemplo, um requisito de desempenho para dar suporte ao \ntráfego sensível ao atraso, como uma aplicação de multimídia interativa, poderia ser que a probabilidade \nde o atraso de fim a fim do pacote ser maior que um atraso máximo tolerável seja menor do que algum \nvalor pequeno [Fraleigh, 2003].\n• Modelos para prever o desempenho de fim a fim para determinado modelo de carga de trabalho, e técnicas \npara encontrar uma alocação de largura de banda de custo mínimo que resulte em cumprir todos os requi-\nsitos do usuário. Aqui, os pesquisadores estão ocupados desenvolvendo modelos de desempenho que \npossam quantificar o desempenho para determinada carga de trabalho, e técnicas de otimização para \nachar alocações de largura de banda de custo mínimo cumprindo requisitos de desempenho.\nSabendo que a Internet de melhor esforço de hoje poderia (do ponto de vista da tecnologia) dar suporte \npara o tráfego de multimídia em um nível de desempenho apropriado, se fosse dimensionada para fazer isso, a \nquestão é por que a Internet de hoje não o faz. As respostas são principalmente econômicas e organizacionais. De \num ponto de vista econômico, os usuários estariam dispostos a pagar a seus ISPs o suficiente para que eles insta-\nlem largura de banda suficiente para dar suporte a aplicações de multimídia por uma Internet de melhor esforço? \nAs questões organizacionais talvez sejam ainda mais assombrosas. Observe que um caminho de fim a fim entre \nduas extremidades de multimídia passará pelas redes de vários ISPs. De um ponto de vista organizacional, esses \nISPs estariam dispostos a cooperar (talvez compartilhando receitas) para garantir que o caminho de fim a fim seja \ndimensionado devidamente para dar suporte a aplicações de multimídia? Para obter uma perspectiva sobre essas \nquestões econômicas e organizacionais, consulte Davies [2005]. Para obter uma perspectiva sobre a provisão de \nredes de backbone da camada 1 para dar suporte ao tráfego sensível a atrasos, consulte Fraleigh [2003].\n7.5.2  Fornecendo múltiplas classes de serviço\nTalvez a melhoria mais simples no serviço de melhor esforço da Internet do tipo “tamanho único” atual-\nmente seja dividir o tráfego em classes e fornecer diferentes níveis de serviço para essas classes. Por exemplo, \n   Redes de computadores e a Internet\n470\num ISP poderia querer oferecer uma classe de serviço mais alta para o tráfego de VoIP sensível ao atraso ou tele-\nconferência (e cobrar mais por esse serviço!) do que para o tráfego elástico, como e-mail ou HTTP. Como alter-\nnativa, um ISP pode simplesmente querer oferecer uma qualidade de serviço mais alta para clientes que queiram \npagar mais por esse serviço melhorado. Diversos ISPs de acesso residencial com fio e ISPs de acesso sem fio por \ncelular adotaram esses níveis de serviço em camadas — com assinantes de serviço “platina” recebendo um serviço \nmelhor do que os assinantes “ouro” ou “prata”\n.\nJá estamos acostumados com diferentes classes de serviço em nossas vidas diárias — passageiros de pri-\nmeira classe em companhias aéreas recebem melhor atendimento do que os da classe executiva, que por sua vez \nrecebem melhor atendimento do que aqueles que voam na classe econômica; VIPs recebem entrada imediata em \neventos, enquanto todos os outros esperam na fila; idosos são respeitados em alguns países e recebem assentos \nde honra e a melhor comida em uma mesa. É importante observar que esse serviço diferencial é fornecido entre \nagregações de tráfego, ou seja, entre classes de tráfego, e não entre conexões individuais. Por exemplo, todos os \npassageiros de primeira classe são tratados da mesma forma (sem que qualquer passageiro da primeira classe \nreceba tratamento melhor do que qualquer outro da mesma classe), assim como todos os pacotes VoIP rece-\nberiam o mesmo tratamento dentro da rede, sem depender da conexão fim a fim à qual pertencem. Conforme \nveremos, lidando com um pequeno número de agregações de tráfego, em vez de um número grande de conexões \nindividuais, novos mecanismos de rede exigidos para fornecer um serviço ainda melhor podem ser mantidos \nrelativamente simples.\nOs primeiros projetistas da Internet claramente tinham essa noção de múltiplas classes de serviço em men-\nte. Relembre do campo tipo de serviço (ToS) no cabeçalho IPv4 na Figura 4.13. IEN123 [ISI 1979] descreve o \ncampo ToS, também presente em um ancestral do datagrama IPv4, como segue: \nO tipo de serviço [campo] fornece uma indicação dos parâmetros abstratos da qualidade de serviço dese-\njada. Esses parâmetros devem ser usados para guiar a escolha de parâmetros através de serviço quando um da-\ntagrama estiver sendo transmitido através de uma rede específica. Muitas redes oferecem serviços prioritários, \nos quais de alguma forma reservam mais atenção ao tráfego de prioridade alta do que aos outros.\nAté mesmo há três décadas, a visão de fornecer diferentes níveis de serviço a diferentes níveis de tráfego estava \nevidente. No entanto, levou um longo período para que pudéssemos perceber essa visão.\nCenários motivadores\nVamos começar nossa discussão sobre mecanismos de rede oferecendo várias classes de serviço com alguns \ncenários motivadores.\nA Figura 7.14 mostra um cenário simples de rede em que dois fluxos de pacotes de aplicação se originam \nnos hospedeiros H1 e H2 em uma LAN e são destinados aos hospedeiros H3 e H4 em outra LAN. Os roteadores \nnas duas LANs são ligados por um enlace de 1,5 Mbits/s. Vamos admitir que as velocidades das LANs sejam \nbem mais altas do que 1,5 Mbits/s e vamos focalizar a fila de saída do roteador R1; é aqui que ocorrerão atraso e \nperda de pacotes se a taxa agregada de envio de H1 e H2 exceder 1,5 Mbits/s. Vamos supor ainda que uma apli-\ncação de áudio de 1 Mbit/s (por exemplo, uma chamada de áudio com qualidade de CD) compartilhe o enlace \nde 1,5 Mbits/s entre R1 e R2 com uma aplicação de navegação Web HTTP que está baixando uma página Web \nde H2 para H4.\nNa Internet de melhor esforço, os pacotes de áudio e HTTP são misturados na fila de saída de R1 e (em \ngeral) transmitidos na ordem primeiro a entrar/primeiro a sair (first-in-first-out — FIFO). Nesse cenário, uma \nrajada de pacotes da origem HTTP poderia lotar a fila, fazendo pacotes IP de áudio sofrerem atraso excessivo ou \nperda, por causa do estouro do buffer em R1. Como resolveríamos esse problema potencial? Dado que a aplica-\nção de navegação Web HTTP não tem limitação de tempo, poderíamos, por intuição, dar prioridade estrita aos \npacotes de áudio em R1. Em uma disciplina estrita de escalonamento por prioridade, um pacote IP de áudio no \nbuffer de saída de R1 seria sempre transmitido antes de qualquer pacote HTTP nesse mesmo buffer. O enlace de \nredes multimídia  471 \nR1 a R2 pareceria um enlace dedicado de 1,5 Mbits/s para o tráfego de áudio e o HTTP usaria o enlace de R1 a R2 \nsomente quando não houvesse nenhum tráfego de áudio na fila. Para R1 distinguir os pacotes de áudio dos HTTP \nem sua fila, cada pacote deve ser marcado como pertencente a uma das duas classes de tráfego. Lembre-se de que \nesse era o objetivo original do campo de tipo de serviço (type-of-service — ToS) do IPv4. Por mais óbvio que pare-\nça, esse é, então, nosso primeiro princípio dos mecanismos necessários para oferecer múltiplas classes de tráfego:\n\u0007\nPrincípio 1: A marcação de pacotes permite que um roteador faça a distinção de pacotes pertencentes a \ndiferentes classes de tráfego.\nObserve que, embora nosso exemplo considere um fluxo concorrente de multimídia e elástico, o mesmo prin-\ncípio se aplica ao caso em que classes de serviço platina, ouro e prata são implementadas — um mecanismo de \nmarcação de pacotes ainda é necessário para indicar a classe de serviço à qual um pacote pertence.\nSuponha agora que o roteador saiba que deve dar prioridade a pacotes da aplicação de áudio de 1 Mbit/s. \nComo a velocidade de saída do enlace é de 1,5 Mbits/s, mesmo que os pacotes HTTP recebam prioridade mais \nbaixa, ainda assim receberão, na média, um serviço de transmissão de 0,5 Mbit/s. Mas o que acontece se a aplica-\nção de áudio começar a enviar pacotes a uma taxa de 1,5 Mbits/s ou mais alta (seja com má intenção, seja devido \na um erro de aplicação)? Nesse caso, os pacotes HTTP morrerão por inanição, isto é, não receberão nenhum ser-\nviço no enlace de R1 a R2. Problemas semelhantes ocorreriam se várias aplicações (por exemplo, várias chamadas \nde áudio), todas com a mesma classe de serviço, estivessem compartilhando a largura de banda de um enlace; elas \ntambém poderiam coletivamente arruinar a sessão HTTP. Idealmente, o que se quer é certo grau de isolamento \nentre classes de tráfego, para proteger uma classe da outra. Essa proteção poderia ser implementada em diferentes \nlocais da rede — em todo e qualquer roteador, na primeira entrada na rede ou nos limites da rede entre domínios. \nLogo, este é o nosso segundo princípio:\n\u0007\nPrincípio 2: É desejável fornecer algum grau de isolamento de tráfego entre as classes, para que uma classe \nnão seja afetada adversamente por outra classe de comportamento inadequado.\nExaminaremos diversos mecanismos específicos que fornecem isolamento entre classes de tráfego. Obser-\nvamos aqui que podem ser adotadas duas abordagens amplas. Com a primeira, é possível realizar a regulação de \ntráfego, como mostra a Figura 7.15. Se uma classe ou fluxo de tráfego deve se ajustar a certos critérios (por exem-\nplo, o fluxo de áudio não deve exceder uma taxa de pico de 1 Mbit/s), então um mecanismo de regulação pode ser \nintroduzido para assegurar que esse critério seja, de fato, observado. Se a aplicação regulada se comportar mal, o \nmecanismo de regulação executará alguma ação (por exemplo, descartará ou atrasará pacotes que estão violando \no critério), de modo que o tráfego que está entrando na rede obedeça aos critérios. O mecanismo do tipo “balde \nfurado” (leaky bucket), que examinaremos mais adiante, talvez seja o mecanismo de regulação mais utilizado. \nFigura 7.14  Competindo aplicações de áudio e HTTP\nKR 07.14.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n18p10 Wide x 14p2 Deep\n11/23/11 rossi\nR1\nEnlace de \n1,5 Mbits/s\nR2\nH2\nH1\nH4\nH3\n   Redes de computadores e a Internet\n472\nNa Figura 7.15, o mecanismo de classificação e marcação de pacotes (Princípio 1) e o mecanismo de regulação \n(Princípio 2) estão localizados juntos na borda da rede, seja no sistema final, seja em um roteador de borda.\nUma abordagem alternativa para prover isolamento entre classes de tráfego é deixar para o mecanismo de \nprogramação de pacotes na camada de enlace a tarefa de alocar explicitamente uma parte fixa da largura de banda \nde enlace a cada classe. Por exemplo, a classe de áudio poderia ser alocada em R1 a 1 Mbit/s, e a classe HTTP, \n0,5 Mbit/s. Nesse caso, os fluxos de áudio e HTTP perceberiam um enlace lógico com capacidade de 1 Mbit/s e \n0,5 Mbit/s, respectivamente, conforme mostra a Figura 7.16. Com imposição estrita de alocação de largura de \nbanda na camada de enlace, um fluxo pode usar apenas a quantidade de largura de banda que lhe foi alocada; em \nparticular, ele não pode utilizar largura de banda que não está sendo usada no momento pelas outras aplicações. \nFigura 7.16  Isolamento lógico das classes de tráfego de áudio e HTTP\nKR 07.16.eps\nAW/Kurose and Ross\nR1\nEnlace de \n1,5 Mbits/s\nEnlace lógico \nde 1 Mbit/s\nEnlace lógico \nde 0,5 Mbit/s\nR2\nH2\nH1\nH4\nH3\nFigura 7.15  Regulação (e marcação) das classes de tráfego de áudio e HTTP\nKR 07.15.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n18p9 Wide x 22p3 Deep\n11/23/11 rossi\nR1\nEnlace de 1,5 Mbits/s\nMarcação e \nregulação de pacotes\nMedição e regulação\nMarcas\nR2\nH2\nH1\nLegenda:\nH4\nH3\nredes multimídia  473 \nPor exemplo, se o fluxo de áudio silenciasse (se o interlocutor fizesse uma pausa e não gerasse nenhum pacote de \náudio, por exemplo), ainda assim o fluxo HTTP não conseguiria transmitir mais do que 0,5 Mbit/s pelo enlace \nde R1 a R2, mesmo que a alocação de largura de banda de 1 Mbit/s para o fluxo de áudio não estivesse sendo \nutilizada naquele momento. Como a largura de banda é um recurso do tipo “use ou perca”\n, não há motivo para \nimpedir que o tráfego HTTP use a largura de banda não usada pelo tráfego de áudio. É desejável usar a largura de \nbanda da maneira mais eficiente possível, nunca a desperdiçando quando puder ser usada para outra finalidade. \nEssa consideração origina nosso terceiro princípio:\n\u0007\nPrincípio 3: Ao fornecer isolamento de classes ou fluxos, é desejável que se usem os recursos (por exemplo, \nlargura de banda de enlace e buffers) da maneira mais eficiente possível.\nMecanismos de escalonamento\nLembre-se de que dissemos na Seção 1.3 e na Seção 4.3 que pacotes pertencentes a vários fluxos de rede \nsão multiplexados e enfileirados para transmissão nos buffers de saída associados a um enlace. O modo como \nos pacotes enfileirados são selecionados para transmissão pelo enlace é conhecido como disciplina de escalo-\nnamento do enlace. Vamos agora examinar mais detalhadamente algumas das mais importantes disciplinas de \nescalonamento.\nPrimeiro a entrar/primeiro a sair (FIFO)\nA Figura 7.17 mostra a representação do modelo de enfileiramento para a disciplina de escalonamento de \nenlace primeiro a entrar/primeiro a sair (FIFO). Pacotes que chegam à fila de saída do enlace esperam pela trans-\nmissão se, naquele momento, o enlace estiver ocupado com a transmissão de outro pacote. Se não houver espaço \nsuficiente de buffer para guardar o pacote que chega, a política de descarte de pacotes da fila então determinará \nse o pacote será descartado (perdido) ou se outros serão retirados da fila para dar espaço ao que está chegando. \nEm nossa discussão a seguir, vamos ignorar o descarte de pacotes. Quando um pacote é transmitido integralmen-\nte pelo enlace de saída (isto é, recebe serviço), ele é retirado da fila.\nA disciplina de escalonamento FIFO — também conhecida como FCFS (first-come-first-served — primeiro \na chegar/primeiro a ser atendido) — seleciona pacotes para transmissão pelo enlace na mesma ordem em que \neles chegaram à fila de saída do enlace. Todos estamos familiarizados com as filas FIFO em pontos de ônibus (em \nFigura 7.17  Representação do enfileiramento FIFO\nR1\nEnlace de \n1,5 Mbits/s\nFila na interface \nde saída de R1\nR2\nH2\nH1\nH4\nH3\n   Redes de computadores e a Internet\n474\nparticular na Inglaterra, onde parece que as filas atingiram a perfeição) ou em agências bancárias e outros órgãos, \nonde os clientes que chegam se juntam ao final da fila de espera, permanecem na ordem e então são atendidos \nquando atingem o início da fila.\nA Figura 7.18 mostra a fila FIFO em operação. As chegadas de pacotes são indicadas por setas numeradas \nacima da linha de tempo superior; os números indicam a ordem em que os pacotes chegaram. As saídas de paco-\ntes individuais são mostradas abaixo da linha de tempo inferior. O tempo que um pacote passa no atendimento \n(sendo transmitido) é indicado pelo retângulo sombreado entre as duas linhas de tempo. Por causa da disciplina \nFIFO, os pacotes saem na mesma ordem em que chegaram. Note que, após a saída do pacote 4, o enlace perma-\nnece ocioso (uma vez que os pacotes 1 a 4 já foram transmitidos e retirados da fila) até a chegada do pacote 5.\nEnfileiramento prioritário\nPela regra do enfileiramento prioritário, pacotes que chegam ao enlace de saída são classificados em classes \nde prioridade na fila de saída, como mostra a Figura 7.19. Como discutimos na seção anterior, a classe de prio-\nridade de um pacote pode depender de uma marca explícita que ele carrega em seu cabeçalho (por exemplo, o \nvalor dos bits de ToS em um pacote IPv4), seu endereço IP de origem ou destino, seu número de porta de destino \nou outro critério. Cada classe de prioridade tem em geral sua própria fila. Ao escolher um pacote para transmitir, \na disciplina de enfileiramento prioritário transmitirá um pacote da classe de prioridade mais alta cuja fila não \nesteja vazia (isto é, tenha pacotes esperando transmissão). A escolha entre pacotes da mesma classe de prioridade \né feita, normalmente, pelo método FIFO.\nA Figura 7.20 ilustra a operação de uma fila prioritária com duas classes de prioridade. Os pacotes 1, 3 e 4 \npertencem à classe de alta prioridade e os pacotes 2 e 5, à classe de baixa prioridade. O pacote 1 chega e, encon-\ntrando o enlace vazio, inicia a transmissão. Durante a transmissão do pacote 1, os pacotes 2 e 3 chegam e são \ncolocados nas filas de baixa e de alta prioridade, respectivamente. Após a transmissão do pacote 1, o pacote 3 \nFigura 7.18  A fila FIFO em operação\nKR 07.18.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n32p7 Wide x 9p5 Deep\n11/23/11 rossi\nTempo\nChegadas\nSaídas\nPacote \nem serviço\nTempo\n1\n1\n2\n3\n4\n5\n2\n3\n1\nt = 0\nt = 2\nt = 4\nt = 6\nt = 8\nt = 10\nt = 12\nt = 14\n2\n3\n4\n5\n4\n5\nFigura 7.19  Modelo de enfileiramento prioritário\nChegadas\nSaídas\nFila de baixa prioridade \n(área de espera)\nClassiﬁcar\nFila de alta prioridade \n(área de espera)\nEnlace \n(servidor)\nredes multimídia  475 \n(um pacote de alta prioridade) é selecionado para transmissão, passando à frente do pacote 2 (que, mesmo ten-\ndo chegado primeiro, é de baixa prioridade). Ao término da transmissão do pacote 3, começa a transmissão do \npacote 2. O pacote 4 (de alta prioridade) chega durante a transmissão do pacote 2 (de baixa prioridade). Em uma \ndisciplina de enfileiramento prioritário não preemptiva, a transmissão de um pacote não será interrompida se já \ntiver começado. Nesse caso, o pacote 4 entra na fila para transmissão e começa a ser transmitido após a conclusão \nda transmissão do pacote 2.\nVarredura cíclica e WQF (enfileiramento justo ponderado)\nNa disciplina de enfileiramento por varredura cíclica, pacotes são classificados do mesmo modo que no \nenfileiramento prioritário. Contudo, em vez de haver uma prioridade estrita de serviço entre as classes, um esca-\nlonador de varredura cíclica alterna serviços entre elas. Na forma mais simples desse escalonamento, um pacote \nde classe 1 é transmitido, seguido por um pacote de classe 2, seguido por um pacote de classe 1, seguido por um \npacote de classe 2 e assim por diante. Uma disciplina de enfileiramento de conservação de trabalho nunca permi-\ntirá que o enlace fique ocioso enquanto houver pacotes (de qualquer classe) enfileirados para transmissão. Uma \ndisciplina de varredura cíclica de conservação de trabalho que procura um pacote de determinada classe, mas \nnão encontra nenhum, verifica imediatamente a classe seguinte na sequência da varredura cíclica.\nA Figura 7.21 ilustra a operação de uma fila de duas classes por varredura cíclica. Nesse exemplo, os pacotes \n1, 2 e 4 pertencem à classe 1 e os pacotes 3 e 5, à classe 2. O pacote 1 inicia a transmissão imediatamente após sua \nchegada à fila de saída. Os pacotes 2 e 3 chegam durante a transmissão do pacote 1 e, assim, entram na fila. Após \na transmissão do pacote 1, o escalonador de enlace procura um pacote de classe 2 e, então, transmite o pacote 3. \nApós a transmissão do pacote 3, o escalonador procura um pacote de classe 1 e, então, transmite o pacote 2. Após \na transmissão do pacote 2, o pacote 4 é o único na fila; assim, ele é transmitido imediatamente após o pacote 2.\nUma abstração generalizada do enfileiramento por varredura cíclica que encontrou considerável utiliza-\nção nas arquiteturas com QoS é a denominada disciplina de enfileiramento justo ponderado (weighted fair \nFigura 7.20  Operação do enfileiramento prioritário\nKR 07.20.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n32p3 Wide x 9p6 Deep\n11/23/11 rossi\nChegadas\nSaídas\nPacote \nem serviço\n1\n1\n2\n3\n4\n5\n2\n3\n1\n2\n3\n4\n5\n4\n5\nTempo\nTempo\nt = 0\nt = 2\nt = 4\nt = 6\nt = 8\nt = 10\nt = 12\nt = 14\nFigura 7.21  Operação de enfileiramento de duas classes por varredura cíclica\nKR 07.21.eps\nChegadas\nPacote \nem serviço\n1\n1\n2\n3\n4\n5\n2\n3\n1\n2\n3\n4\n5\n4\n5\nSaídas\nTempo\nTempo\nt = 0\nt = 2\nt = 4\nt = 6\nt = 8\nt = 10\nt = 12\nt = 14\n   Redes de computadores e a Internet\n476\nqueuing — WFQ) [Demers, 1990; Parekh, 1993]. A WFQ é ilustrada na Figura 7.22. Os pacotes que chegam são \nclassificados e enfileirados por classe em suas áreas de espera apropriadas. Como acontece no escalonamento por \nvarredura cíclica, um programador WFQ atende às classes de modo cíclico — atende primeiro à classe 1, depois \nà classe 2 e, em seguida, à classe 3; e então (supondo que haja três classes) repete o esquema de serviço. A WFQ \ntambém é uma disciplina de enfileiramento de conservação de trabalho; assim, ao encontrar uma fila de classe \nvazia, ela imediatamente passa para a classe seguinte na sequência de atendimento.\nA WFQ é diferente da varredura cíclica, pois cada classe pode receber uma quantidade de serviço diferen-\nciado a qualquer intervalo de tempo. Em particular, a cada classe i é atribuído um peso wi. A WFQ garante que, \nem qualquer intervalo de tempo durante o qual houver pacotes da classe i para transmitir, a classe i receberá \numa fração de serviço igual a wi/(\nwj), onde o denominador é a soma de todas as classes que também têm \npacotes enfileirados para transmissão. No pior caso, mesmo que todas as classes tenham pacotes na fila, a classe \ni ainda terá garantido o recebimento de uma fração wi / (\nwj) da largura de banda. Assim, para um enlace com \ntaxa de transmissão R, a classe i sempre conseguirá uma vazão de, no mínimo, R ⋅ wi/(\nwj). Descrevemos a \nWFQ em condições ideais, pois não consideramos o fato de que os pacotes são unidades discretas de dados e \nque a transmissão de um pacote não será interrompida para dar início à transmissão de outro; Demers [1990] e \nParekh [1993] discutem essa questão do empacotamento. Como veremos nas seções seguintes, a WFQ tem um \npapel fundamental nas arquiteturas com QoS. Ela também está disponível nos roteadores fabricados atualmente \n[Cisco QoS, 2012].\nRegulação: o “balde furado”\nUm dos nossos princípios foi que a regulação da taxa com a qual é permitido que um fluxo (vamos supor que \numa unidade de regulação é um fluxo na nossa discussão a seguir) injete pacotes na rede, é um importante meca-\nnismo de QoS. Mas quais características da taxa de pacotes de um fluxo devem ser reguladas? Podemos identificar \ntrês critérios importantes de regulação diferentes entre si pela escala de tempo durante a qual o pacote é regulado:\n• Taxa média. A rede pode desejar limitar a taxa média durante um período de tempo (pacotes por in-\ntervalo de tempo) com a qual os pacotes de um fluxo podem ser enviados para a rede. Uma questão \ncrucial aqui é o intervalo de tempo durante o qual a taxa média será regulada. Um fluxo cuja taxa \nmédia está limitada a 100 pacotes por segundo é mais restringido do que uma origem limitada a 6.000 \npacotes por minuto, mesmo que ambos tenham a mesma taxa média durante um intervalo de tempo \nlongo o suficiente. Por exemplo, a última limitação permitiria que um fluxo enviasse 1.000 pacotes em \ndado intervalo de tempo de um segundo de duração, enquanto a limitação anterior desautorizaria esse \ncomportamento de envio.\n• Taxa de pico. Enquanto a limitação da taxa média restringe a quantidade de tráfego que pode ser enviada \npara a rede durante um período de tempo relativamente longo, uma limitação de taxa de pico restringe o \nFigura 7.22  Enfileiramento justo ponderado (WFQ)\nKR 07.22.eps\nAW/Kurose and Ross\nClassiﬁcar \nchegadas\nSaídas\nw1\nw2\nw3\nEnlace\nredes multimídia  477 \nnúmero máximo de pacotes que podem ser enviados durante um período de tempo mais curto. Usando \no mesmo exemplo anterior, a rede pode regular um fluxo a uma taxa média de 6.000 pacotes por minuto \ne, ao mesmo tempo, limitar a taxa de pico do fluxo a 1.500 pacotes por segundo.\n• Tamanho da rajada. A rede também pode limitar o número máximo de pacotes (a “rajada” de pacotes) \nque podem ser enviados para dentro dela durante um intervalo curtíssimo de tempo. No limite, à me-\ndida que o comprimento do intervalo se aproxima de zero, o tamanho da rajada limita o número de \npacotes que podem ser enviados instantaneamente para a rede. Mesmo que seja fisicamente impossível \nenviar vários pacotes para a rede instantaneamente (afinal, cada enlace tem uma taxa de transmissão \nfísica que não pode ser ultrapassada!), a abstração de um tamanho máximo de rajada é útil.\nO mecanismo de balde furado (leaky bucket) é uma abstração que pode ser usada para caracterizar esses li-\nmites da regulação. Como mostra a Figura 7.23, um balde furado é um balde que pode conter até b permissões. As \npermissões são adicionadas da seguinte forma. Novas permissões, que potencialmente seriam adicionadas ao bal-\nde, estão sempre sendo geradas a uma taxa de r permissões por segundo. (Para facilitar, nesse caso admitimos que a \nunidade de tempo seja 1 s.) Se o balde estiver cheio com menos de b permissões quando for gerada uma permissão, \nesta será adicionada ao balde; caso contrário, será ignorada e o balde permanecerá cheio com as b permissões.\nVamos considerar agora como o balde furado pode ser usado para regular um fluxo de pacotes. Suponha \nque, antes de um pacote ser transmitido para a rede, primeiro ele tenha de retirar uma permissão de dentro do \nbalde de permissões. Se este estiver vazio, o pacote terá de esperar por uma permissão. (Uma alternativa é o \npacote ser descartado, mas não vamos considerar essa opção aqui.) Vamos considerar agora como esse compor-\ntamento regula o fluxo de tráfego. Como pode haver no máximo b permissões no balde, o tamanho máximo da \nrajada para um fluxo regulado pela técnica do balde furado é b pacotes. Além disso, como a taxa de geração de \npermissões é r, o número máximo de pacotes que pode entrar na rede para qualquer intervalo de tempo t \né rt + b. Assim, a taxa de geração de permissões, r, serve para limitar a taxa média a longo prazo com a qual pa-\ncotes podem entrar na rede. Também é possível usar baldes furados (especificamente, dois baldes em série) para \nregular a taxa de pico de um fluxo e a taxa média a longo prazo; veja os exercícios ao final deste capítulo.\nBalde furado + enfileiramento justo ponderado = máximo atraso provável em uma fila\nVamos fechar nossa discussão sobre escalonamento e regulação mostrando como os dois podem ser com-\nbinados para oferecer um limite sobre o atraso na fila de um roteador. Vamos considerar o enlace de saída de \num roteador que multiplexa n fluxos, cada um regulado por um balde furado com parâmetros bi e ri, i = 1, … n, \nFigura 7.23  O regulador do balde furado\nKR 07.23.eps\nPara a rede\nPacotes\nRetirar \npermissão\nÁrea de \nespera de \npermissão\nBalde contém \naté b permissões\nr permissões/segundo\n   Redes de computadores e a Internet\n478\nusando o escalonamento WFQ. Usamos aqui o termo fluxo de modo um pouco impreciso para denominar os \nconjuntos de pacotes que não são distinguidos uns dos outros pelo escalonador. Na prática, um fluxo pode conter \no tráfego de uma única conexão fim a fim ou um conjunto de muitas conexões desse tipo; veja a Figura 7.24.\nLembre-se de que mencionamos, em nossa discussão sobre a WFQ, que cada fluxo i tem a garantia de rece-\nber uma parcela da largura de banda do enlace igual a, no mínimo, R ∙ wi /(\nwj), sendo R a taxa de transmissão \ndo enlace em pacotes por segundo. Qual é, então, o atraso máximo que sofrerá um pacote enquanto espera ser \natendido na WFQ (isto é, após ter passado pelo balde furado)? Vamos considerar o fluxo 1. Suponha que o bal-\nde de permissões do fluxo 1 esteja de início cheio. Uma rajada de b1 pacotes então chega ao regulador de balde \nfurado para o fluxo 1. Os pacotes retiram todas as permissões do balde (sem esperar) e, em seguida, juntam-se à \nárea de espera WFQ para o fluxo 1. Como esses b1 pacotes são atendidos a uma taxa de, no mínimo, R ∙ wi /(\nwj) \npacotes por segundo, o último deles sofrerá um atraso máximo, dmáx, até que sua transmissão seja concluída, onde\nA razão dessa fórmula é que, se houver b1 pacotes na fila e eles estiverem sendo atendidos (retirados) na fila \na uma taxa de, no mínimo, R ∙ wi /(\nwj) por segundo, então a quantidade de tempo até que o último bit do últi-\nmo pacote seja transmitido não poderá ser maior do que b1/(R ∙ wi /(\nwj)). Um exercício ao final deste capítulo \nsolicita que você demonstre que, se r1 < R ∙ wi /(\nwj), então dmax é o atraso máximo que qualquer pacote do fluxo \n1 pode sofrer na fila WFQ.\n7.5.3  Diffserv\nTendo visto a motivação, os princípios e os mecanismos específicos para oferecer múltiplas classes de serviço, \nvamos encerrar nosso estudo das técnicas para oferecer múltiplas classes de serviços com um exemplo — a arquite-\ntura Diffserv [RFC 2475; Kilkki, 1999]. Diffserv oferece diferenciação de serviço — isto é, a capacidade de lidar com \nas diversas classes de tráfego de diferentes modos na Internet, de modo escalável. A necessidade da escalabilidade \nvem do fato que centenas de milhares de fluxos simultâneos de tráfego de origem-destino estão presentes em um \nroteador backbone. Veremos de modo breve que essa necessidade é satisfeita ao colocarmos uma funcionalidade \ndentro do núcleo da rede, com operações de controle mais complexas sendo implementadas na borda da rede.\nVamos começar com a rede simples mostrada na Figura 7.25. Descreveremos um possível uso da arquite-\ntura Diffserv; outras variações são possíveis, como descreve o RFC 2475. A arquitetura Diffserv consiste em dois \nconjuntos de elementos funcionais:\nFigura 7.24  n fluxos multiplexados com balde furado com escalonamento WFQ\nKR 07.24.eps\nb1\nr1\nw1\nwn\nbn\nrn\nredes multimídia  479 \n• Funções de borda: classificação de pacotes e condicionamento de tráfego. Na borda de entrada da rede (isto \né, ou em um hospedeiro habilitado para Diffserv que gera o tráfego, ou no primeiro roteador habilitado \npara Diffserv pelo qual o tráfego passa), os pacotes que chegam são marcados. Mais especificamente, o \ncampo Differentiated Service (DS) do cabeçalho do pacote IPv4 ou IPv6 é definido para algum valor \n[RFC 3260]. A definição do campo DS teve por finalidade substituir as definições mais antigas do campo \nde tipo de serviço do IPv4 e os campos de classe de tráfego do IPv6 que discutimos no Capítulo 4. Por \nexemplo, na Figura 7.25, os pacotes que estão sendo enviados de H1 para H3 poderiam ser marcados em \nR1, ao passo que os pacotes enviados de H2 para H4 poderiam ser marcados em R2. A marca que um pa-\ncote recebe identifica a classe de tráfego à qual ele pertence. Assim, diferentes classes de tráfego receberão \nserviços diferenciados dentro do núcleo da rede.\n• Função central: envio. Quando um pacote marcado com DS chega a um roteador habilitado para Diffserv, \nele é repassado até seu próximo salto de acordo com o comportamento por salto (PHB) associado à classe \ndo pacote. O comportamento por salto influencia a maneira pela qual os buffers e a largura de banda de \nenlace de um roteador são compartilhados entre as classes de tráfego concorrentes. Um dogma crucial \nda arquitetura Diffserv é que o comportamento por salto do roteador se baseará somente nas marcas dos \npacotes, isto é, na classe de tráfego a que o pacote pertence. Assim, se os pacotes que estão sendo envia-\ndos de H1 para H3 na Figura 7.25 receberem a mesma marca dos que estão sendo enviados de H2 para \nH4, os roteadores da rede tratarão esses pacotes como um agregado, sem distinguir se eles se originam \nde H1 ou H2. Por exemplo, R3 não faria nenhuma distinção entre pacotes de H1 e H2 ao transmiti-los \na R4. Portanto, a arquitetura de serviço diferenciado evita a necessidade de manter o estado do roteador \npara pares origem–destino individuais — uma consideração importante para atender aos requisitos de \nescalabilidade do Diffserv.\nUma analogia poderia ser útil neste ponto. Em muitos eventos sociais de grande escala (por exemplo, uma \nrecepção com muitos convidados, uma boate ou discoteca, um concerto ou uma partida de futebol), as pessoas \nque participarão do evento adquirem algum tipo de entrada. Há ingressos VIP para pessoas muito importantes; \nhá bilhetes para maiores de 18 anos (por exemplo, para eventos em que serão servidas bebidas alcoólicas), há \ncredenciais que dão direito a visitar o camarim dos artistas; há ingressos especiais para repórteres e imprensa em \ngeral; há até mesmo um ingresso comum para o cidadão comum. Esses ingressos são, em geral, adquiridos em \nbilheterias, isto é, na borda do evento. E é aqui, na borda, que são realizadas as operações que requerem intensa \nFigura 7.25  Exemplo simples de rede Diffserv\nKR 07.25.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n25p7 Wide x 17p8 Deep\n11/23/11 rossi\nR4\nRoteador de borda\nLegenda:\nRoteador de núcleo\nR2\nR1\nR6\nR7\nR3\nR5\nH1\nH2\nH4\nH3\nR2\nR3\n   Redes de computadores e a Internet\n480\natividade computacional, como pagar pelo ingresso, verificar o tipo adequado de ingresso, comparar o ingresso \ncom um documento de identidade. Além disso, pode haver um limite para o número de pessoas de um tipo que \npoderão entrar no evento. Se houver esse limite, as pessoas talvez tenham de esperar antes de entrar. Uma vez lá \ndentro, o tipo de ingresso determina o tipo de serviço diferenciado recebido nos diversos locais do evento — um \nVIP recebe bebidas gratuitas, mesa melhor, refeição gratuita, dispõe de salas exclusivas e recebe serviço especial, \nao passo que uma pessoa comum não pode entrar em certas áreas, paga sua bebida e recebe apenas o serviço \nbásico. Em ambos os casos, o serviço recebido no evento depende apenas do tipo de ingresso da pessoa. Além \ndisso, todas as pessoas pertencentes à mesma classe recebem tratamento igual.\nA Figura 7.26 apresenta uma visão lógica da função de classificação e marcação no roteador de borda. Os \npacotes que chegam ao roteador de borda são primeiro classificados. O classificador seleciona os pacotes com \nbase nos valores de um ou mais campos de cabeçalho de pacote (por exemplo, endereço de origem, endereço de \ndestino, porta de origem, porta de destino e ID de protocolo) e dirige o pacote à função de marcação apropriada. \nComo já dissemos, a marca de um pacote é carregada dentro do campo DS no cabeçalho de pacote.\nEm alguns casos, um usuário final pode ter concordado em limitar sua taxa de envio de pacotes conforme \num perfil de tráfego declarado. Esse perfil poderia conter um limite à taxa de pico, bem como às rajadas do fluxo \nde pacotes, como já antes quando tratamos do mecanismo de balde furado. Enquanto o usuário enviar pacotes \npara a rede de um modo que esteja de acordo com o perfil de tráfego negociado, os pacotes receberão sua marca \nde prioridade e serão transmitidos ao longo de sua rota até o destino. Por outro lado, se o perfil de tráfego for vio-\nlado, pacotes que estão fora do perfil poderão ser marcados de modo diferente, ajustados (por exemplo, atrasados \nde modo que seja observada a limitação da taxa máxima) ou descartados na borda da rede. O papel da função de \nmedição mostrada na Figura 7.26 é comparar o fluxo de entrada de pacotes com o perfil de tráfego negociado. A \ndecisão de imediatamente remarcar, repassar, atrasar ou descartar um pacote é uma questão de política determi-\nnada pelo administrador da rede e não está especificada na arquitetura Diffserv.\nAté aqui, examinamos as funções de marcação e regulação da arquitetura Diffserv. O segundo componente \nfundamental dessa arquitetura envolve o comportamento por salto (PHB — per-hop behavior) realizado pelos \nroteadores habilitados para Diffserv. O PHB é definido de maneira cuidadosa, se bem que um tanto enigmática, \ncomo “uma descrição do comportamento de repasse de um nó Diffserv, que possa ser observado externamente, \naplicado a um comportamento agregado Diffserv em particular” [RFC 2475]. Explorando essa definição mais a \nfundo, podemos ver diversas considerações importantes nela embutidas:\n• Um PHB pode resultar no recebimento de diferentes desempenhos por diferentes classes de tráfego (isto \né, comportamentos de repasse diferentes que possam ser observados externamente).\n• Embora um PHB defina diferenças de desempenho (comportamento) entre classes, ele não determina \nnenhum mecanismo específico para conseguir esses comportamentos. Desde que os critérios de de-\nsempenho observados externamente sejam cumpridos, quaisquer mecanismos de execução e quaisquer \nFigura 7.26  Exemplo de uma rede Diffserv simples\nKR 07 26\nPacotes\nDescarte\nClassiﬁcador\nMarcador\nEnvio\nConﬁgurador/\nDescartador\nMedidor\nredes multimídia  481 \npolíticas de alocação de buffer/largura de banda podem ser usados. Por exemplo, um PHB não exigiria \nque fosse utilizada uma disciplina de enfileiramento de pacotes em particular (por exemplo, um en-\nfileiramento prioritário versus um enfileiramento WFQ versus um enfileiramento FCFS) para atingir \ndeterminado comportamento. O PHB é o fim para o qual os mecanismos de alocação e implementação \nde recursos são os meios.\n• As diferenças de comportamento devem ser observáveis e, por conseguinte, mensuráveis.\nForam definidos dois PHBs: um PHB de repasse acelerado (expedited forwarding — EF) [RFC 3246] e um \nPHB de repasse assegurado (assured forwarding — AF) [RFC 2597]. O PHB de repasse acelerado especifica que \na taxa de partida de uma classe de tráfego de um roteador deve ser igual ou maior do que uma taxa configurada. \nO PHB de repasse assegurado divide o tráfego em quatro classes e garante, a cada classe AF, o fornecimento de \nalguma quantidade mínima de largura de banda e de buffer.\nVamos encerrar nossa discussão do Diffserv com algumas observações com relação ao seu modelo de servi-\nço. Primeiro, admitimos que ele é disponibilizado dentro de um único domínio administrativo. O caso mais típico \né o do serviço fim a fim, que tem de ser configurado para vários ISPs localizados entre os sistemas finais comu-\nnicantes. Para prover serviço Diffserv fim a fim, todas os ISPs entre o sistema final devem não apenas prover esse \nserviço, mas, acima de tudo, cooperar e fazer acordos para oferecer aos clientes finais um serviço verdadeiramente \nfim a fim. Sem esse tipo de cooperação, ISPs que vendem serviço Diffserv diretamente a clientes terão sempre de \nse justificar perante eles dizendo: “Sim, sabemos que você pagou um extra, mas não temos um acordo de serviço \ncom o ISPs que descartou e adiou o seu tráfego. Pedimos desculpas pelas muitas lacunas na sua chamada VoIP!”\n. \nSegundo, se o Diffserv estivesse mesmo disponível e a rede trabalhasse apenas sob carga moderada, na maior parte \ndo tempo não se perceberia nenhuma diferença entre um serviço de melhor esforço e um serviço Diffserv. Na \nverdade, hoje, o atraso fim a fim em geral é dominado pelas taxas de acesso e saltos de roteadores e não por atrasos \nde fila em roteadores. Imagine o infeliz cliente do Diffserv, que pagou a mais por um serviço diferenciado, mas \ndescobre que o serviço de melhor esforço oferecido por outros quase sempre apresenta o mesmo desempenho!\n7.5.4  \u0007\nGarantias de QoS por conexão: reserva de recurso e admissão de \nchamada\nNa seção anterior, vimos que a marcação de pacotes e regulação, isolamento de tráfego, e escalonamento \nem nível de enlace podem prover uma classe de serviço com melhor desempenho do que outra. Em certas dis-\nciplinas de escalonamento, como escalonamento de prioridade, as classes inferiores de tráfego são basicamente \n“invisíveis” à classe de tráfego com a prioridade mais alta. Com dimensionamento adequado da rede, a classe \nde serviço mais alta pode, de fato, atingir taxas de perda de pacote e atraso baixíssimas — basicamente um de-\nsempenho semelhante ao circuito. Mas a rede pode garantir que um fluxo contínuo em uma classe de tráfego de \nalta prioridade continuará a receber tal atendimento pela duração do fluxo usando apenas os mecanismos que \ndescrevemos até agora? Não pode. Nesta seção, veremos por que mecanismos e protocolos de rede adicionais são \nainda necessários quando uma garantia de serviço fixa é oferecida a conexões individuais.\nVamos voltar ao cenário da Seção 7.5.2 e considerar duas aplicações de áudio de 1 Mbit/s que transmitem \nseus pacotes através do enlace de 1,5 Mbits/s, conforme ilustrado na Figura 7.27. A taxa de dados combinada \ndos dois fluxos (2 Mbits/s) excede a capacidade do enlace. Mesmo com classificação e marcação, isolamento de \nfluxos e o compartilhamento de largura de banda não utilizada (que não existe nenhum), este é sem dúvida um \ncaso perdido. Simplesmente não existe largura de banda suficiente para acomodar as necessidades de ambas as \naplicações ao mesmo tempo. Se as duas aplicações compartilharem igualmente a largura de banda, cada uma \nperderia 25% de seus pacotes transmitidos. Essa QoS é tão inaceitavelmente baixa que ambas as aplicações de \náudio são completamente inutilizáveis: não há necessidade sequer de transmitir quaisquer pacotes de áudio, em \nprimeiro lugar.\n   Redes de computadores e a Internet\n482\nSabendo que essas duas aplicações, na Figura 7.27, não podem ser atendidas simultaneamente, o que a rede \ndeve fazer? Permitir que as duas prossigam com uma QoS baixa desperdiça os recursos da rede em fluxos de \naplicação que, no fim, não oferecem utilidade alguma ao usuário final. A resposta, felizmente, está evidente — \num dos fluxos de aplicação deve ser bloqueado (ou seja, deve ter o acesso negado à rede), enquanto o outro deve \nprosseguir, usando o 1 Mbit/s inteiro necessário pela aplicação. A rede telefônica é um exemplo de uma rede que \nrealiza tal bloqueio de chamada — se os recursos necessários (um circuito fim a fim no caso de uma rede telefô-\nnica) não podem ser alocados para a chamada, ela é então bloqueada (impedida de entrar na rede) e o usuário \nrecebe um sinal de ocupado. Em nosso exemplo, não há ganho em permitir que um fluxo entre na rede se ele não \nvai receber uma QoS suficiente para ser considerada utilizável. De fato, há um custo em admitir um fluxo que \nnão recebe a QoS necessária, pois os recursos da rede estão sendo usados para suportar um fluxo que não oferece \nutilidade ao usuário final.\nAdmitindo ou bloqueando de modo explícito os fluxos com base em suas necessidades de recursos e nas \nnecessidades dos fluxos já admitidos, a rede pode garantir que os fluxos admitidos serão capazes de receber sua \nQoS solicitada. Implícita com a necessidade de prover uma QoS garantida a um fluxo está a necessidade do fluxo \nde declarar seus requisitos de QoS. O processo de um fluxo declarar seu requisito de QoS e a rede aceitar o fluxo \n(na QoS solicitada) ou bloqueá-lo é denominado processo de admissão de chamada. Este, então, é o nosso quar-\nto princípio (além dos três anteriores da Seção 7.5.2) em relação aos mecanismos necessários para prover a QoS.\n\u0007\nPrincípio 4: Se recursos suficientes nem sempre estiverem disponíveis e a QoS tiver de ser garantida, é necessá-\nrio um processo de admissão de chamada no qual os fluxos declaram seus requisitos de QoS e, então, são admi-\ntidos à rede (na QoS solicitada) ou bloqueados da rede (se a QoS solicitada não puder ser fornecida pela rede).\nNosso exemplo motivador na Figura 7.27 enfatiza a necessidade de diversos novos mecanismos e protocolos \nde rede, se uma chamada (um fluxo fim a fim) tiver de garantir determinada qualidade de serviço uma vez iniciada:\n• Reserva de recurso. A única maneira de garantir que uma chamada tenha os recursos (largura de banda \nde enlace, buffers) necessários para satisfazer a QoS desejada é alocar explicitamente esses recursos à \nchamada — um processo conhecido na linguagem de redes como reserva de recursos. Uma vez que os \nrecursos são reservados, a chamada possui acesso sob demanda a esses recursos por toda a sua duração, \nindependentemente das demandas de outras chamadas. Se uma chamada reserva e recebe uma garantia \nde x Mbits/s de largura de banda de enlace, e nunca transmite a uma taxa maior do que x, a chamada terá \num desempenho sem perda e sem atraso.\n• Admissão de chamada. Se os recursos forem reservados, então a rede deve ter um mecanismo de cha-\nmadas para solicitar e reservar recursos. Visto que os recursos não são infinitos, uma chamada que faz \numa solicitação de admissão de chamada terá sua admissão negada, ou seja, bloqueada, se os recursos \nsolicitados não estiverem disponíveis. Tal admissão de chamada é realizada pela rede telefônica — so-\nFigura 7.27  \u0007\nDuas aplicações de áudio concorrentes sobrecarregando o enlace de R1 a R2\nKR 07.27.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n19p7 Wide x 12p10 Deep\n1/23/11,  11/29/11 rossi\nR1\nEnlace de 1,5 Mbits/s\nÁudio de \n1 Mbit/s\nÁudio de \n1 Mbit/s\nR2\nH2\nH1\nH4\nH3\nredes multimídia  483 \nlicitamos recursos quando discamos um número. Se os circuitos (compartimentos TDMA) necessários \npara completar a chamada estiverem disponíveis, os circuitos são alocados e a chamada é concluída. Se \nos circuitos não estiverem disponíveis, então a chamada é bloqueada e recebemos um sinal de ocupado. \nUma chamada bloqueada pode tentar novamente ganhar admissão à rede, mas não tem permissão para \nenviar tráfego à rede até ter completado com sucesso o processo de admissão de chamada. Claro, um ro-\nteador que aloca uma largura de banda de enlace não deve alocar mais do que está disponível no enlace. \nEm geral, uma chamada pode reservar somente uma fração da largura de banda do enlace, e um roteador \npode alocar sua largura de banda do enlace para mais de uma chamada. Entretanto, a soma de largura \nde banda alocada a todas as chamadas deve ser menor do que a capacidade do enlace, se forem dadas \ngarantias fixas de qualidade de serviço.\n• Sinalização do estabelecimento de chamada. O processo de admissão de chamada descrito acima exige \nque uma chamada seja capaz de reservar recursos suficientes em todo e qualquer roteador da rede em seu \ncaminho entre os remetentes e os destinatários para garantir que sua exigência da QoS fim a fim seja satis-\nfeita. Cada roteador deve determinar os recursos locais necessários pela sessão, considerar as quantidades \nde seus recursos que já estão comprometidos com outras sessões em andamento e determinar se ele possui \nrecursos suficientes para satisfazer a exigência da QoS por salto da sessão naquele roteador sem violar as \ngarantias de QoS feitas a uma sessão que já foi admitida. Um protocolo de sinalização é necessário para \ncoordenar essas diversas atividades — a alocação por salto dos recursos locais, bem como a decisão geral \nde fim a fim de se a ligação foi capaz ou não de reservar recursos necessários em todo e qualquer roteador \nno caminho fim a fim. Esse é o trabalho do protocolo de estabelecimento de chamada, como mostra a \nFigura 7.28. O protocolo RSVP [Zhang, 1993; RFC 2210] foi proposto para essa finalidade dentro da ar-\nquitetura da Internet, para oferecer garantias de qualidade de serviço. Em redes ATM, o protocolo Q2931b \n[Black, 1995] transporta essa informação entre os comutadores da rede ATM e o ponto final.\nApesar de uma tremenda quantidade de pesquisa e desenvolvimento, e até mesmo produtos que oferecem \ngarantias de qualidade de serviço por conexão, quase não tem havido implementação estendida desses serviços. \nHá muitas razões possíveis. Primeiro, e mais importante, pode ser que os mecanismos simples em nível de apli-\ncação, que estudamos nas seções de 7.2 a 7.4, combinados com o dimensionamento apropriado da rede (Seção \n7.5.1), ofereçam um serviço de rede pelo melhor esforço “bom o bastante” para aplicações de multimídia. Além \ndisso, a complexidade e o custo adicional de implementação e gerenciamento de uma rede que ofereça garantias \nde qualidade de serviço por conexão podem ser considerados pelos ISPs simplesmente como muito altos em \ncomparação com as receitas previstas vindas do cliente para esse serviço.\nFigura 7.28  O processo de estabelecimento de chamada\nKR 07.28.eps\nAW/Kurose and Ross\nEstabelecimento de sinalização \nde chamada com QoS\nRequisição/resposta\n   Redes de computadores e a Internet\n484\n7.6  Resumo\nA rede multimídia é um dos desenvolvimentos mais interessantes da Internet de hoje. Pessoas em todo o \nmundo estão passando menos tempo diante de seus aparelhos de rádio ou televisão e usando mais a Internet para \nreceber transmissões de áudio e vídeo, ao vivo ou pré-gravadas. À medida que o acesso à Internet sem fio de alta \nvelocidade se tornar cada vez mais prevalente, essa tendência decerto continuará. Além do mais, com sites do tipo \nYouTube, os usuários se tornaram produtores e também consumidores de conteúdo de multimídia na Internet. \nAlém da distribuição de vídeo, a Internet está sendo usada para transmitir chamadas telefônicas. De fato, nos pró-\nximos dez anos, a Internet, junto com o acesso à Internet sem fio, poderá vir a substituir o quase obsoleto sistema \nde telefonia de comutação de circuitos. VoIP não somente proverá serviço telefônico mais barato, mas também \nnumerosos serviços de valor agregado, como videoconferência, lista telefônica on-line, serviço de mensagens de \nvoz e integração com redes sociais, como Facebook e Google+.\nNa Seção 7.1, descrevemos as características intrínsecas do vídeo e da voz, e depois classificamos as apli-\ncações multimídia em três categorias: (i) áudio/vídeo de fluxo contínuo armazenado, (ii) voz e vídeo-sobre-IP \ninterativos e (iii) áudio e vídeo de fluxo contínuo ao vivo.\nNa Seção 7.2, estudamos o vídeo de fluxo contínuo armazenado com mais profundidade. Para aplicações de \nvídeo de fluxo contínuo, vídeos pré-gravados são armazenados em servidores, e os usuários enviam solicitações a \nesses servidores para que vejam os vídeos por demanda. Vimos que os sistemas de vídeo de fluxo contínuo podem \nser classificados em três categorias: UDP de fluxo contínuo, HTTP de fluxo contínuo e HTT de fluxo contínuo adap-\ntativo. Embora todos esses três tipos de sistemas sejam usados na prática, a maioria dos sistemas de hoje emprega o \nHTTP de fluxo contínuo e o HTTP de fluxo contínuo adaptativo. Observamos que a medida de desempenho mais \nimportante para o vídeo de fluxo contínuo é a vazão média. Na Seção 7.2, também investigamos as CDNs, que aju-\ndam a distribuir enormes quantidades de dados de vídeo aos usuários do mundo inteiro. Também analisamos a tec-\nnologia por trás de três das principais empresas de vídeo de fluxo contínuo na Internet: Netflix, YouTube e Kankan.\nNa Seção 7.3, examinamos como as aplicações de multimídia interativas, como VoIP, podem ser proje-\ntadas para trabalhar sobre uma rede de melhor esforço. Para a multimídia interativa, considerações de tempo \nsão importantes, pois as aplicações interativas são altamente sensíveis ao atraso. Por outro lado, aplicações de \nmultimídia interativas são tolerantes a perdas — a perda ocasional só causa lacunas ocasionais na reprodução de \náudio/vídeo, e essas perdas em geral podem ser ocultadas parcial ou totalmente. Vimos como uma combinação \nde buffers do cliente, números de sequência de pacote e marcas de tempo podem aliviar bastante os efeitos da \nvariação de atraso induzido pela rede. Também estudamos a tecnologia por trás da Skype, uma das principais \nempresas de voz e vídeo sobre IP. Na Seção 7.4, examinamos dois dos protocolos padronizados mais importantes \npara VoIP, a saber, RTP e SIP.\nNa Seção 7.5, introduzimos como vários mecanismos de rede (disciplinas de escalonamento de nível de enla-\nce e regulação de tráfego) podem ser usados para fornecer serviços diferenciados entre diversas classes de tráfego.\nExercícios  \nde fixação e perguntas\nQuestões de revisão do Capítulo 7\nSEÇÃO 7.1\n\t\nR1.\t Reconstrua a Tabela 7.1 para quando Vítor está assistindo um vídeo de 4 Mbits/s, Frank está vendo uma \nimagem nova de 100 Kbytes a cada 20 segundos e Marta está escutando um fluxo de áudio a 200 kbits/s.\n\t\nR2.\t Existem dois tipos de redundância no vídeo. Descreva-os e discuta como eles podem ser explorados para \ncompressão eficiente.\nredes multimídia  485 \n\t\nR3.\t Suponha que um sinal de áudio seja amostrado 16 mil vezes por segundo, e cada amostra seja quantizada em \num de 1.024 níveis. Qual seria a taxa de bits resultante do sinal de áudio digital PCM?\n\t\nR4.\t Aplicações de multimídia podem ser classificadas em três categorias. Relacione e descreva cada uma dessas \ncategorias.\nSEÇÃO 7.2\n\t\nR5.\t Sistemas de vídeo de fluxo contínuo podem ser classificados em três categorias. Relacione e descreva de modo \nresumido cada uma dessas categorias.\n\t\nR6.\t Relacione três desvantagens do UDP de fluxo contínuo.\n\t\nR7.\t Com o HTTP de fluxo contínuo, o buffer de recepção do TCP e o buffer da aplicação cliente são a mesma \ncoisa? Se não forem, como eles interagem?\n\t\nR8.\t Considere o modelo simples para o HTTP de fluxo contínuo. Suponha que o servidor envie bits a uma taxa \nconstante de 2 Mbits/s e a reprodução comece quando 8 milhões de bits tiverem sido recebidos. Qual é o \natraso de buffer inicial tp?\n\t\nR9.\t CDNs geralmente adotam uma de duas filosofias de posicionamento de servidor diferentes. Relacione e \ndescreva resumidamente essas duas filosofias.\n\t\nR10.\t Diversas estratégias de seleção de cluster foram descritas na Seção 7.2.4. Qual dessas estratégias acha um \ncluster bom com relação ao LDNS do cliente? Qual dessas estratégias acha um cluster bom com relação ao \npróprio cliente?\n\t\nR11.\t Além das considerações relacionadas à rede, como atraso, perda e desempenho da largura de banda, existem \nmuitos fatores adicionais que entram no projeto de uma estratégia de seleção de cluster. Quais são eles?\nSEÇÃO 7.3\n\t\nR12.\t Qual é a diferença entre atraso fim a fim e variação de atraso do pacote? Quais são as causas da variação de \natraso? Quais são as causas da variação de atraso do pacote?\n\t\nR13.\t Por que um pacote recebido após seu tempo de reprodução programado é considerado perdido?\n\t\nR14.\t Na Seção 7.3, descrevemos dois esquemas FEC. Faça um pequeno resumo deles. Ambos os esquemas \naumentam a taxa de transmissão do fluxo adicionando sobrecarga. A intercalação também aumenta a taxa de \ntransmissão?\nSEÇÃO 7.4\n\t\nR15.\t Como os diferentes fluxos RTP em sessões diferentes são identificados por um receptor? Como os diferentes \nfluxos internos à mesma sessão são identificados?\n\t\nR16.\t Qual é o papel de um registro SIP? Qual é a diferença entre o papel de um registro SIP e um agente nativo no \nIP móvel?\nSEÇÃO 7.5\n\t\nR17.\t Na Seção 7.5, discutimos o enfileiramento prioritário não preemptivo. O que seria um enfileiramento \nprioritário preemptivo? O enfileiramento prioritário preemptivo teria sentido em redes de computadores?\n\t\nR18.\t Dê um exemplo de disciplina de escalonamento que não é conservadora de trabalho.\n\t\nR19.\t Dê um exemplo de enfileiramentos que você vivencia em sua rotina diária, com disciplina FIFO, prioridade, \nRR e WFQ.\n   Redes de computadores e a Internet\n486\nproblemas\n\t\nP1.\t Considere a figura a seguir. De modo semelhante à nossa discussão da Figura 7.1, suponha que o vídeo seja \ncodificado a uma taxa de bits fixa, e assim cada bloco de vídeo contenha quadros de vídeo que devem ser \nreproduzidos por algum período de tempo fixo, D. O servidor transmite o primeiro bloco de vídeo em t0, o \nsegundo bloco em t0 + D, o terceiro bloco em t0 2D, e assim por diante. Quando o cliente inicia a reprodução, \ncada bloco deve ser reproduzido D unidades de tempo após o bloco anterior.\na.\t Suponha que o cliente inicie a reprodução assim que o primeiro bloco chega em t1. na figura a seguir, \nquantos blocos de vídeo (incluindo o primeiro) terão chegado ao cliente em tempo para sua reprodução? \nExplique como você chegou a essa resposta.\nb.\t Suponha que o cliente inicie a reprodução agora em t1 + D. Quantos blocos de vídeo (incluindo o primeiro \nbloco) terão chegado ao cliente em tempo para sua reprodução? Explique como você chegou a essa resposta.\nc.\t No mesmo cenário do item “b”\n, qual é o maior número de blocos que chega a ser armazenado no buffer do \ncliente, aguardando a reprodução? Explique como você chegou a essa resposta.\nd.\t Qual é o menor atraso de reprodução no cliente, tal que cada bloco de vídeo tenha chegado em tempo \npara sua reprodução? Explique como você chegou a essa resposta.\nKR 07.P01.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n18p9 Wide x 11p4 Deep\n11/18/11, 11/29/11 rossi\nTransmissão de \nvídeo com taxa \nde bits constante \npelo servidor\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTempo\nΔ Δ\nΔ Δ Δ Δ Δ Δ Δ Δ Δ\nNúmero do bloco de vídeo\nt0\nt1\nRecepção \nde vídeo \nno cliente\n\t\nP2.\t Lembre-se do modelo simples para o HTTP de fluxo contínuo mostrado na Figura 7.3. Lembre-se de que \nB indica o tamanho do buffer de aplicação do cliente, e Q indica o número de bits que devem ser mantidos \nem buffer antes que a aplicação cliente inicie a reprodução. Além disso, r indica a taxa de consumo de vídeo. \nSuponha que o servidor envie bits a uma taxa constante x sempre que o buffer do cliente não está cheio.\na.\t Suponha que x < r. Conforme discutimos no texto, neste caso a reprodução alternará entre períodos de \nreprodução contínua e períodos de congelamento. Determine a extensão de cada reprodução contínua e \nperíodo de congelamento como uma função de Q, r e x.\nb.\t Agora, suponha que x > r. Em que momento t = tf o buffer da aplicação cliente se torna cheio?\n\t\nP3.\t Lembre-se do modelo simples para o HTTP de fluxo contínuo mostrado na Figura 7.3. Suponha que o \ntamanho do buffer seja infinito, mas o servidor envie bits na taxa variável x(t). Especificamente, suponha que \nx(t) tenha a seguinte forma de dente de serra. A taxa é de início zero no instante t = 0 e sobe de modo linear \naté H no instante t = T. Depois, esse padrão é repetido indefinidamente, como mostra a figura a seguir.\na.\t Qual é a taxa de envio média do servidor?\nb.\t Suponha que Q = 0, de modo que o cliente começa a reproduzir assim que recebe um quadro de vídeo. O \nque acontecerá?\nc.\t Agora, suponha que Q > 0. Determine, como uma função de Q, H e T, o instante em que a reprodução inicia.\nd.\t Suponha que H > 2r e Q = HT/2. Prove que não haverá congelamento após o atraso de reprodução inicial.\nredes multimídia  487 \ne.\t Suponha que H > 2r. Ache o menor valor de Q tal que não haverá congelamento após o atraso de \nreprodução inicial.\nf.\t Agora, suponha que o tamanho do buffer B seja finito. Considere que H > 2r. Como uma função de Q, B, \nT e H, determine o instante t = tf em que o buffer da aplicação cliente se torna cheio inicialmente.\nKR 07.P03.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n18p10 Wide x 11p2 Deep\n12/20/11 rossi\nH\nTempo\nT\n2T\n3T\n4T\nTaxa de bits x(t)\n\t\nP4.\t Lembre-se do modelo simples para o HTTP de fluxo contínuo mostrado na Figura 7.3. Suponha que o buffer \nda aplicação cliente seja infinito, o servidor envie à taxa constante x e a taxa de consumo de vídeo seja r com \nr < x. Considere também que a reprodução comece imediatamente. Suponha que o usuário termine o vídeo \nantecipadamente, no instante t = E. No momento do término, o servidor para de enviar bits (se ainda não \ntiver enviado todos os bits do vídeo).\na.\t Suponha que o vídeo seja infinitamente longo. Quantos bits são desperdiçados (isto é, enviados, mas \nnão vistos)?\nb.\t Suponha que o vídeo tenha T segundos de duração com T > E. Quantos bits são desperdiçados (isto é, \nenviados, mas não vistos)?\n\t\nP5.\t Considere um sistema DASH para o qual existem N versões de vídeo (em N diferentes taxas e qualidades) \ne N versões de áudio (em N taxas e versões diferentes). Suponha que queiramos permitir que o dispositivo \nde reprodução escolha, a qualquer momento, qualquer uma das N versões de vídeo e qualquer uma das N \nversões de áudio.\na.\t Se criarmos arquivos de modo que o áudio seja misturado com o vídeo, de modo que o servidor envia \nsomente um fluxo de mídia em determinado momento, quantos arquivos o servidor precisará armazenar \n(cada um com um URL diferente)?\nb.\t Se o servidor, em vez disso, envia os fluxos de áudio e vídeo separadamente e o cliente sincroniza os fluxos, \nquantos arquivos o servidor precisa armazenar?\n\t\nP6.\t No exemplo de VoIP da Seção 7.3, seja h o número total de bytes de cabeçalho adicionados a cada trecho, \nincluindo o cabeçalho UDP e IP.\na.\t Supondo que um datagrama IP seja emitido a cada 20 ms, ache a taxa de transmissão em bits por segundo \npara os datagramas gerados por um lado dessa aplicação.\nb.\t Qual é um valor típico de h quando o RTP é usado?\n\t\nP7.\t Considere o procedimento descrito na Seção 7.3 para estimar o atraso médio di. Suponha que u = 0,1. Seja r1 \n– t1 o atraso da amostra mais recente, seja r2 – t2 o atraso da amostra mais recente seguinte e assim por diante.\na.\t Para uma dada aplicação de áudio, suponha que quatro pacotes tenham chegado ao receptor com atrasos de \namostra de r4 – t4, r3 – t3, r2 – t2 e r1 – t1. Expresse a estimativa de atraso d em termos das quatro amostras.\nb.\t Generalize sua fórmula para n atrasos de amostra.\nc.\t Para a fórmula do item “b”\n, suponha que n tenda ao infinito e dê a fórmula resultante. Comente por que \nesse procedimento de determinação de média é denominado média móvel exponencial.\n   Redes de computadores e a Internet\n488\n\t\nP8.\t Repita os itens “a” e “b” da questão anterior para a estimativa do desvio médio do atraso.\n\t\nP9.\t No exemplo de VoIP na Seção 7.3 apresentamos um procedimento on-line (média móvel exponencial) para \nestimar o atraso. Neste problema examinaremos um procedimento alternativo. Seja ti a marca de tempo do \ni-ésimo pacote recebido; ri o tempo em que o i-ésimo pacote é recebido. Seja dn nossa estimativa do atraso \nmédio após o recebimento do enésimo pacote. Após a recepção do primeiro pacote, estabelecemos a estimativa \nde atraso como d1 = r1 – t1.\na.\t Suponha que gostaríamos que dn = (r1 – t1 + r2 – t2 + ... + rn – tn)/n para todo n. Deduza uma fórmula \nrecursiva para dn em termos de dn–1, rn e tn.\nb.\t Descreva por que, para a telefonia por Internet, a estimativa de atraso descrita na Seção 7.3 é mais \napropriada do que a esboçada no item “a”\n.\n\t\nP10.\t Compare o procedimento descrito na Seção 7.3 para a estimativa do desvio médio do atraso com o \nprocedimento descrito na Seção 3.5 para a estimativa do tempo de ida e volta. O que os procedimentos têm \nem comum? Em que são diferentes?\n\t\nP11.\t Considere a figura a seguir (que é semelhante à Figura 7.7). Um transmissor começa a enviar áudio empacotado \nperiodicamente em t = 1. O primeiro pacote chega quando o transmissor está em t = 8.\nKR 07.P12.eps\nKurose and Ross\nComputer Networking 6/e\nsize:  27p0  x  15p9\n11/23/11 rossi\nPacotes \ngerados\nTempo\nPacotes\n1\n8\nPacotes \nrecebidos\na.\t Quais são os atrasos (do transmissor ao receptor, ignorando qualquer atraso de transmissão) dos pacotes \nde 2 a 8? Observe que cada segmento de linha vertical e horizontal na figura tem o comprimento de 1, 2 \nou 3 unidades de tempo.\nb.\t Se a reprodução de áudio começa assim que o primeiro pacote chega no receptor em t = 8, qual dos \nprimeiros oito pacotes enviados não chegará em tempo para a reprodução?\nc.\t Se a reprodução de áudio começa em t = 9, qual dos primeiros oito pacotes enviados não chegará em \ntempo para a reprodução?\nd.\t Qual o mínimo de atraso de reprodução no receptor que resulta em todos os primeiros oito pacotes \nchegando a tempo para a reprodução?\n\t\nP12.\t Considere novamente a figura em P11, que mostra os tempos de transmissão dos pacotes de áudio e os \ntempos de recepção.\na.\t Calcule o atraso estimado para os pacotes de 2 a 8, usando a fórmula para di, da Seção 7.3.2. Use o valor \nde u = 0,1.\nb.\t Calcule o desvio estimado do atraso para a média estimada para os pacotes 2 a 8, usando a fórmula para \nvi da Seção 7.3.2. Use o valor u = 0,1.\nredes multimídia  489 \n\t\nP13.\t Lembre-se dos dois esquemas FEC para VoIP descritos na Seção 7.3. Suponha que o primeiro esquema gere \num trecho redundante para cada quatro trechos originais. Suponha que o segundo use uma codificação de \nbaixa taxa de bits, cuja taxa de transmissão seja 25% da taxa de transmissão do fluxo nominal.\na.\t Quanta largura de banda adicional cada esquema requer? E quanto atraso de reprodução cada \nesquema adiciona?\nb.\t Como os dois esquemas funcionarão se, em cada grupo de cinco pacotes, o primeiro for perdido? Qual \nesquema terá melhor qualidade de áudio?\nc.\t Como os dois esquemas funcionarão se, em cada grupo de dois pacotes, o primeiro for perdido? Qual \nesquema terá melhor qualidade de áudio?\n\t\nP14.\t a.\t \u0007\nConsidere uma chamada de audioconferência no Skype com N > 2 participantes. Suponha que cada \nparticipante gera um fluxo constante com taxa de r bits/s. Quantos bits por segundo o iniciador da \nchamada precisa enviar? Quantos bits por segundo cada um dos outros N – 1 participantes precisará \nenviar? Qual é a taxa de envio total, agregada por todos os participantes?\nb.\t Repita o item “a” para uma chamada de videoconferência no Skype usando um servidor central.\nc.\t Repita o item “b”\n, mas agora para quando cada par enviar uma cópia de seu fluxo de vídeo a cada um dos \nN – 1 outros pares.\n\t\nP15.\t a.\t \u0007\nSuponha que enviemos dois datagramas IP para a Internet, cada um portando um segmento UDP \ndiferente. O primeiro tem endereço IP de origem A1, endereço IP de destino B, porta de origem P1 e \nporta de destino T. O segundo tem endereço IP de origem A2, endereço IP de destino B, porta de origem \nP2 e porta de destino T. Suponha que A1 é diferente de A2 e P1 é diferente de P2. Admitindo que ambos \nos datagramas cheguem a seu destino final, os dois datagramas UDP serão recebidos pelo mesmo socket? \nJustifique sua resposta.\nb.\t Suponha que Alice, Bob e Claire queiram fazer uma audioconferência usando SIP e RTP. Para Alice enviar \ne receber pacotes RTP de e para Bob e Claire, somente uma porta UDP é suficiente (além da necessária \npara as mensagens SIP)? Caso a resposta seja positiva, então como o cliente SIP de Alice distingue entre \nos pacotes RTP recebidos de Bob e Claire?\n\t\nP16.\t Verdadeiro ou falso:\na.\t Se um vídeo armazenado é entregue diretamente de um servidor Web a um reprodutor de mídia, então a \naplicação está usando TCP como protocolo de transporte subjacente.\nb.\t Ao usar RTP, é possível que um remetente mude a codificação no meio de uma sessão.\nc.\t Todas as aplicações que usam RTP devem usar a porta 87.\nd.\t Suponha que uma sessão RTP tenha fluxos separados de áudio e vídeo para cada remetente. Então, os \nfluxos de áudio e vídeo usam o mesmo SSRC.\ne.\t Em serviços diferenciados, ainda que o comportamento por salto defina diferenças de desempenho entre \nclasses, ele não impõe nenhum mecanismo particular para alcançar esses desempenhos.\nf.\t Suponha que Alice queira estabelecer uma sessão SIP com Bob. Ela inclui, na sua mensagem INVITE, a \nlinha m=audio 48753 RTP/AVP 3 (AVP 3 indica áudio GSM). Portanto, Alice indicou em sua mensagem \nque ela deseja enviar áudio GSM.\ng.\t Com referência à declaração anterior, Alice indicou em sua mensagem INVITE que enviará áudio para a \nporta 48753.\nh.\t Mensagens SIP são enviadas tipicamente entre entidades SIP usando um número default para a porta SIP.\ni.\t Para manter seu registro, clientes SIP têm de enviar mensagens REGISTER periodicamente.\nj.\t SIP impõe que todos os clientes SIP suportem codificação de áudio G.711.\n\t\nP17.\t Suponha que a política de escalonamento WFQ seja aplicada a um buffer que suporta três classes; suponha \nque os pesos para essas três classes sejam 0,5, 0,25 e 0,25.\na.\t Suponha que cada classe tenha um grande número de pacotes no buffer. Em que sequência poderiam \n   Redes de computadores e a Internet\n490\nser atendidas essas três classes para atingir os pesos WFQ descritos? (Para escalonamento por varredura \ncíclica, uma sequência natural é 123123123…).\nb.\t Suponha que as classes 1 e 2 tenham um grande número de pacotes no buffer e que não haja pacotes de classe \n3 no buffer. Em que sequência as três classes poderiam ser atendidas para alcançar os pesos WFQ descritos?\n\t\nP18.\t Considere a figura a seguir. Responda as seguintes perguntas:\nKR 07.P18.eps\nAW/Kurose and Ross\nComputer Networking  6/e\nsize:  32p3  x  11p2\n11/23/11  rossi\nTempo\nChegadas\nSaídas\nPacote \nem serviço\nTempo\n1\n6\n5\n9\n3\n2\n8\n10\n11\n7\n4\n12\nt = 0\n1\nt = 2\nt = 4\nt = 6\nt = 8\nt = 10\nt = 12\nt = 14\n1\na.\t Supondo que o serviço é FIFO, indique o tempo em que os pacotes de 2 a 12 deixam a fila. Para cada \npacote, qual o atraso entre a chegada e o início do compartimento no qual é transmitido? Qual o atraso \nmédio sobre todos os 12 pacotes?\nb.\t Suponha agora um serviço com prioridades, e admita que os números ímpares são de prioridade alta, e os \npares de prioridade baixa. Indique o tempo em que cada pacote de 2 a 12 deixará a fila. Para cada pacote, \nqual o atraso entre a chegada e o início do compartimento no qual é transmitido? Qual o atraso médio \nsobre todos os 12 pacotes?\nc.\t Suponha agora um serviço de varredura cíclica, e admita que os pacotes 1, 2, 3, 6, 11 e 12 sejam de classe \n1, e os pacotes 4, 5, 7, 8, 9 e 10 de classe 2. Indique o tempo em que cada pacote de 2 a 12 deixará a fila. \nPara cada um, qual o atraso entre a chegada e a partida? Qual o atraso médio para todos os 12 pacotes?\nd.\t Suponha agora a disciplina de serviço de enfileiramento justo ponderado (WFQ), e admita que os números \nímpares são de prioridade alta, e os pares de prioridade baixa, a partir da classe 2. A classe 1 tem um peso \nWFQ de 2, enquanto a classe 2 tem um peso WFQ de 1. Observe que pode não ser possível alcançar \numa sincronização WFQ idealizada como vimos no texto, então indique por que você escolheu o pacote \nespecífico para ser atendido em cada compartimento de tempo. Para cada pacote, qual é o atraso entre a \nchegada e a partida? Qual é o atraso médio para todos os 12 pacotes?\ne.\t O que você pode observar sobre o tempo de atraso médio nos quatro casos (FIFO, RR, prioridade e WFQ)?\n\t\nP19.\t Considere novamente a figura de P18.\na.\t Suponha um serviço prioritário, com os pacotes 1, 4, 5, 6 e 11 sendo de prioridade alta. Os pacotes restantes \nsão de prioridade baixa. Indique os compartimentos nos quais cada pacote de 2 a 12 deixará a fila.\nb.\t Agora suponha que um serviço de varredura cíclica seja usado, com os pacotes 1, 4, 5, 6 e 11 pertencentes \na uma classe de tráfego, e os restantes pertencendo a uma segunda classe de tráfego. Indique os \ncompartimentos nos quais cada pacote de 2 a 11 deixará a fila.\nc.\t Suponha agora um serviço WFQ, com os pacotes 1, 4, 5, 6 e 11 pertencentes a uma classe de tráfego, e \nos restantes pertencendo a uma segunda classe de tráfego. A classe 1 tem um peso WFQ de 1, enquanto \na classe 2 tem um peso WFQ de 2 (observe que estes pesos são diferentes daqueles na questão anterior). \nIndique os compartimentos nos quais cada pacote de 2 a 12 deixará a fila. Veja também a advertência na \nquestão anterior relativa ao serviço WFQ.\nredes multimídia  491 \n\t\nP20.\t Considere a figura abaixo, que mostra um regulador de balde furado sendo alimentado por um fluxo contínuo \nde pacotes. O buffer de permissões pode segurar, no máximo, duas permissões e está inicialmente cheio em \nt = 0. Novas permissões chegam em uma velocidade de uma permissão por compartimento. A velocidade do \nenlace de saída é tal que se dois pacotes obtêm permissões no início de um compartimento de tempo, ambos \npodem ir ao enlace de saída no mesmo compartimento. Os detalhes de temporização são os seguintes:\nKR 07.P20.eps\nAW/Kurose and Ross\nComputer Networking  6/e\nsize:  36p0  x  12p0  \n11/23/11 rossi\nChegadas\nEnﬁleiramento de pacotes \n(espera permissões)\n9\n10\n7\n6\n4\n8\n5\n1\n3\n2\nt = 8\nt = 6\nt = 4\nt = 2\nt = 0\nt = 4\nt = 2\nt = 0\nr = 1 permissão/compartimento\nb = 2 permissões\n1.\t Os pacotes (se houver) chegam no começo do compartimento. Sendo assim, na figura, os pacotes 1, 2 e 3 \nchegam no compartimento 0. Se já existirem pacotes na fila, os que vão chegar se juntam no final da fila. \nOs pacotes prosseguem em direção ao começo da fila de modo FIFO.\n2.\t Depois das chegadas serem adicionadas à fila, se já existirem pacotes, um ou dois deles (dependendo do \nnúmero de permissões disponíveis) removerão uma permissão do buffer de permissões e irão ao enlace \nde saída durante esse compartimento. Sendo assim, os pacotes 1 e 2 removem uma permissão do buffer \n(já que existem inicialmente duas permissões) e vão ao enlace de saída durante o compartimento 0.\n3.\t Uma nova permissão é adicionada ao buffer de permissões se este não estiver cheio, já que a velocidade de \ncriação de permissões é de r = 1 permissão/compartimento.\n4.\t Tempo avança até a próxima entrada, e as etapas se repetem.\nResponda às seguintes perguntas:\na.\t Para cada compartimento de tempo, identifique os pacotes que estão na fila e o número de permissões no \nbalde, imediatamente depois que as chegadas foram processadas (etapa 1 acima) mas antes que os pacotes \npassem pela fila e removam uma permissão. Assim, para o compartimento de tempo t = 0 no exemplo \nacima, os pacotes 1, 2 e 3 estão na fila, e existem duas permissões no buffer.\nb.\t Para cada compartimento de tempo, indique qual pacote aparecerá na saída depois que as permissões \nforem removidas da fila. Assim, para o compartimento t = 0 no exemplo acima, os pacotes 1 e 2 aparecem \nno enlace de saída do balde furado durante a entrada 0.\n\t\nP21.\t Repita o problema P20, mas suponha que r = 2. Admita novamente que o balde está cheio inicialmente.\n\t\nP22.\t Considere o problema P21, mas suponha agora que r = 3, e que b = 2, como antes. Sua resposta para a questão \nacima é diferente?\n\t\nP23.\t Considere o regulador de balde furado, que regula a taxa média e o tamanho da rajada de um fluxo de pacotes. \nQueremos agora também regular a taxa de pico p. Mostre como a saída desse regulador de balde furado pode \nser alimentada para um segundo regulador de balde furado, de modo que os dois, em série, regulem a taxa \nmédia, a taxa de pico e o tamanho da rajada. Não se esqueça de atribuir um tamanho e uma taxa de geração \nde permissão ao balde do segundo regulador.\n   Redes de computadores e a Internet\n492\n\t\nP24.\t Diz-se que um fluxo de pacotes está de acordo com a especificação de um balde furado (r,b) com tamanho \nde rajada b e taxa média r se o número de pacotes que chega ao balde furado for menor do que rt + b \npacotes a cada intervalo de tempo de duração t para todo t. Um fluxo de pacotes que esteja de acordo com \na especificação do balde furado (r,b) alguma vez terá de esperar em um regulador de balde furado com \nparâmetros r e b? Justifique sua resposta.\n\t\nP25.\t Demonstre que, enquanto r1 < R w1/(\n wj), então dmax é, na verdade, o atraso máximo que qualquer pacote \ndo fluxo 1 sofrerá na fila WFQ.\nTarefa de programação\nNeste laboratório você implementará um servidor e um cliente de vídeo de fluxo contínuo. O cliente usará \no protocolo de fluxo contínuo em tempo real (RTSP) para controlar as ações do servidor. O servidor usará o pro-\ntocolo de tempo real (RTP) para empacotar o vídeo para transporte por UDP. Você receberá um código Python \nque executará parcialmente RTSP e RTP no cliente e no servidor. Sua tarefa será concluir o código para o cliente \ne também para o servidor. Quando terminar, você terá criado uma aplicação cliente-servidor que faz o seguinte:\n• O cliente envia comandos RTSP SETUP\n, PLAY, PAUSE e TEARDOWN e o servidor responde aos comandos.\n• Quando o servidor estiver no estado de reprodução, ele pega periodicamente um quadro JPEG armaze-\nnado, empacota o quadro com RTP e envia o pacote RTP para um socket UDP.\n• O cliente recebe os pacotes RTP, extrai os quadros JPEG, descompacta os quadros e os apresenta no \nmonitor do cliente.\nO código que você receberá implementa o protocolo RTSP no servidor e o desempacotamento RTP no \ncliente, e também cuida da apresentação do vídeo transmitido. Você precisará executar RTSP no cliente e RTP \nno servidor. Esta tarefa de programação aprimorará de modo significativo a compreensão de RTP, RTSP e vídeo \nde fluxo contínuo para o aluno. Recomendamos veementemente que ela seja executada. A tarefa também sugere \nvários exercícios opcionais, incluindo implementação do comando RTSP DESCRIBE no cliente e também no ser-\nvidor. Você encontrará informações completas sobre a tarefa, bem como trechos importantes de código Python, \nno site de apoio deste livro.\nHenning Schulzrinne\nHenning Schulzrinne é professor, diretor do Departamento de Ciências da Compu-\ntação e chefe do Internet Real-Time Laboratory da Universidade Columbia. É coautor \ndos protocolos RTP\n, RTSP\n, SIP e GIST, fundamentais para comunicação de áudio \ne vídeo pela Internet. É bacharel em engenharia elétrica e industrial pela Universi-\ndade de Darmstadt, na Alemanha, mestre em engenharia elétrica e de computação \npela Universidade de Cincinnati e doutor em engenharia elétrica pela Universidade de \nMassachusetts, em Amherst.\nENTREVISTA\nO que o fez se decidir pela especialização em tec-\nnologia de redes multimídia?\nAconteceu quase por acidente. Quando fazia dou-\ntorado, acabei me envolvendo com a DARTnet, uma \nrede experimental que abrangia os Estados Unidos \ncom linhas T1. A DARTnet era usada como campo de \nprova para ferramentas de transmissão para um grupo \n(multicast) e de Internet em tempo real. Isso me levou \nredes multimídia  493 \na desenvolver minha primeira ferramenta de áudio, \na NeVoT. Por intermédio de alguns participantes da \nDARTnet, acabei me envolvendo com a IETF, com o \ngrupo de trabalho Audio Vídeo Transport, que se for-\nmara havia pouco tempo naquela época. O grupo ter-\nminou por padronizar o RTP.\nQual foi seu primeiro emprego no setor de compu-\ntação? O que implicava?\nNo meu primeiro emprego no setor de computação \nfiz a soldagem das peças de um computador Altair. Eu \nainda era estudante do ensino médio em Livermore, \nna Califórnia. Quando voltei à Alemanha, montei uma \npequena empresa de consultoria que projetou um pro-\ngrama de gerenciamento de endereços para uma agên-\ncia de viagens — armazenando dados em fitas cassete \npara nosso TRS-80 e usando uma máquina de escrever \nelétrica Selectric da IBM com uma interface de hard­\nware feita em casa servindo de impressora.\nMeu primeiro emprego verdadeiro foi no AT&T Bell \nLaboratories, no desenvolvimento de um emulador \nde redes para a construção de redes experimentais em \nambiente de laboratório.\nQuais os objetivos do Internet Real-Time Lab?\nNosso objetivo é fornecer componentes e construir \nblocos para a Internet como uma única infraestrutu-\nra de comunicação. Isso inclui o desenvolvimento de \nnovos protocolos, como o GIST (para sinalização de \ncamadas de rede) e o LoST (para encontrar recursos \npor localização), ou aprimorar os protocolos nos quais \njá trabalhamos anteriormente, como o SIP, por meio \nde trabalhos de presença aprimorada, sistemas peer­\n‑to-peer, chamadas de emergência de próxima geração \ne ferramentas de criação de serviços. Recentemente, \ntambém voltamos nossa atenção aos sistemas sem fio \nVoIP, como as redes 802.11b e 802.11n, e talvez as re-\ndes WiMax se tornem importantes tecnologias de úl-\ntima milha para telefonia. Também estamos tentando \naprimorar a capacidade de os usuários diagnosticarem \nfalhas nos confusos equipamentos e provedores, usan-\ndo um sistema de diagnóstico de falhas peer-to-peer \nchamado DYSWIS (Do You See What I See [você vê \no que eu vejo]).\nTentamos fazer trabalhos relevantes, ao construir \nprotótipos e sistemas de código-fonte aberto, ao medir \no desempenho de sistemas reais, e contribuindo para \nos padrões da IETF.\nEm sua opinião, qual é o futuro das redes multi-\nmídia?\nEstamos agora em uma fase de transição; poucos \nanos nos separam do IP como plataforma universal \npara serviços de multimídia, do IPTV ao VoIP. Espe-\nramos que o rádio, o telefone e a TV continuem fun-\ncionando mesmo durante tempestades de neve e terre-\nmotos; portanto, quando a Internet assumir os papéis \ndessas redes dedicadas, os usuários poderão esperar o \nmesmo nível de confiabilidade.\nTeremos de aprender a projetar tecnologias de rede \nem um ecossistema de operadoras, provedores de \nserviço e conteúdo concorrentes, servindo muitos \nusuários sem treinamento e defendendo-os de um \npequeno, porém destrutivo, punhado de usuários ma-\nliciosos e criminosos. A mudança de protocolos está \nse tornando cada vez mais difícil. Estão se tornando \ntambém mais complexas, já que precisam levar em \nconta os lucros competitivos de negócios, segurança, \nprivacidade e a falta de transparência das redes, cau-\nsada por firewalls e tradutores de endereços de rede.\nDesde que a rede multimídia se tornou a base para \ntodo entretenimento ao consumidor, existirá uma ên-\nfase na administração de grandes redes, a um preço \nbaixo. Os usuários esperarão um uso fácil, como ter o \nmesmo conteúdo em todos seus dispositivos.\nPor que o SIP tem um futuro promissor?\nEnquanto prossegue a evolução das redes sem fio \nexistentes hoje para 3G, há a esperança de um único \nmecanismo de sinalização para multimídia que abranja \ntodos os tipos de rede, desde modem a cabo até redes \ntelefônicas de empresas e redes públicas sem fio. Junto \ncom softwares de rádio, isso possibilitará que, no futuro, \num dispositivo único, tal como um telefone Bluetooth \nsem fio, possa ser utilizado em uma rede residencial, em \numa rede empresarial via 802.11 e em redes de longa \ndistância por meio de redes 3G. Mesmo antes de existir \ntal dispositivo único, universal, sem fio, os mecanismos \nde mobilidade pessoal permitem ocultar as diferenças \nentre redes. Um identificador torna-se o meio universal \nde alcançar uma pessoa, em vez de ter de lembrar ou ter \nde passar por meia dúzia de números de telefones espe-\ncíficos para tecnologias ou localizações.\nO SIP também separa o fornecimento de transporte de \nvoz (bit) dos serviços de voz. Agora ficou tecnicamente \npossível acabar com o monopólio local de telefonia, no \nqual uma única empresa fornece transporte neutro de \n   Redes de computadores e a Internet\n494\nbits enquanto outras fornecem “o tom de discar” IP e os \nserviços clássicos de telefonia, como gateways, transfe-\nrência de chamadas e identificador de chamadas.\nAlém da sinalização de multimídia, o SIP oferece \num novo serviço que estava faltando na Internet: no-\ntificação de eventos. Algo parecido com esse serviço \njá vem sendo oferecido por soluções improvisadas \nHTTP e por e-mail, mas nunca foi muito satisfatório. \nUma vez que eventos são uma abstração comum para \nsistemas distribuídos, isso pode simplificar a constru-\nção de novos serviços.\nVocê pode dar algum conselho aos estudantes \nque estão ingressando no campo das redes?\nO trabalho com redes é quase interdisciplinar. Ele \ntira subsídios da engenharia elétrica, da ciência da \ncomputação, da pesquisa operacional e de outras dis-\nciplinas. Assim, os pesquisadores de redes têm de estar \nfamiliarizados com assuntos que estão além de proto-\ncolos e algoritmos de roteamento.\nJá que as redes estão se tornando partes tão impor-\ntantes do nosso dia a dia, estudantes que procuram \nfazer a diferença no campo deveriam pensar em limi-\ntações de novos recursos em rede: tempo e esforço hu-\nmano, em vez de banda larga e armazenamento.\nO trabalho com redes pode trazer imensa satisfação, \njá que se trata de permitir que as pessoas se comuni-\nquem e troquem ideias, um dos valores essenciais do \nser humano. A Internet se tornou a terceira maior in-\nfraestrutura global, seguindo o sistema de transporte e \na distribuição de energia. Poucos setores da economia \nconseguem trabalhar sem redes de alto desempenho, \nentão deverão existir muitas oportunidades para o fu-\nturo próximo.\nNa Seção 1.6 descrevemos algumas das categorias mais predominantes e prejudiciais dos ataques na In-\nternet, incluindo ataques malware, recusa de serviço, analisador de pacotes, disfarce da origem e modificação e \nexclusão de mensagem. Embora tenhamos aprendido muito sobre redes de computadores, ainda não analisamos \ncomo protegê-las dos ataques descritos na Seção 1.6. Com nosso conhecimento recém-adquirido em rede de \ncomputadores e protocolos da Internet, estudaremos minuciosamente a comunicação segura e, em particular, \ncomo as redes podem ser protegidas desses vilões.\nQueremos lhe apresentar Alice e Bob, duas pessoas que desejam se comunicar, porém “com segurança”\n. \nComo esse texto refere-se a redes, gostaríamos de observar que Alice e Bob podem ser dois roteadores que que-\nrem trocar tabelas de roteamento com segurança, um cliente e um servidor que querem estabelecer uma cone-\nxão de transporte segura ou duas aplicações de e-mail que querem trocar e-mails com segurança — todos esses \ntópicos serão examinados mais adiante neste capítulo. Alice e Bob são componentes conhecidos da comunidade \nde segurança, talvez porque o nome deles seja mais interessante do que uma entidade genérica denominada “\nA” \nque quer se comunicar com segurança com uma entidade genérica denominada “B”\n. Amores proibidos, comuni-\ncações em tempo de guerra e transações financeiras são as necessidades dos seres humanos mais citadas quando \no assunto é segurança nas comunicações; preferimos a primeira necessidade às duas últimas, e vamos usar, com \nmuito prazer, Alice e Bob como nosso remetente e nosso destinatário e imaginá-los nesse primeiro cenário.\nDissemos que Alice e Bob querem se comunicar, porém “com segurança”\n, mas o que isso significa exata-\nmente? Como veremos, a segurança (assim como o amor) é repleta de maravilhas; isto é, ela tem muitas facetas. \nÉ certo que Alice e Bob gostariam que o conteúdo de sua comunicação permanecesse secreto, a salvo de um bis-\nbilhoteiro. Provavelmente, eles também gostariam de ter certeza de que estão se comunicando mesmo um com o \noutro e de que, caso algum bisbilhoteiro interfira na comunicação, essa interferência seja detectada. Na primeira \nparte deste capítulo, estudaremos as técnicas que permitem criptografar/decriptar comunicações, autenticar a \nparte com quem estamos nos comunicando e assegurar a integridade da mensagem.\nNa segunda parte, examinaremos como os princípios da criptografia podem ser usados para criar protoco-\nlos de rede seguros. Utilizando mais uma vez uma abordagem top-down, examinaremos os protocolos seguros \nem cada uma das (quatro principais) camadas, iniciando pela camada de aplicação. Verificaremos como proteger \no e-mail e uma conexão TCP, como prover segurança total na camada de rede, e como proteger uma LAN sem \nfio. Na terceira parte deste capítulo, avaliaremos a segurança operacional, que tem o objetivo de proteger redes \norganizacionais de ataques. Em particular, verificaremos, de forma minuciosa, como os firewalls e os sistemas de \ndetecção de invasão podem aprimorar a segurança de uma rede organizacional.\nSegurança em redes\nde computadores\n1\n3\n6\n9\n7\nc\n \na\n \np\n \ní\n \nt\n \nu\n \nl\n \no\n2\n4\n8\n5\n   Redes de computadores e a Internet\n496\n8.1  O que é segurança de rede?\nVamos iniciar nosso estudo de segurança de redes voltando aos namorados citados, Alice e Bob, que que-\nrem se comunicar “com segurança”\n. O que isso significa ao certo? Com certeza, Alice quer que somente Bob en-\ntenda a mensagem que ela enviou, mesmo que eles estejam se comunicando por um meio inseguro, em que um \nintruso (Trudy, a intrusa) pode interceptar qualquer dado que seja transmitido. Bob também quer ter certeza de \nque a mensagem que recebe de Alice foi de fato enviada por ela, e Alice quer ter certeza de que a pessoa com quem \nestá se comunicando é de fato Bob. Alice e Bob também querem ter certeza de que o conteúdo de suas mensagens \nnão foi alterado em trânsito. Também querem, antes de tudo, ter certeza de que podem se comunicar (isto é, de \nque ninguém lhes negue acesso aos recursos necessários à comunicação). Dadas essas considerações, podemos \nidentificar as seguintes propriedades desejáveis da comunicação segura:\n• Confidencialidade. Apenas o remetente e o destinatário pretendido devem poder entender o conteúdo da \nmensagem transmitida. O fato de intrusos conseguirem interceptar a mensagem exige, necessariamente, \nque esta seja cifrada de alguma maneira para impedir que seja entendida por um interceptador. Esse \naspecto de confidencialidade é, provavelmente, o significado mais comumente percebido na expressão \ncomunicação segura. Estudaremos técnicas de criptografia para cifrar e decifrar dados na Seção 8.2.\n• Integridade de mensagem. Alice e Bob querem assegurar que o conteúdo de sua comunicação não seja \nalterado, por acidente ou por má intenção, durante a transmissão. Extensões das técnicas de soma de ve-\nrificação que encontramos em protocolos de transporte e de enlace confiáveis podem ser utilizadas para \nproporcionar integridade à mensagem. Estudaremos autenticação do ponto de chegada e integridade da \nmensagem na Seção 8.3.\n• Autenticação do ponto final. O remetente e o destinatário precisam confirmar a identidade da outra parte \nenvolvida na comunicação — confirmar que a outra parte é de verdade quem alega ser. A comunicação \npessoal entre seres humanos resolve facilmente esse problema por reconhecimento visual. Quando enti-\ndades comunicantes trocam mensagens por um meio pelo qual não podem ver a outra parte, a autentica-\nção não é assim tão simples. Por que, por exemplo, você deveria acreditar que o e-mail que recebeu e que \ncontém uma sentença afirmando que aquele e-mail veio de um amigo seu vem mesmo dele? Estudamos \na autenticação do ponto final na Seção 8.4.\n• Segurança operacional. Hoje quase todas as organizações (empresas, universidades etc.) possuem redes \nconectadas à Internet pública. Essas redes podem ser comprometidas por atacantes que ganham acesso \na elas por meio da Internet pública. Os atacantes podem tentar colocar worms nos hospedeiros na rede, \nadquirir segredos corporativos, mapear as configurações internas da rede e lançar ataques de DoS. Ve-\nremos na Seção 8.9 que os mecanismos operacionais, como firewalls e sistemas de detecção de invasão, \nsão usados para deter ataques contra a rede de uma organização. Um firewall localiza-se entre a rede da \norganização e a rede pública, controlando os acessos de pacote de e para a rede. Um sistema de detecção \nde invasão realiza uma “profunda inspeção de pacote”\n, alertando os administradores da rede sobre algu-\nma atividade suspeita.\nAgora que já determinamos o que significa segurança de rede, vamos considerar em seguida quais são, exa-\ntamente, as informações às quais um intruso pode ter acesso e que ações podem ser executadas por ele. A Figura \n8.1 ilustra o cenário. Alice, a remetente, quer enviar dados a Bob, o destinatário. Para trocar dados com segurança, \nalém de atender aos requisitos de confidencialidade, autenticação e integridade de mensagens, Alice e Bob trocarão \nmensagens de controle e de dados (algo muito semelhante ao modo como remetentes e destinatários TCP trocam \nsegmentos de controle e segmentos de dados). Todas ou algumas dessas mensagens costumam ser criptografadas. \nConforme discutimos na Seção 1.6, um intruso passivo consegue, potencialmente, fazer o seguinte:\n• monitorar — identificar e gravar as mensagens de controle e de dados no canal;\n• modificar, inserir ou eliminar mensagens ou conteúdo de mensagens.\nSegurança em redes de computadores  497 \nComo veremos, a menos que sejam tomadas contramedidas adequadas, essas capacidades permitem que \num intruso monte uma grande variedade de ataques à segurança: monitorar comunicações (talvez roubando \nsenhas e dados), fazer-se passar por outra entidade, sequestrar uma sessão em curso, recusar serviço a usuários \nlegítimos da rede sobrecarregando os recursos do sistema e assim por diante. O CERT Coordination Center \n[CERT, 2012] mantém um resumo de ataques comunicados.\nAgora que já temos certeza de que há ameaças reais à solta na Internet, quais são os equivalentes de Alice \ne Bob na Internet, esses nossos amigos que precisam se comunicar com segurança? Decerto, Bob e Alice podem \nser dois usuários humanos em dois sistemas finais, por exemplo, uma Alice real e um Bob real que de fato querem \ntrocar e-mails seguros. Eles podem também ser os participantes de uma transação de comércio eletrônico. Por \nexemplo, um Bob real pode querer transmitir com segurança o número de seu cartão de crédito a um servidor \nWeb para comprar um produto pela rede. As partes que necessitam de uma comunicação segura podem fazer \nparte de uma infraestrutura da rede. Lembre-se de que o sistema de nomes de domínio (DNS, veja a Seção 2.5) \nou daemons roteadores que trocam informações de roteamento (veja a Seção 4.6) requerem comunicação segura \nentre dois participantes. O mesmo é válido para aplicações de gerenciamento de rede, um tópico que examinare-\nmos no Capítulo 9. Um intruso que conseguisse interferir em consultas do DNS (como discutido na Seção 2.5), \nprocessamentos de roteamento [RFC 4272] ou funções de gerenciamento de rede [RFC 3414] poderia causar \numa devastação na Internet.\nEstabelecida a estrutura, apresentadas algumas das definições mais importantes e justificada a necessidade de \nsegurança na rede, vamos examinar a criptografia mais a fundo. Embora sua utilização para prover confidenciali-\ndade seja evidente por si só, veremos em breve que a criptografia também é essencial para prover autenticação do \nponto final e integridade de mensagem — o que faz dela uma pedra fundamental da segurança na rede.\n8.2  Princípios de criptografia\nEmbora a criptografia tenha uma longa história que remonta, no mínimo, a Júlio César, técnicas modernas, \nincluindo muitas das usadas na Internet, são baseadas em progressos feitos nos últimos 30 anos. O livro de Kahn, \nThe codebreakers [Kahn, 1967], e o livro de Singh, The Code Book: The Science of Secrecy from Ancient Egypt to \nQuantum Cryptography [Singh, 1999], nos oferecem um panorama fascinante dessa longa história. Uma discus-\nsão completa sobre a criptografia exige um livro inteiro [Kaufman, 1995; Schneier, 1995]; portanto, trataremos \napenas de seus aspectos essenciais, em particular do modo como as técnicas criptográficas são postas em prática \nna Internet. Observamos também que, conquanto nessa seção focalizemos a utilização da criptografia aplicada à \nconfidencialidade, em breve veremos que as técnicas criptográficas estão inextricavelmente entrelaçadas com a \nautenticação, a integridade de mensagens, o não repúdio e outras.\nTécnicas criptográficas permitem que um remetente disfarce os dados de modo que um intruso não consiga \nobter nenhuma informação dos dados interceptados. O destinatário, é claro, deve estar habilitado a recuperar os \nFigura 8.1  Remetente, destinatário e intruso (Alice, Bob e Trudy)\nRemetente \nseguro\nAlice\nTrudy\nCanal\nMensagens de controle \ne de dados\nDestinatário \nseguro\nBob\nDados\nDados\nKR 08.01.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n27p4 Wide x 10p7 Deep\n11/18/11 rossi\n   Redes de computadores e a Internet\n498\ndados originais a partir dos dados disfarçados. A Figura 8.2 apresenta alguns dos componentes mais importantes \nda terminologia usada em criptografia.\nSuponha agora que Alice queira enviar uma mensagem a Bob. A mensagem de Alice em sua forma original \n(por exemplo, “Bob, I love you. Alice”) é conhecida como texto aberto ou texto claro. Alice criptografa \nsua mensagem em texto aberto usando um algoritmo de criptografia, de modo que a mensagem criptografada, \nconhecida como texto cifrado, pareça ininteligível para qualquer intruso. O interessante é que em muitos siste-\nmas criptográficos modernos, incluindo os usados na Internet, a técnica de codificação é conhecida — publicada, \npadronizada e disponível para qualquer um (por exemplo, [RFC 1321; RFC 3447; RFC 2420; NIST, 2001]), mes-\nmo para um potencial intruso! Evidentemente, se todos conhecem o método para codificar dados, então deve \nhaver alguma informação secreta que impede que um intruso decifre os dados transmitidos. É aqui que entra a \nchave.\nNa Figura 8.2, Alice fornece uma chave, KA, uma cadeia de números ou de caracteres, como entrada para o \nalgoritmo de criptografia. O algoritmo pega essa chave e o texto aberto da mensagem, m, como entrada e produz \ntexto cifrado como saída. A notação KA(m) refere-se à forma do texto cifrado (criptografado usando a chave KA) \nda mensagem em texto aberto, m. O algoritmo criptográfico propriamente dito, que usa a chave KA, ficará evi-\ndente do próprio contexto. De maneira semelhante, Bob fornecerá uma chave, KB, ao algoritmo de decriptação, \nque pega o texto cifrado e a chave de Bob como entrada e produz o texto aberto original como saída. Isto é, se \nBob receber uma mensagem criptografada KA(m), ele a decriptará calculando KB(KA(m)) = m. Em sistemas de \nchaves simétricas, as chaves de Bob e de Alice são idênticas e secretas. Em sistemas de chaves públicas, é usado \num par de chaves. Uma delas é conhecida por Bob e por Alice (na verdade, é conhecida pelo mundo inteiro). A \noutra chave é conhecida apenas por Bob ou por Alice (mas não por ambos). Nas duas subseções seguintes, exa-\nminaremos com mais detalhes sistemas de chaves simétricas e de chaves públicas.\n8.2.1  Criptografia de chaves simétricas\nTodos os algoritmos criptográficos envolvem a substituição de um dado por outro, como tomar um trecho \nde um texto aberto e então, calculando e substituindo esse texto por outro cifrado apropriado, criar uma mensa-\ngem cifrada. Antes de estudar um sistema criptográfico moderno baseado em chaves, vamos abordar um algorit-\nmo de chaves simétricas muito antigo, muito simples, atribuído a Júlio César e conhecido como cifra de César \n(uma cifra é um método para codificar dados).\nA cifra de César funciona tomando cada letra da mensagem do texto aberto e substituindo-a pela k-ésima \nletra sucessiva do alfabeto (permitindo a rotatividade do alfabeto, isto é, a letra “z” seria seguida novamente da le-\ntra “a”). Por exemplo, se k = 3, então a letra “a” do texto aberto fica sendo “d” no texto cifrado; “b” no texto aberto \nFigura 8.2  Componentes criptográficos\nAlgoritmo de \ncriptograﬁa\nTexto cifrado\nCanal\nTrudy\nAlice\nBob\nAlgoritmo de \ndecriptação\nTexto aberto\nKey:\nKey\nTexto aberto\nKA\nKB\nSegurança em redes de computadores  499 \nse transforma em “e” no texto cifrado, e assim por diante. Nesse caso, o valor de k serve de chave. Por exemplo, a \nmensagem “bob, i love you. alice” se torna “ere, l oryh brx. dolfh” em texto cifrado. Embora \no texto cifrado na verdade pareça não ter nexo, você não levaria muito tempo para quebrar o código se soubesse \nque foi usada a cifra de César, pois há somente 25 valores possíveis para as chaves.\nUm aprimoramento da cifra de César é a cifra monoalfabética, que também substitui uma letra do alfabeto \npor outra. Contudo, em vez de fazer isso seguindo um padrão regular (por exemplo, substituição por um des-\nlocamento de k para todas as letras), qualquer letra pode ser substituída por qualquer outra, contanto que cada \nletra tenha uma única substituta e vice-versa. A regra de substituição apresentada na Figura 8.3 mostra uma regra \npossível para codificar textos abertos.\nA mensagem do texto aberto “bob, I love you. alice” se torna “nkn, s gktc wky. mgsbc.” \nAssim, como aconteceu no caso da cifra de César, o texto parece sem nexo. A cifra monoalfabética também pa-\nrece ser melhor que a cifra de César, pois há 26! (da ordem de 1026) possíveis pares de letras, em vez de 25 pares \npossíveis. Uma técnica de força bruta que experimentasse todos os 1026 pares possíveis demandaria um esforço \ngrande demais e impediria que esse fosse um método viável para quebrar o algoritmo criptográfico e decodificar \na mensagem. Contudo, pela análise estatística da linguagem do texto aberto, por exemplo, e sabendo que as le-\ntras “e” e ‘t” são as mais frequentes em textos em inglês (13% e 9% das ocorrências de letras, respectivamente) e \nsabendo também que determinados grupos de duas e de três letras aparecem com bastante frequência em inglês \n(por exemplo, “in”\n, “it”\n, “the”\n, “ion”\n, “ing”\n, e assim por diante), torna-se relativamente fácil quebrar esse código. Se \no intruso tiver algum conhecimento sobre o possível texto da mensagem, então ficará mais fácil ainda quebrar o \ncódigo. Por exemplo, se a intrusa Trudy for a esposa de Bob e suspeitar que ele está tendo um caso com Alice, ela \npoderá facilmente imaginar que os nomes “bob” e “alice” apareçam no texto. Se Trudy tivesse certeza de que esses \ndois nomes aparecem no texto cifrado e tivesse uma cópia do texto cifrado da mensagem do exemplo, então ela \npoderia determinar imediatamente sete dos 26 pares de letras, o que resultaria em 109 possibilidades a menos \npara verificar pelo método da força bruta. Na verdade, se Trudy suspeitasse que Bob estava tendo um caso, ela \npoderia muito bem esperar encontrar algumas outras palavras preferenciais no texto também.\nConsiderando como seria fácil para Trudy quebrar o código criptográfico de Bob e Alice, podemos distin-\nguir três cenários diferentes, dependendo do tipo de informação que o intruso tem:\n• Ataque exclusivo a texto cifrado. Em alguns casos, o intruso pode ter acesso somente ao texto cifrado \ninterceptado, sem ter nenhuma informação exata sobre o conteúdo do texto aberto. Já vimos como a \nanálise estatística pode ajudar o ataque exclusivo ao texto cifrado em um esquema criptográfico.\n• Ataque com texto aberto conhecido. Vimos anteriormente que, se Trudy, por alguma razão, tivesse certeza \nde que “bob” e “alice” apareciam no texto cifrado, ela poderia determinar os pares (texto cifrado, texto \naberto) para as letras a, l, i, c, e, b e o. Trudy poderia também ser muito sortuda e ter gravado todas as \ntransmissões de texto cifrado e descoberto uma versão decifrada pelo próprio Bob escrita em um pedaço \nde papel. Quando um intruso conhece alguns dos pares (texto aberto, texto cifrado), referimo-nos a isso \ncomo ataque ao esquema criptográfico a partir de texto aberto conhecido.\n• Ataque com texto aberto escolhido. Nesse tipo de ataque, o intruso pode escolher a mensagem em texto \naberto e obter seu texto cifrado correspondente. Para os algoritmos criptográficos simples que vimos \naté aqui, se Trudy conseguisse que Alice enviasse a mensagem “the quick fox jumps over the \nlazy brown dog”\n, ela poderia decifrar completamente o esquema criptográfico. Veremos em breve \nque, para técnicas de criptografia mais sofisticadas, um ataque com um texto aberto escolhido não signi-\nfica necessariamente que a técnica criptográfica possa ser decifrada.\nFigura 8.3  Um cifra monoalfabética\nLetra no texto aberto:\na b c d e f g h i j k l m n o p q r s t u v w x y z\nLetra no texto cifrado:\nm n b v c x z a s d f g h j k l p o i u y t r e w q\n   Redes de computadores e a Internet\n500\nQuinhentos anos atrás foram inventadas técnicas que aprimoravam a cifra monoalfabética, conhecidas \ncomo cifras polialfabéticas. A ideia subjacente à criptografia polialfabética é usar várias cifras monoalfabéticas \ncom uma cifra monoalfabética específica para codificar uma letra em uma posição específica no texto aberto da \nmensagem. Assim, a mesma letra, quando aparece em posições diferentes no texto aberto da mensagem, pode ser \ncodificada de maneira diferente. Um exemplo de esquema criptográfico polialfabético é mostrado na Figura 8.4, \nna qual há duas cifras de César (com k = 5 e k = 19), que aparecem nas linhas da figura. Podemos optar pelo uso \ndessas duas cifras de César, C1 e C2, seguindo o modelo de repetição C1, C2, C2, C1, C2. Isto é, a primeira letra \ndo texto deve ser cifrada usando-se C1, a segunda e a terceira, C2, a quarta, C1 e a quinta, C2. O modelo, então, \nse repete, com a sexta letra sendo cifrada usando-se C1; a sétima, com C2, e assim por diante. Dessa maneira, a \nmensagem em texto aberto “bob, I love you.” é cifrada como “ghu, n etox dhz.”\n. Note que o primeiro \n“b” da mensagem em texto aberto é cifrado usando-se C1, ao passo que o segundo “b” é cifrado usando-se C2. Nesse \nexemplo, a “chave” da codificação e da decodificação é o conhecimento das duas cifras de César (k = 5, k = 19) e do \nmodelo C1, C2, C2, C1, C2.\nCifras de bloco\nVamos agora nos direcionar a tempos modernos e analisar como a criptografia de chave simétrica é feita \natualmente. Existem duas classes gerais de técnicas de criptografia simétrica: cifras de fluxo e cifras de bloco. Na \nSeção 8.7, examinaremos as cifras de fluxo de forma breve, ao investigarmos a segurança para LANs sem fio. Nesta \nseção, focaremos em cifras de bloco, que são utilizadas em muitos protocolos seguros da Internet, incluindo PGP \n(para e-mail seguro), SSL (para conexões TCP seguras) e IPsec (para proteger o transporte da camada de rede).\nNa cifra de bloco, a mensagem a ser criptografada é processada em blocos de k bits. Por exemplo, se k = 64, \nentão a mensagem é dividida em blocos de 64 bits, e cada bloco é criptografado de maneira independente. Para \ncriptografar um bloco, a cifra utiliza um mapeamento um para um para mapear o bloco de k bits de texto aberto \npara um bloco de k bits de texto cifrado. Vamos examinar um exemplo. Suponha que k = 3, de modo que a cifra \nde bloco mapeie entradas de 3 bits (texto aberto) para saídas de 3 bits (texto cifrado). Um possível mapeamento é \ndeterminado na Tabela 8.1. Observe que esse é um mapeamento um para um; ou seja, há uma saída diferente para \ncada entrada. Essa cifra de bloco divide a mensagem em blocos de até 3 bits e criptografa cada bloco de acordo com \no mapeamento acima. Você deve verificar que a mensagem 010110001111 é criptografada para 101000111001.\nAinda no exemplo do bloco de 3 bits, observe que o mapeamento na Tabela 8.1 é um de muitos possíveis \nmapeamentos. Quantos deles existem? Para responder a essa questão, observe que um mapeamento nada mais é \ndo que uma permutação de todas as possíveis entradas. Há 23 (= 8) possíveis entradas (relacionadas nas colunas de \nentrada). Elas podem ser permutadas em 8! = 40.320 formas diferentes. Uma vez que cada permutação especifique \nFigura 8.4  Uma cifra polialfabética que utiliza duas cifras de César\nLetra do texto aberto:\na b c d e f g h i j k l m n o p q r s t u v w x y z\nC1(k = 5): \nC2(k = 19): \nf g h i j k l m n o p q r s t u v w x y z a b c d e\nt u v w x y z a b c d e f g h i j k l m n o p q r s\nKR 08.04.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p0 Wide x 4p0 Deep\n11/18/11 rossi\nTabela 8.1  Uma cifra de bloco de 3 bits específica\nEntrada\nSaída\nEntrada\nSaída\n000\n110\n100\n011\n001\n111\n101\n010\n010\n101\n110\n000\n011\n100\n111\n001\nSegurança em redes de computadores  501 \num mapeamento, há 40.320 mapeamentos possíveis. Podemos ver cada mapeamento como uma chave — se Alice \ne Bob sabem o mapeamento (a chave), conseguem criptografar e decodificar as mensagens enviadas entre eles.\nO ataque de força bruta para essa cifra é tentar decodificar o texto cifrado usando todos os mapeamentos. \nCom apenas 40.320 mapeamentos (quando k = 3), isso pode ser rapidamente realizado em um computador de \nmesa. Para impedir os ataques de força bruta, as cifras de bloco normalmente usam blocos muito maiores, con-\nsistindo em k = 64 bits ou ainda maior. Observe que o número de mapeamentos possíveis para uma cifra de bloco \nk geral é 2k!, o qual é extraordinário para até mesmo valores moderados de k (como k = 64).\nEmbora as cifras de bloco da tabela completa, como descritas, com valores moderados de k possam produ-\nzir esquemas robustos de criptografia de chave simétrica, eles infelizmente são difíceis de implementar. Para k = \n64 e para um determinado mapeamento, Alice e Bob precisariam manter uma tabela com 264 valores de entrada, \numa tarefa impraticável. Além disso, se Alice e Bob quiserem trocar chaves, cada um teria de renovar a tabela. \nAssim, a cifra de bloco da tabela completa, que fornece mapeamentos predeterminados entre todas as entradas e \nsaídas (como no exemplo anterior), está simplesmente fora de cogitação.\nEm vez disso, as cifras de bloco costumam utilizar funções que simulam, de maneira aleatória, tabelas \npermutadas. Um exemplo (adaptado de Kaufman [1995]) de tal função para k = 64 bits é mostrado na Figura \n8.5. A função primeiro divide um bloco de 64 bits em 8 blocos, cada qual consistindo de 8 bits. Cada bloco de \n8 bits é processado por uma tabela de 8 bits para 8 bits, a qual possui um tamanho controlável. Por exemplo, \no primeiro bloco é processado pela tabela denotada por T1. Em seguida, os oito blocos de saída são reunidos \nem um bloco de 64 bits. As posições dos 64 bits no bloco são, então, permutadas para produzir uma saída de \n64 bits. Essa saída é devolvida à entrada de 64 bits, onde se inicia outro ciclo. Após n ciclos, a função apresenta \num bloco de 64 bits de texto cifrado. O objetivo de cada ciclo é fazer cada bit de entrada afetar a maioria (se \nnão todos) dos bits finais de saída. (Se somente um ciclo fosse usado, um determinado bit de entrada afetaria so-\nmente 8 dos 64 bits de saída.) A chave para esse algoritmo das cifras de bloco seria as oito tabelas de permutação \n(admitindo que a função de permutação seja publicamente conhecida).\nHoje, existem diversas cifras de bloco conhecidas, incluindo DES (abreviação de Data Encryption Standard \n[Padrão de Criptografia de Dados]), 3DES e AES (abreviação de Advanced Encryption Standard [Padrão de \nCriptografia Avançada]). Cada padrão utiliza funções em vez de tabelas predeterminadas, segundo a Figura 8.5 \n(embora mais complexa e específica para cada cifra). Cada um desses algoritmos também utiliza uma cadeia de \nbits para chave. Por exemplo, o DES usa blocos de 64 bits com uma chave de 56 bits. O AES usa blocos de 128 bits \n \ne pode operar com chaves de 128, 192 e 256 bits de comprimento. A chave de um algoritmo determina os ma-\npeamentos da “minitabela” e permutações dentro do algoritmo. O ataque de força bruta para cada uma dessas \nFigura 8.5  Exemplo de uma cifra de bloco\nSaída de 64 bits\nLoop \npara n \nciclos\nKR 08.05.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n8 bits\n8 bits\nT1\n8 bits\n8 bits\nT2\n8 bits\n8 bits\nT3\n8 bits\nEntrada de 64 bits\n8 bits\nT4\n8 bits\n8 bits\nT5\n8 bits\n8 bits\nT6\n8 bits\n8 bits\nT7\n8 bits\n8 bits\nT8\nCodiﬁcador de 64 bits\n   Redes de computadores e a Internet\n502\ncifras é percorrer todas as chaves, aplicando o algoritmo de decriptografia com cada chave. Observe que com o \ncomprimento de chave n, há 2n chaves possíveis. NIST [2001] estima que uma máquina que pudesse decifrar um \nDES de 56 bits em um segundo (ou seja, testar 256 chaves em um segundo) levaria, mais ou menos, 149 trilhões \nde anos para decifrar uma chave AES de 128 bits.\nEncadeamento de blocos de cifras\nEm aplicações de redes de computadores, em geral precisamos criptografar mensagens longas (ou fluxos de \ndados longos). Se aplicarmos uma cifra de bloco, como descrita, apenas partindo a mensagem em blocos de k bits \ne criptografando de modo independente cada bloco, um problema sutil mas importante ocorrerá. Para enten-\ndê-lo, note que dois ou mais blocos de texto aberto podem ser idênticos. Por exemplo, o texto aberto em dois ou \nmais blocos poderia ser “HTTP/1.1”\n. Em relação a esses blocos idênticos, uma cifra de bloco produziria, é claro, o \nmesmo texto cifrado. Um atacante poderia talvez adivinhar o texto aberto ao ver blocos de texto cifrado idênticos \ne ser capaz de decodificar a mensagem inteira identificando os blocos de texto cifrado idênticos e usando o que \nsabe sobre a estrutura do protocolo subjacente [Kaufman, 1995].\nPara abordar esse problema, podemos associar um pouco de aleatoriedade ao texto cifrado para que blocos \nde texto aberto idênticos produzam blocos de texto cifrado diferentes. Para explicar essa ideia, m(i) represen-\ntará o i-ésimo bloco de texto aberto, c(i) representará o i-ésimo bloco de texto cifrado, e a \n b representará o \nou-exclusivo (XOR) das duas cadeias de bits, a e b. (Lembre-se de que 0 \n 0 = 1 \n 1 = 0 e 0 \n 1 = 1 \n 0 = 1, \ne o XOR das duas cadeias de bits é feito em uma base de bit por bit. Então, por exemplo, 10101010 \n 11110000 \n= 01011010.) Ademais, denote o algoritmo de criptografia da cifra de bloco com a chave S como KS. Eis a ideia \nbásica. O emissor cria um número aleatório r(i) de k bits para o i-ésimo bloco e calcula c(i) = KS (m(i) \n r(i)). \nObserve que um novo número aleatório de k bits é escolhido para cada bloco. O emissor envia, então, c(1), r(1), \nc(2), r(2), c(3), r(3) etc. Visto que o receptor recebe c(i) e r(i), ele pode recuperar cada bloco de texto aberto \ncomputando m(i) = KS (c(i)) \n r(i). É importante observar que, embora r(i) seja enviado de modo inocente e, \nportanto, pode ser analisada por Trudy, ela não consegue obter blocos de texto aberto m(i), pois não conhece a \nchave KS. Observe também que se dois blocos de texto aberto m(i) e m (j) são iguais, os blocos de texto cifrado \ncorrespondentes c(i) e c(j) serão diferentes (contanto que os números r(i) e r(j) aleatórios sejam diferentes, o que \nacontece com alta probabilidade).\nComo um exemplo, considere a cifra de bloco de 3 bits na Tabela 8.1. Suponha que o texto aberto seja \n010010010. Se Alice criptografá-lo diretamente, sem incluir a aleatoriedade, o texto cifrado resultante se torna \n101101101. Se Trudy analisar esse texto cifrado, como cada uma das três cifras de blocos é igual, ela pode pensar \nque cada um dos blocos de texto aberto são iguais. Agora suponha que em vez disso Alice cria blocos aleatórios \nr(1) = 001, r(2) = 111, e r(3) = 100 e use a técnica anterior para criar o texto cifrado c(1) = 100, c(2) = 010 e c(3) = \n000. Observe que os três blocos de texto cifrado são diferentes mesmo se os blocos de texto aberto forem iguais. \nAlice então envia c(1), r(1), c(2) e r(2). Você deve verificar que Bob pode obter o texto aberto original usando a \nchave KS compartilhada.\nO leitor atento observará que introduzir a aleatoriedade resolve o problema, mas cria outro: ou seja, Alice \ndeve transmitir duas vezes mais bits que antes. De fato, para cada bit de cifra, ela deve agora enviar um bit aleató-\nrio, dobrando a largura de banda exigida. Para obtermos o melhor dos dois mundos, as cifras de bloco em geral \nusam uma técnica chamada Encadeamento do Bloco de Cifra (CBC — Cipher Block Chaining). A ideia básica \né enviar somente um valor aleatório junto com a primeira mensagem e, então, fazer o emissor e o receptor usarem \nblocos codificados em vez do número aleatório subsequente. O CBC opera da seguinte forma:\n1.\t Antes de criptografar a mensagem (ou o fluxo de dados), o emissor cria uma cadeia de k bits, chamada \nVetor de Inicialização (IV). Indique esse vetor de inicialização por c(0). O emissor envia o IV ao receptor \nem texto aberto.\n2.\t Em relação ao primeiro bloco, o emissor calcula m(1) \n c(0), ou seja, calcula o ou-exclusivo do primeiro \nbloco de texto aberto com o IV. Ele então codifica o resultado através do algoritmo de cifra de bloco para \nSegurança em redes de computadores  503 \nobter o bloco de texto cifrado correspondente; isto é, c(1) = KS(m(1) \n c(0)). O emissor envia o bloco \ncriptografado c(1) ao receptor.\n3.\t Para o i-ésimo bloco, o emissor cria o i-ésimo bloco de texto cifrado de c(i) = KS(m(i) \n c(i–1)).\nVamos agora examinar algumas das consequências dessa abordagem. Primeiro, o receptor ainda será capaz \nde recuperar a mensagem original. De fato, quando o receptor recebe c(i), ele o decodifica com KS para obter s(i) \n= m(i) \n c(i–1); uma vez que o receptor também conhece c(i–1), ele então obtém o bloco de texto aberto de \nm(i) = s(i) \n c(i–1). Segundo, mesmo se dois blocos de texto aberto forem idênticos, os texto cifrados corres-\npondentes (quase sempre) serão diferentes. Terceiro, embora o emissor envie o IV aberto, um invasor ainda não \nserá capaz de decodificar os blocos de texto cifrado, visto que o invasor não conhece a chave secreta, S. Por fim, o \nemissor somente envia um bloco auxiliar (o IV), fazendo aumentar, de forma insignificante, o uso da largura de \nbanda para longas mensagens (consistindo de centenas de blocos).\nComo um exemplo, vamos determinar o texto cifrado para uma cifra de bloco de 3 bits na Tabela 8.1 com \ntexto aberto 010010010 e IV = c(0) = 001. O emissor primeiro usa o IV para calcular c(1) = KS (m(1) \n c(0)) = \n100. O emissor calcula, então, c(2) = KS(m(2) \n c(1)) = KS(010 \n 100) = 000, e c(3) = KS (m(3) \n c(2)) = KS (010 \n 000) = 101. O leitor deve verificar que o receptor, conhecendo o IV e KS, pode recuperar o texto aberto original.\nO CBC possui uma consequência importante ao projetar protocolos de rede seguros: precisaremos fornecer \num mecanismo dentro do protocolo para distribuir o IV do emissor ao receptor. Veremos como isso é feito para \nvários protocolos mais adiante, neste capítulo.\n8.2.2  Criptografia de chave pública\nPor mais de dois mil anos (desde a época da cifra de César até a década de 1970), a comunicação cifrada exigia \nque as duas partes comunicantes compartilhassem um segredo — a chave simétrica usada para cifrar e decifrar. Uma \ndificuldade dessa abordagem é que as duas partes têm de concordar, de alguma maneira, com a chave compartilhada, \nmas, para fazê-lo, é preciso comunicação (presumivelmente segura)! Talvez as partes pudessem se encontrar antes, es-\ncolher a chave (por exemplo, dois dos centuriões de César poderiam se encontrar nos banhos romanos) e, mais tarde, \nse comunicar de modo cifrado. Em um mundo em rede, contudo, o mais provável é que as partes comunicantes nunca \npossam se encontrar e jamais consigam conversar a não ser pela rede. É possível que elas se comuniquem por cripto-\ngrafia sem compartilhar uma chave comum secreta conhecida com antecedência? Em 1976, Diffie e Hellman [Diffie, \n1976] apresentaram um algoritmo (conhecido como Troca de Chaves Diffie Hellman — Diffie Hellman Key Exchange) \nque faz exatamente isso — uma abordagem da comunicação segura radicalmente diferente e de uma elegância mara-\nvilhosa que levou ao desenvolvimento dos atuais sistemas de criptografia de chaves públicas. Veremos em breve que \nos sistemas de criptografia de chaves públicas também têm diversas propriedades maravilhosas que os tornam úteis \nnão só para criptografia, mas também para autenticação e assinaturas digitais. É interessante que, há pouco, veio à luz \nque ideias semelhantes às de [Diffie, 1976] e às da [RSA, 1978] foram desenvolvidas independentemente no início da \ndécada de 1970 em uma série de relatórios secretos escritos por pesquisadores do Grupo de Segurança para Comuni-\ncação e Eletrônica (Communications-Electronics Security Group) do Reino Unido [Ellis, 1987]. Como acontece com \nfrequência, grandes ideias podem surgir de modo independente em diversos lugares; felizmente, os progressos da \ncriptografia de chaves públicas ocorreram não apenas no âmbito privado, mas também no público.\nA utilização de criptografia de chaves públicas, como conceito, é bastante simples. Suponha que Alice queira \nse comunicar com Bob. Como ilustra a Figura 8.6, em vez de Bob e Alice compartilharem uma única chave secreta \n(como no caso dos sistemas de chaves simétricas), Bob (o destinatário das mensagens de Alice) tem duas chaves \n— uma chave pública, que está à disposição do mundo todo (inclusive de Trudy, a intrusa), e uma chave privada, \nque apenas ele (Bob) conhece. Usaremos a notação KB\n+ e KB\n– para nos referirmos às chaves pública e privada de Bob, \nrespectivamente. Para se comunicar com Bob, Alice busca primeiro a chave pública de Bob. Em seguida, ela crip-\ntografa sua mensagem, m, usando a chave pública de Bob e um algoritmo criptográfico conhecido (por exemplo, \npadronizado), isto é, Alice calcula KB\n+(m). Bob recebe a mensagem criptografada de Alice e usa sua chave privada \ne um algoritmo de decriptação conhecido (por exemplo, padronizado) para decifrar a mensagem de Alice, \n   Redes de computadores e a Internet\n504\nisto é, Bob calcula KB\n–(KB\n+(m)). Veremos, mais adiante, que há algoritmos e técnicas de criptografia/decriptação \npara escolher chaves públicas e privadas de modo que KB\n–(KB\n+(m)) = m; isto é, aplicando a chave pública de Bob, \nKB\n+, à mensagem m, para obter KB\n+(m), e então aplicando a chave privada de Bob, KB\n–, à versão criptografada de m, \nisto é, calculando KB\n–(KB\n+(m)), obtemos m novamente. Esse resultado é notável! Dessa maneira, Alice pode utilizar \na chave de Bob disponível publicamente para enviar uma mensagem secreta a Bob sem que nenhum deles tenha de \npermutar nenhuma chave secreta! Veremos em breve que nós podemos permutar a chave pública e a chave privada \nde criptografia e obter o mesmo resultado notável, isto é, KB\n–(KB\n+(m)) = KB\n+(KB\n–(m)) = m.\nA utilização de criptografia de chave pública é, portanto, conceitualmente simples. Mas há duas preocupações \nque podem nos ocorrer de imediato. A primeira é que, embora um intruso que intercepte a mensagem cifrada de \nAlice veja apenas dados ininteligíveis, ele conhece tanto a chave (a chave pública de Bob, disponível a todos) quanto \no algoritmo que Alice usou para a criptografia. Assim, Trudy pode montar um ataque com texto aberto escolhido \nutilizando o algoritmo criptográfico padronizado conhecido e a chave criptográfica de acesso público de Bob para \ncodificar a mensagem que quiser! Ela pode até tentar, por exemplo, codificar mensagens, ou partes delas, que sus-\npeita que Alice poderia enviar. Fica claro que, para a criptografia de chave pública funcionar, a escolha de chaves e \nde códigos de criptografia/decriptação deve ser feita de tal maneira que seja impossível (ou, ao menos, tão difícil que \nse torne quase impossível) para um intruso determinar a chave privada de Bob ou conseguir decifrar ou adivinhar a \nmensagem de Alice a Bob. A segunda preocupação é que, como a chave criptográfica de Bob é pública, qualquer um \npode enviar uma mensagem cifrada a Bob, incluindo Alice ou alguém afirmando ser Alice. No caso de uma única \nchave secreta compartilhada, o fato de o remetente conhecer a chave secreta identifica implicitamente o remetente \npara o destinatário. No caso da criptografia de chave pública, contudo, isso não acontece, já que qualquer um pode \nenviar uma mensagem cifrada a Bob usando a chave dele, que está publicamente disponível a todos. É preciso uma \nassinatura digital, um tópico que estudaremos na Seção 8.3, para vincular um remetente a uma mensagem.\nRSA\nEmbora existam muitos algoritmos e chaves que tratam dessas preocupações, o algoritmo RSA (cujo nome \nse deve a seus inventores, Ron Rivest, Adi Shamir e Leonard Adleman) tornou-se quase um sinônimo de crip-\ntografia de chave pública. Vamos, primeiro, ver como o RSA funciona e, depois, examinar por que ele funciona. \nO RSA faz uso extensivo das operações aritméticas usando a aritmética de módulo-n. Vamos revisar de \nmaneira breve a aritmética modular. Lembre-se de que x mod n simplesmente significa o resto de x quando divi-\ndido por n, de modo que, por exemplo, 19 mod 5 = 4. Na aritmética modular, uma pessoa executa as operações \ncomuns de adição, multiplicação e exponenciação. Entretanto, o resultado de cada operação é substituído pelo \nresto inteiro que sobra quando o resultado é dividido por n. A adição e a multiplicação com a aritmética modular \nsão facilitadas com as seguintes propriedades úteis:\nFigura 8.6  Criptografia de chaves públicas\nAlgoritmo de \ncriptograﬁa\nTexto cifrado\nAlgoritmo de \ndecriptação\nMensagem em \ntexto aberto, m\nMensagem em \ntexto aberto, m\nChave criptográﬁca privada\nm = KB\n–(KB\n+(m))\nKB\n–\nKB\n+(m)\nChave criptográﬁca pública\nKB\n+\nKR 08.06.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p0 Wide x 12p10 Deep\n11/18/11 rossi\nSegurança em redes de computadores  505 \n[(a mod n) + (b mod n)] mod n = (a + b) mod n\n[(a mod n) – (b mod n)] mod n = (a – b) mod n\n[(a mod n) • (b mod n)] mod n = (a • b) mod n\nSegue da terceira propriedade que (a mod n)d mod n = ad mod n, uma identidade que em breve acharemos \nmuito útil.\nAgora suponha que Alice queira enviar a Bob uma mensagem criptografada por meio do RSA, conforme \nilustrado na Figura 8.6. Em nossa discussão sobre o RSA, vamos sempre manter em mente que uma mensagem \nnão é nada mais do que um padrão de bits, e cada padrão de bits pode ser representado unicamente por um nú-\nmero inteiro (junto com o comprimento do padrão de bits). Por exemplo, suponha que uma mensagem tenha o \npadrão de bits 1001; essa mensagem pode ser representada pelo número inteiro decimal 9. Assim, criptografar \numa mensagem com RSA é equivalente a criptografar um número inteiro que representa a mensagem. \nExistem dois componentes inter-relacionados do RSA:\n• A escolha da chave pública e da chave privada.\n• O algoritmo de criptografia/decriptação.\nPara escolher as chaves pública e privada no RSA, Bob deve executar as seguintes etapas:\n1.\t Escolher dois números primos grandes, p e q. Que ordem de grandeza devem ter p e q? Quanto maiores \nos valores, mais difícil será quebrar o RSA, mas mais tempo se levará para realizar a codificação e a deco-\ndificação. O RSA Laboratories recomenda que o produto de p e q seja da ordem de 1.024 bits. Para uma \ndiscussão sobre como achar números primos grandes, consulte Caldwell [2012].\n2.\t Calcular n = pq e z = (p – 1)(q – 1).\n3.\t Escolher um número e menor do que n que não tenha fatores comuns (exceto o 1) com z. (Nesse caso, \ndizemos que e e z são números primos entre si.) A letra “e” é usada já que esse valor será utilizado na crip-\ntografia (“encryption”\n, em inglês).\n4.\t Achar um número d, tal que ed – 1 seja divisível exatamente (isto é, não haja resto na divisão) por z. A letra \n“d” é usada porque seu valor será utilizado na decriptação. Em outras palavras, dado e, escolhemos d tal que\ned mod z = 1\n5.\t A chave pública que Bob põe à disposição de todos, KB\n+, é o par de números (n, e); sua chave privada, KB\n–, \né o par de números (n, d).\nA criptografia feita por Alice e a decriptação feita por Bob acontecem da seguinte forma:\n• Suponha que Alice queira enviar a Bob um padrão de bits, ou número m, tal que m < n. Para codificar, \nAlice calcula a potência me e, então, determina o resto inteiro da divisão de me por n. Assim, o valor ci-\nfrado, c, da mensagem em texto aberto de Alice, m, é:\nc = me mod n\n\t\nO padrão de bits correspondente a esse texto cifrado c é enviado a Bob.\n• Para decifrar a mensagem em texto cifrado recebida, c, Bob calcula\nm = cd mod n\n\t\nque exige o uso de sua chave secreta (n, d).\nComo exemplo simples de RSA, suponha que Bob escolha p = 5 e q = 7. (Admitimos que esses valores são \nmuito pequenos para ser seguros.) Então, n = 35 e z = 24. Bob escolhe e = 5, já que 5 e 24 não têm fatores comuns. \nPor fim, ele escolhe d = 29, já que 5 ⋅ 29 – 1 (isto é, ed – 1) é divisível exatamente por 24. Ele divulga os dois valo-\nres, n = 35 e e = 5, e mantém em segredo o valor d = 29. Observando esses dois valores públicos, suponha que \nAlice queira agora enviar as letras l, o, v e e a Bob. Interpretando cada letra como um número entre 1 e 26 (com \na sendo 1 e z sendo 26), Alice e Bob realizam a criptografia e a decriptação mostradas nas tabelas 8.2 e 8.3, \n \n   Redes de computadores e a Internet\n506\nrespectivamente. Observe que neste exemplo consideramos cada uma das quatro letras como uma mensagem \ndistinta. Um exemplo mais realista seria converter as quatro letras em suas representações ASCII de 8 bits e então \ncodificar o número inteiro correspondente ao padrão de bits de 32 bits resultante. (Esse exemplo realista cria \nnúmeros muito longos para publicar neste livro!)\nDado que o exemplo fictício das tabelas 8.2 e 8.3 já produziu alguns números extremamente grandes e visto \nque sabemos, porque vimos antes, que p e q devem ter, cada um, algumas centenas de bits de comprimento, várias \nquestões práticas nos vêm à mente no caso do RSA. Como escolher números primos grandes? Como escolher e e \nd? Como calcular exponenciais de números grandes? A discussão desses assuntos está além do escopo deste livro; \nconsulte Kaufman [1995] e as referências ali citadas para obter mais detalhes.\nTabela 8.2  Criptografia RSA para Alice, e = 5, n = 35\nLetra do texto aberto\nm: representação numérica\nm e\nTexto cifrado \nc = m e mod n\nl\n12\n248832\n17\no\n15\n759375\n15\nv\n22\n5153632\n22\ne\n5\n3125\n10\nChaves de sessão\nObservamos aqui que a exponenciação exigida pelo RSA é um processo que consome tempo considerável. \nO DES, ao contrário, é, no mínimo, cem vezes mais veloz em software e entre mil e dez mil vezes mais veloz em \nhardware [RSA Fast, 2012]. Como resultado, o RSA é frequentemente usado na prática em combinação com a \ncriptografia de chave simétrica. Por exemplo, se Alice quer enviar a Bob uma grande quantidade de dados ci-\nfrados a alta velocidade, ela pode fazer o seguinte. Primeiro escolhe uma chave que será utilizada para codificar \nos dados em si; essa chave às vezes é denominada chave de sessão, representada por KS. Alice deve informar a \nBob essa chave de sessão, já que essa é a chave simétrica compartilhada que eles usarão com uma cifra de chave \nsimétrica (por exemplo, DES ou AES). Alice criptografa o valor da chave de sessão usando a chave pública RSA \nde Bob, isto é, ela processa c = (KS)e mod n. Bob recebe a chave de sessão codificada RSA, c, e a decifra para obter \na chave de sessão KS. Ele agora conhece a chave que Alice usará para transferir dados cifrados.\nPor que o RSA funciona?\nA criptografia/decriptação do RSA parece mágica. Por que será que, aplicando o algoritmo de criptografia e, \nem seguida, o de decriptação, podemos recuperar a mensagem original? Para entender por que o RSA funciona, \ntome de novo n = pq, onde p e q são os números primos grandes usados no algoritmo RSA.\nTabela 8.3  Decriptação RSA para Bob, d = 29, n = 35\nTexto cifrado c\nc d\nm = c d mod n\nLetra do texto \naberto\n17\n4819685721067509150915091411825223071697\n12\nl\n15\n127834039403948858939111232757568359375\n15\no\n22\n851643319086537701956194499721106030592\n22\nv\n10\n1000000000000000000000000000000\n5\ne\nSegurança em redes de computadores  507 \nLembre-se de que, na criptografia RSA, uma mensagem (representada exclusivamente por um número in-\nteiro) m é elevada primeiro à potência e usando-se aritmética de módulo n, ou seja,\nc = me mod n\nA decriptação é feita elevando-se esse valor à potência d, novamente usando a aritmética de módulo n. O \nresultado de uma etapa de criptografia, seguida de uma etapa de decriptação, é então (me mod n)d mod n. Vamos \nver agora o que podemos dizer sobre essa quantidade. Como mencionado antes, uma propriedade importante da \naritmética modular é (a mod n)d mod n = ad mod n para quaisquer valores a, n e d. Então, usando a = me nesta \npropriedade, temos:\n(me mod n)d mod n = med mod n\nPortanto, falta mostrar que med mod n = m. Embora estejamos tentando eliminar um pouco da mágica do \nmodo de funcionamento do RSA, para explicá-lo, precisaremos usar, aqui, outro resultado bastante mágico da \nteoria dos números. Especificamente, precisamos do resultado que diga que, se p e q forem primos, n = pq, e z = \n(p – 1)(q – 1), então xy mod n será o mesmo que x(y mod z) mod n [Kaufman, 1995]. Aplicando esse resultado com \nx = m e y = ed, temos\nmed mod n = m(ed mod z) mod n\nMas lembre-se de que escolhemos e e d tais que ed mod z = 1. Isso nos dá\nmed mod n = m1 mod n = m\nque é exatamente o resultado que esperávamos! Efetuando primeiro a exponenciação da potência e (isto é, crip-\ntografando) e depois a exponenciação da potência d (isto é, decriptando), obtemos o valor original m. E mais \nnotável ainda é o fato de que, se elevarmos primeiro à potência d e, em seguida, à potência e, isto é, se invertermos \na ordem da criptografia e da decriptação, realizando primeiro a operação de decriptação e, em seguida, aplicando \na de criptografia — também obteremos o valor original m. Esse resultado extraordinário resulta imediatamente \nda aritmética modular:\n(md mod n)e mod n = mde mod n = med mod n = (me mod n)d mod n\nA segurança do RSA reside no fato de que não se conhecem algoritmos para fatorar rapidamente um núme-\nro, nesse caso, o valor público n, em números primos p e q. Se alguém conhecesse os números p e q, então, dado o \nvalor público e, poderia com facilidade processar a chave secreta d. Por outro lado, não se sabe se existem ou não \nalgoritmos rápidos para fatorar um número e, nesse sentido, a segurança do RSA não é garantida.\nOutro conhecido algoritmo de criptografia de chave pública é o Diffie-Hellman, que será explorado resumi-\ndamente nos Problemas. O Diffie-Hellman não é tão versátil quanto o RSA, pois não pode ser usado para cifrar \nmensagens de comprimento arbitrário; ele pode ser usado, entretanto, para determinar uma chave de sessão \nsimétrica, que, por sua vez, é utilizada para codificar mensagens.\n8.3  \u0007\nIntegridade de mensagem e assinaturas digitais\nNa seção anterior, vimos como a criptografia pode ser usada para oferecer sigilo a duas entidades em comu-\nnicação. Nesta seção, nos voltamos ao assunto igualmente importante da criptografia que é prover integridade \nda mensagem (também conhecida como autenticação da mensagem). Além da integridade da mensagem, discu-\ntiremos dois assuntos parecidos nesta seção: assinaturas digitais e autenticação do ponto final.\nDefinimos o problema de integridade da mensagem usando, mais uma vez, Alice e Bob. Suponha que Bob \nreceba uma mensagem (que pode ser cifrada ou estar em texto aberto) acreditando que tenha sido enviada por \nAlice. Para autenticar a mensagem, Bob precisa verificar se:\n1.\t A mensagem foi, realmente, enviada por Alice.\n2.\t A mensagem não foi alterada em seu caminho para Bob.\n   Redes de computadores e a Internet\n508\nVeremos, nas seções 8.4 a 8.7, que esse problema de integridade da mensagem é uma preocupação impor-\ntante em todos os protocolos de rede seguros.\nComo um exemplo específico, considere uma rede de computadores que está utilizando um algoritmo de \nroteamento de estado de enlace (como OSPF) para determinar rotas entre cada dupla de roteadores na rede (veja \nCapítulo 4). Em um algoritmo de estado de enlace, cada roteador precisa transmitir uma mensagem de estado de \nenlace a todos os outros roteadores na rede. Uma mensagem de estado de enlace do roteador inclui uma relação \nde seus vizinhos diretamente conectados e os custos diretos a eles. Uma vez que o roteador recebe mensagens de \nestado de enlace de todos os outros roteadores, ele pode criar um mapa completo da rede, executar seu algoritmo \nde roteamento de menor custo e configurar sua tabela de repasse. Um ataque relativamente fácil no algoritmo de \nroteamento é Trudy distribuir mensagens de estado de enlace falsas com informações incorretas sobre o estado \nde enlace. Assim, a necessidade de integridade da mensagem — quando o roteador B recebe uma mensagem de \nestado de enlace do roteador A, deve verificar que o roteador A na verdade criou a mensagem e que ninguém a \nalterou em trânsito.\nNesta seção descrevemos uma técnica conhecida sobre integridade da mensagem usada por muitos proto-\ncolos de rede seguros. Mas, antes disso, precisamos abordar outro importante tópico na criptografia — as funções \nde hash criptográficas.\n8.3.1  Funções de hash criptográficas\nComo mostrado na Figura 8.7, a função de hash recebe uma entrada, m, e calcula uma cadeia de tamanho \nfixo H(m) conhecida como hash. A soma de verificação da Internet (Capítulo 3) e os CRCs (Capítulo 4) satisfa-\nzem essa definição. Uma função hash criptográfica deve apresentar a seguinte propriedade adicional:\n• Em termos de processamento, é impraticável encontrar duas mensagens diferentes x e y tais que H(x) = H(y).\nInformalmente, essa propriedade significa que, em termos de processamento, é impraticável que um invasor \nsubstitua uma mensagem por outra que está protegida pela função hash. Ou seja, se (m, H(m)) é a mensagem e \no hash da mensagem criada pelo emissor, um invasor não pode forjar os conteúdos de outra mensagem, y, que \npossui o mesmo valor de hash da mensagem original.\nÉ bom nos convencermos de que uma soma de verificação simples, como a soma de verificação da Internet, \ncriaria uma função de hash criptográfica fraca. Em vez de processar a aritmética de complemento de 1 (como é fei-\nto para a soma de verificação da Internet), vamos efetuar uma soma de verificação tratando cada caractere como um \nbyte e somando os bytes usando porções de 4 bytes por vez. Suponha que Bob deva a Alice 100,99 dólares e lhe envie \num vale contendo a cadeia de texto “IOU100.99BOB” (IOU — I Owe You — Eu lhe devo.) A representação ASCII \n(em notação hexadecimal) para essas letras é 49, 4F, 55, 31, 30, 30, 2E, 39, 39, 42, 4F, 42.\nFigura 8.7  Funções de hash\nFunção de hash \nmuitos-para-um\nMensagem longa: m\nMensagem longa:\nEssa carta é MUITO longa\npois tenho muito a dizer...\n..........\n..........\n..........\nBob\nResumo de mensagem \nde tamanho ﬁxo: H(m)\nOpgmdvboijrtnsd\ngghPPdogm;lcvkb\nSegurança em redes de computadores  509 \nA Figura 8.8 (parte de cima) mostra que a soma de verificação de 4 bytes para essa mensagem é B2 C1 D2 \nAC. Uma mensagem ligeiramente diferente (e que sairia muito mais cara para Bob) é mostrada na parte de baixo \nda Figura 8.8. As mensagens “IOU100.99BOB” e “IOU900.19BOB” têm a mesma soma de verificação. Assim, \nesse algoritmo de soma de verificação simples viola a exigência anterior. Fornecidos os dados originais, é simples \ndescobrir outro conjunto de dados com a mesma soma de verificação. É claro que, para efeito de segurança, pre-\ncisaremos de uma função de hash muito mais poderosa do que uma soma de verificação.\nO algoritmo de hash MD5 de Ron Rivest [RFC 1321] é amplamente usado hoje. Ele processa um resumo \nde mensagem de 128 bits por meio de um processo de quatro etapas, constituído de uma etapa de enchimento \n(adição de um “um” seguido de “zeros” suficientes, para que o comprimento da mensagem satisfaça determinadas \ncondições), uma etapa de anexação (anexação de uma representação de 64 bits do comprimento da mensagem \nantes do enchimento), uma etapa de inicialização de um acumulador e uma etapa final iterativa, na qual os blocos \nde 16 palavras da mensagem são processados (misturados) em quatro rodadas de processamento. Para ver uma \ndescrição do MD5 (incluindo uma implementação em código fonte C), consulte [RFC 1321].\nO segundo principal algoritmo de hash em uso atualmente é o SHA-1 (Secure Hash Algorithm — algoritmo \nde hash seguro) [FIPS, 1995]. Esse algoritmo se baseia em princípios similares aos usados no projeto do MD4 \n[RFC 1320], o predecessor do MD5. O uso do SHA-1, um padrão federal norte-americano, é exigido sempre que \num algoritmo de hash criptográfico for necessário para aos federais. Ele produz uma mensagem de resumo de 160 \nbits. O resultado mais longo torna o SHA-1 mais seguro.\nFigura 8.8  Mensagem inicial e mensagem fraudulenta têm a mesma soma de verificação!\nMensagem\nI O U 1\n0 0 . 9\n9 B O B\nASCII\nRepresentação\n49\n4F\n55\n31\n30\n30\n2E\n39\n39\n42\n4F\n42\nB2\nC1\nD2\nAC\nVeriﬁcação\nMensagem\nI O U 9\n0 0 . 1\n9 B O B\nASCII\n49\n4F\n55\n39\n30\n30\n2E\n31\n39\n42\n4F\n42\nB2\nC1\nD2\nAC\nRepresentação\nVeriﬁcação\n8.3.2  Código de autenticação da mensagem\nRetornemos ao problema da integridade da mensagem. Agora que compreendemos as funções de hash, \nvamos tentar entender como podemos garantir a integridade da mensagem:\n1.\t Alice cria a mensagem m e calcula o hash H(m) (por exemplo, com SHA-1).\n2.\t Alice então anexa H(m) à mensagem m, criando uma mensagem estendida (m, H(m)), e a envia para Bob.\n3.\t Bob recebe uma mensagem estendida (m, h) e calcula H(m). Se H(m) = h, Bob conclui que está tudo certo.\nEssa abordagem é, obviamente, errônea. Trudy pode criar uma mensagem m΄ falsa, passar-se por Alice, \ncalcular H(m΄) e enviar (m΄, H(m΄)) a Bob. Quando Bob receber a mensagem, tudo se encaixa na etapa 3, então \nele não suspeita de nada.\n   Redes de computadores e a Internet\n510\nPara realizar a integridade da mensagem, além de usar as funções de hash criptográficas, Alice e Bob precisa-\nrão de um segredo compartilhado s, que não é nada mais do que uma cadeia de bits denominada chave de autenti-\ncação. Utilizando esse segredo compartilhado, a integridade da mensagem pode ser realizada da seguinte maneira:\n1.\t Alice cria a mensagem m, concatena s com m para criar m + s, e calcula o hash H(m+s) (por exemplo, com \nSHA-1). H(m+s) é denominado o código de autenticação da mensagem (MAC).\n2.\t\nAlice então anexa o MAC à mensagem m, criando uma mensagem estendida (m, H(m+s)), e a envia para Bob.\n3.\t Bob recebe uma mensagem estendida (m, h) e, conhecendo s, calcula o MAC H(m+s). Se H(m+s) = h, Bob \nconclui que está tudo certo.\nUm resumo desse processo é ilustrado na Figura 8.9. É importante observar que o MAC (abreviação de \nmessage authentication code [código de autenticação da mensagem]), neste caso, não é o mesmo MAC utilizado \nnos protocolos da camada de enlace (abreviação de medium access control [controle de acesso ao meio])!\nUm bom recurso do MAC é o fato de ele não exigir um algoritmo de criptografia. De fato, em muitas apli-\ncações, incluindo o algoritmo de roteamento de estado do enlace, descrito antes, as entidades de comunicação \nsomente estão preocupadas com a integridade da mensagem e não com o seu sigilo. Utilizando um MAC, as \nentidades podem autenticar as mensagens que enviam uma à outra sem ter de integrar algoritmos de criptografia \ncomplexos ao processo de integridade.\nComo você pode esperar, muitos padrões diferentes para os MACs foram propostos ao longo dos anos. \nHoje, o mais popular é o HMAC, que pode ser usado com o MD5 ou SHA-1. O HMAC, na verdade, passa os \ndados e a chave de autenticação duas vezes pela função de hash [Kaufman, 1995; RFC 2104].\nAinda resta um assunto importante. Como distribuímos a chave de autenticação compartilhada às entida-\ndes de comunicação? No algoritmo de roteamento de estado do enlace, por exemplo, precisaríamos, de alguma \nforma, distribuir a chave de autenticação secreta a cada um dos roteadores no sistema independente. (Observe \nque os roteadores podem usar a mesma chave de autenticação.) Um administrador de rede conseguiria, de fato, \nesse feito visitando fisicamente cada um dos roteadores. Ou, se o administrador de rede for preguiçoso, e se cada \nroteador possuir sua própria chave pública, ele poderia distribuir a chave de autenticação a qualquer um dos \nroteadores criptografando-a com a chave pública e, então, enviar a chave cifrada por meio da rede ao roteador.\nFigura 8.9  Código de autenticação de mensagem (MAC)\nH(.)\nH(.)\nm\nm\nm\nm\ns\ns\ns\n+\nInternet\nCompa-\nração\nKR 08.09.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n26p7  x  12p0\n11/18/11 rossi\nLegenda:\n= Mensagem\n= Segredo compartilhado\nH(m+s)\nH(m+s)\n8.3.3  Assinaturas digitais\nPense no número de vezes em que você assinou seu nome em um pedaço de papel durante a última semana. \nVocê assina cheques, comprovantes de operação de cartões de crédito, documentos legais e cartas. Sua assinatura \natesta o fato de que você (e não outra pessoa) conhece o conteúdo do documento e/ou concorda com ele. No \nmundo digital, com frequência deseja-se indicar o dono ou o criador de um documento ou deixar claro que alguém \nSegurança em redes de computadores  511 \nconcorda com o conteúdo de um documento. A assinatura digital é uma técnica criptográfica que cumpre essas \nfinalidades no mundo digital.\nExatamente como acontece com as assinaturas por escrito, a assinatura digital deve ser verificável e não fal-\nsificável. Isto é, deve ser possível provar que um documento assinado por um indivíduo foi na verdade assinado \npor ele (a assinatura tem de ser verificável) e que somente aquele indivíduo poderia ter assinado o documento (a \nassinatura não pode ser falsificada).\nVamos considerar agora como podemos criar um método de assinatura digital. Observe que, quando Bob \nassina uma mensagem, deve colocar algo nela que seja único para ele. Bob poderia adicionar um MAC à assina-\ntura, sendo o MAC criado ao adicionar sua chave (única para ele) à mensagem e, depois, formar o hash. Mas, para \nAlice verificar a assinatura, ela deve também ter uma cópia da chave, que não seria única para Bob. Portanto, os \nMACs não se incluem nesse processo.\nLembre-se de que, com a criptografia de chave pública, Bob possui tanto uma chave pública como uma pri-\nvada, as quais são únicas para ele. Dessa maneira, a criptografia de chave pública é uma excelente candidata para \nprover assinaturas digitais. Vamos verificar como isso é feito.\nSuponha que Bob queira assinar digitalmente um documento, m. Imagine que o documento seja um arquivo \nou uma mensagem que ele vai assinar e enviar. Como mostra a Figura 8.10, para assinar esse documento Bob apenas \nusa sua chave criptográfica privada KB\n– para processar KB\n–(m). A princípio, pode parecer estranho que Bob esteja \nusando sua chave privada (que, como vimos na Seção 8.2, foi usada para decriptar uma mensagem que tinha sido \ncriptografada com sua chave pública) para assinar um documento. Mas lembre-se de que criptografia e decriptação \nnada mais são do que uma operação matemática (exponenciação à potência e ou d no RSA; veja a Seção 8.2) e que \na intenção de Bob não é embaralhar ou disfarçar o conteúdo do documento, mas assiná-lo de maneira que este seja \nverificável, não falsificável e incontestável. Bob tem o documento, m, e sua assinatura digital do documento é KB\n–(m).\nA assinatura digital KB\n–(m) atende às exigências de ser verificável e não falsificável? Suponha que Alice tenha \nm e KB\n–(m). Ela quer provar na Justiça (em ação litigiosa) que Bob de fato assinou o documento e que ele era a \núnica pessoa que poderia tê-lo assinado. Alice pega a chave pública de Bob, KB\n+(m), e lhe aplica a assinatura digital \nKB\n–(m) associada ao documento m. Isto é, ela processa KB\n+(KB\n–(m)), e, voilà, com dramática encenação, produz m, \nque é uma reprodução exata do documento original! Ela então argumenta que somente Bob poderia ter assinado \no documento pelas seguintes razões:\n• Quem quer que tenha assinado o documento deve ter usado a chave criptográfica privada, KB\n–, para pro-\ncessar a assinatura KB\n–(m), de modo que KB\n+(KB\n–(m)) = m.\n• A única pessoa que poderia conhecer a chave privada, KB\n–, é Bob. Lembre-se de que dissemos em nossa \ndiscussão do RSA, na Seção 8.2, que conhecer a chave pública, KB\n+, não serve para descobrir a chave priva-\nda, KB\n–. Portanto, a única pessoa que poderia conhecer KB\n– é aquela que gerou o par de chaves (KB\n+, KB\n–) em \nFigura 8.10  Criando uma assinatura digital para um documento\nAlgoritmo \ncriptográﬁco\nMensagem: m\nChave privada\nde Bob, KB\n–\nQuerida Alice:\nDesculpe-me por ter\ndemorado tanto para\nescrever. Desde que\nnós....\n..........\nBob\nMensagem assinada \nKB\n– (m)\nfadfg54986fgnzmcnv\nT98734ngldskg02j\nser09tugkjdﬂg\n..........\n   Redes de computadores e a Internet\n512\nprimeiro lugar, Bob. (Note que, para isso, admitimos que Bob não passou KB\n– a ninguém e que ninguém \nroubou KB\n– de Bob.)\nTambém é importante notar que, se o documento original, m, for modificado para alguma forma alterna-\ntiva, m', a assinatura que Bob criou para m não será válida para m', já que KB\n+(KB\n– (m)) não é igual a m'. Assim, \nobservamos que as assinaturas digitais também oferecem integridade da mensagem, permitindo que o receptor \nverifique se ela foi alterada, bem como sua origem.\nUma preocupação com os dados de assinatura é que a criptografia e a decriptação prejudicam o desempe-\nnho do computador. Sabendo do trabalho extra de criptografia e decriptação, os dados de assinatura por meio \nde criptografia/decriptação completa podem ser excessivos. Uma técnica mais eficiente é introduzir as funções \nde hash na assinatura digital. Na Seção 8.3.2, um algoritmo de hash pega uma mensagem, m, de comprimento \nqualquer e calcula uma “impressão digital” de comprimento fixo da mensagem, representada por H(m). Usando \numa função de hash, Bob assina o hash da mensagem em vez de assinar a própria mensagem, ou seja, Bob calcula \nKB\n–(H(m)). Como H(m) é em geral muito menor do que a mensagem original m, o esforço computacional neces-\nsário para criar a assinatura digital é reduzido consideravelmente.\nEm relação a Bob enviar uma mensagem para Alice, a Figura 8.11 apresenta um resumo do procedimento \noperacional de criar uma assinatura digital. Bob coloca sua longa mensagem original em uma função de hash. \nEle, então, assina digitalmente o hash resultante com sua chave privada. A mensagem original (em texto claro) \njunto com o resumo de mensagem assinada digitalmente (de agora em diante denominada assinatura digital) é \nentão enviada para Alice. A Figura 8.12 apresenta um resumo do processo operacional da assinatura. Alice aplica \na função de hash à mensagem para obter um resultado de hash. Ela também aplica a função de hash à mensagem \nem texto claro para obter um segundo resultado de hash. Se os dois combinarem, então Alice pode ter certeza \nsobre a integridade e o autor da mensagem.\nAntes de seguirmos em frente, vamos fazer uma breve comparação entre as assinaturas digitais e os MACs, visto \nque eles são paralelos, mas também têm diferenças sutis e importantes. As assinaturas digitais e os MACs iniciam com \numa mensagem (ou um documento). Para criar um MAC por meio de mensagem, inserimos uma chave de autenti-\ncação à mensagem e, depois, pegamos o hash do resultado. Observe que nem a chave pública nem a criptografia de \nFigura 8.11  Enviando uma mensagem assinada digitalmente\nChave privada de Bob:\nKB\n–\nFunção de hash \nmuitos-para-um\nMensagem longa\nCara Alice:\nEssa carta é MUITO longa,\npois tenho muito a dizer...\n..........\n..........\n..........\nBob\nResumo de mensagem \nde tamanho ﬁxo\nOpgmdvboijrtnsd\ngghPPdogm;lcvkb\nResumo de \nmensagem assinado\nPacote a enviar \npara Alice\nFgkopdgoo69cmxw\n54psdterma[asofmz\nAlgoritmo \ncriptográﬁco\nSegurança em redes de computadores  513 \nchave simétrica estão envolvidas na criação do MAC. Para criar uma assinatura digital, primeiro pegamos o hash da \nmensagem e, depois, ciframos a mensagem com nossa chave privada (usando a criptografia de chave pública). Assim, \numa assinatura digital é uma técnica “mais pesada”\n, pois exige uma Infraestrutura de Chave Pública (PKI) subjacente \ncom autoridades de certificação, conforme descrito abaixo. Veremos na Seção 8.4 que o PGP — um sistema seguro de \ne-mail e popular — utiliza assinaturas digitais para a integridade da mensagem. Já vimos que o OSPF usa MACs para a \nintegridade da mensagem. Veremos nas seções 8.5 e 8.6 que os MACs são também usados por conhecidos protocolos \nde segurança da camada de transporte e da camada de rede.\nCertificação de chaves públicas\nUma aplicação importante de assinaturas digitais é a certificação de chaves públicas, ou seja, certificar \nque uma chave pública pertence a uma entidade específica. A certificação de chaves públicas é usada em muitos \nprotocolos de rede seguros, incluindo o IPsec e o SSL.\nPara compreender mais sobre o problema, consideremos uma versão de comércio eletrônico para o clássico \n“trote da pizza”\n. Suponha que Alice trabalhe no ramo de pizzas para viagem e que aceite pedidos pela Internet. \nBob, que adora pizza, envia a Alice uma mensagem em texto aberto que contém o endereço de sua casa e o tipo \nde pizza que quer. Nessa mensagem, ele inclui também uma assinatura digital (isto é, um hash assinado extraído \nda mensagem original em texto aberto) para provar a Alice que ele é a origem verdadeira da mensagem. Para \nverificar a assinatura, Alice obtém a chave pública de Bob (talvez de um servidor de chaves públicas ou de uma \nmensagem de e-mail) e verifica a assinatura digital. Dessa maneira, ela se certifica de que foi Bob, e não algum \nadolescente brincalhão, quem fez o pedido.\nTudo parece caminhar bem até que entra em cena a esperta Trudy. Como mostrado na Figura 8.13, Trudy \ndecide fazer uma travessura. Ela envia uma mensagem a Alice na qual diz que é Bob, fornece o endereço de Bob e \nFigura 8.12  Verificando uma mensagem assinada\nChave pública \nde Bob: KB\n+\nMensagem longa\nCara Alice:\nEssa carta é MUITO longa\npois tenho muito a dizer\n...........\n..........\n..........\nBob\nResumo de mensagem \nde tamanho ﬁxo\nOpgmdvboijrtnsd\ngghPPdogm;lcvkb\nResumo de \nmensagem assinado\nFgkopdgoo69cmxw\n54psdterma[asofmz\nFunção de hash \nmuitos-para-um\nComparar\nResumo de mensagem \nde tamanho ﬁxo\nOpgmdvboijrtnsd\ngghPPdogm;lcvkb\nKR 08.12.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n32p9 Wide x 24p8 Deep\n11/18/11 rossi\nAlgoritmo \ncriptográﬁco\n   Redes de computadores e a Internet\n514\npede uma pizza. Nessa mensagem ela também inclui sua (de Trudy) chave pública, embora Alice suponha, claro, \nque seja a de Bob. Trudy também anexa uma assinatura digital, a qual foi criada com sua própria (de Trudy) chave \nprivada. Após receber a mensagem, Alice aplica a chave pública de Trudy (pensando que é a de Bob) à assinatura \ndigital e concluirá que a mensagem em texto aberto foi, na verdade, criada por Bob. Este ficará muito surpreso \nquando o entregador aparecer em sua casa com uma pizza de calabresa e enchovas!\nPor esse exemplo, vemos que, para que a criptografia de chaves públicas seja útil, você precisa ser capaz de \nverificar se tem a chave pública verdadeira da entidade (pessoa, roteador, navegador e outras) com a qual quer se \ncomunicar. Por exemplo, quando Alice estiver se comunicando com Bob usando criptografia de chaves públicas, \nela precisará saber, com certeza, que a chave pública que supostamente é de Bob é, de fato, dele.\nA vinculação de uma chave pública a uma entidade particular é feita, em geral, por uma Autoridade Cer-\ntificadora (Certification Authority — CA), cuja tarefa é validar identidades e emitir certificados. Uma CA tem as \nseguintes funções:\n1.\t\nUma CA verifica se uma entidade (pessoa, roteador e assim por diante) é quem diz ser. Não há procedimentos \nobrigatórios para o modo como deve ser feita a certificação. Quando tratamos com uma CA, devemos con-\nfiar que ela tenha realizado uma verificação de identidade adequadamente rigorosa. Por exemplo, se Trudy \nconseguisse entrar na autoridade certificadora Fly-by-Night, e apenas declarasse “Eu sou Alice” e recebesse \ncertificados associados à identidade de Alice, então não se deveria dar muita credibilidade a chaves públicas \ncertificadas pela autoridade certificadora Fly-by-Night. Por outro lado, seria mais sensato (ou não!) estar mais \ninclinado a confiar em uma CA que faz parte de um programa federal ou estadual. O grau de confiança que \nse tem na identidade associada a uma chave pública equivale apenas ao grau de confiança depositada na CA e \nem suas técnicas de verificação de identidades. Que teia emaranhada de confiança estamos tecendo!\n2.\t Tão logo verifique a identidade da entidade, a CA cria um certificado que vincula a chave pública da enti-\ndade à identidade verificada. O certificado contém a chave pública e a informação exclusiva que identifica \nmundialmente o proprietário da chave pública (por exemplo, o nome de alguém ou um endereço IP). O \ncertificado é assinado digitalmente pela CA. Essas etapas são mostradas na Figura 8.14.\nFigura 8.13  Trudy se passa por Bob usando criptografia de chaves públicas\nChave privada \nde Trudy: KT\n–\nChave pública \nde Trudy: KT\n+\nResumo de mensagem \nassinado (utilizando a \nchave privada de Trudy)\nFgkopdgoo69cmxw\n54psdterma[asofmz\nMensagem\nAlice, \nEnvie-me uma pizza.\n                               Bob\nFunção de hash \nmuitos-para-um\nAlice usa a chave \npública de Trudy \npensando que é a de \nBob e conclui que a \nmensagem é de Bob\nPIZZA\nAlgoritmo \ncriptográﬁco\nSegurança em redes de computadores  515 \nVejamos, agora, como certificados podem ser usados para combater os espertinhos das pizzas, como Trudy \ne outros indesejáveis. Quando Bob faz seu pedido, ele também envia seu certificado assinado por uma CA. Alice \nusa a chave pública da CA para verificar a validade do certificado de Bob e extrair a chave pública dele.\nTanto a International Telecommunication Union (ITU) como a IETF desenvolveram padrões para autori-\ndades certificadoras. Na recomendação ITU X.509 [ITU, 2005a], encontramos a especificação de um serviço de \n \nautenticação, bem como uma sintaxe específica para certificados. O RFC 1422 descreve um gerenciamento \n \nde chaves baseado em CA para utilização com o e-mail seguro pela Internet. Essa recomendação é compatível \ncom X.509, mas vai além, pois estabelece procedimentos e convenções para uma arquitetura de gerenciamento \nde chaves. A Tabela 8.4 apresenta alguns campos importantes de um certificado.\nTabela 8.4  Campos selecionados de uma chave pública X.509 e RFC 1422\nNome do campo\nDescrição\nVersão\nNúmero da versão da especificação X.509\nNúmero de série\nIdentificador exclusivo emitido pela CA para um certificado\nAssinatura\nEspecifica o algoritmo usado pela CA para assinar esse certificado\nNome do emissor\nIdentidade da CA que emitiu o certificado, em formato de nome distinto (DN) [RFC 2253]\nPeríodo de validade\nInício e fim do período de validade do certificado\nNome do sujeito\nIdentidade da entidade cuja chave pública está associada a esse certificado, em formato DN\nChave pública do sujeito\nA chave pública do sujeito, bem como uma indicação do algoritmo de chave pública (e parâmetros do algoritmo) \na ser usado com essa chave\n8.4  Autenticação do ponto final\nA autenticação do ponto final é o processo de provar a identidade de uma entidade a outra entidade por \numa rede de computadores, por exemplo, um usuário provando sua identidade a um servidor de correio eletrô-\nnico. Como seres humanos, autenticamos uns aos outros de diversas maneiras: reconhecemos o rosto do outro \nao nos encontrarmos, reconhecemos a voz no telefone, somos autenticados por um oficial da Alfândega que \ncompara a nossa foto com a do passaporte.\nFigura 8.14  Bob obtém sua chave pública certificada pela CA\nCertiﬁcado de Bob \nassinado pela CA, \ncontendo sua \nchave pública: KB\n+\nAutoridade \nCertiﬁcadora (CA)\n(KB\n+, B)\nChave pública \nda CA: KCA\n–\nKR 08.14.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n23p10 Wide x 16p0 Deep\n11/18/11 rossi\nAlgoritmo \ncriptográﬁco\n   Redes de computadores e a Internet\n516\nNesta seção, vamos considerar como uma entidade pode autenticar outra quando os dois estão se comuni-\ncado por uma rede. Vamos nos atentar aqui para a autenticação de uma entidade “ao vivo”\n, no momento em que \na comunicação está de fato ocorrendo. Um exemplo concreto é um usuário autenticando-se em um servidor de \ncorreio eletrônico. Este é um problema sutilmente diferente de provar que uma mensagem recebida em algum \nponto no passado veio mesmo do remetente afirmado, conforme estudamos na Seção 8.3.\nAo realizar autenticação pela rede, as entidades comunicantes não podem contar com informações biométri-\ncas, como aparência visual ou timbre de voz. Na verdade, veremos em nossos estudos de caso que, geralmente, são \nos elementos da rede como roteadores e processos cliente/servidor que precisam se autenticar. Aqui, a autenticação \nprecisa ser feita unicamente com base nas mensagens e dados trocados como parte de um protocolo de autentica-\nção. Em geral, um protocolo de autenticação seria executado antes que as duas entidades comunicantes executassem \nalgum outro protocolo (por exemplo, um protocolo de transferência confiável de dados, um de troca de informações \nde roteamento ou um de correio eletrônico). O protocolo de autenticação primeiro estabelece as identidades das \npartes para a satisfação mútua; somente depois da autenticação é que as entidades “põem as mãos” no trabalho real.\nAssim como no caso do nosso desenvolvimento de um protocolo de transferência confiável de dados (rdt) \nno Capítulo 3, será esclarecedor desenvolvermos aqui diversas versões de um protocolo de autenticação, que \nchamaremos de ap (authentication protocol — protocolo de autenticação), explicando cada versão enquanto pros-\nseguimos. (Se você gostar da evolução passo a passo de um projeto, também poderá gostar de Bryant [1988], que \nrelata uma narrativa fictícia entre projetistas de um sistema aberto de autenticação de rede e sua descoberta das \nmuitas questões sutis que estão envolvidas.)\nVamos imaginar que Alice precise se autenticar com Bob.\n8.4.1  Protocolo de autenticação ap1.0\nTalvez o protocolo de autenticação mais simples que possamos imaginar seja aquele onde Alice apenas en-\nvia uma mensagem a Bob dizendo ser Alice. Esse protocolo aparece na Figura 8.15. A falha aqui é óbvia — não \nhá como Bob realmente saber que a pessoa enviando a mensagem “Eu sou Alice” é mesmo Alice. Por exemplo, \nTrudy (a intrusa) poderia enviar tal mensagem.\nFigura 8.15  Protocolo ap1.0 e um cenário de falha\nAlice\nEu sou Alice\nBob\nTrudy\nTrudy\nAlice\nEu sou Alice\nBob\nKR 08.15.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n26p10 Wide x 12p0 Deep\n11/18/11 rossi\n8.4.2  Protocolo de autenticação ap2.0\nSe Alice possui um endereço de rede conhecido (por exemplo, um endereço IP) do qual ela sempre se co-\nmunica, Bob poderia tentar autenticar Alice verificando se o endereço de origem no datagrama IP que transporta \na mensagem de autenticação combina com o endereço conhecido de Alice. Nesse caso, Alice seria autenticada. \nIsso poderia impedir que um intruso muito ingênuo em redes se passe por Alice, mas não o aluno determinado, \nque está estudando este livro, ou muitos outros!\nSegurança em redes de computadores  517 \nPelo estudo das camadas de rede e enlace de dados, sabemos que não é difícil (por exemplo, se alguém ti-\nvesse acesso ao código do sistema operacional e pudesse criar seu próprio núcleo do sistema operacional, como \nacontece com o Linux e vários outros sistemas operacionais disponíveis livremente) criar um datagrama IP, colo-\ncar qualquer endereço de origem IP que se desejar (por exemplo, o endereço IP conhecido de Alice) no datagra-\nma IP e enviar o datagrama pelo protocolo da camada de enlace para o roteador do primeiro salto. Daí em diante, \no datagrama com endereço de origem incorreto seria zelosamente repassado para Bob. Essa técnica, mostrada na \nFigura 8.16, é uma forma de falsificação de IP. Tal falsificação pode ser evitada se o roteador do primeiro salto de \nTrudy for configurado para encaminhar apenas datagramas contendo o endereço IP de origem de Trudy [RFC \n2827]. Porém, essa capacidade não é implementada ou imposta de modo universal. Assim, Bob poderia ser enga-\nnado a achar que o gerente de rede de Trudy (que poderia ser a própria Trudy) configurou o roteador do primeiro \nsalto de Trudy para repassar somente datagramas corretamente endereçados.\nFigura 8.16  Protocolo ap2.0 e um cenário de falha\nAlice\nEu sou Alice\nEndereço IP de Alice.\nBob\nTrudy\nAlice\nEu sou Alice\nEndereço IP de Alice.\nBob\nTrudy\nKR 08.16.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n27p0 Wide x 11p7 Deep\n11/18/11 rossi\n8.4.3  Protocolo de autenticação ap3.0\nUma técnica clássica de autenticação é usar uma senha secreta. A senha é um segredo compartilhado entre \no autenticador e a pessoa sendo autenticada. Gmail, Facebook, telnet, FTP e muitos outros serviços usam a auten-\nticação por senha. No protocolo ap3.0, Alice envia sua senha secreta a Bob, como mostra a Figura 8.17.\nFigura 8.17  Protocolo ap3.0 e um cenário de falha\nAlice\nEu sou Alice, \nsenha\nOK\nBob\nTrudy\nAlice\nEu sou Alice, \nsenha\nOK\nBob\nTrudy\nGravador de voz\nLegenda:\nKR 08 17\n   Redes de computadores e a Internet\n518\nComo as senhas são muito utilizadas, poderíamos supor que o protocolo ap3.0 é bastante seguro. Nesse \ncaso, estaríamos errados! A falha de segurança aqui é clara. Se Trudy interceptar a comunicação de Alice, então \nela poderá descobrir a senha de Alice. Se você acha que isso é improvável, considere o fato de que, quando você \nusa Telnet com outra máquina e se autentica, a senha de autenticação é enviada abertamente para o servidor \nTelnet. Alguém conectado ao cliente Telnet ou à LAN do servidor possivelmente poderia investigar (ler e arma-\nzenar) todos os pacotes transmitidos na LAN e, assim, roubar a senha de autenticação. De fato, essa é uma técnica \nbem conhecida para roubar senhas (veja, por exemplo, Jimenez [1997]). Essa ameaça, obviamente, é bastante real, \nde modo que ap3.0 certamente não funcionará.\n8.4.4  Protocolo de autenticação ap3.1\nNossa próxima ideia para consertar o ap3.0, naturalmente, é criptografar a senha. Criptografando a senha, \npodemos impedir que Trudy descubra a senha de Alice. Se considerarmos que Alice e Bob compartilham uma \nchave secreta simétrica, KA–B, então Alice pode criptografar a senha e enviar sua mensagem de identificação, \n“Eu sou Alice”\n, e sua senha criptografada para Bob. Então, Bob decodifica a senha e, supondo que a senha \nesteja correta, autentica Alice. Bob está confiante na autenticação de Alice, pois Alice não apenas conhece a \nsenha, mas também sabe o valor da chave secreta compartilhada necessária para criptografar a senha. Vamos \nchamar esse protocolo de ap3.1.\nEmbora seja verdade que ap3.1 impede que Trudy descubra a senha de Alice, o uso da criptografia aqui não \nresolve o problema da autenticação. Bob está sujeito a um ataque de reprodução: Trudy só precisa investigar a \ncomunicação de Alice, gravar a versão criptografada da senha e reproduzir a versão criptografada da senha para \nBob, fingindo ser Alice. O uso de uma senha criptografada em ap3.1 não torna a situação muito diferente daquela \ndo protocolo ap3.0, na Figura 8.17.\n8.4.5  Protocolo de autenticação ap4.0\nO cenário de falha na Figura 8.17 resultou do fato de Bob não conseguir distinguir entre a mensagem origi-\nnal enviada por Alice e a reprodução dessa mensagem. Ou seja, Bob não conseguiu saber se Alice estava ao vivo \n(isto é, realmente estava no outro ponto da conexão) ou se a mensagem que ele recebeu foi uma reprodução de \numa autenticação anterior. O leitor muito (muito) atento se lembrará de que o protocolo de apresentação de três \nvias precisou resolver o mesmo problema — o lado do servidor de uma conexão TCP não queria aceitar uma co-\nnexão se o segmento SYN recebido fosse uma cópia antiga (retransmissão) de um segmento SYN de uma conexão \nanterior. Como o lado do servidor TCP resolveu o problema de determinar se o cliente estava mesmo ao vivo? \nEle escolheu um número de sequência inicial que não havia sido usado por um bom tempo, enviou esse número \nao cliente, e então aguardou que o cliente respondesse com um segmento ACK contendo esse número. Podemos \nadotar a mesma ideia para fins de autenticação.\nUm nonce é um número que o protocolo usará somente uma vez por toda a vida. Ou seja, uma vez que \no protocolo utilizar um nonce, esse número nunca mais será utilizado. Nosso protocolo ap4.0 usa um nonce da \nseguinte forma:\n1.\t Alice envia a mensagem “Eu sou Alice” para Bob.\n2.\t Bob escolhe um nonce, R, e o envia a Alice.\n3.\t Alice criptografa o nonce usando a chave secreta simétrica de Alice e Bob, KA–B, e envia o nonce criptogra-\nfado, KA–B(R), de volta a Bob. Assim como no protocolo ap3.1, o fato de Alice conhecer KA–B e o utilizar \npara criptografar o valor permite a Bob saber que a mensagem recebida foi gerada por Alice. O nonce é \nusado para garantir que Alice está ao vivo.\n4.\t Bob decodifica a mensagem recebida. Se o nonce decodificado for igual ao que ele enviou a Alice, então \nAlice está autenticada.\nSegurança em redes de computadores  519 \nO protocolo ap4.0 é ilustrado na Figura 8.18. Usando o valor exclusivo de R e depois verificando o valor \nretornado, KA–B(R), Bob pode estar certo de que Alice tanto é quem ela diz ser (pois conhece o valor da chave se-\ncreta necessária para criptografar R) quanto está ao vivo (pois criptografou o nonce, R, que Bob acabou de criar).\nO uso de um nonce e das formas de criptografia por chave simétrica forma a base do ap4.0. Uma pergunta \nnatural é se podemos usar um nonce e a criptografia de chave pública (em vez da de chave simétrica) para resolver \no problema de autenticação. Essa questão é explorada nos problemas ao final deste capítulo.\nFigura 8.18  Protocolo ap4.0 e um cenário de falha\nAlice\nR\nKA–B(R)\nEu sou Alice\nBob\nKR 08.18.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n10p8 Wide x 10p1 Deep\n11/18/11 rossi\n8.5  Protegendo o e-mail\nNas seções anteriores, analisamos as questões fundamentais em segurança de redes, incluindo a cripto-\ngrafia de chave simétrica e de chave pública, autenticação do ponto final, distribuição de chave, integridade da \nmensagem e assinaturas digitais. Agora vamos analisar como essas ferramentas estão sendo usadas para oferecer \nsegurança na Internet.\nCuriosamente, é possível oferecer serviços de segurança em cada uma das quatro principais camadas da \npilha de protocolos da Internet. Quando a segurança é fornecida para um protocolo específico da camada de \naplicação, a aplicação que o emprega utilizará um ou mais serviços de segurança, como sigilo, autenticação ou \nintegridade. Quando é oferecida para um protocolo da camada de transporte, todas as aplicações que usam esse \nprotocolo aproveitam os seus serviços de segurança. Quando a segurança é fornecida na camada de rede em base \nhospedeiro para hospedeiro, todos os segmentos da camada de transporte (e, portanto, todos os dados da camada \nde aplicação) aproveitam os serviços de segurança dessa camada. Quando a segurança é oferecida em um enlace, \nos dados em todos os quadros que percorrem o enlace recebem os serviços de segurança do enlace.\nNas seções 8.5 a 8.8, verificamos como as ferramentas de segurança estão sendo usadas nas camadas de \nenlace, rede, transporte e aplicação. Obedecendo à estrutura geral deste livro, começamos no topo da pilha \nde protocolo e discutimos segurança na camada de aplicação. Nossa técnica é usar uma aplicação específica, \ne-mail, como um estudo de caso para a segurança da camada de aplicação. Então, descemos a pilha de pro-\ntocolo. Examinaremos o protocolo SSL (que provê segurança na camada de transporte), o IPsec (que provê \nsegurança na camada de rede) e a segurança do protocolo LAN IEEE 802.11 sem fio.\nVocê deve estar se perguntando por que a funcionalidade da segurança está sendo fornecida em mais de \numa camada na Internet. Já não bastaria prover essa funcionalidade na camada de rede? Há duas respostas para \na pergunta. Primeiro, embora a segurança na camada de rede possa oferecer “cobertura total” cifrando todos os \ndados nos datagramas (ou seja, todos os segmentos da camada de transporte) e autenticando todos os endere-\nços IP destinatários, ela não pode prover segurança no nível do usuário. Por exemplo, um site de comércio não \npode confiar na segurança da camada IP para autenticar um cliente que vai comprar mercadorias. Assim, existe \na necessidade de uma funcionalidade da segurança em camadas superiores bem como cobertura total em canais \ninferiores. Segundo, em geral é mais fácil implementar serviços da Internet, incluindo os de segurança nas cama-\ndas superiores da pilha de protocolo. Enquanto aguardamos a ampla implementação da segurança na camada de \n   Redes de computadores e a Internet\n520\nrede, o que ainda levará muitos anos, muitos criadores de aplicação “já fazem isso” e introduzem a funcionalida-\nde da segurança em suas aplicações favoritas. Um exemplo clássico é o Pretty Good Privacy (PGP), que oferece \ne-mail seguro (discutido mais adiante nesta seção). Necessitando de apenas um código de aplicação do cliente e \ndo servidor, o PGP foi uma das primeiras tecnologias de segurança a ser amplamente utilizada na Internet.\n8.5.1  E-mail seguro\nAgora usamos os princípios de criptografia das seções 8.2 a 8.3 para criar um sistema de e-mail seguro. Cria-\nmos esse projeto de alto nível de maneira incremental, introduzindo, a cada etapa, novos serviços de segurança. \nEm nosso projeto de um sistema de e-mail seguro, vamos manter em mente o exemplo picante apresentado na \nSeção 8.1 — o caso de amor entre Alice e Bob. Imagine que Alice quer enviar uma mensagem de e-mail para Bob \ne Trudy quer bisbilhotar.\nAntes de avançar e projetar um sistema de e-mail seguro para Alice e Bob, devemos considerar quais carac-\nterísticas de segurança seriam as mais desejáveis para eles. A primeira, e mais importante, é a confidencialidade. \nComo foi discutido na Seção 8.1, nem Alice nem Bob querem que Trudy leia a mensagem de e-mail de Alice. A \nsegunda característica que Alice e Bob provavelmente gostariam de ver no sistema de e-mail seguro é a autenticação \ndo remetente. Em particular, quando Bob receber a seguinte mensagem: “Eu não o amo mais. Nunca mais \nquero vê-lo. Da anteriormente sua, Alice”\n, ele naturalmente gostaria de ter certeza de que a mensa-\ngem veio de Alice, e não de Trudy. Outra característica de segurança de que os dois amantes gostariam de dispor é \na integridade de mensagem, isto é, a certeza de que a mensagem que Alice enviar não será modificada no trajeto até \nBob. Por fim, o sistema de e-mail deve fornecer autenticação do receptor, isto é, Alice quer ter certeza de que de fato \nestá enviando a mensagem para Bob, e não para outra pessoa (por exemplo, Trudy) que esteja se passando por ele.\nPortanto, vamos começar abordando a preocupação mais premente, a confidencialidade. A maneira mais di-\nreta de conseguir confidencialidade é Alice criptografar a mensagem com tecnologia de chaves simétricas (como \nDES ou AES) e Bob decriptar a mensagem ao recebê-la. Como discutido na Seção 8.2, se a chave simétrica for \nlonga o suficiente e se apenas Alice e Bob possuírem a chave, então será dificílimo que alguém (incluindo Trudy) \nleia a mensagem. Embora essa seja uma abordagem direta, ela apresenta a dificuldade fundamental que discuti-\nmos na Seção 8.2 — é difícil distribuir uma chave simétrica de modo que apenas Bob e Alice tenham cópias dela. \nPortanto, é natural que consideremos uma abordagem alternativa — a criptografia de chaves públicas (usando, \npor exemplo, RSA). Nessa abordagem, Bob disponibiliza publicamente sua chave pública (por exemplo, em um \nservidor de chaves públicas ou em sua página pessoal) e Alice criptografa sua mensagem com a chave pública \nde Bob, e envia a mensagem criptografada para o endereço de e-mail de Bob. Quando recebe a mensagem, ele \nsimplesmente a decodifica com sua chave privada. Supondo que Alice tenha certeza de que aquela chave pública \né a de Bob, essa técnica é um meio excelente de fornecer a confidencialidade desejada. Um problema, contudo, é \nque a criptografia de chaves públicas é relativamente ineficiente, sobretudo para mensagens longas.\nPara superar o problema da eficiência, vamos fazer uso de uma chave de sessão (discutida na Seção 8.2.2). \nEm particular, Alice (1) escolhe uma chave simétrica, KS, aleatoriamente, (2) criptografa sua mensagem m com \na chave simétrica, Ks, (3) criptografa a chave simétrica com a chave pública de Bob, KB\n+, (4) concatena a mensa-\ngem criptografada e a chave simétrica criptografada de modo que formem um “pacote” e (5) envia o pacote ao \nendereço de e-mail de Bob. Os passos estão ilustrados na Figura 8.19. (Nessa figura e nas subsequentes, o sinal \n“+” dentro de um círculo representa formar a concatenação e o sinal “–” dentro de um círculo, desfazer a conca-\ntenação.) Quando Bob receber o pacote, ele (1) usará sua chave privada KB\n–, para obter a chave simétrica KS, e (2) \nutilizará a chave simétrica KS para decodificar a mensagem m.\nAgora que projetamos um sistema de e-mail seguro que fornece confidencialidade, vamos desenvolver um \noutro sistema que forneça autenticação do remetente e também integridade de mensagem. Vamos supor, por \nenquanto, que Alice e Bob não estejam mais preocupados com confidencialidade (querem compartilhar seus \nsentimentos com todos!) e que só estejam preocupados com a autenticação do remetente e com a integridade \nda mensagem. Para realizar essa tarefa, usaremos assinaturas digitais e resumos de mensagem, como descrito na \nSegurança em redes de computadores  521 \nSeção 8.3. Especificamente, Alice (1) aplica uma função de hash H (por exemplo, MD5) à sua mensagem m, para \nobter um resumo, (2) assina o resultado da função de hash com sua chave privada KA\n– para criar uma assinatura \ndigital, (3) concatena a mensagem original (não criptografada) com a assinatura para criar um pacote e (4) envia \no pacote ao endereço de e-mail de Bob. Quando Bob recebe o pacote, ele (1) aplica a chave pública de Alice, KA\n+, \nao resumo de mensagem assinado e (2) compara o resultado dessa operação com o próprio hash H da mensagem. \nAs etapas são ilustradas na Figura 8.20. Como discutimos na Seção 8.3, se os dois resultados forem iguais, Bob \npoderá ter razoável certeza de que a mensagem veio de Alice e não foi alterada.\nVamos considerar agora o projeto de um sistema de e-mail que forneça confidencialidade, autenticação de \nremetente e integridade de mensagem. Isso pode ser feito pela combinação dos procedimentos das figuras 8.19 e \n8.20. Primeiro, Alice cria um pacote preliminar, exatamente como ilustra a Figura 8.20, constituído de sua mensa-\ngem original junto com um hash da mensagem assinado digitalmente. Em seguida, ela trata esse pacote prelimi-\nnar como uma mensagem em si e a envia seguindo as etapas do remetente mostradas na Figura 8.19, criando um \nnovo pacote, que é enviado a Bob. As etapas seguidas são mostradas na Figura 8.21. Quando Bob recebe o pacote, \nele aplica primeiro seu lado da Figura 8.19 e depois seu lado da Figura 8.20. É preciso ficar claro que esse projeto \natinge o objetivo de fornecer confidencialidade, autenticação de remetente e integridade de mensagem. Note que \nnesse esquema Alice utiliza criptografia de chaves públicas duas vezes: uma vez com sua chave privada e uma vez \ncom a chave pública de Bob. De maneira semelhante, Bob também aplica a criptografia de chaves públicas duas \nvezes — uma vez com sua chave privada e uma vez com a chave pública de Alice.\nO projeto de e-mail seguro ilustrado na Figura 8.21 provavelmente fornece segurança satisfatória para os \nusuários de e-mail na maioria das ocasiões. Mas ainda resta uma questão importante a ser abordada. O projeto \nda Figura 8.21 requer que Alice obtenha a chave pública de Bob e que este obtenha a chave pública de Alice. \nA distribuição dessas chaves é um problema nada trivial. Por exemplo, Trudy poderia se disfarçar de Bob e \nFigura 8.19  \u0007\nAlice usou uma chave de sessão simétrica, KS, para enviar um e-mail secreto para Bob\nKS(.)\nKS(.)\nKS(m)\nKS(m)\nKS\nKS\nKB\n+(.)\nKB\n+(KS)\nKB\n+(KS)\nm\nm\n+\n–\nInternet\nKB\n–(.)\nKR 08.19.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n27p0 Wide x 10p3 Deep\n11/18/11 rossi\nAlice envia uma mensagem de e-mail, m\nBob recebe uma mensagem de e-mail, m\nFigura 8.20  \u0007\nUsando funções de hash e assinaturas digitais para fornecer autenticação de \nremetente e integridade de mensagem\nH(.)\nKA\n–(.)\nKA\n+(.)\nKA\n–(H(m))\nKA\n–(H(m))\nm\nm\nm\n+\n–\nInternet\nAlice envia uma mensagem de e-mail, m\nBob recebe uma mensagem de e-mail, m\nH(.)\nCompare\n   Redes de computadores e a Internet\n522\ndar a Alice sua própria chave pública, dizendo que é a de Bob. Como aprendemos na Seção 8.3, uma tática \npopular para distribuir chaves públicas com segurança é certificar as chaves públicas usando uma autoridade \ncertificadora (CA).\n8.5.2  PGP\nProjetado originalmente por Phil Zimmermann em 1991, o PGP (Pretty Good Privacy — privacidade \nrazoável) é um esquema de criptografia para e-mail que se tornou um padrão de fato. O site do PGP é acessado \nmais de um milhão de vezes por mês por usuários de 166 países [PGPI, 2012]. Versões do PGP estão disponíveis \nem domínio público; por exemplo, você pode encontrar o software PGP para sua plataforma favorita, bem como \ngrande quantidade de material de leitura interessante, na home page internacional do PGP [PGPI, 2012]. (Um \nensaio de particular interesse, escrito pelo autor do PGP, é encontrado em Zimmermann [2012].) O projeto do \nPGP é, basicamente, idêntico ao projeto apresentado na Figura 8.21. Dependendo da versão, o software do PGP \nusa MD5 ou SHA para processar o resumo de mensagem; CAST, DES triplo ou IDEA para criptografar chaves \nsimétricas, e RSA para criptografar chaves públicas.\nQuando o PGP é instalado, o software cria um par de chaves públicas para o usuário. A chave pública pode \nentão ser colocada no site do usuário ou em um servidor de chaves públicas. A chave privada é protegida pelo uso \nde uma senha. A senha tem de ser informada todas as vezes que o usuário acessar a chave privada. O PGP oferece \nao usuário a opção de assinar digitalmente a mensagem, criptografar a mensagem ou, ainda, ambas as opções: \nassinar digitalmente e criptografar a mensagem. A Figura 8.22 mostra uma mensagem PGP assinada. Essa men-\nsagem aparece após o cabeçalho MIME. Os dados codificados da mensagem correspondem a KA\n–(H(m)), isto é, \nao resumo de mensagem assinado digitalmente. Como discutimos antes, para que Bob verifique a integridade da \nmensagem, ele precisa ter acesso à chave pública de Alice.\nFigura 8.21  \u0007\nAlice usa criptografia de chaves simétricas, criptografia de chaves públicas, uma \nfunção de hash e uma assinatura digital para fornecer sigilo, autenticação de \nremetente e integridade de mensagem\nH(.)\nKA\n–(.)\nKS(.)\nKS\nKA\n–(H(m))\nm\nm\n+\n+\nto Internet\nKB\n+(.)\nKR 08.21.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n26p0 Wide x 11p1 Deep\n11/18/11 rossi\nFigura 8.22  Uma mensagem PGP assinada \n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\nBob:\nCan I see you tonight?\nPassionately yours, Alice\n-----BEGIN PGP SIGNATURE-----\nVersion: PGP for Personal Privacy 5.0\nCharset: noconv\nyhHJRHhGJGhgg/12EpJ+lo8gE4vB3mqJhFEvZP9t6n7G6m5Gw2\n-----END PGP SIGNATURE-----\nSegurança em redes de computadores  523 \nA Figura 8.23 mostra uma mensagem PGP secreta. Essa mensagem também aparece após o cabeçalho \nMIME. É claro que a mensagem em texto aberto não está incluída na de e-mail secreta. Quando um remetente \n(como Alice) quer não apenas a confidencialidade, mas também a integridade, o PGP contém uma mensagem \ncomo a da Figura 8.23 dentro daquela da Figura 8.22.\nO PGP também fornece um mecanismo para certificação de chaves públicas, mas esse mecanismo é bem \ndiferente daquele da CA mais convencional. As chaves públicas do PGP são certificadas por uma rede de con-\nfiança. A própria Alice pode certificar qualquer par chave/usuário quando ela achar que esse par realmente está \ncorreto. Além disso, o PGP permite que Alice declare que ela confia em outro usuário para atestar a autenticidade \nde mais chaves. Alguns usuários do PGP assinam reciprocamente suas chaves montando grupos de assinatura \nde chaves. Os usuários se reúnem fisicamente, trocam chaves públicas e certificam suas chaves reciprocamente, \nassinando-as com suas chaves privadas.\nFigura 8.23  Uma mensagem PGP secreta\n-----BEGIN PGP MESSAGE-----\nVersion: PGP for Personal Privacy 5.0\nu2R4d+/jKmn8Bc5+hgDsqAewsDfrGdszX68liKm5F6Gc4sDfcXyt\nRfdS10juHgbcfDssWe7/K=lKhnMikLo0+1/BvcX4t==Ujk9PbcD4\nThdf2awQfgHbnmKlok8iy6gThlp\n-----END PGP MESSAGE\n8.6  Protegendo conexões TCP: SSL\nNa seção anterior, vimos como as técnicas criptográficas podem prover sigilo, integridade dos dados e \nautenticação do ponto final a aplicações específicas, ou seja, e-mail. Nesta, desceremos uma camada na pilha de \nPhil Zimmermann e o PGP\nPhilip R. Zimmermann é o criador do Pretty Good \nPrivacy (PGP). Por causa disso, foi alvo de uma in-\nvestigação criminal que durou três anos, porque o go-\nverno norte-americano sustentava que as restrições \nsobre a exportação de software de criptografia tinham \nsido violadas quando o PGP se espalhou por todo o \nmundo após sua publicação como software de uso \nlivre em 1991. Após ter sido liberado como software \ncompartilhado, alguém o colocou na Internet e estran-\ngeiros o baixaram. Nos Estados Unidos, os programas \nde criptografia são classificados como artefatos mili-\ntares por lei federal e não podem ser exportados.\nA despeito da falta de financiamento, da inexistên-\ncia de representantes legais, da falta de uma empresa \npara lhe dar apoio e das intervenções governamen-\ntais, mesmo assim o PGP se tornou o software de \ncriptografia para e-mail mais usado no mundo. O mais \nestranho é que o governo dos Estados Unidos pode \nter contribuído inadvertidamente para a disseminação \ndo PGP por causa do caso Zimmermann.\nO governo norte-americano arquivou o caso no iní-\ncio de 1996. O anúncio foi festejado pelos ativistas da \nInternet. O caso Zimmermann tinha se transformado \nna história de um inocente que lutava por seus direi-\ntos contra os desmandos de um governo poderoso. \nA desistência do governo foi uma notícia bem-vinda, \nem parte por causa da campanha em favor da censura \nna Internet desencadeada pelo Congresso e em parte \npor causa dos esforços feitos pelo FBI para conseguir \nmaior amplitude para a espionagem governamental.\nApós o governo ter arquivado o caso, Zimmer-\nmann fundou a PGP Inc., que foi adquirida pela \nNetwork Associates em dezembro de 1997. Zim-\nmermann é agora membro do conselho da Network \nAssociates, bem como consultor independente para \nassuntos de criptografia.\nHistória\n   Redes de computadores e a Internet\n524\nprotocolo e examinaremos como a criptografia pode aprimorar o TCP com os serviços de segurança, incluin-\ndo sigilo, integridade dos dados e autenticação do ponto final. Essa versão aprimorada do TCP é denominada \nCamada Segura de Sockets (Secure Sockets Layer — SSL). Uma versão levemente modificada da versão 3 do \nSSL, denominada Segurança na Camada de Transporte (Transport Layer Security —TLS), foi padronizada pelo \nIETF [RFC 4346].\nO SSL foi na origem projetado pela Netscape, mas as ideias básicas por trás da proteção do TCP ante-\ncedem o trabalho da Netscape (por exemplo, consulte Woo [1994]). Desde sua concepção, o SSL obteve uma \nampla implementação. Ele é suportado por todos os navegadores Web e servidores Web, e usado basicamente \npor todos os sites populares (incluindo Amazon, eBay, Yahoo!, MSN etc.). Dezenas de bilhões de dólares são \ngastos com o SSL a cada ano. Na verdade, se você já comprou qualquer coisa pela Internet com seu cartão de \ncrédito, a comunicação entre seu navegador e servidor para essa compra foi quase certamente por meio do \nSSL. (Você pode identificar que o SSL está sendo usado por seu navegador quando a URL se iniciar com https: \nem vez de http:.)\nPara entender a necessidade do SSL, vamos examinar um cenário de comércio pela Internet típico. Bob está \nnavegando na Web e acessa o site Alice Incorporated, que está vendendo perfume. O site Alice Incorporated exi-\nbe um formulário no qual Bob deve inserir o tipo de perfume e quantidade desejados, seu endereço e o número \nde seu cartão de crédito. Bob insere essas informações, clica em Enviar, e espera pelo recebimento (via correio \npostal comum) de seus perfumes; ele também espera pelo recebimento de uma cobrança pelo seu pedido na pró-\nxima fatura do cartão de crédito. Até o momento, tudo bem, mas se nenhuma medida de segurança for tomada, \nBob poderia esperar por surpresas.\n• Se nenhum sigilo (encriptação) for utilizado, um invasor poderia interceptar o pedido de Bob e receber \nsuas informações sobre o cartão. O invasor poderia, então, fazer compras à custa de Bob.\n• Se nenhuma integridade de dados for utilizada, um invasor poderia modificar o pedido de Bob, fazendo\n-o comprar dez vezes mais frascos de perfumes que o desejado.\n• Finalmente, se nenhuma autenticação do servidor for utilizada, um servidor poderia exibir o famoso logo-\ntipo da Alice Incorporated, quando na verdade o site é mantido por Trudy, que está se passando por Alice \nIncorporated. Após receber o pedido de Bob, Trudy poderia ficar com o dinheiro dele e sumir. Ou Trudy \npoderia realizar um roubo de identidade obtendo o nome, endereço e número do cartão de crédito de Bob.\nO SSL resolve essas questões aprimorando o TCP com sigilo, integridade dos dados, autenticação do servi-\ndor e autenticação do cliente.\nMuitas vezes, o SSL é usado para oferecer segurança em transações que ocorrem pelo HTTP. Entretanto, \ncomo o SSL protege o TCP, ele pode ser empregado por qualquer aplicação que execute o TCP. O SSL provê uma \nInterface de Programação de Aplicação (API) com sockets, semelhante à API do TCP. Quando uma aplicação \nquer empregar o SSL, ela inclui classes/bibliotecas SSL. Como mostrado na Figura 8.24, embora o SSL resida \ntecnicamente na camada de aplicação, do ponto de vista do desenvolvedor, ele é um protocolo de transporte que \nprovê serviços do TCP aprimorados com serviços de segurança.\nFigura 8.24  \u0007\nEmbora o SSL resida tecnicamente na camada de aplicação, do ponto de vista do \ndesenvolvedor, ele é um protocolo da camada transporte\nKR 08.24.eps\nTCP\nSubcamada SSL\nIP\nAplicação\nCamada de \naplicação\nTCP avançado com SSL\nSocket SSL\nSocket TCP\nTCP\nIP\nAplicação\nTCP API\nSocket TCP\nSegurança em redes de computadores  525 \n8.6.1  Uma visão abrangente\nComeçamos descrevendo uma versão simplificada do SSL, que nos permitirá obter uma visão abrangente \nde por que e como funciona o SSL. Vamos nos referir a essa versão simplificada do SSL como “quase-SSL”\n. Após \ndescrever o quase-SSL, na próxima subseção, descreveremos o SSL de verdade, preenchendo as lacunas. O qua-\nse-SSL (e o SSL) possui três fases: apresentação (handshake), derivação de chave e transferência de dados. Agora \ndescreveremos essas três fases para uma sessão de comunicação entre um cliente (Bob) e um servidor (Alice), o \nqual possui um par de chaves pública/privada e um certificado que associa sua identidade à chave pública.\nApresentação (handshake)\nDurante a fase de apresentação, Bob precisa (a) estabelecer uma conexão TCP com Alice, (b) verificar se ela \né realmente Alice, e (c) enviar-lhe uma chave secreta mestre, que será utilizada por Alice e Bob para criar todas \nas chaves simétricas de que eles precisam para a sessão SSL. Essas três etapas estão ilustradas na Figura 8.25. \nObserve que, uma vez a conexão TCP sendo estabelecida, Bob envia a Alice uma mensagem “hello”\n. Alice, então, \nresponde com seu certificado, que contém sua chave pública. Conforme discutido na Seção 8.3, como o certifica-\ndo foi assinado por uma CA, Bob sabe, com certeza, que a chave pública no certificado pertence a Alice. Ele então \ncria um Segredo Mestre (MS) (que será usado somente para esta sessão SSL), codifica o MS com a chave pública \nde Alice para criar o Segredo Mestre Cifrado (EMS), enviando-o para Alice, que o decodifica com sua chave pri-\nvada para obter o MS. Após essa fase, Bob e Alice (e mais ninguém) sabem o segredo mestre para esta sessão SSL.\nDerivação de chave\nA princípio, o MS, agora compartilhado por Bob e Alice, poderia ser usado como a chave de sessão simé-\ntrica para toda a verificação subsequente de criptografia e integridade dos dados. Entretanto, em geral é conside-\nrado mais seguro para Alice e Bob usarem, individualmente, chaves criptográficas diferentes, bem como chaves \ndiferentes para criptografia e verificação da integridade. Assim, Alice e Bob usam o MS para criar quatro chaves:\n• EB = chave de criptografia de sessão para dados enviados de Bob para Alice\n• MB = chave MAC de sessão para dados enviados de Bob para Alice\n• EA = chave de criptografia de sessão para dados enviados de Alice para Bob\n• MA = chave MAC de sessão para dados enviados de Alice para Bob\nFigura 8.25  A apresentação quase-SSL, iniciando com uma conexão TCP\nTCP SYN\nTCP/SYNACK\nDecodiﬁcar EMS com\nKA\n–  para obter MS\nEMS = KA\n+(MS)\nTCP ACK\nSSL hello\ncertiﬁcado\nKR 08.25.eps\n(b)\n(a)\n(c) \nCriar segredo \nmestre (MS)\n   Redes de computadores e a Internet\n526\nAlice e Bob podem criar quatro chaves a partir do MS. Isso poderia ser feito apenas dividindo o MS em \nquatro chaves. (Mas, no SSL real, é um pouco mais complicado, como veremos.) Ao final da fase de derivação \nde chave, Alice e Bob têm todas as quatro chaves. As duas de criptografia serão usadas para cifrar dados; as duas \nchaves MAC serão usadas para verificar a integridade dos dados.\nTransferência de dados\nAgora que Alice e Bob compartilham as mesmas quatro chaves de sessão (EB, MB, EA e MA), podem come-\nçar a enviar dados protegidos um ao outro por meio da conexão TCP. Como o TCP é um protocolo de fluxo de \nbytes, uma abordagem natural seria o SSL cifrar dados da aplicação enquanto são enviados e, então, transmitir os \ndados cifrados para o TCP. Mas, se fizéssemos isso, onde colocaríamos o MAC para a verificação da integridade? \nDecerto não queremos esperar até o fim da sessão TCP para verificar a integridade de todos os dados de Bob que \nforam enviados por toda a sessão! Para abordar essa questão, o SSL divide o fluxo de dados em registros, anexa \num MAC a cada registro para a verificação da integridade, e cifra o registro+MAC. Para criar o MAC, Bob insere \nos dados de registro junto com a chave MB em uma função de hash, conforme discutido na Seção 8.3. Para cifrar \no pacote registro+MAC, Bob usa sua chave de criptografia de sessão EB. Esse pacote cifrado é então transmitido \nao TCP para o transporte pela Internet.\nEmbora essa abordagem vá longe, o fornecimento de integridade dos dados para um fluxo inteiro de men-\nsagem ainda não é à prova de falhas. Em particular, suponha que Trudy seja uma mulher no meio e possua a \nhabilidade de inserir, excluir e substituir segmentos no fluxo dos segmentos TCP enviados entre Alice e Bob. \nTrudy, por exemplo, poderia capturar dois segmentos enviados por Bob, inverter sua ordem, ajustar os números \nde sequência TCP (que não estão cifrados) e enviar dois segmentos na ordem inversa para Alice. Supondo que \ncada segmento TCP encapsula exatamente um registro, vamos verificar como Alice os processaria.\n1.\t O TCP que está sendo executado em Alice pensaria que está tudo bem e passaria os dois registros para a \nsubcamada SSL.\n2.\t O SSL em Alice decodificaria os dois registros.\n3.\t O SSL em Alice usaria o MAC em cada registro para verificar a integridade dos dados dos dois registros.\n4.\t O SSL passaria, então, os fluxos de bytes decifrados dos dois registros à camada de aplicação; mas o fluxo \nde bytes completo recebido por Alice não estaria na ordem correta por causa da inversão dos registros!\nRecomendamos que você analise cenários diferentes para quando Trudy remove segmentos e quando Trudy \nrepete segmentos.\nA solução para esse problema, como você deve ter imaginado, é usar números de sequência. O SSL os utiliza \nda seguinte maneira. Bob mantém um contador de números de sequência, que se inicia no zero e vai aumentando \npara cada registro SSL que ele envia. Bob, na verdade, não inclui um número de sequência no próprio registro, mas \nno cálculo do MAC. Desse modo, o MAC é agora um hash dos dados mais uma chave MAC MB mais o número de se-\nquência atual. Alice rastreia os números de sequência de Bob, permitindo que ela verifique a integridade dos dados \nde um registro incluindo o número de sequência apropriado no cálculo do MAC. O uso de números de sequência \nSSL impede que Trudy realize um ataque da mulher do meio, como reordenar ou repetir os segmentos. (Por quê?)\nRegistro SSL\nO registro SSL (assim como o registro quase-SSL) é ilustrado na Figura 8.26. Ele consiste em um campo de \ntipo, um campo de versão, um campo de comprimento, um campo de dados e um campo MAC. Observe que os \nprimeiros três campos não estão cifrados. O campo de tipo indica se o registro é uma mensagem de apresentação \nou uma mensagem que contém dados da aplicação. É também usado para encerrar a conexão SSL, como discuti-\ndo a seguir. Na extremidade receptora, o SSL usa o campo de comprimento para extrair os registros SSL do fluxo \nde bytes TCP da entrada. O campo de versão não precisa de explicações.\nSegurança em redes de computadores  527 \n8.6.2  Uma visão mais completa\nA subseção anterior abordou o protocolo quase-SSL; serviu para nos dar uma compreensão básica de por \nque e como funciona o SSL. Agora que temos essa compreensão básica sobre o SSL, podemos nos aprofundar \nmais e examinar os princípios básicos do verdadeiro protocolo SSL. Além da leitura desta descrição, recomenda-\nmos que você complete o laboratório Wireshark SSL, disponível no site de apoio deste livro.\nApresentação SSL\nO SSL não exige que Alice e Bob usem um algoritmo específico de chave simétrica, um algoritmo de chave \npública ou um MAC específico. Em vez disso, permite que Alice e Bob combinem os algoritmos criptográficos no \ninício da sessão SSL, durante a fase de apresentação. Ademais, durante essa fase, Alice e Bob enviam nonces um ao \noutro, que são usados na criação de chaves de sessão (EB, MB, EA e MA), As etapas da apresentação do SSL real são:\n1.\t O cliente envia uma lista de algoritmos criptográficos que ele suporta, junto com um nonce do cliente.\n2.\t A partir da lista, o servidor escolhe um algoritmo simétrico (por exemplo, AES), um algoritmo de chave \npública (por exemplo, RSA com um comprimento de chave específico) e um algoritmo MAC. Ele devolve \nao cliente suas escolhas, bem como um certificado e um nonce do servidor.\n3.\t O cliente verifica o certificado, extrai a chave pública do servidor, gera um Segredo Pré-Mestre (PMS), \ncifra o PMS com a chave pública do servidor e envia o PMS cifrado ao servidor.\n4.\t Utilizando a mesma função de derivação de chave (conforme especificado pelo padrão SSL), o cliente e o \nservidor calculam independentemente o Segredo Mestre (MS) do PMS e dos nonces. O MS é então divi-\ndido para gerar as duas chaves de criptografia e duas chaves MAC. Além disso, quando a cifra simétrica \nselecionada emprega o CBC (como 3DES ou AES), então dois Vetores de Inicialização (IVs) — um para \ncada lado da conexão — são também obtidos do MS. De agora em diante, todas as mensagens enviadas \nentre o cliente e o servidor são cifradas e autenticadas (com o MAC).\n5.\t O cliente envia um MAC de todas as mensagens de apresentação.\n6.\t O servidor envia um MAC de todas as mensagens de apresentação.\nAs duas últimas etapas protegem a apresentação da adulteração. Para entender, observe que no passo 1 o \ncliente normalmente oferece uma lista de algoritmos — alguns fortes e outros fracos. Essa lista é enviada em texto \naberto, visto que os algoritmos de criptografia e chaves ainda não foram consentidos. Trudy, como uma mulher \nno meio, poderia excluir os algoritmos mais fortes da lista, obrigando o cliente a selecionar um algoritmo fraco. \nPara evitar o ataque de adulteração, na etapa 5 o cliente envia um MAC da concatenação de todas as mensagens \nde apresentação que ele enviou e recebeu. O servidor pode comparar esse MAC com o das mensagens de apre-\nsentação que ele enviou e recebeu. Se houver uma inconsistência, o servidor pode finalizar a conexão. De maneira \nsemelhante, o servidor envia um MAC das mensagens de apresentação que encontrou, permitindo que o cliente \nverifique a presença de inconsistências.\nVocê talvez esteja questionando por que existem nonces nas etapas 1 e 2. Os números de sequência não são \nsuficientes para prevenir o ataque de repetição de segmento? A resposta é sim, mas eles não previnem sozinhos \n“o ataque de repetição de conexão”\n. Considere o seguinte ataque de repetição de conexão. Suponha que Trudy \nanalise todas as mensagens entre Alice e Bob. No dia seguinte, ela se passa por Bob e envia a Alice exatamente a \nFigura 8.26  Formato de registro para o SSL\nVersão\nComprimento\nTipo\nDados\nMAC\nCertiﬁcado com  EB\nKR 08.26.eps\nAW/Kurose and Ross\nComputer Networking  6/e\n28p0  x  4p6  \n11/18/11 rossi\n   Redes de computadores e a Internet\n528\nmesma sequência de mensagens que Bob enviou a Alice no dia anterior. Se Alice não usar os nonces, ela respon-\nderá exatamente com a mesma sequência de mensagens que enviou no dia anterior. Alice não suspeitará de nada \nestranho, pois cada mensagem que ela recebe passará pela verificação de integridade. Se Alice for um servidor de \ncomércio eletrônico, pensará que Bob está efetuando um segundo pedido (para a mesma coisa). Por outro lado, \nao incluir um nonce no protocolo, Alice enviará nonces diferentes para cada sessão TCP, fazendo que as chaves \nde criptografia sejam diferentes nos dois dias. Portanto, quando Alice recebe os registros SSL repetidos de Trudy, \neles falharão na verificação de integridade, e a transação do site de comércio eletrônico falso não acontecerá. Em \nresumo, no SSL, os nonces são usados para proteger o “ataque de repetição de conexão”\n, e os números de sequên-\ncia são usados para defender a repetição de pacotes individuais durante uma sessão em andamento.\nEncerramento de conexão\nEm algum momento, Bob ou Alice desejarão finalizar a sessão SSL. Um método seria deixar Bob finalizar \na sessão SSL apenas encerrando a conexão TCP subjacente — ou seja, enviando um segmento TCP FIN para \nAlice. Mas esse plano ingênuo prepara o terreno para o ataque por truncamento pelo qual Trudy, mais uma vez, se \nlocaliza no meio de uma sessão SSL em andamento e a finaliza antes da hora com um TCP FIN. Se Trudy fizesse \nisso, Alice acharia que recebeu todos os dados de Bob quando, na verdade, recebeu só uma parte deles. A solução \npara esse problema é indicar, no campo de tipo, se o registro serve para encerrar a sessão SSL. (Embora o tipo \nSSL seja enviado em aberto, ele é autenticado no receptor usando o MAC do registro.) Ao incluir esse campo, se \nAlice fosse receber um TCP FIN antes de receber um registro SSL de encerramento, ela saberia que algo estranho \nestava acontecendo.\nIsso conclui nossa introdução ao SSL. Vimos que ele usa muitos dos princípios da criptografia discutidos \nnas seções 8.2 e 8.3. Os leitores que querem explorar o SSL mais profundamente podem consultar o livro de Res-\ncorla sobre o SSL [Rescorla, 2001].\n8.7  \u0007\nSegurança na camada de rede: IPsec e redes virtuais \nprivadas\nO protocolo IP de segurança, mais conhecido como IPsec, provê segurança na camada de rede. O IPsec pro-\ntege os datagramas IP entre quaisquer entidades da camada de rede, incluindo hospedeiros e roteadores. Como \ndescreveremos em breve, muitas instituições (corporações, órgãos do governo, organizações sem fins lucrativos \netc.) usam o IPsec para criar redes virtuais privadas (VPNs) que trabalham em cima da Internet pública.\nAntes de falar sobre o IPsec, vamos voltar e considerar o que significa prover sigilo na camada de rede. \nCom o sigilo da camada de rede entre um par de entidades da rede (por exemplo, entre dois roteadores, entre \ndois hospedeiros, ou entre um roteador e um hospedeiro), a entidade remetente cifra as cargas úteis de todos os \ndatagramas que envia à entidade destinatária. A carga útil cifrada poderia ser um segmento TCP, um segmento \nUDP e uma mensagem ICMP etc. Se esse serviço estivesse em funcionamento, todos os dados enviados de uma \nentidade a outra — incluindo e-mail, páginas Web, mensagens de apresentação TCP e mensagens de gerencia-\nmento (como ICMP e SNMP) — ficariam ocultos de qualquer intruso que estivesse investigando a rede. Por essa \nrazão, a segurança na camada de rede é conhecida por prover “cobertura total”\n.\nAlém do sigilo, um protocolo de segurança da camada de rede poderia prover outros serviços de segurança. \nPor exemplo, fornecer autenticação da origem, de modo que a entidade destinatária possa verificar a origem do \ndatagrama protegido. Um protocolo de segurança da camada de rede poderia oferecer integridade dos dados, \npara que a entidade destinatária verificasse qualquer adulteração do datagrama que pudesse ter ocorrido enquan-\nto o datagrama estava em trânsito. Um serviço de segurança da camada de rede também poderia oferecer a pre-\nvenção do ataque de repetição, querendo dizer que Bob conseguiria detectar quaisquer datagramas duplicados \nque um atacante pudesse inserir. Veremos em breve que o IPsec provê mecanismos para todos esses serviços de \nsegurança, ou seja, para sigilo, autenticação da origem, integridade dos dados e prevenção do ataque de repetição.\nSegurança em redes de computadores  529 \n8.7.1  IPsec e redes virtuais privadas (VPNs)\nUma instituição que se estende por diversas regiões geográficas muitas vezes deseja ter sua própria rede IP, \npara que seus hospedeiros e servidores consigam enviar dados um ao outro de uma maneira segura e sigilosa. Para \nalcançar essa meta, a instituição poderia, na verdade, empregar uma rede física independente — incluindo roteado-\nres, enlaces e uma infraestrutura de DNS — que é completamente separada da Internet pública. Essa rede disjunta, \nreservada a uma instituição particular, é chamada de rede privada. Como é de se esperar, uma rede privada pode ser \nmuito cara, já que a instituição precisa comprar, instalar e manter sua própria infraestrutura de rede física.\nEm vez de implementar e manter uma rede privada, hoje muitas instituições criam VPNs em cima da Inter-\nnet pública. Com uma VPN, o tráfego interdepartamental é enviado por meio da Internet pública e não de uma \nrede fisicamente independente. Mas, para prover sigilo, esse tráfego é criptografado antes de entrar na Internet \npública. Um exemplo simples de VPN é mostrado na Figura 8.27. Aqui, a instituição consiste em uma matriz, \numa filial e vendedores viajantes, que normalmente acessam a Internet do seu quarto de hotel. (A figura mostra \nsó um vendedor.) Nesta VPN, quando dois hospedeiros dentro da matriz enviam datagramas IP um ao outro ou \nquando dois hospedeiros dentro de uma filial querem se comunicar, eles usam o bom e velho IPv4 (ou seja, sem \nos serviços IPsec). Entretanto, quando dois dos hospedeiros da instituição se comunicam por um caminho que \ncruza a Internet pública, o tráfego é codificado antes de entrar na Internet.\nPara entender como a VPN funciona, vamos examinar um exemplo simples no contexto da Figura 8.27. \nQuando um hospedeiro na matriz envia um datagrama IP a um vendedor no hotel, o roteador de borda na ma-\ntriz converte o datagrama IPv4 em um IPsec e o encaminha à Internet. Esse datagrama IPsec, na verdade, possui \num cabeçalho IPv4 tradicional, de modo que os roteadores na Internet pública processam o datagrama como se \nele fosse um IPv4 comum — para eles, o datagrama é perfeitamente comum. Mas, conforme ilustrado na Figura \n8.27, a carga útil do datagrama IPsec inclui um cabeçalho IPsec, que é utilizado para o processamento do IPsec; \nalém disso, a carga útil do datagrama IPsec está codificada. Quando o datagrama IPsec chegar ao notebook do \nvendedor, o sistema operacional no notebook decodifica a carga útil (e provê outros serviços de segurança, como \na verificação da integridade dos dados) e passa a carga útil não codificada para o protocolo da camada superior \n(por exemplo, para o TCP ou UDP).\nFigura 8.27  Rede virtual privada (VPN)\nKR 08.27.eps\nKUROSE/ROSS\nComputer Networking 6/e\nIP\nCabeçalho\nIPsec\nCabeçalho\nCarga útil \nsegura\nIP\nCabeçalho\nIPsec\nCabeçalho\nCarga útil \nsegura\nIP\nCabeçalho\nIPsec\nCabeçalho\nCarga útil \nsegura\nIP\nCabeçalho\nCarga útil\nIP\nCabeçalho Carga útil\nNotebook com IPsec\nRoteador \ncom IPv4 \ne IPsec\nRoteador \ncom IPv4 \ne IPsec\nFilial\nMatriz\nVendedor \nno hotel\nInternet \npública\n   Redes de computadores e a Internet\n530\nAcabamos de dar uma visão geral de alto nível de como uma instituição pode implementar o IPsec para \ncriar uma VPN. Para ver a floresta por entre as árvores, deixamos de lado muitos detalhes importantes. Vamos \nagora dar uma olhada neles mais de perto.\n8.7.2  Os protocolos AH e ESP\nO IPsec é um material um tanto complexo — ele é definido em mais de uma dúzia de RFCs. Dois RFCs im-\nportantes são RFC 4301, o qual descreve a arquitetura de segurança IP geral, e RFC 6071, que fornece uma visão \ngeral dos protocolos IPsec. Nossa meta neste livro, como de costume, não é apenas reprocessar os RFCs secos e \ncomplexos, mas fazer uma abordagem mais operacional e pedagógica para descrever os protocolos.\nNo conjunto dos protocolos IPsec, existem dois principais: o protocolo Cabeçalho de Autenticação (AH) \ne o protocolo Carga de Segurança de Encapsulamento (ESP). Quando uma entidade remetente IPsec (em geral \num hospedeiro ou um roteador) envia datagramas seguros a uma entidade destinatária (também um hospedeiro \nou um roteador), ela utiliza o protocolo AH ou o protocolo ESP. O protocolo AH provê autenticação da origem \ne integridade dos dados, mas não provê sigilo. O protocolo ESP provê autenticação da origem, integridade dos \ndados e sigilo. Visto que o sigilo é muitas vezes essencial às VPNs e outras aplicações IPsec, o protocolo ESP é \nmuito mais utilizado do que o protocolo AH. Para desmistificar o IPsec e evitar suas complexidades, a partir de \nagora estaremos focados exclusivamente no protocolo ESP. Os leitores que desejam aprender também sobre o \nprotocolo AH devem explorar os RFCs e outros recursos on-line.\n8.7.3  Associações de segurança\nOs datagramas IPsec são enviados entre pares de entidades da rede, tal como entre hospedeiros, entre dois \nroteadores, ou entre um hospedeiro e um roteador. Antes de enviar datagramas IPsec da entidade remetente à \ndestinatária, essas entidades criam uma conexão lógica da camada de rede, denominada associação de segu-\nrança (SA). Uma SA é uma conexão lógica simples; ou seja, ela é unidirecional do remetente ao destinatário. Se \nas duas entidades querem enviar datagramas seguros entre si, então duas SAs (ou seja, duas conexões lógicas) \nprecisam ser estabelecidas, uma em cada direção.\nPor exemplo, considere mais uma vez uma VPN institucional na Figura 8.27. Essa instituição consiste em \numa filial, uma matriz e, digamos, n vendedores viajantes. Por exemplo, vamos supor que haja tráfego IPsec bidi-\nrecional entre a matriz e a filial e tráfego IPsec bidirecional entre a matriz e os vendedores viajantes. Quantas SAs \nexistem nessa VPN? Para responder a essa questão, observe que há duas SAs entre o roteador de borda da matriz \ne o de borda da filial (uma em cada direção); para cada notebook do vendedor, há duas SAs entre o roteador de \nborda da matriz e o notebook (de novo, uma em cada direção). Portanto, no total, há (2 + 2n) SAs. Mantenha em \nmente, entretanto, que nem todo o tráfego enviado à Internet pelos roteadores de borda ou pelos notebooks será pro-\ntegido por IPsec. Podemos citar como exemplo um hospedeiro na matriz que quer acessar ao servidor Web (como \nAmazon ou Google) na Internet pública. Assim, o roteador de borda (e os notebooks) emitirão na Internet tanto \ndatagramas IPv4 comuns como datagramas IPsec seguros.\nVamos agora olhar “por dentro” de uma SA. Para tornar essa discussão tangível e concreta, utilizaremos o \ncontexto de uma SA do roteador R1 ao roteador R2 na Figura 8.28. (Você pode pensar no Roteador R1 como \no roteador de borda da matriz, e o Roteador R2 como o roteador de borda da filial, conforme a Figura 8.27.) O \nRoteador R1 manterá as informações de estado sobre essa SA, as quais incluirão:\n• Um identificador de 32 bits para a SA, denominado Índice de Parâmetro de Segurança (SPI)\n• A interface remetente da SA (neste caso 200.168.1.100) e a interface destinatária da SA (neste caso \n193.68.2.23)\n• O tipo de criptografia a ser usada (por exemplo, 3DES com CBC)\n• A chave de criptografia\nSegurança em redes de computadores  531 \n• O tipo de verificação de integridade (por exemplo, HMAC com MD5)\n• A chave de autenticação\nQuando o roteador R1 precisa construir um datagrama IPsec para encaminhar por essa SA, ele acessa as in-\nformações de estado para determinar como deveria autenticar e criptografar o datagrama. De maneira semelhan-\nte, o roteador R2 manterá as mesmas informações de estado sobre essa SA e as usará para autenticar e decodificar \nqualquer datagrama IPsec que chegar da SA.\nUma entidade IPsec (roteador ou hospedeiro) muitas vezes guarda essas informações de estado para muitas \nSAs. No exemplo sobre a VPN na Figura 8.27 com n vendedores, o roteador de borda guarda informações de esta-\ndo para (2 + 2n) SAs. Uma entidade IPsec armazena as informações de estado para todas as suas SAs em seu Banco \nde Dados de Associação de Segurança (SAD), o qual é uma estrutura de dados do núcleo do sistema operacional.\n8.7.4  O datagrama IPsec\nApós descrever sobre as SAs, podemos agora descrever o verdadeiro datagrama IPsec. Ele possui duas for-\nmas diferentes de pacotes, uma para o modo túnel e outra para o modo transporte. O modo túnel, o mais apro-\npriado para as VPNs, é mais implementado do que o modo transporte. Para desmistificar o IPsec e evitar suas \ncomplexidades, a partir de agora vamos nos concentrar exclusivamente no modo túnel. Uma vez que você tiver \numa compreensão sólida do modo túnel, poderá com facilidade aprender, por sua conta, o modo transporte.\nO formato do pacote do datagrama IPsec é ilustrado na Figura 8.29. Você pode pensar que os formatos do pa-\ncote são maçantes e sem graça, mas logo veremos que o datagrama IPsec na verdade parece e tem gosto de uma igua-\nria mexicana! Vamos examinar os campos do IPsec no contexto da Figura 8.28. Suponha que o roteador R1 receba \num datagrama IPv4 comum do hospedeiro 172.16.1.17 (na rede da matriz) destinado ao hospedeiro 172.16.2.48 (na \nrede da filial). R1 utiliza a seguinte receita para converter esse “datagrama IPv4 original” em um datagrama IPsec:\n• Anexa ao final do datagrama IPv4 original (que inclui os campos de cabeçalho originais) um campo de \n“trailer ESP”\nFigura 8.28  Associação de segurança (SA) de R1 a R2\nKR 08.28.eps\nKUROSE/ROSS\nComputer Networking 6/e\n28p0 Wide x 8p2 Deep\n11/18/11 ROSSI ILLUSTRATION\nInternet\nSA\nR1\n172.16.1/24\nMatriz\nFilial\n200.168.1.100\n193.68.2.23\n172.16.2/24\nR2\nFigura 8.29  Formato do datagrama IPsec\nNovo \ncabeçalho IP\nCabeçalho \nESP\nCabeçalho \nESP\nESP\nMAC\nCabeçalho \nIP original\nCarga útil do \ndatagrama IP original\nCifrado\nEnchilada autenticada\nTamanho do\nenchimento\nEnchimento\nPróximo\ncabeçalho\nSPI\nSeq #\nKR 08.29.eps\nKUROSE/ROSS\nC\n/\n   Redes de computadores e a Internet\n532\n• Codifica o resultado utilizando o algoritmo e a chave especificados pela SA\n• Anexa na frente dessa quantidade codificada um campo chamado “cabeçalho ESP”; o pacote resultante \né chamado de “enchilada”\n• Cria uma autenticação MAC sobre a enchilada inteira, usando o algoritmo e a chave especificados na SA\n• Anexa a MAC atrás da enchilada, formando a carga útil\n• Por fim, cria um cabeçalho IP novo com todos os campos de cabeçalho IPv4 clássicos (juntos, normal-\nmente com 20 bytes de comprimento), o qual se anexa antes da carga útil\nObserve que o datagrama IP resultante é um autêntico datagrama IPv4, com os tradicionais campos de \ncabeçalho IPv4 acompanhados por uma carga útil. Mas, neste caso, a carga útil contém um cabeçalho ESP, o \ndatagrama IP original, um trailer ESP e um campo de autenticação ESP (com o datagrama original e o trailer \nESP cifrados). O datagrama IP original possui o endereço IP remetente 172.16.1.17 e o endereço IP destinatário \n172.16.2.48. Em razão de o datagrama IPsec incluir o IP original, esses endereços são inclusos (e cifrados) como \nparte da carga útil do pacote IPsec. Mas e os endereços IP remetente e destinatário que estão no novo cabeçalho \nIP, ou seja, o cabeçalho localizado à esquerda do datagrama IPsec? Como você pode esperar, eles são estabelecidos \npara as interfaces do roteador remetente e destinatário nas duas extremidades dos túneis, isto é, 200.168.1.100 e \n193.68.2.23. Além disso, o número do protocolo nesse novo campo de cabeçalho IPv4 não será o número do TCP, \nUDP ou SMTP, mas igual a 50, determinando que esse é um datagrama IPsec que está utilizando o protocolo ESP.\nApós R1 enviar um datagrama IPsec à Internet pública, ele passará por diversos roteadores antes de chegar \nao R2. Cada um desses roteadores processará o datagrama como se fosse um datagrama comum — eles são in-\nconscientes do fato de que o datagrama está transportando dados cifrados pelo IPsec. Para esses roteadores da \nInternet pública, como o endereço IP remetente no cabeçalho externo é R2, o destino final do datagrama é R2.\nDepois de acompanhar um exemplo de como o datagrama IPsec é construído, vamos olhar de perto os in-\ngredientes da enchilada. Vemos na Figura 8.29 que o trailer ESP consiste em três campos: enchimento, tamanho \ndo enchimento e próximo cabeçalho. Lembre-se de que cifras de bloco exigem que a mensagem a ser cifrada seja \num múltiplo inteiro do comprimento de bloco. O enchimento (que consiste em bytes sem significado) é usado \nde modo que, quando adicionada ao datagrama original (junto com o tamanho do enchimento e próximo cabe-\nçalho), a “mensagem” resultante tenha um número inteiro de blocos. O campo tamanho do enchimento indica à \nentidade destinatária quanto enchimento foi inserido (e, portanto, precisa ser removido). O próximo cabeçalho \nidentifica o tipo (por exemplo, UDP) de dados contidos no campo de dados da carga útil. Os dados da carga útil \n(em geral o datagrama IP original) e o trailer ESP são concatenados e, então, cifrados.\nAnexado na frente dessa unidade cifrada está o cabeçalho ESP, o qual é enviado em aberto e consiste em dois \ncampos: o SPI e o campo de número de sequência. O SPI indica à entidade destinatária a SA à qual o datagrama \npertence; essa entidade pode, então, indexar seu SAD com o SPI para determinar os algoritmos e chaves apropriados \nde autenticação/decriptação. O campo de número de sequência é usado para a proteção contra ataques de repetição.\nA entidade remetente também anexa uma autenticação MAC. Como já dissemos, a entidade remetente cal-\ncula um MAC por toda a enchilada (que consiste em um cabeçalho ESP, o datagrama IP original e o trailer ESP — \n \ncom a criptografia do datagrama e do trailer). Lembre-se de que, para calcular um MAC, o remetente anexa uma \nchave MAC secreta à enchilada e calcula um hash de tamanho fixo do resultado.\nQuando R2 recebe o datagrama IPsec, ele observa que o endereço IP destinatário do datagrama é o próprio \nR2, que, então, processa o datagrama. Como o campo de protocolo (no cabeçalho IP à esquerda) é 50, R2 entende \nque deve aplicar o processamento IPsec ESP ao datagrama. Primeiro, alinhando na enchilada, E2 usa o SPI para \ndeterminar a qual SA o datagrama pertence. Segundo, ele calcula o MAC da enchilada e verifica se o MAC é \ncompatível com o valor do campo MAC ESP. Se for, ele sabe que a enchilada vem de R1 e que não foi adulterada. \nTerceiro, verifica o campo número de sequência para ver se o datagrama é novo (e não repetido). Quarto, ele de-\ncodifica a unidade cifrada utilizando o algoritmo e a chave de criptografia associados à SA. Quinto, ele remove o \nenchimento e extrai o datagrama IP original básico. E, por fim, sexto, ele encaminha o datagrama original à rede \nda filial em direção a seu destino final. Ufa, que receita complicada, não é? Ninguém disse que era fácil preparar \ne desvendar uma enchilada!\nSegurança em redes de computadores  533 \nNa verdade, existe outra importante sutileza que precisa ser abordada. Ela se baseia na seguinte questão: \nQuando R1 recebe um datagrama (inseguro) de um hospedeiro na rede da matriz, e esse datagrama é destinado a \nalgum endereço IP destinatário fora da matriz, como R1 sabe se ele deve ser convertido em um datagrama IPsec? \nE se ele vai ser processado por um IPsec, como R1 sabe qual SA (de muitas SAs em seu SAD) deve ser usada para \nconstruir o datagrama IPsec? O problema é resolvido da seguinte maneira. Junto com um SAD, a entidade IPsec \ntambém mantém outra estrutura de dados denominada Banco de Dados de Política de Segurança (SPD). Este \nindica que tipos de datagramas (como uma função do endereço IP remetente, endereço IP destinatário e tipo do \nprotocolo) serão processados pelo IPsec; e para aqueles que serão processados pelo IPsec, qual SA deve ser usa-\nda. De certa forma, as informações em um SPD indicam “o que” fazer com um datagrama que está chegando; as \ninformações no SAD indicam “como” fazer isso.\nResumo dos serviços IPsec\nQuais serviços o IPsec provê, exatamente? Vamos examinar tais serviços do ponto de vista de um atacante, \ndigamos Trudy, que é uma mulher no meio, situada entre R1 e R2 na Figura 8.28. Suponha por toda essa discus-\nsão que Trudy não conhece as chaves de autenticação e criptografia usadas pela SA. O que Trudy pode e não pode \nfazer? Primeiro, ela não pode ver o datagrama original. Na verdade, não só os dados do datagrama original estão \nocultos de Trudy como também o número de protocolo, o endereço IP remetente e o endereço IP destinatário. \nEm relação aos datagramas enviados através da SA, Trudy sabe apenas que eles vieram de algum hospedeiro em \n172. 16.1.0/24 e são destinados a algum hospedeiro em 172.16.2.0/24. Ela não sabe se está carregando dados TCP, \nUDP ou ICMP; ela não sabe que está carregando HTTP, SMTP ou quaisquer outros tipos de dados de aplicação. \nEsse sigilo, portanto, vai além do SSL. Segundo, suponha que Trudy tente adulterar um datagrama na SA alteran-\ndo alguns de seus bits. Quando o datagrama alterado chegar a R2, a verificação de integridade falhará (usando um \nMAC), impedindo a tentativa maliciosa de Trudy mais uma vez. Terceiro, imagine que Trudy tente se passar por \nR1, criando um datagrama IPsec com remetente 200.168.1.100 e destinatário 193.68.2.23. O ataque será inútil, \npois a verificação da integridade do datagrama em R2 falhará novamente. Por fim, em razão de o IPsec incluir nú-\nmeros de sequência, Trudy não poderá criar um ataque de repetição bem-sucedido. Em resumo, conforme afir-\nmado no início desta seção, o IPsec oferece — entre qualquer par de dispositivos que processam pacotes através \nda camada de rede — sigilo, autenticação da origem, integridade dos dados e proteção contra ataque de repetição.\n8.7.5  IKE: Gerenciamento de chave no IPsec\nQuando uma VPN possui um número pequeno de pontos finais (por exemplo, apenas dois roteadores como \nna Figura 8.28), o administrador de rede pode inserir manualmente informações sobre a SA (algoritmos e chaves \nde criptografia/autenticação e os SPIs) nos SADs dos pontos de chegada. Essa “teclagem manual” é claramente \nimpraticável para uma VPN grande, a qual pode consistir em centenas ou mesmo milhares de roteadores e hos-\npedeiros IPsec. Implementações grandes e geograficamente distribuídas exigem um mecanismo automático para \na criação das SAs. O IPsec o faz com o protocolo de Troca de Chave (IKE), especificado em RFC 5996.\nO IKE tem semelhanças com a apresentação (handshake) em SSL (consulte a Seção 8.6). Cada entidade \nIPsec possui um certificado, o qual inclui a chave pública da entidade. Da mesma forma que o SSL, o protocolo \nIKE tem os dois certificados de troca de entidades, autenticação de negociação e algoritmos de criptografia, e \ntroca informações de chave com segurança para criar chaves de sessão nas SAs IPsec. Diferente do SSL, o IKE \nemprega duas fases para realizar essas tarefas.\nVamos investigar essas duas fases no contexto de dois roteadores, R1 e R2, na Figura 8.28. A primeira fase \nconsiste em duas trocas de pares de mensagem entre R1 e R2:\n• Durante a primeira troca de mensagens, os dois lados usam Diffie-Hellman (consulte os Problemas no \nfinal do capítulo) para criar um IKE SA bidirecional entre os roteadores. Para nos confundir, esse IKE \n   Redes de computadores e a Internet\n534\nSA bidirecional é totalmente diferente da SA IPsec discutida nas seções 8.6.3 e 8.6.4. O IKE SA provê \num canal cifrado e autenticado entre os dois roteadores. Durante a primeira troca de par de mensagem, \nas chaves são estabelecidas para a criptografia e autenticação para o IKE SA. Também é estabelecido um \nsegredo mestre que será usado para calcular chaves SA IPsec mais adiante na fase 2. Observe que, durante \na primeira etapa, as chaves públicas e privadas RSA não são usadas. Em particular, nem R1 nem R2 reve-\nlam sua identidade assinando uma mensagem com sua chave privada.\n• Durante a segunda troca de mensagens, ambos os lados revelam sua identidade assinando suas mensa-\ngens. Entretanto, as identidades não são reveladas a um analisador passivo, pois são enviadas por um \ncanal seguro IKE SA. Também nessa fase, os dois lados negociam os algoritmos de autenticação e cripto-\ngrafia que serão empregados pelas SAs IPsec.\nNa fase 2 do IKE, os dois lados criam uma SA em cada direção. Ao fim da fase 2, as chaves de sessão de \ncriptografia e autenticação são estabelecidas em ambos os lados para as duas SAs. Eles podem, então, usar as SAs \npara enviar datagramas seguros, como descrito nas seções 8.6.3 e 8.6.4. A principal motivação de ter duas fases \nno IKE é o custo computacional — visto que a segunda fase não envolve qualquer chave pública de criptografia, \no IKE pode criar um grande número de SAs entre as duas entidades IPsec com um custo computacional relati-\nvamente pequeno.\n8.8  Segurança de LANs sem fio\nSegurança é uma preocupação importante em redes sem fio, em que as ondas de rádio carregando quadros \npodem se propagar muito além do prédio que contém os hospedeiros e as estações base sem fio. Nesta seção, \napresentaremos uma breve introdução em segurança sem fio. Para obter mais informações, veja o livro de fácil \nleitura de Edney e Arbaugh [Edney, 2003].\nO tema de segurança em 802.11 tem atraído muita atenção em círculos técnicos e na mídia. Apesar de \ndiscussões consideráveis terem sido feitas, poucos debates aconteceram — parece existir um entendimento uni-\nversal de que a especificação 802.121 original contém uma série de falhas graves na segurança. De fato, podemos \nfazer o download de softwares de domínio público que tiram proveito dessas falhas, fazendo aqueles que usam \nmecanismos de segurança 802.11 comuns ficarem expostos a ataques de segurança tanto quanto os que não usam \nnenhum tipo de atributos de segurança.\nNa seção seguinte, discutiremos os mecanismos de segurança inicialmente padronizados na especificação \n802.11, conhecida como Privacidade Equivalente Cabeada (WEP, do inglês Wired Equivalent Privacy). Como \no nome sugere, a WEP tem como propósito fornecer um nível de segurança semelhante ao que é encontrado em \nredes cabeadas. Então, discutiremos algumas falhas de segurança da WEP e examinaremos o padrão 802.11i, \numa versão fundamentalmente mais segura do que 802.11, adotado em 2004.\n8.8.1  Privacidade Equivalente Cabeada (WEP)\nO protocolo IEEE 802.11 WEP foi criado em 2009 para fornecer autenticação e criptografia de dados entre \num hospedeiro e um ponto de acesso sem fio (ou seja, a estação-base) usando uma técnica de chave compartilha-\nda simétrica. A WEP não especifica um algoritmo de gerenciamento de chave, então supomos que o hospedeiro \ne o ponto de acesso sem fio de alguma forma concordam sobre a chave através de um método fora da banda. A \nautenticação é realizada da seguinte forma:\n1.\t Um hospedeiro sem fio requisita uma autenticação por um ponto de acesso.\n2.\t Um ponto de acesso responde ao pedido de autenticação com um valor de nonce de 128 bytes.\n3.\t O hospedeiro sem fio criptografa o nonce usando uma chave simétrica que compartilha com o ponto \nde acesso.\n4.\t O ponto de acesso decodifica o nonce criptografado do hospedeiro.\nSegurança em redes de computadores  535 \nSe o nonce decodificado for compatível com o valor nonce originalmente enviado ao hospedeiro, então o \nhospedeiro é autenticado pelo ponto de acesso.\nO algoritmo criptografado de dados WEP é ilustrado na Figura 8.30. Uma chave simétrica secreta de 40 bits, \nKS, é presumidamente conhecida por ambos, hospedeiro e ponto de acesso. Além disso, um Vetor de Inicialização \n(IV, do inglês Initialization Vector) de 24 bits é anexado a uma chave de 40 bits para criar uma chave de 64 bits que \nserão usados para criptografar um único quadro. O IV mudará de um quadro para o outro e, por conseguinte, \ncada quadro será criptografado com uma chave de 64 bits diferente. A criptografia é efetuada da seguinte forma. \nPrimeiro, um valor de 4 bytes de CRC (veja a Seção 5.2) é calculado para a carga útil de dados. Então, a carga útil \ne o CRC de quatro bytes são criptografados usando uma cifra de fluxo RC4. Não veremos os detalhes de um RC4 \naqui (veja Schneier [1995] e Edney [2003] para obter mais detalhes). Para nossos objetivos, é suficiente saber que, \nquando o valor de uma chave (nesse caso, a chave de 64 bits (KS, IV)) é apresentado ao algoritmo RC4, este produz \num fluxo de valores de chave k1\nIV, k2\nIV, k3\nIV,… que são usados para criptografar os dados e o valor CRC em um \nquadro. Para propósitos práticos, podemos pensar nessas operações como se fossem executadas em um byte por \nvez. A criptografia é realizada por OU-exclusivo no i-ésimo byte de dados, di, com a i-ésima chave, ki\nIV, no fluxo \nde valores de chaves produzido pelo par (Ks, IV) para produzir o i-ésimo byte do texto cifrado, ci:\nci = di \n ki\nIV\nO valor de IV muda de um quadro ao próximo e é incluído no texto aberto do cabeçalho de cada quadro \n802.11 criptografado por WEP, como ilustra a Figura 8.30. O receptor aceita a chave simétrica secreta de 40 bits \nque compartilha com o transmissor, anexa o IV e usa a chave de 64 bits resultante (que é idêntica à usada pelo \ntransmissor para executar a criptografia) para decriptografar o quadro:\ndi = ci \n ki\nIV\nO uso adequado do algoritmo RC4 necessita que o mesmo valor da chave de 64 bits nunca seja usado mais \nde uma vez. Lembre-se de que a chave WEP muda em uma base quadro a quadro. Para determinado KS (que \nmuda em raras ocasiões), isso significa que existem somente 224 chaves únicas. Se estas são escolhidas de modo \naleatório, podemos mostrar que a probabilidade de ter escolhido o mesmo valor de IV (e, portanto, usado a mes-\nma chave de 64 bits) é de mais de 99% depois de somente 12.000 quadros [Walker, 2000; Edney, 2003]. Com um \nquadro de 1 Kbyte e a transmissão de dados de velocidade 11 Mbits/s, apenas alguns segundos são necessários \nantes que 12.000 quadros sejam transmitidos. Além do mais, já que IV é transmitido em texto aberto no quadro, \num curioso saberá quando um valor IV duplicado for usado.\nPara notar um dos vários problemas que ocorrem quando uma chave duplicada é usada, considere o se-\nguinte ataque ao texto aberto escolhido por Trudy contra Alice. Suponha que Trudy (usando provavelmente uma \nfalsificação de IP) envia uma solicitação a Alice (por exemplo, uma solicitação HTTP ou uma solicitação FTP) \npara transmitir um arquivo de conteúdo declarado, d1, d2, d3, d4… Trudy também nota os dados criptografados \nc1, c2, c3, c4… Já que di = ci \n ki\nIV, se OU-exclusivo ci com cada lado dessa igualdade, teremos\ndi \n ci = ki\nIV\nFigura 8.30  Protocolo 802.11 WEP\nKR 08.30.eps\nKurose and Ross\nGerador de sequência de chaves\n(para Ks, IV dados)\nk1\nIV\nd1\nc1\nk2\nIV\nk3\nIV\nkN\nIV\nIV\nkN+1\nIV\nkN+4\nKs: chave simétrica secreta de 40 bits\nQuadro de dados em texto aberto mais CRC\nIV (por quadro)\nCabeçalho \n802.11\nIV\nDados criptografados \npor WEP mais CRC\nd2\nc2\nd3\nc3\ndN\ncN\nCRC1\ncN+1\ncN+4\nCRC4\n   Redes de computadores e a Internet\n536\nNessa relação, Trudy pode usar os valores conhecidos de di e ci para calcular ki\nIV. Da próxima vez que Trudy \nvir o mesmo valor de IV sendo usado, ela saberá a sequência de chave k1\nIV, k2\nIV, k3\nIV,… e assim será capaz de de-\ncriptografar a mensagem criptografada.\nExistem outras diversas preocupações adicionais sobre a segurança da WEP. Fluhrer [2001] descreve um \nataque aproveitando-se de uma fraqueza conhecida no RC4 quando certas chaves são escolhidas. Stubblefield \n[2002] discute maneiras eficazes de programar e tirar proveito desse ataque. Outra preocupação com a WEP \nenvolve os bits CRC, mostrados na Figura 8.30, e transmitidos no quadro 802.11 para detectar bits alterados \nna carga útil. No entanto, um atacante que muda o conteúdo criptografado (por exemplo, substituindo textos \nininteligíveis por dados criptografados originais), calcula o CRC através de textos ininteligíveis e coloca o CRC \nem um quadro WEP, pode produzir um quadro 802.11 que será aceito pelo receptor. O que é necessário nesse \ncaso são técnicas de integridade de mensagem, como as estudadas na Seção 8.3 para detectar intercepções físicas \nou substituições de conteúdo. Para obter mais informações sobre segurança da WEP, veja Edney [2003]; Walker \n[2000] Weatherspoon [2000] e suas referências a esse respeito.\n8.8.2  IEEE 802.11i\nLogo após o lançamento do IEEE 802.11 em 1999, começou o trabalho no desenvolvimento de uma ver-\nsão nova e aprimorada do 802.11, com mecanismos de segurança mais fortes. O novo padrão, conhecido como \n802.11i, passou por uma ratificação final em 2004. Como veremos, enquanto a WEP fornecia uma criptografia \nrelativamente fraca, somente um meio de executar autenticação e nenhum mecanismo de distribuição de chaves, \no IEEE 802.11i fornece formas de criptografia muito mais fortes, um conjunto extenso de mecanismos de auten-\nticação e um mecanismo de distribuição de chaves. A seguir, apresentamos uma síntese do 802.11i; um excelente \npanorama técnico (em fluxo de áudio) sobre 802.11i é TechOnline [2012].\nA Figura 8.31 dá uma visão geral sobre a estrutura do 802.11i. Além do cliente sem fio e do ponto de acesso, \no 802.11i define um servidor de autenticação, com o qual o AP se comunica. Separar o servidor de comunicação \ndo AP permite que um servidor de autenticação atenda a muitos APs, centralizando as decisões (muitas vezes \nFigura 8.31  802.11i: quatro fases de operação\nSTA:\nestação cliente\nAP:\nponto de acesso\nRede \ncom fio\nAS:\nservidor de \nautenticação\nKR 08.31.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n25p0 Wide x 19p2 Deep\n1\nDescoberta de \ncapacidades de segurança\n4\nSTA, AP usam PMK para derivar \nChave Temporária (Temporal Key — TK) \nutilizada para criptograﬁa, \nintegridade de mensagem\nAS deriva a mesma PMK, \nenvia a APLICAÇÃO\nSTA deriva Chave \nMestra do par (PMK – Peer Master Key)\n2\n3\n3\nSTA e AS autenticam-se mutuamente, juntos geram a Chave Mestra\n(Master Key — MK). AP funciona como “passagem”\nSegurança em redes de computadores  537 \nconfidenciais) relativas à autenticação e acesso em um único servidor, e mantendo os custos e a complexidade do \nAP menores. O 802.11i opera em quatro fases:\n1.\t Descoberta. Na fase de descoberta, o AP anuncia sua presença e as formas de autenticação e criptografia \nque podem ser fornecidas ao nó do cliente sem fio. Então, o cliente solicita as formas específicas de au-\ntenticação e criptografia que deseja. Apesar de o cliente e o AP já estarem trocando mensagens, o cliente \nainda não foi autenticado, nem tem uma chave criptografada, então vários passos ainda serão necessários \nantes que o cliente possa se comunicar com um hospedeiro remoto qualquer por um canal sem fio.\n2.\t Autenticação mútua e geração da Chave Mestra (MK). A autenticação ocorre entre o cliente sem fio e o \nservidor de autenticação. Nesta fase, o ponto de acesso age essencialmente como um repassador, encami-\nnhando mensagens entre o cliente e o servidor de autenticação. O Protocolo de Autenticação Extensível \n(EAP, do inglês Extensible Authentication Protocol) [RFC 3748] define o formato da mensagem fim a fim \nusado em um modo simples de requisição/resposta de interação entre o cliente e o servidor de autentica-\nção. Como mostrado na Figura 8.32, as mensagens EAP são encapsuladas usando um EAPoL (EAP atra-\nvés da LAN, [IEEE 802.1X]) e enviadas através de um enlace 802.11 sem fio. Então, estas mensagens EAP \nsão desencapsuladas no ponto de acesso, e reencapsuladas usando um protocolo RADIUS para a trans-\nmissão por UDP/IP ao servidor de autenticação. Embora o protocolo e o servidor RADIUS [RFC 2865] \nnão sejam requisitados pelo protocolo 802.11i, eles são componentes-padrão na prática para o 802.11i. \nO protocolo há pouco padronizado DIAMETER [RFC 3588] é propenso a substituir o RADIUS em um \nfuturo próximo.\n\t\nCom o EAP, o servidor de autenticação pode escolher diversos modos para realizar a autenticação. Embo-\nra o 802.11i não exija um método específico de autenticação, o esquema de autenticação EAP-TLS [RFC \n5216] muitas vezes é utilizado. O EAP-TLS usa técnicas de chaves públicas (incluindo a criptografia nonce \ne resumos de mensagens) semelhantes às que estudamos na Seção 8.3, que permitem que o cliente e o ser-\nvidor de autenticação se autentiquem mutuamente, e produzam uma Chave Mestra (MK, do inglês Master \nKey) que é conhecida por ambas as partes.\n3.\t Geração de Chave Mestra Pareada (PMK). A MK é compartilhada secretamente apenas para o cliente e \npara o servidor de autenticação, sendo usada por eles para gerar uma segunda chave, a Chave Mestra Pa-\nreada (PMK, do inglês Pairwise Master Key). Então, o servidor de autenticação envia a PMK ao AP. E este \né o ponto a que queríamos chegar! O cliente e o AP têm agora uma chave compartilhada (lembre-se de \nque em WEP o problema da distribuição de chaves não foi tratado) e se autenticaram mutuamente. Agora \neles estão quase prontos para entrar em ação.\nFigura 8.32  \u0007\nO EAP é um protocolo fim a fim. As mensagens EAP são encapsuladas usando um \nEAPol através de um enlace sem fio entre o cliente e o ponto de acesso, e usando \nRADIUS através de UDP/IP entre o ponto de acesso e o servidor de autenticação\nSTA:\nestação cliente\nAP:\nponto de acesso\nRede \ncom fio\nAS:\nservidor de \nautenticação\nEAP TLS\nEAP\nEAP por LAN (EAPoL)\nRADIUS\nIEEE 802.11\nUDP/IP\nKR 08.32.eps\nAW/Kurose and Ross\n   Redes de computadores e a Internet\n538\n4.\t Geração de Chave Temporal (TK). Com a PMK, o cliente sem fio e o AP podem agora gerar chaves adicio-\nnais que serão usadas para comunicação. De interesse particular temos a Chave Temporal (TK, do inglês \nTemporal Key), que será usada para executar a criptografia dos dados enviados em nível de enlace pelo \nenlace sem fio e um hospedeiro remoto qualquer.\nO 802.11i oferece várias formas de criptografia, incluindo um esquema de criptografia baseada em AES e \numa versão reforçada da criptografia WEP.\n8.9  \u0007\nSegurança operacional: firewalls e sistemas de \ndetecção de invasão\nVimos, em todo este capítulo, que a Internet não é um lugar muito seguro — os delinquentes estão por \ntoda parte, criando todo tipo de destruição. Sabendo da natureza hostil da Internet, vamos considerar a rede \nde uma organização e um administrador de rede que a administra. Do ponto de vista de um administrador, o \nmundo está dividido claramente em dois campos — os mocinhos (que pertencem à organização que administra \na rede e que deveriam poder acessar recursos dentro da rede que ele administra de um modo relativamente \nlivre de restrições) e os bandidos (todo o resto, cujo acesso aos recursos da rede deve ser cuidadosamente ins-\npecionado). Em muitas organizações, que vão de castelos medievais a modernos escritórios de empresas, há um \núnico ponto de entrada/saída onde ambos, mocinhos e bandidos que entram e saem, passam por inspeção de \nsegurança. Em castelos medievais, essa inspeção era feita em um portão, na extremidade de uma ponte levadiça; \nem escritórios empresariais, ela é feita na central de segurança. Em redes de computadores, quando o tráfego \nque entra/sai de uma rede passa por inspeção de segurança, é registrado, descartado ou transmitido; isso é feito \npor mecanismos operacionais conhecidos como firewalls, sistemas de detecção de invasão (IDSs) e sistemas de \nprevenção de invasão (IPSs).\n8.9.1  Firewalls\nUm firewall é uma combinação de hardware e software que isola a rede interna de uma organização da \nInternet em geral, permitindo que alguns pacotes passem e bloqueando outros. O firewall permite a um adminis-\ntrador de rede controlar o acesso entre o mundo externo e os recursos da rede que ele administra, gerenciando o \nfluxo de tráfego de e para esses recursos. Um firewall possui três objetivos:\n• Todo o tráfego de fora para dentro, e vice-versa, passa por um firewall. A Figura 8.33 mostra um firewall, \nsituado diretamente no limite entre a rede administrada e o resto da Internet. Embora grandes organiza-\nções possam usar diversos níveis de firewalls ou firewalls distribuídos [Skoudis, 2006], alocar um firewall \nem um único ponto de acesso à rede, conforme mostrado na Figura 8.33, facilita o gerenciamento e a \nexecução de uma política de acesso seguro.\n• Somente o tráfego autorizado, como definido pela política de segurança local, poderá passar. Com todo \no tráfego que entra e sai da rede institucional passando pelo firewall, este pode limitar o acesso ao \ntráfego autorizado.\n• O próprio firewall é imune à penetração. O próprio firewall é um mecanismo conectado à rede. Se não pro-\njetado ou instalado de modo adequado, pode ser comprometedor, oferecendo apenas uma falsa sensação \nde segurança (pior do que não ter nenhum firewall!).\nCisco e Check Point são dois dos principais fornecedores atuais de firewall. Você pode criar um firewall \n(filtro de pacotes) facilmente a partir de um sistema Linux usando iptables (software de domínio público que em \ngeral acompanha o Linux).\nOs firewalls podem ser classificados em três categorias: filtros de pacotes tradicionais, filtros de estado e \ngateways de aplicação. Abordaremos cada um nas subseções seguintes.\nSegurança em redes de computadores  539 \nFiltros de pacotes tradicionais\nComo ilustra a Figura 8.33, uma organização normalmente tem um roteador de borda que conecta sua \nrede interna com seu ISP (e dali com a Internet pública, mais ampla). Todo o tráfego que sai ou que entra na \nrede interna passa por esse roteador e é nele que ocorre a filtragem de pacotes. Um filtro de pacotes examina \ncada datagrama que está sozinho, determinando se deve passar ou ficar baseado nas regras específicas defini-\ndas pelo administrador. As decisões de filtragem costumam ser baseadas em:\n• Endereço IP de origem e de destino\n• Tipo de protocolo no campo do datagrama IP: TCP, UDP, ICMP, OSPF etc.\n• Porta TCP ou UDP de origem e de destino\n• Bits de flag do TCP: SYN, ACK etc.\n• Tipo de mensagem ICMP\n• Regras diferentes para datagramas que entram e saem da rede\n• Regras diferentes para diferentes interfaces do roteador\nUm administrador de rede configura o firewall com base na política da organização. A política pode con-\nsiderar a produtividade do usuário e o uso de largura de banda, bem como as preocupações com segurança da \norganização. A Tabela 8.5 lista diversas políticas que uma organização pode ter, e como elas seriam endereçadas \ncom um filtro de pacotes. Por exemplo, se a organização não quer nenhuma conexão TCP de entrada, exceto \naquelas para o servidor Web público, ela pode bloquear todos os segmentos TCP SYN de entrada, com exceção \ndos segmentos TCP SYN com porta de destino 80 e endereço IP de destino correspondente ao servidor Web. Se \nela não quer que seus usuários monopolizem a largura de banda de acesso com aplicações de rádio via Internet, \npode bloquear todo o tráfego UDP não importante (já que o rádio via Internet é em geral enviado por UDP). Se \nnão quer que sua rede interna seja mapeada (por traceroute) por um estranho, pode bloquear todas as mensagens \nICMP TTL expiradas que saem da rede da organização.\nUma política de filtragem também pode ser baseada na combinação de endereços e números de porta. Por \nexemplo, o roteador de filtragem poderia bloquear todos os datagramas Telnet (os que têm número de porta 23), \nexceto os que estão vindo ou indo de ou para uma lista de endereços IP específicos. Essa política permite cone-\nxões Telnet de e para os hospedeiros que estão na lista. Infelizmente, basear a política em endereços externos não \noferece nenhuma proteção contra datagramas cujo endereço de origem foi falsificado.\nFigura 8.33  Posição do firewall entre a rede administrada e o mundo exterior\nRede \nadministrada\nFirewall\nInternet \npública\nKR 08.33.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n28p0 Wide x 18p3 Deep\n11/18/11, 11/23/11 rossi\n   Redes de computadores e a Internet\n540\nA filtragem pode também ser baseada em o bit TCP ACK estar ou não marcado. Esse truque é bastante útil \nquando uma organização quer permitir que seus clientes internos se conectem com servidores externos, mas impe-\ndir que clientes externos se conectem com servidores internos. Lembre-se de que o primeiro segmento de todas as \nconexões TCP (veja a Seção 3.5) tem o bit ACK com valor 0, ao passo que todos os outros segmentos da conexão têm \no bit ACK com valor 1. Assim, se uma organização quiser impedir que clientes externos iniciem uma conexão com \nservidores internos, ela apenas filtrará todos os segmentos que entram que tenham o bit ACK definido como 0. Essa \npolítica elimina todas as conexões TCP originadas do exterior, mas permite conexões que se originam internamente.\nAs regras do firewall são executadas em roteadores com listas de controle de acesso, tendo cada interface \ndo roteador sua própria lista. Um exemplo de uma lista de controle de acesso para uma organização 222.22/16 é \nilustrado na Tabela 8.6. Essa lista é para uma interface que conecta o roteador aos ISPs externos da organização. \nAs regras são aplicadas a cada datagrama que atravessa a interface de cima para baixo. As primeiras duas regras \njuntas permitem que usuários internos naveguem na Web: a primeira permite que qualquer pacote TCP com \nporta de destino 80 saia da rede da organização; a segunda autoriza que qualquer pacote TCP com porta de ori-\ngem 80 e o bit ACK marcado entrem na rede. Observe que se uma fonte externa tentar estabelecer uma conexão \nTCP com um hospedeiro interno, a conexão será bloqueada, mesmo que a porta de origem ou de destino seja 80. \nAs duas regras juntas permitem que pacotes DNS entrem e saiam da rede da organização. Em resumo, essa lista \nde controle de acesso limitada bloqueia todo o tráfego exceto o tráfego Web iniciado de dentro da organização e \ndo tráfego DNS. [CERT Filtering, 2012] apresenta uma lista de portas/protocolos para a filtragem de pacotes para \nevitar diversas brechas de segurança conhecidas nas aplicações de rede existentes.\nTabela 8.6  Lista de controle de acesso para uma interface do roteador\nAção\nEndereço de origem\nEndereço de destino\nProtocolo\nPorta de origem\nPorta de destino\nBit de flag\nPermitir\n222.22/16\nFora de 222.22/16\nTCP\n> 1023\n80\nQualquer um\nPermitir\nFora de 222.22/16\n222.22/16\nTCP\n80\n> 1023\nACK\nPermitir\n222.22/16\nFora de 222.22/16\nUDP\n> 1023\n53\n—\nPermitir\nFora de 222.22/16\n222.22/16\nUDP\n53\n> 1023\n—\nNegar\nTodos\nTodos\nTodos\nTodos\nTodos\nTodos\nFiltros de pacote com controle de estado\nEm um filtro de pacotes tradicional, as decisões de filtragem são feitas em cada pacote isolado. Os filtros de \nestado rastreiam conexões TCP e usam esse conhecimento para tomar decisões sobre filtragem.\nPara entender esses filtros de estado, vamos reexaminar a lista de controle de acesso da Tabela 8.6. Embora \num tanto restritiva, essa lista, no entanto, permite que qualquer pacote que chegue de fora com um ACK = 1 e porta \nTabela 8.5  \u0007\nPolíticas e regras de filtragem correspondentes para uma rede da organização \n130.27/16 com servidor Web 130.207.244.203\nPolítica\nConfiguração de firewall\nNão há acesso exterior à Web\nDescartar todos os pacotes de saída para qualquer endereço IP, \nporta 80\nNão há conexões TCP de entrada, exceto aquelas apenas para o \nservidor Web público da organização\nDescartar todos os pacotes TCP SYN para qualquer IP exceto \n130.207.244.203, porta 80\nImpedir que rádios Web devorem a largura de banda disponível\nDescartar todos os pacotes UDP de entrada — exceto pacotes DNS\nImpedir que sua rede seja usada por um ataque DoS smurf\nDescartar todos os pacotes ping que estão indo para um endereço de \ndifusão (por exemplo, 130.207.255.255)\nImpedir que a rota de sua rede seja rastreada\nDescartar todo o tráfego de saída ICMP com TTL expirado\nSegurança em redes de computadores  541 \nde origem 80 atravesse o filtro. Esses pacotes poderiam ser usados em tentativas de destruir o sistema interno com \npacotes defeituosos, realizar ataques de recusa de serviço, ou mapear a rede interna. A solução ingênua é bloquear \npacotes TCP ACK também, mas tal método impediria que os usuários internos da organização navegassem na Web.\nOs filtros de estado resolvem esse problema rastreando todas as conexões TCP de entrada em uma tabela de \nconexão. Isso é possível porque o firewall pode notar o início de uma nova conexão observando uma apresentação \nde três vias (SYN, SYNACK e ACK); ele pode observar o fim de uma conexão ao ver um pacote FIN para a conexão. \nO firewall também consegue (de forma conservadora) admitir que a conexão está finalizada quando não observou \nnenhuma atividade no decorrer da conexão, digamos, por 60 segundos. Um exemplo de tabela de conexão para um \nfirewall é mostrado na Tabela 8.7. Essa tabela indica que, no momento, há três conexões TCP em andamento, as \nquais foram iniciadas dentro da organização. Ademais, o filtro de estado inclui uma nova coluna, “verificar conexão”\n, \nem sua lista de controle de acesso, como mostrado na Tabela 8.8. Observe que essa tabela é idêntica à lista de contro-\nle de acesso da Tabela 8.6, exceto por ela indicar que a conexão deve ser verificada para duas das regras.\nVamos analisar alguns exemplos e ver como a tabela de conexão e a lista de controle de acesso funcionam em \nconjunto. Suponha que um atacante tente enviar um pacote defeituoso para a rede da organização por meio de um \ndatagrama com porta de origem TCP 80 e com o flag ACK marcado. Suponha ainda que ele possua um número \nde porta de origem 12543 e endereço IP remetente 150.23.23.155. Quando o pacote chega ao firewall, este verifica \na lista de controle de acesso da Tabela 8.8, que indica que a tabela de conexão deve também ser verificada antes \nde permitir que esse pacote entre na rede da organização. O firewall verifica devidamente a tabela de conexão e \nobserva que esse pacote não faz parte de uma conexão TCP em andamento, e o rejeita. Como segundo exemplo, \nimagine que um usuário interno queira navegar em um site externo. Como o usuário primeiro envia um segmento \nTCP SYN, sua conexão TCP é registrada na tabela de conexão. Quando o servidor envia pacotes de volta (com \no bit ACK necessariamente definido), o firewall verifica a tabela e observa que uma conexão correspondente está \nem andamento. O firewall, então, deixará esses pacotes passarem, sem interferir na navegação do usuário interno.\nTabela 8.7  Tabela de conexão para o filtro de estado\nEndereço de origem\nEndereço de destino\nPorta de origem\nPorta de destino\n222.22.1.7\n37.96.87.123\n12699\n80\n222.22.93.2\n199.1.205.23\n37654\n80\n222.22.65.143\n203.77.240.43\n48712\n80\nTabela 8.8  Lista de controle de acesso para filtro de estado\nAção\nEndereço de origem\nEndereço de destino\nProtocolo\nPorta de \norigem\nPorta de \ndestino\nBit de flag\nVerificar \nconexão\nPermitir\n222.22/16\nFora de 222.22/16\nTCP\n>1023\n80\nQualquer um\nPermitir\nFora de 222.22/16\n222.22/16\nTCP\n80\n>1023\nACK\nX\nPermitir\n222.22/16\nFora de 222.22/16\nUDP\n>1023\n53\n—\nPermitir\nFora de 222.22/16\n222.22/16\nUDP\n53\n>1023\n—\nX\nNegar\nTodos\nTodos\nTodos\nTodos\nTodos\nTodos\nGateway de aplicação\nNos exemplos que acabamos de mostrar, vimos que a filtragem de pacotes permite que uma organização \nfaça uma filtragem grosseira de conteúdos de cabeçalhos IP e TCP/UDP, incluindo endereços IP, números de \nporta e bits de reconhecimento. Mas, e se a organização quiser fornecer o serviço Telnet a um conjunto restrito \nde usuários internos (em vez de a endereços IP)? E se quiser que esses usuários privilegiados se autentiquem antes \nde obter permissão para criar sessões Telnet com o mundo externo? Essas tarefas estão além das capacidades de \n   Redes de computadores e a Internet\n542\num filtro. Na verdade, informações sobre a identidade de usuários internos não estão incluídas nos cabeçalhos \nIP/TCP/UDP; elas estão nos dados da camada de aplicação.\nPara assegurar um nível mais refinado de segurança, os firewalls têm de combinar filtro de pacotes com \ngateways de aplicação. Gateways de aplicação fazem mais do que examinar cabeçalhos IP/TCP/UDP e tomam de-\ncisões com base em dados da aplicação. Um gateway de aplicação é um servidor específico de aplicação, através \ndo qual todos os dados da aplicação (que entram e que saem) devem passar. Vários gateways de aplicação podem \nexecutar no mesmo hospedeiro, mas cada gateway é um servidor separado, com seus próprios processos.\nPara termos uma ideia melhor desses gateways, vamos projetar um firewall que permite a apenas um con-\njunto restrito de usuários executar Telnet para o exterior e impede que todos os clientes externos executem Telnet \npara o interior. Essa política pode ser aplicada pela execução da combinação de um filtro de pacotes (em um ro-\nteador) com um gateway de aplicação de Telnet, como mostra a Figura 8.34. O filtro do roteador está configurado \npara bloquear todas as conexões Telnet, exceto as que se originam do endereço IP do gateway de aplicação. Essa \nconfiguração de filtro força todas as conexões Telnet de saída a passarem pelo gateway de aplicação. Considere \nagora um usuário interno que quer executar Telnet com o mundo exterior. Primeiro, ele tem de estabelecer uma \nsessão Telnet com o gateway de aplicação. Uma aplicação que está executando no gateway — e que fica à escuta \nde sessões Telnet que entram — solicita ao usuário sua identificação e senha. Quando o usuário fornece essas \ninformações, o gateway de aplicação verifica se ele tem permissão para executar Telnet com o mundo exterior. \nSe não tiver, a conexão Telnet do usuário interno ao gateway será encerrada pelo gateway. Se o usuário tiver \npermissão, o gateway (1) pedirá ao usuário o nome do computador externo com o qual ele quer se conectar, (2) \nestabelecerá uma sessão Telnet entre o gateway e o hospedeiro externo e (3) repassará ao hospedeiro externo \ntodos os dados que chegam do usuário e ao usuário todos os dados que chegam do hospedeiro externo. Assim, o \ngateway de aplicação Telnet não só autoriza o usuário, mas também atua como um servidor Telnet e um cliente \nTelnet, repassando informações entre o usuário e o servidor Telnet remoto. Note que o filtro permitirá a etapa 2, \nporque é o gateway que inicia a conexão Telnet com o mundo exterior.\nRedes internas frequentemente têm vários gateways de aplicação, como gateways para Telnet, HTTP, FTP e \ne-mail. De fato, o servidor de correio (veja a Seção 2.4) e o cache Web de uma organização são gateways de aplicação.\nGateways de aplicação não estão isentos de desvantagens. Primeiro, é preciso um gateway de aplicação dife-\nrente para cada aplicação. Segundo, há um preço a pagar em termos de desempenho, visto que todos os dados serão \nFigura 8.34  Firewall composto de um gateway de aplicação e um filtro\nKR 08.34.eps\nAW/Kurose and Ross \nComputer Networking 6/e\nGateway de \naplicação, G\nSessão Telnet \nhospedeiro-gateway\nSessão Telnet gateway \nhospedeiro remoto\nRoteador \ne ﬁltro\nSegurança em redes de computadores  543 \nAnonimato e privacidade\nSuponha que você queira visitar aquela polêmica \npágina Web (por exemplo, o site de um ativista po-\nlítico) e você (1) não quer revelar seu endereço IP à \npágina Web, (2) não quer que o seu ISP (que pode ser \no de sua casa ou do escritório) saiba que você está \nvisitando esse site, e (3) não quer que o seu IP local \nveja os dados que você está compartilhando com o \nsite. Se você usar o método tradicional de conexão \ndireta à página Web sem nenhuma criptografia, então \nfalhará em seus três objetivos. Mesmo que use SSL, \nvocê falhará nas duas primeiras questões: seu ende-\nreço IP de origem é apresentado à página Web em \ntodo datagrama enviado; e o endereço de destino de \ncada pacote enviado pode facilmente ser analisado \npelo seu ISP local.\nPara obter anonimato e privacidade, você pode \nusar uma combinação de um servidor proxy confiá-\nvel e SSL, como mostrado na Figura 8.35. Com essa \ntécnica, você faz uma conexão SSL com o proxy con-\nfiável. Depois envia, nessa conexão SSL, uma solicita-\nção HTTP para o site desejado. Quando o proxy rece-\nber essa solicitação HTTP criptografada por SSL, ele \na decodificará e encaminhará o texto claro da solicita-\nção HTTP à página Web. Esta responde ao proxy, que \npor sua vez encaminha a resposta a você pelo SSL. \nComo a página Web só vê o endereço IP do proxy, e \nnão o do seu cliente, você está de fato obtendo um \nacesso anônimo à página Web. E devido ao tráfego \nentre você e o proxy ser criptografado, seu ISP local \nnão pode invadir sua privacidade ao logar no site que \nvocê visitou ou gravar os dados que estavam sendo \ncompartilhados. Hoje, muitas empresas disponibili-\nzam tais serviços proxy (como a proxify.com).\nÉ claro que, ao usar esta solução, seu proxy sa-\nberá tudo: seu endereço IP e o endereço IP do site \nque você está visitando; pode ver todo o tráfego \nem texto claro compartilhado entre você e a página. \nUma técnica melhor, adotada pelo serviço de ano-\nnimato e privacidade TOR, é sequenciar seu tráfe-\ngo através de uma série de servidores proxys que \nnão compartilham informações entre si [TOR 2012]. \nEm particular, o TOR permite que indivíduos inde-\npendentes contribuam com proxys para seu acervo. \nQuando um usuário se conecta a um servidor usan-\ndo o TOR, ele escolhe aleatoriamente (de seu acervo \nde proxys) uma corrente de três proxys e sequencia \ntodo o tráfego entre cliente e servidor por essa cor-\nrente. Dessa maneira, supondo que os proxys não \ntrocam informações entre si, ninguém percebe que \nocorreu uma comunicação entre seu endereço IP e \na página da Web desejada. Além disso, apesar de o \ntexto claro ser enviado entre o último proxy e o ser-\nvidor, o último proxy não sabe qual endereço IP está \nenviando ou recebendo o texto claro.\nFigura 8.35  Fornecendo anonimato e privacidade com um proxy\nAlice\nObtendo anonimato \ndo proxy\nSSL\nTexto aberto\nKR 08.35.eps\nKUROSE/ROSS\nComputer Networking 6/e\n28p0 Wide x 9p9 Deep\n11/18/11 ROSSI ILLUSTRATION\nHistória\n   Redes de computadores e a Internet\n544\nrepassados por meio do gateway. Isso se torna uma preocupação em particular quando vários usuários ou aplicações \nestão utilizando o mesmo gateway. Por fim, o software cliente deve saber como entrar em contato com o gateway \nquando o usuário fizer uma solicitação, e deve saber como dizer ao gateway de aplicação a qual servidor se conectar.\n8.9.2  Sistemas de detecção de invasão\nAcabamos de ver que um filtro de pacotes (tradicional e de estado) inspeciona campos de cabeçalho IP, TCP, \nUDP e ICMP quando está decidindo quais pacotes deixará passar através do firewall. No entanto, para detectar \nmuitos tipos de ataque, precisamos executar uma inspeção profunda de pacote, ou seja, precisamos olhar atra-\nvés dos campos de cabeçalho e dentro dos dados da aplicação que o pacote carrega. Como vimos na Seção 8.9.1, \ngateways de aplicação frequentemente fazem inspeções profundas de pacote. Mas um gateway de aplicação só \nexecuta isso para uma aplicação específica.\nDecerto existe espaço para mais um dispositivo — um dispositivo que não só examina os cabeçalhos de todos \nos pacotes ao passar por eles (como um filtro de pacotes), mas também executa uma inspeção profunda de pacote \n(diferente do filtro de pacotes). Quando tal dispositivo observa o pacote suspeito, ou uma série de pacotes suspei-\ntos, ele impede que tais pacotes entrem na rede organizacional. Ou, quando a atividade só é vista como suspeita, \no dispositivo pode deixar os pacotes passarem, mas envia um alerta ao administrador de rede, que pode examinar \no tráfego minuciosamente e tomar as ações necessárias. Um dispositivo que gera alertas quando observa tráfegos \npotencialmente mal-intencionados é chamado de sistema de detecção de invasão (IDS, do inglês intrusion de-\ntection system). Um dispositivo que filtra o tráfego suspeito é chamado de sistema de prevenção de invasão (IPS, \ndo inglês intrusion prevention system). Nesta seção, estudaremos ambos os sistemas — IDS e IPS —, já que o mais \ninteressante aspecto técnico desses sistemas é como eles detectam tráfego suspeito (em vez de enviarem alertas ou \nabandonarem pacotes). Daqui para a frente vamos nos referir ao sistema IDS e ao sistema IPS como sistema IDS.\nUm IDS pode ser usado para detectar uma série de tipos de ataques, incluindo mapeamento de rede \n(provindo, por exemplo, de nmap), varreduras de porta, varreduras de pilha TCP, ataques de DoS, ataques de \ninundação de largura de banda, worms e vírus, ataques de vulnerabilidade de OS e ataques de vulnerabilidade \nde aplicações. (Veja, na Seção 1.6, um tutorial sobre ataques de rede.) Hoje, milhares de organizações empregam \nsistemas de IDS. Muitos desses sistemas são patenteados, comercializados pela Cisco, Check Point, e outros \nfornecedores de equipamentos de segurança. Mas muitos dos sistemas de IDS implementados são de domínio \npúblico, como o extremamente popular Snort IDS (o qual discutiremos em breve).\nUma organização pode pôr em prática um ou mais sensores IDS em sua rede organizacional. A Figura 8.36 \nmostra uma organização que tem três sensores IDS. Quando múltiplos sistemas são executados, eles costumam \ntrabalhar em harmonia, enviando informações sobre atividades de tráfegos suspeitos ao processador IDS central, \nque as coleta e integra e envia alarmes aos administradores da rede quando acharem apropriado. Na Figura 8.36, \na organização dividiu sua rede em duas regiões: uma de segurança máxima, protegida por um filtro de pacotes e \num gateway de aplicação e monitorada por sensores IDS; e uma região de segurança baixa — referida como zona \ndesmilitarizada (DMZ, do inglês demilitarized zone) — protegida apenas por um filtro de pacotes, mas também \nmonitorada por sensores IDS. Observe que a DMZ inclui os servidores da organização que precisam se comuni-\ncar com o mundo externo, como um servidor Web e seus servidores autoritativos.\nNeste ponto, você deve estar imaginando: mas por que sensores IDS? Por que não colocar um sensor IDS \nlogo atrás do filtro de pacotes (ou até integrá-lo ao filtro de pacotes) da Figura 8.36? Logo veremos que um \nIDS não só precisa fazer uma inspeção profunda do pacote, como também comparar cada pacote que passa \ncom milhares de “assinaturas”; isso pode ser um volume de processamento significativo, em particular se a \norganização recebe gigabits/s de tráfego da Internet. Ao colocar sensores IDS mais à frente, cada sensor só \nvê uma fração do tráfego da organização, e pode facilmente acompanhar o ritmo. No entanto, hoje existem \nsistemas IDS e IPS de alto desempenho, e muitas organizações podem acompanhar com apenas um sensor \nlocalizado próximo ao roteador de acesso.\nSistemas IDS são classificados de modo geral tanto como sistemas baseados em assinatura, ou sistemas \nbaseados em anomalia. Um IDS baseado em assinatura mantém um banco de dados extenso de ataques de assi-\nSegurança em redes de computadores  545 \nnaturas. Cada assinatura é um conjunto de regras relacionadas a uma atividade de invasão. Uma assinatura pode \nser uma lista de características sobre um único pacote (por exemplo, números de portas de origem e destino, \ntipo de protocolo, e uma sequência de bits em uma carga útil de um pacote), ou estar relacionada a uma série de \npacotes. As assinaturas são normalmente criadas por engenheiros habilidosos em segurança de rede que tenham \npesquisado ataques conhecidos. O administrador de rede de uma organização pode personalizar as assinaturas \nou inserir suas próprias no banco de dados.\nOperacionalmente, uma IDS baseada em assinatura analisa cada pacote que passa, comparando cada um \ncom as assinaturas no banco de dados. Se um pacote (ou uma série deles) corresponder a uma assinatura no ban-\nco de dados, o IDS gera um alerta. O alerta pode ser enviado ao administrador da rede por uma mensagem de \ncorreio eletrônico, pode ser enviado ao sistema de gerenciamento da rede, ou pode simplesmente ser registrado \npara futuras inspeções.\nApesar de os sistemas IDS baseados em assinaturas serem amplamente executados, eles têm uma série de \nlimitações. Acima de tudo, eles requerem conhecimento prévio do ataque para gerar uma assinatura precisa. Ou \nseja, um IDS baseado em assinatura é completamente cego a novos ataques que ainda não foram registrados. \nOutra desvantagem é que, mesmo que uma assinatura combine, isso pode não ser o resultado de um ataque, mas \nmesmo assim um alarme é gerado. Por fim, pelo fato de cada pacote ser comparado com uma extensa coleção de \nassinaturas, o IDS fica atarefado com o processamento e deixa de detectar muitos pacotes malignos.\nUm IDS baseado em anomalias cria um perfil de tráfego enquanto observa o tráfego em operação normal. \nEle procura então por fluxos de pacotes que são estatisticamente incomuns, por exemplo, uma porcentagem \nirregular de pacotes ICMP ou um crescimento exponencial de análises de porta e varreduras de ping. O mais \ninteressante sobre sistemas de IDS baseados em anomalias é que eles não recorrem a conhecimentos prévios de \noutros ataques — ou seja, potencialmente, eles conseguem detectar novos ataques, que não foram documentados. \nPor outro lado, é um problema extremamente desafiador distinguir o tráfego normal de tráfegos estatisticamente \nincomuns. Até hoje, a maioria das implementações de IDS são principalmente baseadas em assinaturas, apesar \nde algumas terem alguns recursos baseados em anomalias.\nFigura 8.36  \u0007\nUma organização implementando um filtro, uma aplicação gateway e sensores IDS\nInternet\nServidor \nWeb\nServidor \nFTP\nServidor \nDNS\nRede \ninterna\nAplicação \ngateway\nZona desmilitarizada\nKR 08.36.eps\nKurose and Ross\nComputer Networking 6/e\n33p0  x   23p3\n11/18/11,  11/13/11 rossi\nFiltro\nLegenda:\n= Sensores IDS\n   Redes de computadores e a Internet\n546\nSnort\nSnort é um IDS de código aberto, de domínio público, com centenas de milhares de execuções [Snort, 2012; \nKoziol, 2003]. Ele pode ser executado em plataformas Linux, UNIX e Windows. Usa uma interface libpcap de \nanálise genérica, que também é empregada pelo Wireshark e muitas outras ferramentas de análises de pacotes. \nPode facilmente lidar com 100 Mbits/s de tráfego; para instalações com velocidades de tráfego de gigabit/s, múl-\ntiplos sensores Snort serão necessários.\nPara termos uma ideia melhor do Snort, vamos observar o exemplo de uma assinatura Snort:\nalert icmp $EXTERNAL_NET any -> $HOME_NET any\n(msg:”ICMP PING NMAP”; dsize: 0; itype: 8;)\nEsta assinatura é compatível com qualquer pacote ICMP que entre na rede da organização ($HOME_NET) e \nque venha do exterior ($EXTERNAL_NET), seja do tipo 8 (ping ICMP), e tenha uma carga útil vazia (dsize=0). Já \nque o nmap (veja a Seção 1.6) gera pacotes ping com características específicas, essa assinatura é projetada para \ndetectar varreduras de ping usadas pelo nmap. Quando um pacote corresponde a essa assinatura, o Snort gera um \nalerta que inclui a mensagem “ICMP PING NMAP”\n.\nTalvez o mais impressionante sobre o Snort seja a vasta comunidade de usuários e especialistas em seguran-\nça que mantêm sua base de dados de assinaturas. Normalmente, em algumas horas do novo ataque, a comunida-\nde escreve e lança uma assinatura de ataque, que então é transferida via download pelas centenas de milhares de \nexecuções de Snort ao redor do mundo. Além disso, ao usarem a sintaxe da assinatura do Snort, os administrado-\nres de rede podem criar suas próprias assinaturas a fim de satisfazer as necessidades da organização, modificando \nassinaturas existentes ou criando novas.\n8.10  Resumo\nNeste capítulo, examinamos os diversos mecanismos que Bob e Alice, os amantes secretos, podem usar para \nse comunicar com segurança. Vimos que estão interessados em confidencialidade (para que somente eles possam \nentender o conteúdo de uma mensagem transmitida), autenticação do ponto final (para terem a certeza de que \nestão falando um com o outro) e integridade de mensagem (para terem certeza de que suas mensagens não sejam \nalteradas em trânsito). É claro que a necessidade de comunicação segura não está limitada a amantes secretos. Na \nverdade, vimos nas seções 8.5 a 8.8 que a segurança é necessária em várias camadas de uma arquitetura de rede \npara proteção contra bandidos que têm à mão um grande arsenal de ataques possíveis.\nNa primeira parte deste capítulo, apresentamos vários princípios subjacentes à comunicação segura. Na \nSeção 8.2, examinamos as técnicas para criptografar e decriptografar dados, incluindo criptografia de chaves \nsimétricas e criptografia de chaves públicas. O DES e o RSA foram examinados como estudos de caso específi-\ncos dessas duas classes mais importantes de técnicas de criptografia em uso nas redes de hoje.\nNa Seção 8.3, examinamos duas técnicas para fornecer a integridade da mensagem: códigos de autenticação \nde mensagem (MACs) e assinaturas digitais. As duas têm uma série de paralelos. Ambas usam funções hash crip-\ntografadas e ambas permitem que verifiquemos a origem, assim como a integridade da própria mensagem. Uma \ndiferença importante é que os MACs não recorrem à criptografia, ao passo que as assinaturas digitais necessitam \nde uma infraestrutura de chave pública. Ambas as técnicas são muito usadas na prática, como vimos nas seções \n8.5 a 8.8. Além disso, as assinaturas digitais são usadas para criar certificados digitais, os quais são importantes \npara a verificação da validade de uma chave pública. Na Seção 8.4, vimos também a autenticação do ponto final \ne como nonces podem ser usados para impedir ataques de repetição.\nNas seções 8.5 a 8.8, examinamos diversos protocolos de segurança de rede que são usados extensivamente \nna prática. Vimos que uma criptografia de chave simétrica está no núcleo do PGP, SSL, IPsec e segurança sem fio. \nVimos que uma criptografia pública é crucial para ambos PGP e SSL. Estudamos que o PGP usa assinaturas digi-\ntais para a integridade da mensagem, ao passo que SSL e IPsec usam MACs. Agora que entendemos os princípios \nSegurança em redes de computadores  547 \nbásicos da criptografia, e tendo estudado como esses princípios são usados, você deve estar pronto para projetar \nseus próprios protocolos de segurança de rede!\nMunidos das técnicas abordadas nas seções 8.2 a 8.8, Bob e Alice podem se comunicar com segurança (espe-\nramos que eles sejam estudantes de rede que aprenderam o que este livro ensinou e consigam, dessa maneira, evitar \nque seus encontros secretos sejam descobertos por Trudy!). Porém, a confiabilidade é apenas uma pequena parte \ndo quadro da segurança na rede. Como vimos na Seção 8.9, o foco está se concentrando cada vez mais em garantir \na segurança da infraestrutura da rede contra o ataque potencial dos bandidos. Assim, na última parte deste capí-\ntulo, estudamos firewalls e sistemas IDS que inspecionam pacotes que entram e saem da rede de uma organização.\nEste capítulo cobriu diversos fundamentos, enquanto focava nos tópicos mais importantes sobre a segu-\nrança nas redes modernas. Os leitores que desejam mais informações podem investigar as referências citadas. \nEm particular, recomendamos Skoudis [2006] sobre ataques e segurança operacional, Kaufmann [1995] para \ncriptografia e como ela se aplica à segurança de redes, Rescorla [2001] para uma explicação profunda, mas de fá-\ncil compreensão sobre SSL e Edney [2003] para uma discussão completa sobre segurança 802.11, incluindo uma \ninvestigação sobre WEP e suas falhas.\nExercícios  \nde fixação e perguntas\nQuestões de revisão do Capítulo 8\nSEÇÃO 8.1\n\t\nR1.\t Quais são as diferenças entre confidencialidade de mensagem e integridade de mensagem? É possível ter \nconfidencialidade sem integridade? É possível ter integridade sem confidencialidade? Justifique sua resposta.\n\t\nR2.\t Equipamentos da Internet (roteadores, comutadores, servidores DNS, servidores Web, sistemas do usuário \nfinal etc.) frequentemente precisam se comunicar com segurança. Dê três exemplos específicos de pares de \nequipamentos da Internet que precisem de uma comunicação segura.\nSEÇÃO 8.2\n\t\nR3.\t Da perspectiva de um serviço, qual é uma diferença importante entre um sistema de chave simétrica e um \nsistema de chave pública?\n\t\nR4.\t Suponha que um intruso tenha uma mensagem criptografada, bem como a versão decodificada dessa \nmensagem. Ele pode montar um ataque somente com texto cifrado, um ataque com texto aberto conhecido \nou um ataque com texto aberto escolhido?\n\t\nR5.\t Considere uma cifra de 8 blocos. Quantos blocos de entrada possíveis uma cifra tem? Quantos mapeamentos \npossíveis existiriam? Se analisarmos cada mapeamento como uma chave, então quantas chaves essa cifra teria?\n\t\nR6.\t Suponha que N pessoas queiram se comunicar com cada uma das outras N — 1 pessoas usando criptografia de \nchaves simétricas. Todas as comunicações entre quaisquer duas pessoas, i e j, são visíveis para todas as outras \ndo grupo de N, e nenhuma outra pessoa desse grupo pode decodificar suas comunicações. O sistema, como \num todo, requer quantas chaves? Agora, suponha que seja usada criptografia de chaves públicas. Quantas \nchaves serão necessárias nesse caso?\n\t\nR7.\t Suponha que n = 10.000, a = 10.023 e b = 10.004. Use uma identidade da aritmética modular para calcular \n(a ∙ b) mod n de cabeça.\n\t\nR8.\t Suponha que você queira criptografar a mensagem 10101111 criptografando um número decimal que \ncorresponda a essa mensagem. Qual seria esse número decimal?\n   Redes de computadores e a Internet\n548\nSEÇÕES 8.3–8.4\n\t\nR9.\t De que maneira um hash fornece um melhor controle de integridade da mensagem do que uma soma de \nverificação (como a soma de verificação da Internet)?\n\t\nR10.\t Você pode decodificar o hash de uma mensagem a fim de obter a mensagem original? Explique sua resposta.\n\t\nR11.\t Considere a variação de um algoritmo MAC (Figura 8.9), em que o transmissor envia (m, H(m) + s), sendo \nH(m) + s a concatenação de H(m) e s. Essa variação é falha? Por que ou por que não?\n\t\nR12.\t O que significa afirmar que um documento é verificável e não falsificável?\n\t\nR13.\t De que modo um resumo de mensagem criptografado por chave pública proporciona uma assinatura digital \nmelhor do que utilizar a mensagem criptografada com chave pública?\n\t\nR14.\t Suponha que a certificador.com crie um certificado para a alguem.com. Normalmente, o certificado inteiro \nseria criptografado com a chave pública de certificador.com. Verdadeiro ou falso?\n\t\nR15.\t Suponha que Alice tenha uma mensagem pronta para enviar para qualquer pessoa que pedir. Milhares de pessoas \nquerem ter a mensagem da Alice, mas cada uma quer ter certeza da integridade da mensagem. Nesse contexto, \nvocê acha que é mais apropriado um esquema baseado em MAC ou um baseado em assinatura digital? Por quê?\n\t\nR16.\t Qual é a finalidade de um nonce em um protocolo de identificação de ponto final?\n\t\nR17.\t O que significa dizer que um nonce é um valor usado uma vez por toda a vida? Pelo tempo de vida de quem?\n\t\nR18.\t O esquema de integridade de mensagem baseado no HMAC é suscetível a ataques de repetição? Se for, como \num nonce pode ser incorporado ao esquema para remover essa suscetibilidade?\nSEÇÕES 8.5–8.8\n\t\nR19.\t Suponha que Bob receba uma mensagem PGP de Alice. Como o Bob sabe com certeza que Alice criou a \nmensagem (e não Trudy, por exemplo)? O PGP usa um MAC para integridade da mensagem?\n\t\nR20.\t Em um registro SSL, há um campo para uma sequência de números SSL. Verdadeiro ou falso?\n\t\nR21.\t Qual é a finalidade de um nonce em um protocolo de autenticação em uma apresentação SSL?\n\t\nR22.\t Suponha que uma sessão SSL utilize uma cifra de bloco com CBC. Verdadeiro ou falso: o servidor envia o IV \nao cliente de forma aberta.\n\t\nR23.\t Suponha que Bob inicie uma conexão TCP com Trudy, que está fingindo ser Alice. Durante a apresentação, \nTrudy envia a Bob um certificado de Alice. Em qual etapa do algoritmo de apresentação SSL Bob descobrirá \nque não está se comunicando com Alice?\n\t\nR24.\t Considere uma cadeia de pacotes do Hospedeiro A ao Hospedeiro B usando IPsec. Em geral, um novo SA \nserá estabelecido para cada pacote enviado na cadeia. Verdadeiro ou falso?\n\t\nR25.\t Suponha que o TCP esteja sendo executado por IPsec entre a matriz e a filial na Figura 8.28. Se o TCP \nretransmitir o mesmo pacote, então os dois pacotes correspondentes enviados por pacotes R1 terão o mesmo \nnúmero sequencial no cabeçalho ESP. Verdadeiro ou falso?\n\t\nR26.\t Um SA IKE e um SA IPsec são a mesma coisa. Verdadeiro ou falso?\n\t\nR27.\t Considere um WEP para 802.11. Suponha que a informação seja 10101100 e o fluxo de chaves seja 1111000. \nQual é o texto cifrado resultante?\n\t\nR28.\t Em WEP, um IV é enviado em aberto em cada quadro. Verdadeiro ou falso?\nSEÇÃO 8.9\n\t\nR29.\t Um filtro de pacotes com estado mantém duas estruturas de dados. Nomeie-as e descreva resumidamente o \nque elas fazem.\n\t\nR30.\t Considere um filtro de pacotes tradicional (sem estado). Ele pode filtrar pacotes baseado em bits de flag TCP \nassim como em outros campos de cabeçalho. Verdadeiro ou falso?\nSegurança em redes de computadores  549 \n\t\nR31.\t Em um filtro de pacotes tradicional, cada interface pode ter sua própria lista de controle de acesso. Verdadeiro \nou falso?\n\t\nR32.\t Por que uma aplicação de gateway deve trabalhar junto com o filtro de roteador para ser eficaz?\n\t\nR33.\t IDSs baseados em assinaturas e IPSs inspecionam a carga útil de segmentos TCP e UDP. Verdadeiro ou falso?\nproblemas\n\t\nP1.\t Usando a cifra monoalfabética da Figura 8.3, codifique a mensagem “This is an easy problem” (este é um \nproblema fácil). Decodifique a mensagem “rmij’u uamu xyj”\n.\n\t\nP2.\t Mostre que o ataque com texto aberto conhecido de Trudy em que ela conhece os pares de tradução (texto \ncifrado, texto aberto) para sete letras reduz em aproximadamente 109 o número de possíveis substituições a \nverificar no exemplo apresentado na Seção 8.2.1.\n\t\nP3.\t Considere o sistema polialfabético mostrado na Figura 8.4. Um ataque com texto aberto escolhido que \nconsiga obter a codificação da mensagem “The quick fox jumps over the lazy brown dog” é suficiente para \ndecifrar todas as mensagens? Explique sua resposta.\n\t\nP4.\t Considere o bloco cifrado na Figura 8.5. Suponha que cada bloco cifrado Ti simplesmente inverta a ordem dos \noito bits de entrada (então, por exemplo, 11110000 se torna 00001111). Além disso, imagine que o misturador \nde 64 bits não modifique qualquer bit (de modo que o valor de saída de m-ésimo bit seja igual ao valor de \nentrada do m-ésimo bit). (a) Sendo n = 3 e a entrada original de 64 bits igual a 10100000 repetidos oito vezes, \nqual é o valor da saída? (b) Repita a parte (a), mas agora troque o último bit da entrada original de 64 bits de 0 \npara 1. (c) Repita as partes (a) e (b), mas agora suponha que o misturador de 64 bits inverta a ordem de 64 bits.\n\t\nP5.\t Considere o bloco cifrado da Figura 8.5 Para uma determinada “chave”\n, Alice e Bob precisariam ter 8 \ntabelas, cada uma com 8 bits por 8 bits. Para Alice (ou Bob) armazenar todas as oito tabelas, quantos bits de \narmazenamento são necessários? Como esse número se compara com o número de bits necessários para um \nbloco cifrado de tabela cheia de 64 bits?\n\t\nP6.\t Considere o bloco cifrado de 3 bits da Tabela 8.1. Suponha que o texto aberto seja 100100100. (a) Inicialmente \nsuponha que o CBC não seja usado. Qual é o texto cifrado resultante? (b) Imagine que Trudy analise o texto \ncifrado. Supondo que ela saiba que um bloco cifrado de bits está sendo usado sem o CBC (mas ela não sabe \na cifra específica), o que ela pode suspeitar? (c) Agora suponha que o CBC é usado com IV = 111. Qual é o \ntexto cifrado resultante?\n\t\nP7.\t (a) Usando RSA, escolha p = 3 e q = 11 e codifique a palavra “dog”\n, criptografando cada letra em separado. \nAplique o algoritmo de decriptação à versão criptografada para recuperar a mensagem original em texto \naberto. (b) Repita a parte (a), mas agora criptografe “dog” como uma mensagem m.\n\t\nP8.\t Considere o RSA com p = 5 e q = 11.\na.\t Quais são n e z?\nb.\t Seja e igual a 3. Por que esta é uma escolha aceitável para e?\nc.\t Encontre d tal que de = 1 (mod z) e d < 160.\nd.\t Criptografe a mensagem m = 8 usando a chave (n, e). Seja c o texto cifrado correspondente. Mostre todo \no processo. Dica: Para simplificar os cálculos, use este fato:\n[(a mod n) • (b mod n)] mod n = (a • b) mod n\n\t\nP9.\t Neste problema, exploraremos o algoritmo criptografado Diffie-Hellman (DH) de chave pública, o qual \npermite que duas entidades concordem em uma chave compartilhada. O algoritmo DH faz uso de um \nnúmero primo grande p e outro número grande g, menor que p. Ambos, p e g, são públicos (de modo que um \natacante os conheça). Em DH, Alice e Bob escolhem, independentemente, chaves secretas, SA e SB. Alice então \ncalcula sua chave pública, TA, elevando g a SA e tomando o mod p. Bob, similarmente, calcula sua própria \nchave pública, TB, elevando g a SB e tomando o módulo p. Então, Alice e Bob trocam suas chaves públicas pela \n   Redes de computadores e a Internet\n550\nInternet. Alice calcula a chave secreta compartilhada S ao elevar TB a SA e tomando o módulo p. Bob, de modo \nsemelhante, calcula a chave compartilhada S´ ao elevar TA a SB e tomando o módulo p.\na.\t Prove que, no geral, Alice e Bob obtêm a mesma chave simétrica, ou seja, prove que S = S´.\nb.\t Com p = 11 e g = 2, suponha que Alice e Bob escolham chaves privadas SA = 5 e SB = 12, respectivamente. \nCalcule as chaves públicas de Alice e Bob, TA e TB. Mostre todo o processo.\nc.\t Seguindo a parte (b), calcule S como a chave simétrica compartilhada. Mostre todo o processo.\nd.\t Forneça um diagrama de tempo que mostre como o Diffie-Hellman pode ser atacado por um homem do meio. \nO diagrama de tempo deve ter três linhas verticais, uma para Alice, uma para Bob e uma para a atacante Trudy.\n\t\nP10.\t Suponha que Alice queira se comunicar com o Bob usando uma chave simétrica criptografada usando uma \nchave de sessão KS. Na Seção 8.2, aprendemos que uma chave pública criptografada pode ser usada para \ndistribuir a chave de sessão da Alice para Bob. Neste problema, exploramos como uma chave de sessão pode \nser distribuída — sem uma chave pública criptografada — usando um centro de distribuição de chaves (KDC, \ndo inglês key distribution center). O KDC é um servidor que compartilha uma única chave simétrica secreta \ncom cada usuário registrado. Para Alice e Bob, indique essas chaves como KA-KDC e KB-KDC. Projete um esquema \nque use o KDC para distribuir KS a Alice e Bob. Seu esquema deverá usar três mensagens para distribuir uma \nchave de sessão: a de Alice ao KDC; a do KDC a Alice; e por fim a de Alice para Bob. A primeira mensagem \né KA-KDC(A, B). Usando a notação, KA-KDC, KB-KDC, S, A e B, responda às seguintes perguntas.\na.\t Qual é a segunda mensagem?\nb.\t Qual é a terceira mensagem?\n\t\nP11.\t Calcule uma terceira mensagem, diferente das duas mensagens na Figura 8.8, que tenha a mesma soma de \nverificação dessas duas mensagens na figura citada.\n\t\nP12.\t Suponha que Alice e Bob compartilhem duas chaves secretas: uma chave de autenticação S1 e uma chave \ncriptografada simétrica S2. Aumente a Figura 8.9 para que ambas, integridade e confidencialidade, sejam \nfornecidas.\n\t\nP13.\t No protocolo de distribuição de arquivo P2P BitTorrent (veja o Capítulo 2), a semente quebra o arquivo \nem blocos, e os pares redistribuem os blocos uns aos outros. Sem proteção alguma, um atacante pode \nfacilmente causar destruição em um torrent ao se mascarar como se fosse um par benevolente e enviar \nblocos falsos aos outros conjuntos de pares no torrent. Esse pares que não são suspeitos redistribuem \nos blocos falsos a mais outros pares, que distribuem os blocos falsos aos outros pares. Sendo assim, é \nimportante para o BitTorrent ter um mecanismo que permita aos pares verificar a integridade de um bloco, \npara que não distribuam blocos falsos. Suponha que, quando um par se junta a um torrent, de início pega \num arquivo .torrent\n de uma fonte inteiramente confiável. Descreva um esquema simples que permita que os \npares verifiquem a integridade dos blocos.\n\t\nP14.\t O protocolo de roteamento OSPF usa um MAC em vez de assinaturas digitais para fornecer a integridade da \nmensagem. Por que você acha que foi escolhido o MAC em vez de assinaturas digitais?\n\t\nP15.\t Considere nosso protocolo de autenticação da Figura 8.18, no qual Alice se autentica para Bob, e que vimos \nque funciona bem (isto é, não encontramos nenhuma falha nele). Agora suponha que, enquanto Alice está se \nautenticando para Bob, este deve se autenticar para Alice. Apresente um cenário no qual Trudy, fazendo-se \npassar por Alice, agora pode se autenticar para Bob como se fosse Alice. (Dica: considere que a sequência \nde operações do protocolo, uma com Trudy e outra com Bob iniciando a sequência, pode ser intercalada \narbitrariamente. Preste atenção particular ao fato de que Bob e Alice usarão um nonce e que, caso não se tome \ncuidado, o mesmo nonce pode ser utilizado de forma maliciosa.)\n\t\nP16.\t Uma questão natural é se podemos usar um nonce e a criptografia de chave pública para resolver o problema de \nautenticação do ponto final na Seção 8.4. Considere o seguinte protocolo natural: (1) Alice envia a mensagem \n“Eu sou Alice” a Bob. (2) Bob escolhe um nonce, R, e o envia a Alice. (3) Alice usa sua chave privada \npara criptografar o nonce e envia o valor resultante a Bob. (4) Bob aplica a chave pública de Alice à mensagem \nrecebida. Assim, Bob calcula R e autentica Alice.\na.\t Faça um diagrama desse protocolo, usando a notação para chaves públicas e privadas, empregada neste livro.\nSegurança em redes de computadores  551 \nb.\t Suponha que os certificados não sejam usados. Descreva como Trudy pode se tornar uma “mulher no \nmeio”\n, interceptando as mensagens de Alice e depois se passando por Alice para Bob.\n\t\nP17.\t A Figura 8.19 mostra as operações que Alice deve realizar com PGP para fornecer confidencialidade, \nautenticação e integridade. Faça um diagrama das operações correspondentes que Bob precisa executar no \npacote recebido por Alice.\n\t\nP18.\t Suponha que Alice queira enviar um e-mail a Bob. Bob tem um par de chave pública-privada (KB\n+,KB\n–), e \nAlice tem o certificado de Bob. Mas Alice não tem um par de chave pública, privada. Alice e Bob (e o mundo \ninteiro) compartilham a mesma função de hash H(.).\na.\t Nessa situação, é possível projetar um esquema para que Bob possa verificar que Alice criou a mensagem? \nSe sim, mostre como, com um diagrama de bloco para Alice e Bob.\nb.\t É possível projetar um esquema que forneça confidencialidade para enviar a mensagem de Alice a Bob? \nSe sim, mostre como com um diagrama de bloco para Alice e Bob.\n\t\nP19.\t Considere a saída Wireshark a seguir para uma parte de uma sessão SSL.\na.\t O pacote Wireshark 112 é enviado pelo cliente ou pelo servidor?\nb.\t Qual o número IP do servidor e seu número de porta?\nc.\t Considerando que nenhuma perda ou retransmissão ocorreram, qual será a sequência de números do \npróximo segmento TCP enviado pelo cliente?\nd.\t Quantos registros SSL existem no pacote Wireshark 112?\ne.\t O pacote 112 contém um Segredo Mestre ou um Segredo Mestre Criptografado, ou nenhum?\nf.\t Supondo que o campo do tipo na apresentação é de 1 byte e cada campo de comprimento é de 3 bytes, quais \nsão os valores do primeiro e do último bytes do Segredo Mestre (ou do Segredo Mestre Criptografado)?\ng.\t A mensagem de apresentação criptografada do cliente leva em conta o número de registros SSL?\nh.\t A mensagem de apresentação criptografada do servidor leva em conta o número de registros SSL?\n(Captura de tela do Wireshark reimpressa com permissão da Wireshark Foundation.)\n   Redes de computadores e a Internet\n552\n\t\nP20.\t Na Seção 8.6.1 mostramos que, sem os números de sequência, Trudy (a mulher do meio) pode causar destruição \nem uma sessão SSL ao trocar os segmentos TCP. Trudy pode fazer algo semelhante ao excluir um segmento \nTCP? O que ela precisa fazer para ter sucesso em seu ataque de exclusão? Quais serão os seus efeitos?\n\t\nP21.\t Suponha que Alice e Bob estão se comunicando por uma sessão SSL. Imagine que um atacante, que não tem \nnenhuma das chaves compartilhadas, insira um segmento TCP falso em um fluxo de pacotes com a soma de \nverificação TCP e números sequenciais corretos (e endereços IP e números de porta corretos). A SSL do lado \nreceptor aceitará o pacote falso e passará a carga útil à aplicação receptora? Por que ou por que não?\n\t\nP22.\t As seguintes questões de Verdadeiro/Falso se referem à Figura 8.28.\na.\t Quando um hospedeiro em 172.16.1/24 envia um datagrama ao servidor Amazon.com, o roteador R1 vai \ncriptografar o datagrama usando IPsec.\nb.\t Quando um hospedeiro em 172.16.1/24 envia um datagrama a um servidor 172.16.2/24, o roteador R1 \nmudará os endereços de origem e destino do datagrama IP.\nc.\t Suponha que um hospedeiro 172.16.1/24 inicie uma conexão TCP com um servidor Web em 172.16.2/24. \nComo parte dessa conexão, todos os datagramas enviados pelo R1 terão o número de protocolo 50 no \ncampo esquerdo do cabeçalho IPv4.\nd.\t Considere o envio de um segmento TCP de um hospedeiro em 172.16.1/24 a um hospedeiro em \n172.16.2/24. Suponha que o reconhecimento desse segmento se perde, então um TCP reenvia o segmento.\n\t\nP23.\t Considere o exemplo na Figura 8.28. Suponha que Trudy é a mulher do meio, que insere datagramas no fluxo \nde datagramas indo de R1 a R2. Como parte do ataque de repetição, Trudy envia uma cópia duplicada de um \ndos datagramas enviados de R1 a R2. R2 decriptografará o datagrama duplicado e o encaminhará à rede da \nfilial? Se não, descreva em detalhes como R2 detecta o datagrama duplicado.\n\t\nP24.\t Considere o seguinte pseudoprotocolo WEP. A chave é de 4 bits e o IV, de 2 bits. O IV é anexado ao final \nda chave quando está gerando o fluxo de chaves. Suponha que uma chave secreta compartilhada é 1010. Os \nfluxos de chaves para quatro entradas possíveis são:\n101000: 0010101101010101001011010100100…\n101001: 1010011011001010110100100101101…\n101010: 0001101000111100010100101001111…\n101011: 1111101010000000101010100010111…\n\t\n\t Suponha que todas as mensagens tenham 8 bits. Considere que o ICV (controle de integridade) seja de 4 bits \ne calculado pelo OU-exclusivo nos primeiros 4 bits de dados com os últimos 4 bits de dados. Suponha que \no pseudopacote WEP consista em três campos: primeiro o campo IV, depois o campo de mensagem e por \núltimo o campo ICV, com alguns deles criptografados.\na.\t Queremos enviar a mensagem m = 10100000 usando IV = 11 e WEP. Qual serão os valores nos três \ncampos WEP?\nb.\t Mostre que, quando o receptor decriptografa o pacote WEP, ele recupera a mensagem e o ICV.\nc.\t Suponha que Trudy intercepte um pacote WEP (não necessariamente com IV = 11) e quer modificá-lo \nantes de encaminhá-lo ao receptor. Considere que Trudy inverta o primeiro bit do ICV. Admitindo que \nela não conheça o fluxo de chaves para quaisquer IVs, que outros bits Trudy precisa gerar também, para \nque os pacotes recebidos passem pela verificação do ICV?\nd.\t Justifique sua resposta modificando os bits do pacote WEP da parte (a), decodificando o pacote resultante \ne verificando o controle de integridade.\n\t\nP25.\t Forneça uma tabela de filtro e uma tabela de conexão para um firewall de estado que seja tão restrito quanto \npossível, mas que efetue o seguinte:\na.\t Permite que todos os usuários internos estabeleçam sessões Telnet com hospedeiros externos.\nb.\t Permite que usuários externos naveguem pela página Web da empresa em 222.22.0.12.\nc.\t Mas, em qualquer outro caso, bloqueia todo o tráfego interno e externo.\nSegurança em redes de computadores  553 \n\t\n\t A rede interna é 222.22/16. Em sua solução, suponha que a tabela de conexão esteja normalmente ocultando \ntrês conexões, de dentro para fora. Você precisará inventar um endereço IP e número de portas apropriados.\n\t\nP26.\t Suponha que Alice queira visitar a página Web activist.com usando um serviço do tipo TOR. Esse serviço usa \ndois servidores proxy colaboradores, Proxy1 e Proxy2. Alice primeiro obtém os certificados (cada um contendo \numa chave pública) para Proxy1 e Proxy2 de um servidor central. Indique com K+\n1( ), K+\n2( ), K1\n–( ) e K2\n–( ) para \na criptografia/decriptografia com chaves RSA pública e privada.\na.\t Usando um diagrama de tempo, forneça um protocolo (o mais simples possível) que permita a Alice \nestabelecer uma sessão compartilhada de chave S1 com Proxy1. Indique com S1(m) a criptografia/\ndecriptografia do dado m com a chave compartilhada S1.\nb.\t Usando um diagrama de tempo, forneça um protocolo (o mais simples possível) que permita a Alice \nestabelecer uma sessão compartilhada da chave S2 com Proxy2, sem revelar seu endereço IP ao Proxy2.\nc.\t Suponha que as chaves compartilhadas S1 e S2 estejam determinadas. Usando um diagrama de tempo, forneça \num protocolo (o mais simples possível e não usando uma criptografia de chave pública) que permita a Alice \nrequisitar uma página HTML de activist.com sem revelar seu endereço IP ao Proxy2 e sem revelar ao Proxy1 \nqual site ela está visitando. Seu diagrama deve terminar com uma requisição HTTP chegando a activist.com.\nWireshark Lab\nNeste laboratório (disponível no site de apoio), investigamos o protocolo SSL (do inglês Secure Socket Layer). \nLembre-se de que na Seção 8.6 o SSL é usado para a segurança de uma conexão TCP, e é extensivamente usada na \nprática para a segurança de transações pela Internet. Neste laboratório, vamos focalizar os registros SSL enviados \npor uma conexão TCP. Tentaremos delinear e classificar cada registro, focando no entendimento do porquê e \ncomo de cada registro. Investigamos os vários tipos de registro SSL, assim como os campos nas mensagens SSL, \nanalisando um relatório dos registros SSL enviados entre seu hospedeiro e um servidor de comércio eletrônico.\nIPsec Lab\nNeste laboratório (disponível na página Web), exploraremos como criar SAs IPsec entre caixas Linux. \nVocê pode fazer a primeira parte com duas caixas Linux simples, cada uma com um adaptador Ethernet. Mas, \nna segunda parte do laboratório, você precisará de quatro caixas Linux, duas tendo dois adaptadores Ethernet. \nNa segunda metade do laboratório, você criará SAs IPsec usando protocolos ESP no modo túnel. Primeiro \nvocê criará as SAs manualmente e depois fazendo o IKE criar as SAs.\nSteven M. Bellovin\nSteven M. Bellovin ingressou no corpo docente da Universidade de Columbia de-\npois de muitos anos no Network Services Research Lab do AT&T Labs Research, em \nFlorham Park, Nova Jersey. Ele trabalha especificamente com redes e segurança, e \ncom as causas que as tornam incompatíveis. Em 1995, Steven recebeu o Usenix Lifeti-\nme Achievement Award por seu trabalho na criação da Usenet, a primeira rede de troca \nde grupos de bate-papo que ligava dois ou mais computadores e permitia que os usuá­\nrios compartilhassem informações e se juntassem em discussões. Steve também foi \neleito membro da National Academy of Engineering. Obteve bacharelado na Columbia \nUniversity e doutorado na University of North Carolina, em Chapel Hill.\nENTREVISTA\n   Redes de computadores e a Internet\n554\nO que o fez se decidir pela especialização na área \nde segurança de redes?\nO que vou dizer parecerá estranho, mas a resposta é \nsimples. Estudei programação de sistemas e administra-\nção de sistemas, o que levou naturalmente à segurança. E \nsempre me interessei por comunicações, desde a época \nem que trabalhava em parte do tempo com programa-\nção de sistemas, quando ainda estava na universidade.\nMeu trabalho na área de segurança continua a ser \nmotivado por duas coisas — um desejo de manter os \ncomputadores úteis, o que significa impedir que sua \nfunção seja corrompida por atacantes, e um desejo de \nproteger a privacidade.\nQual era sua visão sobre a Usenet na época em \nque o senhor a estava desenvolvendo? Qual é sua \nvisão agora?\nNo início considerávamos a Usenet como um meio \npara discutir ciência da computação e programação de \ncomputadores em todo o país e para utilizar em ques-\ntões administrativas locais, como recurso de vendas, e \nassim por diante. Na verdade, minha previsão original \nera de uma ou duas mensagens por dia, de 50 a 100 sites \nno máximo. Mas o crescimento real ocorreu em tópicos \nrelacionados a pessoas, incluindo — mas não se limi-\ntando a — interações de seres humanos com compu-\ntadores. Meus grupos de bate-papo preferidos foram, \ndurante muitos anos, coisas como rec.woodworking \n(marcenaria), bem como sci.crypt (criptografia).\nAté certo ponto, as redes de notícias foram substi-\ntuídas pela Web. Se eu fosse iniciar o projeto hoje, ele \nseria muito diferente. Mas ainda é um excelente meio \npara alcançar um público muito amplo interessado no \nassunto, sem ter de passar por sites Web particulares.\nQuais pessoas o inspiraram profissionalmente? De \nque modo?\nO professor Fred Brooks — fundador e primeiro \nchefe do departamento de ciência da computação da \nUniversity of North Carolina, em Chapel Hill, gerente \nda equipe que desenvolveu o IBM S/360 e o OS/360, \ne autor do livro The Mythical Man-Month — exerceu \numa tremenda influência sobre minha carreira. Acima \nde tudo, ele me ensinou observação e compromissos \n— como considerar problemas no contexto do mundo \nreal (e como o mundo real é muito mais confuso do \nque qualquer teórico gostaria que fosse), e como equi-\nlibrar interesses conflitantes ao elaborar uma solução. \nGrande parte do trabalho com computadores é enge-\nnharia — a arte de fazer os compromissos certos de \nmodo a satisfazer muitos objetivos contraditórios.\nEm sua opinião, qual é o futuro das redes e da \nsegurança?\nAté agora, grande parte da segurança que temos vem \ndo isolamento. Um firewall, por exemplo, funciona \nimpedindo o acesso a certas máquinas e serviços. Mas \nvivemos em uma época na qual a conectividade é cada \nvez maior — ficou mais difícil isolar coisas. Pior ainda, \nnossos sistemas de produção requerem um número \nmuito maior de componentes isolados, interconecta-\ndos por redes. Garantir a segurança de tudo isso é um \nde nossos maiores desafios.\nEm sua opinião, tem havido grandes avanços na \nárea de segurança? Até onde teremos de ir?\nAo menos do ponto de vista científico, sabemos como \nfazer criptografia. E isso é uma grande ajuda. Mas a \nmaioria dos problemas de segurança se deve a códi-\ngos defeituosos e este é um problema muito mais sério. \nNa verdade, é o problema mais antigo da ciência da \ncomputação que ainda não foi resolvido — e acho que \ncontinuará sendo. O desafio é descobrir como manter \na segurança de sistemas quando temos de construí-los \ncom componentes inseguros. Hoje já podemos fazer \nisso no que tange às falhas de hardware; mas podemos \nfazer o mesmo em relação à segurança?\nO senhor pode dar algum conselho aos estudan-\ntes sobre a Internet e segurança em redes?\nAprender os mecanismos é a parte fácil. Aprender a \n“pensar como um paranoico” é mais difícil. Você tem \nde lembrar que as distribuições de probabilidade não \nse aplicam — os atacantes podem encontrar e encon-\ntrarão condições improváveis. E os detalhes são im-\nportantes — e muito!\nApós termos percorrido nosso caminho pelos oito primeiros capítulos deste livro, estamos agora conscien-\ntes de que uma rede consiste em muitas peças complexas de hardware e software que interagem umas com as \noutras — desde os enlaces, comutadores, roteadores, hospedeiros e outros dispositivos, que são os componentes \nfísicos, até os muitos protocolos (tanto em hardware quanto em software) que controlam e coordenam esses com-\nponentes. Quando centenas ou milhares de componentes são montados em conjunto por alguma organização \npara formar uma rede, não é nada surpreendente que por vezes eles apresentem defeitos, que elementos da rede \nsejam mal configurados, que recursos da rede sejam utilizados em excesso ou que componentes simplesmente \n“quebrem” (por exemplo, um cabo pode ser cortado, uma latinha de refrigerante pode ser derramada sobre um \nroteador). O administrador de rede, cuja tarefa é mantê-la “viva e atuante”\n, deve estar habilitado a reagir a esses \ncontratempos (e, melhor ainda, a evitá-los). Com potencialmente milhares de componentes espalhados por uma \ngrande área, ele, em sua central de operações (network operations center — NOC), evidentemente necessita de \nferramentas que o auxiliem a monitorar, administrar e controlar a rede. Neste capítulo, examinaremos a arquite-\ntura, os protocolos e as bases de informação que um administrador de rede utiliza para realizar essa tarefa.\n9.1  O que é gerenciamento de rede?\nAntes de discutir o gerenciamento de redes em si, vamos considerar alguns cenários ilustrativos do “mundo \nreal” que não são de redes, mas nos quais um sistema complexo com muitos componentes em interação deve ser \nmonitorado, gerenciado e controlado por um administrador. As usinas de geração de energia elétrica têm uma \nsala de controle, onde mostradores, medidores e lâmpadas monitoram o estado (temperatura, pressão, vazão) de \nválvulas, tubulações, tanques e outros componentes remotos da instalação industrial. Esses dispositivos permi-\ntem que o operador monitore os muitos componentes da planta e podem alertá-lo (o famoso pisca-alerta) caso \nhaja algum problema iminente. O operador responsável executa certas ações para controlar esses componentes. \nDe maneira semelhante, a cabine de um avião é equipada com instrumentos para que o piloto possa monitorar e \ncontrolar os muitos componentes de uma aeronave. Nesses dois exemplos, o “administrador” monitora equipa-\nmentos remotos e analisa os dados para garantir que os equipamentos estejam funcionando e operando dentro \ndos limites especificados (por exemplo, que a fusão do núcleo de uma usina nuclear não esteja na iminência de \nacontecer ou que o combustível do avião não esteja prestes a acabar), controla reativamente o sistema fazendo \najustes de acordo com as modificações ocorridas no sistema ou em seu ambiente e gerencia proativamente o siste-\nma (por exemplo, detectando tendências ou comportamentos anômalos que permitem executar uma ação antes \nGerenciamento\nde rede\n1\n3\n6\n8\n7\nc\n \na\n \np\n \ní\n \nt\n \nu\n \nl\n \no\n2\n4\n9\n5\n   Redes de computadores e a Internet\n556\nque surjam problemas sérios). De modo semelhante, o administrador de rede vai monitorar, gerenciar e controlar \nativamente o sistema do qual está encarregado.\nNos primórdios das redes de computadores, quando elas ainda eram artefatos de pesquisa, e não uma infra-\nestrutura usada por milhões de pessoas por dia, “gerenciamento de rede” era algo de que nunca se tinha ouvido \nfalar. Se alguém descobrisse um problema, poderia realizar alguns testes, como o ping, para localizar a fonte do \nproblema e, em seguida, modificar os ajustes do sistema, reiniciar o software ou o hardware ou chamar um colega \npara fazer isso. (O RFC 789 traz uma discussão de fácil leitura sobre a primeira grande “queda” da ARPAnet em \n27 de outubro de 1980, muito antes da existência de ferramentas de gerenciamento, e sobre os esforços realizados \npara entender os motivos dessa queda e recuperar a rede.) Como a Internet pública e as intranets privadas cres-\nceram e se transformaram de pequenas redes em grandes infraestruturas globais, a necessidade de gerenciar mais \nsistematicamente a enorme quantidade de componentes de hardware e software nessas redes também se tornou \nmais importante.\nCom o intuito de motivar nosso estudo de gerenciamento de redes, vamos começar com um exemplo sim-\nples. A Figura 9.1 ilustra uma pequena rede constituída de três roteadores e alguns hospedeiros e servidores. Mes-\nmo para uma rede tão simples, há muitos cenários em que o administrador muito se beneficiará por ter à mão as \nferramentas de gerenciamento adequadas:\n• Detecção de falha em uma placa de interface em um hospedeiro ou roteador. Com ferramentas de geren-\nciamento apropriadas, uma entidade de rede (por exemplo, o roteador A) pode indicar ao administrador \nque uma de suas interfaces não está funcionando. (Isso decerto é melhor do que receber um telefonema, \nno NOC (centro de operações), de um usuário zangado dizendo que a conexão com a rede caiu!) Um \nadministrador de rede que monitora e analisa de maneira ativa o tráfego pode realmente impressionar \no usuário (aquele, o zangado) detectando problemas na interface bem antes e substituindo a placa de \ninterface antes que ela caia. Isso poderá ser feito, por exemplo, se o administrador notar um aumento \nde erros de somas de verificação em quadros que estão sendo enviados por uma placa de interface que \nestá prestes a falhar.\n• Monitoração de hospedeiro. O administrador de rede pode verificar periodicamente se todos os hospe-\ndeiros da rede estão ativos e operacionais. Mais uma vez, ele pode, de fato, impressionar um usuário da \nrede, reagindo proativamente a um problema (falha em um hospedeiro) antes de o defeito ser relatado \npelo usuário.\n• Monitoração de tráfego para auxiliar o oferecimento de recursos. Um administrador de rede pode moni-\ntorar padrões de tráfego entre origens e destinos e notar, por exemplo, que, trocando servidores entre \nsegmentos de LAN, o total de tráfego que passa por várias LANs poderia ser reduzido de maneira signi-\nficativa. Imagine a felicidade geral com um desempenho melhor sem o custo de novos equipamentos. De \nmodo semelhante, monitorando a utilização de um enlace, um administrador de rede pode determinar \nque um segmento de LAN ou o enlace com o mundo externo está sobrecarregado e que um enlace de \nmaior largura de banda deve ser providenciado (ainda que com um custo mais alto). Ele também poderia \nser alertado automaticamente quando o nível de congestionamento de um enlace ultrapassasse determi-\nnado limite, para providenciar um enlace de maior largura de banda antes que o congestionamento se \ntornasse sério.\n• Detecção de mudanças rápidas em tabelas de roteamento. A alternância de rotas — mudanças frequentes \nem tabelas de roteamento — pode indicar instabilidades no roteamento ou um roteador mal configu-\nrado. Evidentemente, um administrador de rede que configurou um roteador de modo inapropriado \nprefere ele mesmo descobrir o erro antes que a rede caia.\n• Monitoração de SLAs. Acordos de Nível de Serviços (Service Level Agreements — SLAs) são contratos \nque definem parâmetros específicos de medida e níveis aceitáveis de desempenho do provedor de rede \nem relação a essas medidas [Huston, 1999a]. Verizon e Sprint são apenas dois dos muitos provedores \nde rede que garantem SLAs a seus clientes [AT&T SLA, 2012; Verizon SLA, 2012]. Alguns desses SLAs \nsão: disponibilidade de serviço (interrupção de serviços), latência, vazão e requisitos para notificação da \nGerenciamento de rede  557 \nocorrência de serviço interrompido. É claro que, se um contrato especificar critérios de desempenho de \nprestação de serviços entre um provedor de rede e seus usuários, então a medição e o gerenciamento do \ndesempenho do sistema também serão de grande importância para o administrador de rede.\n• Detecção de invasão. Um administrador de rede provavelmente vai querer ser avisado quando chegar \ntráfego de uma fonte suspeita ou quando se destinar tráfego a ela (por exemplo, hospedeiro ou número \nde porta). Da mesma maneira, ele talvez queira detectar (e, em muitos casos, filtrar) a existência de cer-\ntos tipos de tráfego (por exemplo, pacotes roteados pela origem ou um grande número de pacotes SYN \ndirigidos a determinado hospedeiro) que são característicos de certos tipos de ataque à segurança que \nconsideramos no Capítulo 8.\nA International Organization for Standardization (ISO) criou um modelo de gerenciamento de rede que é útil \npara situar os cenários apresentados em um quadro mais estruturado. São definidas cinco áreas de gerenciamento \nde rede:\n• Gerenciamento de desempenho. Sua meta é quantificar, medir, informar, analisar e controlar o desempe-\nnho (por exemplo, utilização e vazão) de diferentes componentes da rede. Entre esses componentes estão \ndispositivos individuais (por exemplo, enlaces, roteadores e hospedeiros), bem como abstrações fim a \nfim, como um trajeto pela rede. Veremos, em breve, que padrões de protocolo como o SNMP (Simple \nNetwork Management Protocol — Protocolo Simples de Gerenciamento de Rede) [RFC 3410] desempe-\nnham um papel fundamental no gerenciamento de desempenho da Internet.\n• Gerenciamento de falhas. Seu objetivo é registrar, detectar e reagir às condições de falha da rede. A linha \ndivisória entre gerenciamento de falha e gerenciamento de desempenho é bastante indefinida. Podemos \nentender o primeiro como o tratamento imediato de falhas transitórias da rede (por exemplo, interrup-\nção de serviço em enlaces, hospedeiros, ou em hardware e software de roteadores), enquanto o segundo \nadota uma abordagem de longo prazo em relação ao desempenho da rede em face de demandas variáveis \nde tráfego e falhas ocasionais na rede. Como acontece no gerenciamento de desempenho, o SNMP tem \num papel fundamental no gerenciamento de falhas.\nFigura 9.1  Um cenário simples que ilustra a utilização do gerenciamento de rede\nHospedeiro\nH1\nA\nB\nC\nHospedeiro\nEnlace com \na rede \nexterna\nServidor\nKR 09.01.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n24p8 Wide x 28p6 Deep\n9/13/11, 10/28/11 rossi\n   Redes de computadores e a Internet\n558\n• Gerenciamento de configuração. O gerenciamento de configuração permite que um administrador de \nrede saiba quais dispositivos fazem parte da rede administrada e quais são suas configurações de hardware \ne software. O [RFC 3139] oferece uma visão geral de gerenciamento e requisitos de configuração para \nredes IP.\n• Gerenciamento de contabilização. Permite que o administrador da rede especifique, registre e controle o \nacesso de usuários e dispositivos aos recursos da rede. Quotas de utilização, cobrança por utilização e \nalocação de acesso privilegiado a recursos fazem parte do gerenciamento de contabilização.\n• Gerenciamento de segurança. Sua meta é controlar o acesso aos recursos da rede de acordo com alguma \npolítica bem definida. As centrais de distribuição de chaves e as autoridades certificadoras que estuda-\nmos na Seção 8.3 são componentes do gerenciamento de segurança. O uso de firewalls para monitorar e \ncontrolar pontos externos de acesso à rede, um tópico que estudamos na Seção 8.9, é outro componente \ncrucial.\nNeste capítulo, abordaremos apenas os rudimentos do gerenciamento de rede. Nosso foco é propositalmen-\nte limitado — examinaremos apenas a infraestrutura do gerenciamento de rede: a arquitetura geral, os protocolos \nde gerenciamento de rede e a base de informações que servem para um administrador de rede mantê-la em pé e \nem funcionamento. Não abordaremos os processos de tomada de decisão do administrador de rede, que é quem \ndeve planejar, analisar e reagir às informações gerenciais dirigidas à NOC. Nessa área estão alguns tópicos, como \nidentificação e gerenciamento de falhas [Katzela, 1995; Medhi, 1997; Labovitz, 1997; Steinder, 2002; Feamster, \n2005; Wu, 2005; Teixeira, 2006], detecção de anomalias [Lakhina, 2004; Lakhina, 2005; Barford, 2009] e outros \nmais. Tampouco abordaremos o tópico mais amplo do gerenciamento de serviços [Saydam, 1996; RFC 3052] — o \nfornecimento de recursos, como largura de banda, capacidade do servidor e outros recursos computacionais e de \ncomunicação necessários para cumprir os requisitos de serviço específicos da missão de uma empresa.\nUma pergunta frequente é: “O que é gerenciamento de rede?”\n. Nossa discussão anterior expôs e ilustrou a \nnecessidade de alguns usos desse gerenciamento. Concluiremos esta seção com uma definição em uma única sen-\ntença (embora um tanto longa), dada por Saydam [1996]:\nGerenciamento de rede inclui a implementação, a integração e a coordenação de elementos de hardware, \nsoftware e humanos, para monitorar, testar, consultar, configurar, analisar, avaliar e controlar os recursos da \nrede, e de elementos, para satisfazer às exigências operacionais, de desempenho e de qualidade de serviço a um \ncusto razoável.\nÉ uma sentença de perder o fôlego, mas é uma definição viável. Nas seções seguintes, acrescentaremos um pouco \nde recheio a essa definição bastante enxuta de gerenciamento de rede.\n9.2  A infraestrutura do gerenciamento de rede\nVimos, na seção anterior, que o gerenciamento de rede exige a capacidade de “monitorar, testar, consultar, \nconfigurar [...] e controlar” os componentes de hardware e software. Como os dispositivos da rede são distribuí-\ndos, fazer isso exigirá, no mínimo, que o administrador consiga coletar dados (por exemplo, para a finalidade de \nmonitoração) de uma entidade remota e efetuar mudanças nessa entidade (por exemplo, controle). Nesta seção, \numa analogia humana será útil para entender a infraestrutura necessária para gerenciamento de rede.\nImagine que você seja o dirigente de uma grande organização que têm filiais em todo o mundo. É tarefa sua \nassegurar que as diversas partes de sua organização operem sem percalços. Como você o fará? No mínimo, você \ncoletará dados periodicamente das filiais por meio de relatórios e de várias medições quantitativas de atividade, \nprodutividade e orçamento. De vez em quando (mas não sempre), você será explicitamente notificado da existên-\ncia de um problema em uma filial; o gerente que quer galgar a escada corporativa (e talvez ficar com seu cargo) \npode enviar relatórios não solicitados mostrando que as coisas estão correndo muito bem na filial a seu cargo. \nVocê examinará os relatórios que receber esperando encontrar operações regulares em todos os lugares, mas, \nGerenciamento de rede  559 \nsem dúvida, achará problemas que demandarão sua atenção. Você poderá iniciar um diálogo pessoal com uma \nde suas filiais problemáticas, coletar mais dados para entender o problema e, então, passar uma ordem executiva \n(“Faça essa mudança!”) ao gerente.\nImplícita nesse cenário humano muito comum está uma infraestrutura para controlar a organização — o \nchefe (você), os locais remotos que estão sendo controlados (as filiais), seus agentes remotos (os gerentes das \nfiliais), protocolos de comunicação (para transmitir relatórios e dados padronizados e para diálogos pessoais) \ne dados (o conteúdo dos relatórios e as medições quantitativas de atividade, produtividade e orçamento). Cada \num desses componentes do gerenciamento organizacional humano tem um correspondente no gerenciamento \nde rede.\nA arquitetura de um sistema de gerenciamento de rede é conceitualmente idêntica a essa analogia simples \ncom uma organização humana. O campo do gerenciamento tem sua terminologia própria para os vários compo-\nnentes de uma arquitetura de gerenciamento de rede; portanto, adotamos aqui essa terminologia. Como mostra \na Figura 9.2, há três componentes principais nessa arquitetura: uma entidade gerenciadora (o chefe, na analogia \napresentada), os dispositivos gerenciados (as filiais) e um protocolo de gerenciamento de rede.\nA entidade gerenciadora é uma aplicação que em geral tem um ser humano no circuito e que é executada \nem uma estação central de gerenciamento de rede na NOC. Ela é o centro da atividade; ela controla a coleta, o \nprocessamento, a análise e/ou a apresentação de informações de gerenciamento de rede. É nela que são iniciadas \nações para controlar o comportamento da rede e é aqui que o administrador humano interage com os dispositi-\nvos da rede.\nUm dispositivo gerenciado é um equipamento de rede (incluindo seu software) que reside em uma \nrede gerenciada. Ele corresponde à filial de nossa analogia humana. Um dispositivo gerenciado pode ser um \nhospedeiro, um roteador, uma ponte, um hub, uma impressora ou um modem. Em seu interior pode haver \ndiversos objetos gerenciados. Estes são, na verdade, as peças de hardware propriamente ditas que estão den-\ntro do dispositivo gerenciado (por exemplo, uma placa de interface de rede) e os conjuntos de parâmetros de \nconfiguração para as peças de hardware e software (por exemplo, um protocolo de roteamento intradomínio, \ncomo o RIP). Em nossa analogia humana, os objetos gerenciados podem ser os departamentos existentes na \nfilial. Esses objetos gerenciados têm informações associadas a eles que são coletadas dentro de uma Base de \nInformações de Gerenciamento (Management Information Base — MIB); veremos que os valores dessas in-\nFigura 9.2  Principais componentes de uma arquitetura de gerenciamento de rede\nAgente\nDados\nAgente\nDados\nAgente\nDados\nAgente\nDados\nDispositivo \ngerenciado\nDispositivo \ngerenciado\nDispositivo \ngerenciado\nDispositivo \ngerenciado\nAgente\nDados\nKR 09.02.eps\nAW/Kurose and Ross\nComputer Networking 6/e\nEntidade \ngerenciadora Dados\n   Redes de computadores e a Internet\n560\nformações estão disponíveis para a entidade gerenciadora (e, em muitos casos, podem ser definidos por ela). \nEm nossa analogia humana, a MIB corresponde aos dados quantitativos (medições de atividade, produtivi-\ndade e orçamento, podendo este último ser estabelecido pela entidade gerenciadora!) que são trocados entre \no escritório central e a filial. Estudaremos as MIBs em detalhes na Seção 9.3. Por fim, reside também em cada \ndispositivo gerenciado um agente de gerenciamento de rede, um processo executado no dispositivo gerencia-\ndo, que se comunica com a entidade gerenciadora e que executa ações locais nos dispositivos gerenciados sob \no comando e o controle da entidade gerenciadora. O agente de gerenciamento de rede é o gerente da filial em \nnossa analogia humana.\nO terceiro componente da arquitetura é o protocolo de gerenciamento de rede. Esse protocolo é executado \nentre a entidade gerenciadora e o agente de gerenciamento de rede dos dispositivos gerenciados, o que permite \nque a entidade gerenciadora investigue o estado dos dispositivos gerenciados e, indiretamente, execute ações so-\nbre eles mediante seus agentes. Estes podem usar o protocolo de gerenciamento de rede para informar à entidade \ngerenciadora a ocorrência de eventos excepcionais (por exemplo, falhas de componentes ou violação de patama-\nres de desempenho). É importante notar que o protocolo de gerenciamento de rede em si não gerencia a rede. Em \nvez disso, ele fornece uma ferramenta com a qual o administrador pode gerenciar (“monitorar, testar, consultar, \nconfigurar, analisar, avaliar e controlar”) a rede. Essa é uma distinção sutil, mas importante.\nEmbora a infraestrutura do gerenciamento de rede seja conceitualmente simples, podemos nos atrapalhar \ncom seu vocabulário especial, como os termos “entidade gerenciadora”\n, “dispositivo gerenciado”\n, “agente de ge-\nrenciamento” e “base de informações de gerenciamento”\n. Por exemplo, nessa terminologia, em nosso cenário sim-\nples de monitoração de hospedeiros, “agentes de gerenciamento” localizados em “dispositivos gerenciados” são \nperiodicamente examinados pela “entidade gerenciadora” — uma ideia simples, mas um problema linguístico! \nMas, com um pouco de sorte, lembrar sempre da analogia com uma organização humana e seus óbvios paralelos \ncom o gerenciamento de rede nos ajudará neste capítulo.\nNossa discussão anterior sobre arquitetura de gerenciamento de rede foi genérica e se aplica, em geral, a \nvários padrões e esforços que vêm sendo propostos há anos. Os padrões de gerenciamento de rede começaram \na amadurecer no final da década de 1980, sendo que o OSI CMISE/CMIP (Common Management Service \nElement/Common Management Information Protocol — Elemento de Serviço de Gerenciamento Comum/\nProtocolo de Informação de Gerenciamento Comum) [Piscatello, 1993; Stallings, 1993; Glitho, 1998] e o \nSNMP (Simple Network Management Protocol — Protocolo Simples de Gerenciamento de Rede) da Internet \n[RFC 3410; Stallings, 1999; Rose, 1996] emergiram como os dois padrões mais importantes [Subramanian, \n2000]. Ambos foram projetados para ser independentes de produtos ou de redes de fabricantes específicos. \nComo o SNMP foi projetado e oferecido rapidamente em uma época em que a necessidade de gerenciamento \nde rede começava a ficar premente, ele encontrou uma ampla aceitação. Hoje, esse protocolo é a estrutura de \ngerenciamento de rede mais usada e disseminada. Abordaremos o SNMP em detalhes na seção seguinte.\nA central de operações de rede da Comcast\nA rede IP baseada em fibra de classe mundial \nComcast oferece produtos e serviços reunidos a 49 \nmilhões de clientes combinados de vídeo, dados e \nvoz. A rede da Comcast inclui mais de 618.000 mi-\nlhas de rota instalada, 138.000 milhas de rota de fi-\nbra, 30.000 milhas de backbone, 122.000 nós óticos e \narmazenamento maciço para a Content Delivery Net-\nwork da Comcast, que entrega um produto de Video \non Demand de mais de 134 Terabytes. Cada parte da \nrede da Comcast, até as residências e empresas dos \nPrincípios na prática\nGerenciamento de rede  561 \nclientes, é monitorada por um dos Centros de Opera-\nções da empresa.\nA Comcast opera dois Centros Nacionais de Ope-\nrações de Rede que controlam o backbone nacional, \nredes locais regionais, aplicações nacionais e plata-\nformas específicas que dão suporte à infraestrutura \nde voz, dados e vídeo para clientes residenciais, co-\nmerciais e atacadistas. Além disso, tem três Centros \nde Operações Divisionais que controlam a infraestru-\ntura local que dá suporte a todos os seus clientes. \nOs Centros de Operações Nacionais e Divisionais \nsão responsáveis por monitorar, de forma proativa, \ntodos os aspectos de sua rede e o desempenho de \nprodutos todos os dias do ano, 24 horas por dia, uti-\nlizando processos e sistemas comuns. Por exemplo, \ndiversos eventos da rede nos níveis nacional e local \ntêm objetivos predefinidos comuns para níveis de se-\nveridade, processos de recuperação e tempo médio \nesperado para restauração. Os centros nacionais e \ndivisionais podem dar apoio uns aos outros se um \nproblema local afetar a operação de um site. Além \ndisso, os Centros de Operações Nacionais e Divi-\nsionais têm uma extensa Rede Virtual Privada, que \npermite aos engenheiros acessarem a rede com se-\ngurança para realizar, remotamente, atividades proa-\ntivas ou reativas de gerenciamento de rede.\nA técnica da Comcast para o gerenciamento de \nrede envolve cinco áreas principais: Gerenciamen-\nto de Desempenho, Gerenciamento de Falhas, Ge-\nrenciamento de Configuração, Gerenciamento de \nContabilização e Gerenciamento de Segurança. \nGerenciamento de Desempenho focaliza o co-\nnhecimento de como a rede/sistemas e aplicações \n(conhecidos como ecossistema) estão trabalhando \ncom relação a medições predefinidas, específicas \nda hora do dia, dia da semana ou eventos especiais \n(por exemplo, tempestades ou eventos importantes, \ncomo uma partida de futebol). Essas medições pre-\ndefinidas existem por todo o caminho do serviço, \ndesde a residência ou empresa do cliente e passando \npela rede inteira, bem como os pontos de interface \ncom parceiros e pares. Além disso, transações sin-\ntéticas são executadas para garantir a continuidade \ndo sistema. Gerenciamento de Falhas é definido \ncomo a capacidade de detectar, registrar e entender \nanomalias que podem afetar os clientes. A Comcast \nutiliza mecanismos de correlação para determinar \ncorretamente a severidade de um evento e atuar de \nmodo apropriado, eliminando ou remediando poten-\nciais problemas antes que afetem os clientes. Ge-\nrenciamento de Configuração garante que versões \napropriadas de hardware e software sejam distribuí-\ndas por todos os elementos do ecossistema. Manter \nesses elementos em seus níveis “dourados” máximos \najuda a evitar consequências indesejadas. Gerencia-\nmento de Contabilização garante que os centros de \noperações tenham um conhecimento claro da pro-\nvisão e utilização do ecossistema. Isso é importante \nespecialmente para garantir que, em todo o tempo, os \ncentros de operações tenham a capacidade de redi-\nrecionar o tráfego com eficiência. Gerenciamento de \nSegurança cuida para que haja controles apropria-\ndos para garantir que o ecossistema esteja protegido \nde modo eficiente contra acesso indevido.\nCentros de Operações de Rede e o ecossistema que \neles apoiam não são estáticos. O pessoal de engenharia \ne operações está sempre reavaliando as medidas de de-\nsempenho e ferramentas predefinidas para garantir que \nas expectativas dos clientes por excelência operacional \nsejam atendidas.\nEssas telas mostram ferramentas que dão suporte a correlação, gerenciamento  \nde patamares e destinos, usadas pelos técnicos da Comcast (Cortesia da Comcast).\n   Redes de computadores e a Internet\n562\n9.3  \u0007\nA estrutura de gerenciamento padrão da Internet\nAo contrário do que o nome SNMP (Protocolo Simples de Gerenciamento de Rede) possa sugerir, o ge-\nrenciamento de rede na Internet é muito mais do que apenas um protocolo para transportar dados de geren-\nciamento entre uma entidade gerenciadora e seus agentes, e o SNMP passou a ser muito mais complexo do que \nsugere a palavra “simples”\n. As raízes da atual estrutura de gerenciamento padrão da Internet (Internet Standard \nManagement Framework) remontam ao SGMP (Simple Gateway Monitoring Protocol — protocolo de monito-\nramento do gateway simples) [RFC 1028]. O SGMP foi projetado por um grupo de pesquisadores, usuários e \nadministradores universitários de rede, cuja experiência com esse protocolo permitiu que eles projetassem, \nimplementassem e oferecessem o SNMP em poucos meses [Lynch, 1993] — um feito muito distante dos pro-\ncessos de padronização atuais, que são bastante prolongados. Desde então, o SNMP evoluiu do SNMPv1 para o \nSNMPv2 e chegou à sua versão mais recente, o SNMPv3 [RFC 3410], lançada em abril de 1999 e atualizada em \ndezembro de 2002.\nNa descrição de qualquer estrutura para gerenciamento de rede, certas questões devem inevitavelmente ser \nabordadas:\n• O que está sendo monitorado (de um ponto de vista semântico)? E que tipo de controle pode ser exercido \npelo administrador de rede?\n• Qual é o modelo específico das informações que serão relatadas e/ou trocadas?\n• Qual é o protocolo de comunicação para trocar essas informações?\nLembre-se de nossa analogia humana apresentada na seção anterior. O chefe e os gerentes das filiais precisa-\nrão acertar entre eles as medições de atividade, produtividade e orçamento que serão usadas para relatar a situa­\nção das filiais. De maneira semelhante, eles terão de concordar sobre que tipos de ações o chefe poderá realizar \n(por exemplo, cortar o orçamento, ordenar que o gerente da filial modifique alguma característica da operação do \nescritório ou demitir o pessoal e fechar a filial). Em um maior nível de detalhamento, eles precisarão concordar \nsobre o modo como esses dados serão relatados. Por exemplo, em que moeda (dólares, reais?) será apresentado \no relatório de orçamento? Em que unidades será medida a produtividade? Embora talvez pareçam triviais, esses \ndetalhes terão de ser acertados. Por fim, a maneira pela qual a informação trafegará entre o escritório central e as \nfiliais (isto é, o protocolo de comunicação) deve ser especificada.\nA estrutura de gerenciamento-padrão da Internet aborda essas questões. Ela é constituída de quatro partes:\n• Definições dos objetos de gerenciamento de rede, conhecidos como objetos MIB. Na Estrutura de Geren-\nciamento de Padrão da Internet, as informações de gerenciamento são representadas como uma coletâ-\nnea de objetos gerenciados que, juntos, formam um banco de informações virtuais, conhecido como MIB \n(Management Information Base). Um objeto MIB pode ser um contador, tal como o número de datagra-\nmas IP descartados em um roteador por causa de erros em cabeçalhos de datagramas IP ou o número \nde erros de detecção de portadora em uma placa de interface Ethernet; um conjunto de informações \ndescritivas, como a versão do software que está sendo executado em um servidor DNS; informações de \nestado, como se um determinado dispositivo está funcionando corretamente; ou informações específicas \nsobre protocolos, como um caminho de roteamento até um destino. Assim, os objetos MIB definem as \ninformações de gerenciamento mantidas por um dispositivo gerenciado. Objetos MIB relacionados são \nreunidos em módulos MIB. Em nossa analogia com uma organização humana, a MIB define a informa-\nção transportada entre a filial e a sede.\n• Uma linguagem de definição de dados, conhecida como SMI (Structure of Management Information — \nEstrutura de Informação de Gerenciamento), que define os tipos de dados, um modelo de objeto e regras \npara escrever e revisar informações de gerenciamento. Objetos MIB são especificados nessa linguagem \nde definição de dados. Em nossa analogia humana, a SMI é usada para definir os detalhes do formato das \ninformações que serão trocadas.\nGerenciamento de rede  563 \n• Um protocolo, SNMP. O SNMP é usado para transmitir informações e comandos entre uma entidade \ngerenciadora e um agente que os executa em nome da entidade dentro de um dispositivo de rede geren-\nciado.\n• Capacidades de segurança e de administração. A adição dessas capacidades representa o aprimoramento \nmais importante do SNMPv3 em comparação com o SNMPv2.\nAssim, a arquitetura de gerenciamento da Internet é modular por projeto, com uma linguagem de de-\nfinição de dados e de MIB independente de protocolo e um protocolo independente de MIB. O interessante \né que essa arquitetura modular foi criada primeiro para facilitar a transição de um gerenciamento de rede \nbaseado em SNMP para uma estrutura de gerenciamento de rede que estava sendo desenvolvida pela ISO, \nque era a arquitetura de gerenciamento que concorria com o SNMP quando este foi projetado — uma tran-\nsição que nunca aconteceu. Com o tempo, contudo, a modularidade do projeto do SNMP permitiu que ele \nevoluísse mediante três importantes revisões, com cada uma das quatro partes do SNMP já discutidas antes \nse desenvolvendo de modo independente. Ficou bem claro que a decisão pela modularidade foi correta, \nainda que tenha sido tomada pela razão errada!\nNas subseções seguintes, examinaremos com mais detalhes os quatro componentes mais importantes da \nestrutura de gerenciamento-padrão da Internet.\n9.3.1  SMI (Estrutura de Informações de Gerenciamento)\nA Estrutura de Informações de Gerenciamento (Structure of Management Information — SMI) (um \ncomponente da estrutura de gerenciamento de rede cujo nome é bastante estranho e não dá nenhuma pista \nquanto à sua funcionalidade) é a linguagem usada para definir as informações de gerenciamento que resi-\ndem em uma entidade gerenciada de rede. Essa linguagem de definição é necessária para assegurar que a \nsintaxe e a semântica dos dados de gerenciamento de rede sejam bem definidas e não apresentem ambigui-\ndade. Note que a SMI não define uma instância específica para os dados em uma entidade gerenciada de \nrede, mas a linguagem na qual a informação está especificada. Os documentos que descrevem a SMI para \no SNMPv3 (confusamente denominada SMIv2) são [RFC 2578; RFC 2579; RFC 2580]. Vamos examinar a \nSMI de baixo para cima, começando com seus tipos de dados básicos. Em seguida, veremos como os objetos \ngerenciados são descritos em SMI e, então, como os objetos gerenciados relacionados entre si são agrupados \nem módulos.\nTipos de dados básicos da SMI\nO RFC 2578 especifica os tipos de dados básicos da linguagem SMI de definição de módulos MIB. Embora \na SMI seja baseada na linguagem de definição de objetos ASN.1 (Abstract Syntax Notation One — notação de \nsintaxe abstrata 1) [ISO X.680, 2002] (ver Seção 9.4), tantos foram os tipos de dados específicos da SMI acres-\ncentados que esta deve ser considerada uma linguagem de definição de dados por direito próprio. Os 11 tipos de \ndados básicos definidos no RFC 2578 aparecem na Tabela 9.1. Além desses objetos escalares, também é possível \nimpor uma estrutura tabular sobre um conjunto ordenado de objetos MIB usando a construção SEQUENCE \nOF; consulte o RFC 2578 para mais detalhes. Grande parte dos tipos de dados da Tabela 9.1 será familiar (ou \nautoexplicativa) para a maioria dos leitores. O único tipo de dado que discutiremos com mais detalhes será o \nOBJECT IDENTIFIER, usado para dar nome a um objeto.\nConstruções SMI de nível mais alto\nAlém dos tipos de dados básicos, a linguagem de definição de dados SMI também fornece construções de \nlinguagem de nível mais alto.\n   Redes de computadores e a Internet\n564\nA construção OBJECT-TYPE é utilizada para especificar o tipo de dado, o status e a semântica de um \nobjeto gerenciado. Esses objetos gerenciados contêm os dados de gerenciamento que estão no núcleo do geren-\nciamento de rede. Há mais de dez mil objetos definidos em diversos RFCs da Internet [RFC 3410]. A construção \nOBJECT-TYPE tem quatro cláusulas. A cláusula SYNTAX de uma definição OBJECT-TYPE especifica o tipo \nde dado básico associado ao objeto. A cláusula MAX-ACCESS especifica se o objeto gerenciado pode ser lido, \nescrito, criado ou ter seu valor incluído em uma notificação. A cláusula STATUS indica se a definição do objeto \né atual e válida, obsoleta (caso em que não deve ser executada, pois sua definição está incluída por motivos \nhistóricos apenas) ou desaprovada (obsoleta, mas implementável por causa de sua interoperabilidade com im-\nplementações mais antigas). A cláusula DESCRIPTION contém uma definição textual e legível do objeto; ela \n“documenta” a finalidade do objeto gerenciado e deve fornecer todas as informações semânticas necessárias \npara executá-lo.\nComo exemplo de uma construção OBJECT-TYPE, considere a definição do tipo de objeto ipSystem- \nStatsInDelivers do RFC 4293. Esse objeto define um contador de 32 bits que monitora o número de datagra-\nmas IP recebidos no dispositivo gerenciado e entregues com sucesso a um protocolo de camada superior. A última \nlinha dessa definição diz respeito ao nome desse objeto, um tópico que consideraremos na subseção seguinte.\nipSystemStatsInDelivers OBJECT-TYPE\n\t\nSYNTAX Counter32\n\t\nMAX-ACCESS read-only\n\t\nSTATUS current\n\t\nDESCRIPTION\n\t\n\t\n\u0007\n“The total number of datagrams successfully  \ndelivered to IPuser-protocols (including ICMP).\n\t\n\t\n\u0007\nWhen tracking interface statistics, the counter \nof the interface to which these datagrams were \naddressed is incremented. This interface might \nnot be the same as the input interface for \nsome of the datagrams.\n\t\n\t\n\u0007\nDiscontinuities in the value of this counter can \noccur at re-initialization of the management \nsystem, and at other times as indicated by the \nvalue of ipSystemStatsDiscontinuityTime.”\n\t\n::= { ipSystemStatsEntry 18 }\nTabela 9.1 \nTipos de dados básicos da SMI\nTipo de dado\nDescrição\nINTEGER\nNúmero inteiro de 32 bits, como definido em ASN.1, com valor entre – 231 e 231 – 1, inclusive, ou um valor de \numa lista de valores constantes possíveis, nomeados.\nInteger32\nNúmero inteiro de 32 bits, com valor entre – 231 e 231 – 1, inclusive.\nUnsigned32\nNúmero inteiro de 32 bits sem sinal na faixa de 0 a 232 – 1, inclusive.\nOCTET STRING\nCadeia de bytes de formato ASN.1 que representa dados binários arbitrários ou de texto de até 65.535 bytes de \ncomprimento.\nOBJECT IDENTIFIER\nFormato ASN.1 atribuído administrativamente (nome estruturado); veja a Seção 9.3.2.\nIPaddress\nEndereço Internet de 32 bits, na ordem de bytes da rede.\nCounter32\nContador de 32 bits que cresce de 0 a 232 – 1 e volta a 0.\nCounter64\nContador de 64 bits.\nGauge32\nNúmero inteiro de 32 bits que não faz contagens além de 232 – 1 nem diminui para menos do que 0.\nTimeTicks\nTempo, medido em centésimos de segundo, transcorrido a partir de algum evento.\nOpaque\nCadeia ASN.1 não interpretada, necessária por compatibilidade com versões anteriores.\nGerenciamento de rede  565 \nA construção MODULE-IDENTITY permite que objetos relacionados entre si sejam agrupados, como \nconjunto, dentro de um “módulo”\n. Por exemplo, o RFC 4293 especifica o módulo MIB que define objetos geren-\nciados (incluindo ipSystemStatsInDelivers) para gerenciar implementações do IP e de seu protocolo \nassociado, o ICMP. O [RFC 4022] especifica o módulo MIB para TCP, e o [RFC 4133] especifica o módulo MIB \npara UDP. O [RFC 4502] define o módulo MIB para monitoração remota, RMON. Além de conter as definições \nOBJECT-TYPE dos objetos gerenciados dentro do módulo, a construção MODULE-IDENTITY contém cláu-\nsulas para documentar informações de contato do autor do módulo, a data da última atualização, um histórico \nde revisões e uma descrição textual do módulo. Como exemplo, considere o módulo de definição para gerencia-\nmento do protocolo IP:\nipMIB MODULE-IDENTITY\n\t\nLAST-UPDATED “200602020000Z”\n\t\nORGANIZATION “IETF IPv6 MIB Revision Team”\n\t\nCONTACT-INFO\n\t\n\t\n“Editor:\n\t\n\t\nShawn A. Routhier\n\t\n\t\nInterworking Labs\n\t\n\t\n108 Whispering Pines Dr. Suite 235\n\t\n\t\nScotts Valley, CA 95066\n\t\n\t\nUSA\n\t\n\t\nEMail: <sar@iwl.com>”\n\t\nDESCRIPTION\n\t\n\t\n\u0007\n“The MIB module for managing IP and ICMP \nimplementations, but excluding their \nmanagement of IP routes.\n\t\n\t\n\u0007\nCopyright (C) The Internet Society (2006). \nThis version of this MIB module is part of \nRFC 4293; see the RFC itself for full legal \nnotices.”\n\t\nREVISION “200602020000Z”\n\t\nDESCRIPTION\n\t\n\t\n\u0007\n“The IP version neutral revision with added \nIPv6 objects for ND, default routers, and \nrouter advertisements. As well as being the \nsuccessor to RFC 2011, this MIB is also the \nsuccessor to RFCs 2465 and 2466. Published \nas RFC 4293.”\n\t\nREVISION “199411010000Z”\n\t\nDESCRIPTION\n\t\n\t\n\u0007\n“A separate MIB module (IP-MIB) for IP and \nICMP management objects. Published as RFC \n2011.”\n\t\nREVISION “199103310000Z”\n\t\nDESCRIPTION\n\t\n\t\n\u0007\n“The initial revision of this MIB module was \npart of MIB-II, which was published as RFC \n1213.”\n\t\n::= { mib-2 48}\nA construção NOTIFICATION-TYPE é usada para especificar informações referentes a mensagens \nSNMPv2 Trap e InformationRequest geradas por um agente ou por uma entidade gerenciadora; veja a \n   Redes de computadores e a Internet\n566\nSeção 9.3.3. Entre essas informações estão um texto de descrição (DESCRIPTION) que especifica quando \ntais mensagens devem ser enviadas, bem como uma lista de valores que devem ser incluídos na mensa-\ngem gerada; veja o [RFC 2578] para obter mais detalhes. A construção MODULE-COMPLIANCE define \no conjunto de objetos gerenciados dentro de um módulo que um agente deve implementar. A construção \nAGENT-CAPABILITIES especifica as capacidades dos agentes relativas às definições de notificação de \nobjetos e de eventos.\n9.3.2  Base de informações de gerenciamento: MIB\nComo já dissemos antes, a Base de Informações de Gerenciamento (Management Information Base — \nMIB) pode ser imaginada como um banco virtual de informações que guarda objetos gerenciados cujos valores, \ncoletivamente, refletem o “estado” atual da rede. Esses valores podem ser consultados e/ou definidos por uma \nentidade gerenciadora por meio do envio de mensagens SNMP ao agente que está rodando em um dispositivo \ngerenciado em nome da entidade gerenciadora. Objetos gerenciados são especificados utilizando a construção \nOBJECT-TYPE da SMI discutida anteriormente e agrupados em módulos MIB utilizando a construção MODU-\nLE-IDENTITY.\nA IETF tem estado muito atarefada com a padronização de módulos MIB associados a roteadores, \nhospedeiros e outros equipamentos de rede, o que inclui dados básicos de identificação sobre determinado \ncomponente de hardware e informações de gerenciamento sobre as interfaces e os protocolos de dispositivos \nda rede. Até 2006 havia mais de duzentos módulos MIB baseados em padrões e um número ainda maior de \nmódulos MIB especificados por fabricantes privados. Com todos esses padrões, a IETF precisava encontrar \num modo de identificar e dar nome aos módulos padronizados, bem como aos objetos gerenciados específi-\ncos dentro de um módulo. Em vez de começar do nada, a IETF adotou uma estrutura padronizada de iden-\ntificação de objetos (nomeação) que já tinha sido publicada pela ISO. Como acontece com muitas entidades \ndedicadas à padronização, a ISO tinha “grandes planos” para sua estrutura padronizada de identificação de \nobjetos — identificar todo e qualquer objeto padronizado possível (por exemplo, formato de dados, protocolo \nou informação) em qualquer rede, independentemente das organizações dedicadas à padronização das redes \n(por exemplo, IETF, ISO, IEEE ou ANSI), do fabricante do equipamento ou do proprietário da rede. Um ob-\njetivo bem grandioso mesmo! A estrutura de identificação de objeto adotada pela ISO é parte da linguagem \nde definição de objetos ASN.1 [ISO X.680, 2002], que discutiremos na Seção 9.4. Os módulos MIB padro-\nnizados já têm seu próprio cantinho confortável dentro dessa estrutura de nomeação bastante abrangente, \ncomo veremos a seguir.\nComo mostra a Figura 9.3, pela estrutura de nomeação da ISO, os objetos são nomeados de modo \nhierárquico. Note que cada ponto de ramo da árvore tem um nome e um número (entre parênteses); assim, \nqualquer ponto da árvore pode ser identificado pela sequência de nomes ou números que especificam o \ntrajeto da raiz até aquele ponto na árvore identificadora. Um programa baseado na Web — divertido, mas \nincompleto e não oficial — que percorre parte da árvore de identificadores de objetos (usando informações \nsobre os ramos, oferecidas por voluntários) pode ser encontrado em OID Repository [2012].\nNo topo da hierarquia estão a ISO e o ITU-T, as duas principais entidades de padronização que tratam \nda ASN.1, bem como um ramo para o esforço conjunto realizado por essas duas organizações. No ramo ISO \nda árvore, encontramos registros para todos os padrões ISO (1.0) e para os emitidos por entidades padroni-\nzadoras de vários países membros da ISO (1.2). Embora não apareça na Figura 9.3, logo abaixo desse ramo \nda árvore (ISO/países membros, também conhecido como 1.2), encontraríamos os Estados Unidos (1.2.840), \nembaixo dos quais encontraríamos um número para os padrões IEEE e ANSI e para os padrões específicos \nde empresas privadas. Entre essas empresas, estão a RSA (1.2.840.11359) e a Microsoft (1.2.840.113556), \nsob a qual encontraríamos os Microsoft File Formats (1.2.840.113556.4) para vários produtos da Microsoft, \n \nGerenciamento de rede  567 \ncomo o Word (1.2.840.113556.4.2). Mas estamos interessados em redes (e não nos arquivos do Microsoft\n \nWord), portanto, vamos voltar nossa atenção ao ramo denominado 1.3, os padrões emitidos por entidades \nreconhecidas pela ISO. Entre elas estão o Departamento de Defesa dos Estados Unidos (6) (sob o qual encon-\ntraremos os padrões da Internet), a Open Software Foundation (22), a associação de empresas aéreas SITA \n(69) e as entidades identificadas pela OTAN (57), além de muitas outras organizações.\nSob o ramo Internet da árvore (1.3.6.1), há sete categorias. Sob o ramo private (1.3.6.1.4), encontra-\nmos uma lista [IANA, 2009b] dos nomes e códigos de empresas privadas para as mais de quatro mil empresas \nprivadas que se registraram na Internet Assigned Numbers Authority (IANA) [IANA, 2009a]. Sob o ramo ma-\nnagement (1.3.6.1.2) e MIB-2 (1.3.6.1.2.1), encontramos a definição dos módulos MIB padronizados. Puxa! É \numa longa jornada até chegarmos ao nosso cantinho no espaço de nomes da ISO!\nMódulos MIB padronizados\nO ramo mais baixo da árvore da Figura 9.3 mostra alguns dos módulos MIB importantes, orientados \npara hardware (system e interface), bem como os associados a alguns dos protocolos mais importan-\ntes da Internet. O RFC 5000 relaciona todos os módulos MIB padronizados. Embora os RFCs referentes às \nMIBs sejam uma leitura tediosa e difícil, é muito instrutivo (isto é, assim como comer verduras, “é bom para \nvocê”) considerar algumas definições de módulos MIB para ter uma ideia do tipo de informação que há em \num módulo.\nOs objetos gerenciados que ficam sob o título system contêm informações gerais sobre o dispositivo que está \nsendo gerenciado; todos os dispositivos gerenciados devem suportar os objetos MIB do grupo system. A Tabela \n9.2 define os objetos gerenciados no grupo system, de acordo com o [RFC 1213]. A Tabela 9.3 define os objetos \ngerenciados no módulo MIB para o protocolo UDP em uma entidade gerenciada.\nFigura 9.3  Árvore de identificadores de objetos ASN.1\nITU-T (0)\nJoint ISO/ITU-T (2)\nISO (1)\nStandard (0)\nISO member\nbody (2)\nISO identiﬁed\norganization (3)\nNATO\nidentiﬁed (57)\nOpen Software\nFoundation (22)\nUS\nDoD (6)\nInternet (1)\ndirectory\n(1)\nexperimental\n(3)\nsecurity\n(5)\nmail\n(7)\nprivate\n(4)\nsnmpv2\n(6)\nmanagement\n(2)\nMIB-2 (1)\nsystem\n(1)\naddress \ntranslation\n(3)\nicmp\n(5)\nudp\n(7)\ncmot\n(9)\ninterface\n(2)\nip\n(4)\ntcp\n(6)\negp\n(8)\ntransmission\n(10)\nrmon\n(16)\nsnmp\n(11)\nKR 09.03.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n33p9 Wide x 22p6 Deep\n11/16/11 rossi\n   Redes de computadores e a Internet\n568\nTabela 9.2  Objetos gerenciados no grupo system da MIB-2\nIdentificador de objeto\nNome\nTipo\nDescrição (segundo RFC 1213)\n1.3.6.1.2.1.1.1\nsysDescr\nOCTET STRING\n“Nome completo e identificação da versão do tipo de \nhardware do sistema, do sistema operacional do software \ne do software de rede.”\n1.3.6.1.2.1.1.2\nsysObjectID\nOBJECT \nIDENTIFIER\nID atribuído pelo fabricante do objeto fornece um meio fácil \ne não ambíguo para determinar que tipo de objeto está \nsendo gerenciado.\n1.3.6.1.2.1.1.3\nsysUpTime\nTimeTicks\n“O tempo (em centésimos de segundo) desde que \na parte de gerenciamento de rede do sistema foi \nreinicializada pela última vez.”\n1.3.6.1.2.1.1.4\nsysContact\nOCTET STRING\n“A pessoa de contato para esse nó gerenciado, \njuntamente com a informação sobre como contatá-la.”\n1.3.6.1.2.1.1.5\nsysName\nOCTET STRING\n“Um nome atribuído administrativamente para esse nó \ngerenciado. Por convenção, esse é o nome de domínio \ntotalmente qualificado do nó.”\n1.3.6.1.2.1.1.6\nsysLocation\nOCTET STRING\n“A localização física do nó.”\n1.3.6.1.2.1.1.7\nsysServices\nInteger32\nUm valor codificado que indica o conjunto de serviços \ndisponível no nó: aplicações físicas (por exemplo, um \nrepetidor), de enlace/sub-rede (por exemplo, ponte), \nde Internet (por exemplo, gateway IP), fim a fim (por \nexemplo, hospedeiro).\nTabela 9.3  Objetos gerenciados no módulo MIB-2 UDP\nIdentificador de objeto\nNome\nTipo\nDescrição (segundo RFC 4113)\n1.3.6.1.2.1.7.1\nudpInDatagrams\nCounter32\n“Número total de datagramas UDP entregues a \nusuários UDP.”\n1.3.6.1.2.1.7.2\nudpNoPorts\nCounter32\n“Número total de datagramas UDP recebidos para \nos quais não havia nenhuma aplicação na porta de \ndestino.”\n1.3.6.1.2.1.7.3\nudpInErrors\nCounter32\n“Número de datagramas UDP recebidos que não \npuderam ser entregues por outras razões que não a \nfalta de uma aplicação na porta de destino.”\n1.3.6.1.2.1.7.4\nudpOutDatagrams\nCounter32\n“Número total de datagramas UDP enviados dessa \nentidade.”\n9.3.3  \u0007\nOperações do protocolo SNMP e mapeamentos \nde transporte\nO Protocolo simples de gerenciamento de rede versão 2 (SNMPv2) [RFC 3416] é usado para transportar \ninformações da MIB entre entidades gerenciadoras e agentes, executando em nome das entidades gerenciado-\nras. A utilização mais comum do SNMP é em um modo comando-resposta, no qual a entidade gerenciadora \nSNMPv2 envia uma requisição a um agente SNMPv2, que a recebe, realiza alguma ação e envia uma resposta \nà requisição. Em geral, uma requisição é usada para consultar (recuperar) ou modificar (definir) valores de \nGerenciamento de rede  569 \nobjetos MIB associados a um dispositivo gerenciado. Um segundo uso comum do SNMP é para um agente \nenviar uma mensagem não solicitada, conhecida como mensagem trap, à entidade gerenciadora. As mensa-\ngens trap são usadas para notificar uma entidade gerenciadora de uma situação excepcional que resultou em \nmudança nos valores dos objetos MIB. Vimos antes, na Seção 9.1, que o administrador de rede pode querer \nreceber uma mensagem trap, por exemplo, quando uma interface cai, quando o congestionamento atinge um \nnível predefinido em um enlace ou quando ocorre qualquer outro evento importante. Observe que há uma \nsérie de compromissos importantes entre consulta de objetos (interação comando-resposta) e trapping; veja \nos exercícios ao final deste capítulo.\nO SNMPv2 define sete tipos de mensagens, conhecidas genericamente como PDUs (protocol data units – \nProtocolo de unidade de dados), conforme apresentado na Tabela 9.4 e descrito em seguida. O formato da PDU \npode ser visto na Figura 9.4.\n• As PDUs GetRequest, GetNextRequest e GetBulkRequest são enviadas de uma entidade ge-\nrenciadora a um agente para requisitar o valor de um ou mais objetos MIB no dispositivo gerenciado do \nagente. Os identificadores de objeto dos objetos MIB cujos valores estão sendo requisitados são especifi-\ncados na parte de vinculação de variáveis da PDU. GetRequest, GetNextRequest e GetBulkRe-\nquest diferem no grau de especificidade de seus pedidos de dados. GetRequest pode requisitar um \nconjunto arbitrário de valores MIB; múltiplas GetNextRequest podem ser usadas para percorrer a \nsequência de uma lista ou tabela de objetos MIB, e GetBulkRequest permite que um grande bloco de \ndados seja devolvido, evitando a sobrecarga incorrida quando tiverem de ser enviadas múltiplas mensa-\ngens GetRequest ou GetNextRequest. Em todos os três casos, o agente responde com uma PDU \nResponse que contém os identificadores de objetos e seus valores associados.\n• A PDU SetRequest é usada por uma entidade gerenciadora para estabelecer o valor de um ou mais \nobjetos MIB em um dispositivo gerenciado. Um agente responde com uma PDU Response que contém \numa mensagem de estado de erro “noError” para confirmar que o valor realmente foi estabelecido.\n• A PDU InformRequest é usada por uma entidade gerenciadora para comunicar a outra entidade \ngerenciadora informações MIB remotas à entidade receptora. A entidade receptora responde com uma \nPDU Response com a mensagem de estado de erro “noError” para reconhecer o recebimento da PDU \nInformRequest.\n• O tipo final de PDU SNMPv2 é a mensagem trap. Mensagens trap são geradas assincronamente, isto \né, não são geradas em resposta a uma requisição recebida, mas em resposta a um evento para o qual a \nentidade gerenciadora requer notificação. O RFC 3418 define tipos conhecidos de trap que incluem uma \npartida a frio ou a quente realizada por um dispositivo, a ativação ou interrupção de um enlace, a perda \nde um vizinho ou um evento de falha de autenticação. Uma requisição de trap recebida não exige respos-\nta de uma entidade gerenciadora.\nSabendo da natureza comando-resposta do SNMPv2, convém observar que, embora as SNMP-PDUs \npossam ser transportadas por muitos protocolos de transporte diferentes, elas normalmente são transporta-\ndas na carga útil de um datagrama UDP. Na verdade, o RFC 3417 estabelece que o UDP é o “mapeamento de \ntransporte preferencial”. Já que o UDP é um protocolo de transporte não confiável, não há garantia de que um \ncomando ou sua resposta será recebido no destino pretendido. O campo request ID da PDU é usado pela en-\ntidade gerenciadora para numerar as requisições que faz a um agente; a resposta de um agente adota a request \nID daquela do comando recebido. Assim, o campo request ID pode ser usado pela entidade gerenciadora para \ndetectar comandos ou respostas perdidos. Cabe à entidade gerenciadora decidir se retransmitirá um comando \nse nenhuma resposta correspondente for recebida após determinado período de tempo. Em particular, o pa-\ndrão SNMP não impõe nenhum procedimento específico de retransmissão, nem mesmo diz que o comando \ndeve ser enviado em primeiro lugar. Ele requer apenas que a entidade gerenciadora “aja com responsabilidade \nem relação à frequência e à duração das retransmissões”\n. Isso, é claro, nos leva a pensar como deve agir um \nprotocolo “responsável”!\n   Redes de computadores e a Internet\n570\n9.3.4  Segurança e administração\nOs projetistas do SNMPv3 têm dito que o “SNMPv3 pode ser considerado um SNMPv2 com capacidades \nadicionais de segurança e de administração” [RFC 3410]. Decerto, há mudanças no SNMPv3 em relação ao \nSNMPv2, mas em nenhum lugar elas são mais evidentes do que nas áreas da administração e da segurança. O \npapel central da segurança no SNMPv3 era de particular importância, já que a falta de segurança adequada re-\nsultava no uso do SNMP primordialmente para monitorar, em vez de controlar (por exemplo, SetRequest é \npouquíssimo usada no SNMPv1).\nÀ medida que o SNMP amadurecia, passando por três versões, sua funcionalidade crescia, mas infelizmente \ncrescia também o número de documentos de padronização relacionados a ele. Isso fica evidenciado pelo fato de \nque há agora um RFC [RFC 3411] que “descreve uma arquitetura para descrever os Ambientes de Gerenciamento \ndo SNMP”! Embora a ideia de uma “arquitetura” para “descrever um ambiente” possa ser um pouco excessiva \npara nossa cabeça, o objetivo do RFC 3411 é admirável — introduzir uma linguagem comum para descrever a \nfuncionalidade e as ações executadas por um agente ou entidade gerenciadora SNMPv3. A arquitetura de uma \nentidade SNMPv3 é direta, e viajar por ela servirá para solidificar nosso entendimento do SNMP.\nTabela 9.4  Tipos de PDU SNMPv2\nTipo de SNMPv2-PDU\nRemetente-receptor\nDescrição\nGetRequest\ngerente a agente\npega valor de uma ou mais instâncias de objetos MIB\nGetNextRequest\ngerente a agente\npega valor da próxima instância de objeto MIB na lista ou tabela\nGetBulkRequest\ngerente a agente\npega valores em grandes blocos de dados, por exemplo, valores em uma grande \ntabela\nInformRequest\ngerente a gerente\ninforma à entidade gerenciadora remota valores da MIB que são remotos para \nseu acesso\nSetRequest\ngerente a agente\ndefine valores de uma ou mais instâncias de objetos MIB\nResponse\nagente a gerente ou \ngerente a gerente\ngerado em resposta a\nGetRequest, \nGetNextRequest, \nGetBulkRequest, \nSetRequest PDU, ou InformRequest\nSNMPv2-Trap\nagente a gerente\ninforma ao gerente um evento excepcional\nFigura 9.4  Formato da PDU SNMP\nTipo de \nPDU \n(0-3)\nID \nrequisição\nStatus \nde erro \n(0-5)\nÍndice de \nerro\nNome\nValor\nNome\nNome\nValor\nTipo de \nPDU (4)\nEmpresa\nEnd. \nagente\nTipo \nde trap\n(0–7)\nCódigo \nespecíﬁco\nMarca de \ntempo\nValor\nCabeçalho para obter/deﬁnir\nCabeçalho de trap\nInformação de trap\nPDU SNMP\nVariáveis a obter/deﬁnir\nKR 09.04.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n29p0 Wide x 13p6 Deep\n11/16/11 rossi\nGerenciamento de rede  571 \nAs denominadas aplicações SNMP consistem em um gerador de comandos, um receptor de notificações \ne um transmissor proxy (todos normalmente encontrados em uma entidade gerenciadora); um elemento res-\npondedor de comandos e um originador de notificações (ambos encontrados em geral em um agente), e na \npossibilidade de outras aplicações. O gerador de comandos gera as PDUs GetRequest, GetNextRequest, \nGetBulkRequest e SetRequest, que examinamos na Seção 9.3.3, e processa as respostas recebidas dessas \nPDUs. O respondedor de comandos executa em um agente e recebe, processa e responde (usando a mensagem \nResponse) às PDUs GetRequest, GetNextRequest, GetBulkRequest e SetRequest recebidas. A \naplicação originadora de notificações de um agente gera PDUs Trap; essas PDUs podem ser recebidas e pro-\ncessadas em uma aplicação receptora de notificações em uma entidade gerenciadora. A aplicação do transmis-\nsor proxy repassa as PDUs de requisição, notificação e resposta.\nUma PDU enviada por uma aplicação SNMP passa, em seguida, por um “processador” SNMP, antes de \nser enviada via protocolo de transporte apropriado. A Figura 9.5 mostra como uma PDU gerada pela aplicação \ngeradora de comandos entra primeiro no módulo de despacho, onde é determinada a versão do SNMP. A PDU \né então processada no sistema de processamento de mensagens, no qual é envolvida em um cabeçalho de men-\nsagem que contém o número da versão do SNMP, uma ID de mensagem e informações sobre o tamanho desta. \nCaso seja necessária criptografia ou autenticação, são incluídos também os campos de cabeçalho apropriados para \nessas informações; veja mais detalhes no [RFC 3411]. Por fim, a mensagem SNMP (a PDU gerada pela aplicação e \nmais as informações do cabeçalho de mensagem) é passada ao protocolo de transporte apropriado. O protocolo de \ntransporte preferencial para transportar mensagens SNMP é o UDP (isto é, as mensagens SNMP são transportadas \ncomo carga útil de um datagrama UDP), e o número de porta preferencial para o SNMP é a porta 161. A porta 162 \né usada para mensagens trap.\nVimos antes que as mensagens SNMP são usadas não só para monitorar, mas também para controlar (por \nexemplo, por meio do comando SetRequest) elementos da rede. É claro que um intruso que conseguisse in-\nterceptar mensagens SNMP e/ou gerar seus próprios pacotes SNMP na infraestrutura de gerenciamento poderia \ncriar um grande tumulto na rede. Assim, é crucial que mensagens SNMP sejam transmitidas com segurança. \nSurpreendentemente, foi apenas na versão mais recente do SNMP que a segurança recebeu a atenção merecida. \nFigura 9.5  Processador e aplicações do SNMPv3\nGerador de \ncomandos\nAplicações \nSNMP\nProcessador \nSNMP\nReceptor de \nnotiﬁcações\nTransmissor \nproxy\nDespacho\nSistema de \nprocessamento \nde mensagens\nTemporização, \nautenticação, \nprivacidade\nSegurança\nCamada de transporte\nControle \nde acesso\nGerador de \ncomandos\nOriginador de \nnotiﬁcações\nOutro\nPDU\nCabeçalho de \nsegurança/mensagem\nPDU\n   Redes de computadores e a Internet\n572\nA segurança SNMPv3 é conhecida como segurança baseada no usuário [RFC 3414], pois utiliza o conceito \ntradicional de um usuário, identificado por um nome de usuário, ao qual as informações de segurança — uma \nsenha, um valor de chave ou acessos privilegiados — são associadas. O SNMPv3 fornece criptografia, autentica-\nção, proteção contra ataques de reprodução (veja a Seção 8.3) e controle de acesso.\n• Criptografia. As PDUs SNMP podem ser criptografadas usando o DES (Criptografia de dados padrão) no \nmodo Encadeamento de Blocos de Cifras (CBC). Note que, como o DES é um sistema de chaves compar-\ntilhadas, o valor da chave secreta dos dados de codificação do usuário deve ser conhecido pela entidade \nreceptora que deve decodificar os dados.\n• Autenticação. O SNMP usa a técnica de MAC (Message Authentication Code) que estudamos na Seção \n8.3.1 para prover proteção e autenticação contra adulteração [RFC 4301]. Lembre-se de que um MAC \nnecessita que um emissor e um receptor conheçam uma chave secreta comum.\n• Proteção contra ataques de reprodução. Lembre-se de que podem ser usados nonces (veja o Capítulo 8) \npara proteção contra ataques de reprodução. O SNMPv3 adota uma técnica relacionada. Para garantir \nque uma mensagem recebida não seja uma reprodução de alguma mensagem anterior, o receptor exige \nque o remetente inclua em cada mensagem um valor baseado em um contador no receptor. Esse conta-\ndor, que funciona como um nonce, reflete o período de tempo decorrido entre a última reinicialização \ndo software de gerenciamento de rede do remetente e o número total de reinicializações desde a última \nvez que o software de gerenciamento de rede do receptor foi configurado. Contanto que o contador em \numa mensagem recebida esteja dentro de uma determinada margem de erro em relação ao próprio valor \ndo receptor, a mensagem é aceita como uma mensagem original e a partir daí pode ser autenticada e/ou \ndecriptada. Consulte o [RFC 3414] para obter mais detalhes.\n• Controle de acesso. O SNMPv3 fornece um controle de acesso baseado em vistas [RFC 3415] que \ncontrola quais das informações de gerenciamento de rede podem ser consultadas e/ou definidas por \nquais usuários. Uma entidade SNMP armazena informações sobre direitos de acesso e políticas em \num banco de dados de configuração local (Local Configuration Datastore — LCD). Partes do LCD são \nacessíveis elas próprias como objetos gerenciados, definidos na MIB de Configuração do Modelo de \nControle de Acesso Baseado em Vistas (View-Based Access Control Model Configuration) [RFC 3415] \ne, portanto, podem ser gerenciados e manipulados remotamente via SNMP.\n9.4  ASN.1\nNeste livro, examinamos uma série de assuntos interessantes sobre redes de computadores. Esta seção so-\nbre a ASN.1, contudo, pode não fazer parte da lista dos dez tópicos mais interessantes. Assim como as verduras, \nconhecer a ASN.1 e a questão mais ampla de serviços de apresentação é algo que “é bom para você”\n. A ASN.1 é \num padrão originado na ISO, usado em uma série de protocolos relacionados à Internet, em particular na área de \ngerenciamento de rede. Por exemplo, vimos na Seção 9.3 que as variáveis MIB do SNMP estão inextricavelmente \nligadas à ASN.1. Portanto, embora o material sobre a ASN.1 apresentado nesta seção seja bastante árido, espera-\nmos que o leitor nos dê um voto de confiança e acredite que o material é importante.\nPara motivar nossa discussão, faça o seguinte exercício mental: suponha que fosse possível copiar dados, \nde maneira confiável, da memória de um computador diretamente para a de outro computador. Se isso fosse \npossível, o problema da comunicação estaria “resolvido”? A resposta a essa pergunta depende de como se define \n“problema de comunicação”\n. Com certeza, uma cópia perfeita, memória a memória, comunicaria com exatidão \nos bits e os bytes de uma máquina para outra. Mas essa cópia exata de bits e bytes significa que, quando o software \nque estiver sendo executado no computador receptor acessar esses dados, ele verá os mesmos valores que esta-\nvam armazenados na memória do computador remetente? A resposta é “não necessariamente”! O ponto crucial \nda questão é que diferentes arquiteturas de computadores, diferentes sistemas operacionais e compiladores têm \ndiferentes convenções de armazenamento e representação de dados. Quando se trata de comunicar e armazenar \nGerenciamento de rede  573 \ndados entre vários computadores (como acontece em todas as redes de comunicação), fica evidente que esse pro-\nblema da representação de dados tem de ser resolvido.\nComo exemplo desse problema, considere o fragmento simples em linguagem C a seguir. Como essa estru-\ntura deveria ser disposta na memória?\nstruct {\n\t\nchar code;\n\t\nint x;\n\t\n} test;\n\t\ntest.x = 259;\n\t\ntest.code = ‘a’;\nO lado esquerdo da Figura 9.6 mostra uma disposição possível para esses dados em uma arquitetura hipo-\ntética: há um único byte de memória que contém o caractere “a”\n, seguido de uma palavra de 16 bits que contém \no valor inteiro 259, armazenado com o byte mais significativo antes. A disposição na memória de outro compu-\ntador é mostrada no lado direito da Figura 9.6. O caractere “a” é seguido pelo valor inteiro armazenado com o \nbyte menos significativo antes e com o inteiro de 16 bits alinhado para começar em um limite de palavra de 16 \nbits. Certamente, se quiséssemos executar uma cópia ao pé da letra entre as memórias desses dois computadores \ne usar a mesma definição de estrutura para acessar os valores armazenados, veríamos resultados diferentes nos \ndois computadores!\nO fato de diferentes arquiteturas terem diferentes formatos para os dados internos é um problema real e \nuniversal. O problema específico do armazenamento de inteiros em diferentes formatos é tão comum que tem até \num nome. A ordem de armazenamento de inteiros “big-endian” armazena primeiro os bytes mais significativos \n(nos endereços de armazenamento mais baixos). A ordem “little-endian” armazena primeiro os bytes menos \nsignificativos. Os processadores Sparc da Sun e os da Motorola são do tipo “big-endian”\n, ao passo que os da Intel \nsão do tipo “little-endian”\n. Como curiosidade, os termos “big-endian” e “little-endian” vêm do livro Viagens de \nGulliver, de Jonathan Swift, no qual dois grupos de pessoas insistem, dogmaticamente, em fazer uma coisa sim-\nples de duas maneiras diferentes (esperamos que a analogia com a comunidade da arquitetura de computadores \nfique clara). Um grupo da Terra de Lilliput insiste em quebrar os ovos pela extremidade maior (os “big-endians”), \nenquanto o outro grupo insiste em quebrá-los pela extremidade menor. Essa diferença foi a causa de um grande \nconflito e de uma rebelião civil.\nSabendo que diferentes computadores armazenam e representam dados de modos diferentes, como os \nprotocolos de rede devem enfrentar o problema? Por exemplo, se um agente SNMP estiver prestes a enviar \numa mensagem Response que contém um número inteiro que representa a contagem do número de datagra-\nmas UDP recebidos, como ele deveria representar o valor inteiro a ser enviado à entidade gerenciadora — na \nordem “big-endian” ou na ordem “little-endian”? Uma alternativa seria o agente enviar os bytes do inteiro na \nmesma ordem em que serão armazenados na entidade gerenciadora. Outra seria o agente enviar o inteiro em \nsua própria ordem de armazenamento, deixando à entidade gerenciadora a responsabilidade de reordenar os \nbytes quando necessário. Qualquer uma das alternativas exigiria que o remetente ou o receptor conhecesse o \nformato de representação de inteiros do outro.\nFigura 9.6  Duas organizações diferentes de dados em duas arquiteturas diferentes\na\n00000001\n00000011\ntest.code\ntest.x\ntest.code\ntest.x\na\n00000011\n00000001\n   Redes de computadores e a Internet\n574\nUma terceira alternativa é ter um método independente de máquina, de sistema operacional e de lingua-\ngem para descrever números inteiros e outros tipos de dados (isto é, uma linguagem de definição de dados) e \nregras que estabeleçam a maneira como cada um desses tipos de dados deve ser transmitido pela rede. Quando \nforem recebidos dados de determinado tipo, eles estarão em um formato conhecido e, assim, poderão ser arma-\nzenados em qualquer formato específico que um dado computador exija. Tanto a SMI, que estudamos na Seção \n9.3, quanto a ASN.1 adotam essa terceira alternativa. No jargão da ISO, os dois padrões descrevem um serviço \nde apresentação — o serviço de transmitir e traduzir informações de um formato específico de uma máquina \npara outro. A Figura 9.7 ilustra um problema de apresentação no mundo real; nenhum dos receptores entende \na ideia essencial que está sendo comunicada — que o interlocutor gosta de algo. Como mostrado na Figura 9.8, \num serviço de apresentação pode resolver esse problema traduzindo a ideia para uma linguagem inteligível (pelo \nserviço de apresentação), independentemente de quem fala, enviando essa informação ao receptor e, em seguida, \ntraduzindo-a para uma linguagem que o receptor entende.\nA Tabela 9.5 mostra alguns tipos de dados definidos pela ASN.1. Lembre-se de que encontramos os tipos \nde dados INTEGER, OCTET STRING e OBJECT IDENTIFIER em nosso estudo anterior da SMI. Como nossa \nmeta aqui (felizmente) não é fornecer uma introdução completa à ASN.1, remetemos o leitor aos padrões ou ao \nlivro impresso e on-line de Larmouth [1996], para a descrição de tipos e construtores ASN.1, como SEQUENCE \ne SET, que permitem definir os tipos de dados estruturados.\nAlém de fornecer uma linguagem de definição de dados, a ASN.1 oferece Regras Básicas de Codificação \n(Basic Encoding Rules — BERs), que especificam como instâncias de objetos que foram definidas usando a lingua-\nFigura 9.7  O problema da apresentação\nHippie de \nmeia-idade \n(dos anos 1960)\nQue barato!\nVovozinha\nAdolescente de 2013\nQue barato!\nKR 09.07.eps\nAW/Kurose and Ross\nComputer Networking 6/e\n20p5 Wide x 12p0 Deep\n9/6/11 rossi\nFigura 9.8  Solução do problema da apresentação\nHippie de\nmeia-idade\n(dos anos 1960)\nServiço de \napresentação\nVovozinha\nAdolescente de 2013\nServiço de \napresentação\nÉ muito bom\nÉ da pontinha!\nÉ da hora!\nQue barato!\nÉ muito bom\nServiço de \napresentação\nGerenciamento de rede  575 \ngem de descrição de dados ASN.1 devem ser enviadas pela rede. A BER adota a abordagem TLV (Type, Length, \nValue — Tipo, Comprimento, Valor) para a codificação de dados para transmissão. Para cada item de dados a \nser remetido, são enviados o tipo dos dados, o comprimento do item de dados e o valor do item de dados, nessa \nordem. Com essa simples convenção, os dados recebidos basicamente se autoidentificam.\nA Figura 9.9 mostra como os dois itens de dados de um exemplo simples seriam enviados. Nesse exemplo, \no remetente quer enviar a cadeia de caracteres “smith” seguida de um valor decimal 259 (que é igual a 00000001 \n00000011 em linguagem binária, ou a um valor de byte 1 seguido de um valor de byte 3) adotando a ordem \nTabela 9.5  Tipos de dados ASN.1 selecionados\nTag\nTipo\nDescrição\n1\nBOOLEAN\nvalor é “verdadeiro” ou “falso”\n2\nINTEGER\npode ser arbitrariamente grande\n3\nBITSTRING\nlista de um ou mais bits\n4\nOCTET STRING\nlista de um ou mais bytes\n5\nNULL\nsem valor\n6\nOBJECT IDENTIFIER\nnome, na árvore de nomeação padrão ASN.1; veja Seção 9.2.2\n9\nREAL\nponto flutuante\nFigura 9.9  Exemplo de codificação BER\nsobrenome ::= OCTET STRING\ncódigo ::= INTEGER\n{código, 259}\n{sobrenome, \"smith\"}\nMódulo de tipos\nde dados escrito\nem ASN.1\nInstâncias de tipos de dados\nespeciﬁcados no módulo\nRegras básicas de codiﬁcação \n(BER)\nCadeia de bytes \ntransmitida\n3\n1\n2\n2\nh\nt\ni\nm\ns\n5\n4\nKR 09 09 eps\n   Redes de computadores e a Internet\n576\n“big-endian”\n. O primeiro byte da cadeia transmitida tem o valor 4, que indica que o tipo do item de dado seguinte \né um OCTET STRING; esse é o “T” do código TLV. O segundo byte da cadeia contém o comprimento do OCTET \nSTRING, nesse caso, 5. O terceiro byte da cadeia transmitida inicia o OCTET STRING de comprimento 5; ele \ncontém a representação ASCII da letra “s”\n. Os valores T, L e V dos dados seguintes são 2 (o valor de tag do tipo \nINTEGER), 2 (isto é, um inteiro de comprimento de 2 bytes) e a representação “big-endian” de 2 bytes do valor \ndecimal 259.\nEm nossa discussão, abordamos apenas um subconjunto pequeno e simples da ASN.1. Entre os recursos \npara aprender mais sobre a ASN.1 estão os documentos padronizados da ASN.1 [ISO X.680, 2002], o livro on-line \nde Larmouth [2012], relativo ao modelo OSI, e os sites relativos ao ASN.1 [OSS, 2012] e OID Repository [2012].\n9.5  Conclusão\nNosso estudo sobre o gerenciamento de redes — e, na verdade, sobre toda a rede — agora está completo! \nNeste capítulo final, começamos apresentando os motivos da necessidade de fornecer ferramentas adequadas ao \nadministrador — a pessoa que tem a tarefa de manter a rede “ligada e funcionando” — para monitorar, testar, \nconsultar, configurar, analisar, avaliar e controlar a operação da rede. Nossas analogias com a administração de \nsistemas complexos, como usinas elétricas, aviões e organizações humanas, ajudaram-nos a fornecer motivos \npara essa necessidade. Vimos que a arquitetura do sistema de gerenciamento de rede gira em torno de cinco com-\nponentes fundamentais: (1) um gerenciador de rede, (2) um conjunto de dispositivos gerenciados remotamente \n(pelo gerenciador de rede), (3) as bases de informações de gerenciamento (MIBs) existentes nesses dispositivos, \ncontendo dados sobre seu estado e sua operação, (4) os agentes remotos que reportam informação das MIBs e \nexecutam ações sob o controle do gerenciador de rede e (5) um protocolo para a comunicação entre o gerencia-\ndor de rede e os dispositivos remotos.\nEm seguida, examinamos em detalhes a estrutura de gerenciamento de rede da Internet e, em particular, o pro-\ntocolo SNMP\n. Vimos como o SNMP apresenta os cinco componentes fundamentais de uma arquitetura de gerencia-\nmento de padrão da Internet e gastamos um bom tempo examinando objetos MIB, a SMI — a linguagem de definição \nde dados para especificação das MIBs — e o protocolo SNMP em si. Observamos que a SMI e a ASN.1 estão inex-\ntricavelmente interligadas e que a ASN.1 desempenha um papel fundamental na camada de apresentação do modelo \nde referência de sete camadas ISO/OSI, e então fizemos um estudo rápido da ASN.1. Talvez mais importante do que \nos detalhes da ASN.1 em si foi a necessidade percebida de fornecer a tradução entre formatos de dados específicos de \ncada computador de uma rede. Embora algumas arquiteturas de rede reconheçam explicitamente a importância desse \nserviço, por terem uma camada de apresentação, essa camada não existe na pilha de protocolos da Internet.\nConvém também observar que há muitos tópicos no gerenciamento de rede que preferimos não abordar — \ntópicos como as falhas de identificação e gerenciamento, a detecção proativa de anomalias, a correlação entre os \nalarmes e os aspectos mais amplos do gerenciamento de serviço (por exemplo, de forma oposta ao gerenciamento \nde rede). Embora sejam importantes, esses tópicos merecem um livro dedicado a eles. O leitor pode consultar as \nreferências apresentadas na Seção 9.1.\nExercícios\nde fixação e perguntas\nQuestões de revisão do Capítulo 9\nSEÇÃO 9.1\n\t\nR1.\t Por que um administrador de rede necessita de ferramentas de gerenciamento de rede? Descreva cinco cenários.\nGerenciamento de rede  577 \n\t\nR2.\t Quais são as cinco áreas de gerenciamento de rede definidas pela ISO?\n\t\nR3.\t Qual a diferença entre gerenciamento de rede e gerenciamento de serviço?\nSEÇÃO 9.2\n\t\nR4.\t Defina os seguintes termos: entidade gerenciadora, dispositivo gerenciado, agente de gerenciamento, MIB e \nprotocolo de gerenciamento de rede.\nSEÇÃO 9.3\n\t\nR5.\t Qual é o papel da SMI no gerenciamento de rede?\n\t\nR6.\t Cite uma diferença importante entre uma mensagem de comando-resposta e uma mensagem trap no SNMP.\n\t\nR7.\t Quais são os sete tipos de mensagens usados no SNMP?\n\t\nR8.\t O que significa um “processador do SNMP”?\nSEÇÃO 9.4\n\t\nR9.\t Qual é a finalidade da árvore de identificadores de objetos ASN.1?\n\t\nR10.\t Qual é o papel da ASN.1 na camada de apresentação nos modelos de referência ISO/OSI?\n\t\nR11.\t A Internet tem uma camada de apresentação? Se não tiver, como são tratadas as questões de diferenças \nentre arquiteturas de máquinas — por exemplo, a representação diferente de números inteiros em máquinas \ndiferentes?\n\t\nR12.\t O que significa codificação TLV?\nproblemas\n\t\nP1.\t Considere as duas maneiras pelas quais ocorrem as comunicações entre uma entidade gerenciadora e um \ndispositivo gerenciado: modo comando-resposta e trapping. Quais são os prós e os contras dessas duas \ntécnicas, em termos de (1) sobrecarga, (2) tempo de notificação quando ocorrem eventos excepcionais e (3) \nrobustez quanto às mensagens perdidas entre a entidade gerenciadora e o dispositivo gerenciado?\n\t\nP2.\t Na Seção 9.3 vimos que era preferível transportar mensagens SNMP em datagramas UDP não confiáveis. Em \nsua opinião, por que os projetistas do SNMP preferiram o UDP ao TCP como protocolo de transporte para o \nSNMP?\n\t\nP3.\t Qual é o identificador de objeto ASN.1 para o protocolo ICMP (veja a Figura 9.3)?\n\t\nP4.\t Suponha que você trabalhe para uma empresa com sede nos Estados Unidos que quer desenvolver sua \nprópria MIB para o gerenciamento de uma linha de produtos. Em que lugar da árvore de identificadores de \nobjetos (veja a Figura 9.3) esse produto seria registrado? (Dica: você deve recorrer a alguns RFCs e a outros \ndocumentos similares para responder a essa pergunta.)\n\t\nP5.\t Lembre-se da Seção 9.3.2, que uma empresa privada (empreendimento) pode criar suas próprias variáveis \nMIB sob o ramo privado 1.3.6.1.4. Suponha que a IBM quisesse criar uma MIB para seu software do servidor \nWeb. Qual seria o próximo qualificador OID após 1.3.6.1.4? (Para responder a essa questão, você precisará \nconsultar IANA [2009b].) Pesquise na Web para tentar descobrir se essa MIB existe para um servidor da IBM.\n\t\nP6.\t Em sua opinião, por que o comprimento precede o valor em uma codificação TLV (ao invés de vir após o \nvalor)?\n\t\nP7.\t Considere a Figura 9.9. Qual seria a codificação BER para {peso, 75} {sobrenome, “Marco”}?\n\t\nP8.\t Considere a Figura 9.9. Qual seria a codificação BER para {peso, 65} {sobrenome, “Dario”}?\n   Redes de computadores e a Internet\n578\nPor favor, descreva um ou dois dos projetos mais \ninteressantes em que você já trabalhou durante \nsua carreira. Quais foram os maiores desafios?\nQuando eu era pesquisadora na AT&T, um grupo nos-\nso projetou uma nova forma de gerenciar o roteamento \nnas redes de backbone dos ISPs. Em geral, os operadores \nde rede configuram cada roteador individualmente, e \nesses roteadores executam protocolos distribuídos para \ncalcular caminhos através da rede. Acreditamos que o \ngerenciamento de rede seria mais simples e mais flexí-\nvel se os operadores da rede pudessem exercer controle \ndireto sobre o modo como os roteadores repassam o trá-\nfego com base em uma visão em nível de rede da topolo-\ngia e do tráfego. A plataforma de controle de roteamen-\nto (RCP — Routing Control Platform) que projetamos \ne montamos poderia calcular as rotas para todo o ba-\nckbone da AT&T em um único computador comercial, e \npoderia controlar roteadores legados sem modificação. \nPara mim, esse projeto foi interessante porque tivemos \numa ideia provocadora, um sistema funcional e por fim \numa execução real em uma rede operacional.\nQue mudanças e inovações você antecipa que \nacontecerão no gerenciamento de redes no \nfuturo?\nEm vez de apenas “aparafusar” o gerenciamento de \nrede em cima das redes existentes, os pesquisadores e \nprofissionais estão começando a projetar redes que são \nfundamentalmente mais fáceis de gerenciar. Como nos-\nso trabalho inicial no RCP, a ideia principal nas chama-\ndas redes definidas por software (SDN — Software De-\nfined Networking) é executar um controlador que possa \ninstalar regras de tratamento de pacotes de baixo nível \nnos comutadores subjacentes, usando um protocolo pa-\ndrão. Esse controlador pode executar diversas aplicações \nde gerenciamento de rede, como o controle de acesso \ndinâmico, mobilidade transparente do usuário, enge-\nnharia de tráfego, balanceamento de carga do servidor, \nuso eficiente das redes em termos de energia e assim por \ndiante. Acredito que a SDN é uma grande oportunida-\nde para acertar o gerenciamento de rede, repensando a \nrelação entre os dispositivos de rede e o software que os \ncontrola.\nComo você vê o futuro das redes e da Internet?\nAs redes compõem um campo muito interessante, \npois as aplicações e as tecnologias utilizadas mudam o \ntempo todo. Estamos sempre reinventando! Quem teria \nprevisto, há apenas cinco ou dez anos, o domínio dos \nsmartphones, permitindo que usuários móveis acessem \naplicações existentes e também novos serviços baseados \nem sua localização? O surgimento da computação em \nnuvem está fundamentalmente mudando a relação entre \nos usuários e as aplicações que eles executam, e os senso-\nJennifer Rexford\nJennifer Rexford é professora no departamento de Ciência da Computação da \nPrinceton University. Sua pesquisa tem o amplo objetivo de tornar as redes de com-\nputadores mais fáceis de projetar e administrar, com ênfase particular nos protocolos \nde roteamento. De 1996 a 2004, foi membro do departamento de Gerenciamento e \nDesempenho de Redes da AT&T Labs-Research. Enquanto estava na AT&T, projetou \ntécnicas e ferramentas para medição de rede, engenharia de tráfego e configuração \nde roteadores, que foram implementadas na rede de backbone da AT&T. Jennifer é \ncoautora do livro Web Protocols and Practice: Networking Protocols, Caching, and \nTraffic Measurement, publicado pela Addison-Wesley em maio de 2001. Foi presiden-\nte da ACM SIGCOMM de 2003 a 2007. Graduou-se como bacharel em engenharia \nelétrica pela Princeton University em 1991 e obteve mestrado e doutorado em engenharia elétrica e ciência da \ncomputação pela Universidade de Michigan em 1993 e 1996, respectivamente. Em 2004, Jennifer foi vencedora \ndo Grace Murray Hopper Award da ACM como jovem profissional de destaque em computação, aparecendo na \nlista TR-100 do MIT dos inovadores com idade menor que 35 anos.\nENTREVISTA\nGerenciamento de rede  579 \nres em rede estão permitindo diversas aplicações novas. \nO ritmo da inovação é mesmo inspirador.\nA rede de apoio é um componente decisivo em todas \nessas inovações. Mesmo assim, a rede está notoriamen-\nte “no caminho” — limitando o desempenho, compro-\nmetendo a confiabilidade, restringindo aplicações e \ncomplicando a implementação e o gerenciamento de \nserviços. Devemos lutar para tornar a rede do futuro \ntão invisível quanto o ar que respiramos, de modo que \nnunca fique no caminho de novas ideias e serviços va-\nliosos. Para isso, precisamos elevar o nível de abstração \nacima dos dispositivos de rede e protocolos individuais \n(e seus respectivos acrônimos!), de modo que possa-\nmos raciocinar sobre a rede como um todo.\nQue pessoas a inspiraram profissionalmente?\nHá muito tempo tenho me inspirado em Sally Floyd, \ndo International Computer Science Institute. Sua pes-\nquisa é sempre significativa, focalizando os desafios \nimportantes enfrentados pela Internet. Ela mergulha a \nfundo em questões difíceis, até que entenda comple-\ntamente o problema e o espaço das soluções, e dedica \nmuita energia para “fazer as coisas acontecerem”\n, como \nempurrar suas ideias para padrões de protocolos e \nequipamentos de rede. Além disso, ela oferece retor-\nno à comunidade, através de serviços profissionais em \ndiversas organizações de padrões e pesquisa, e tam-\nbém criando ferramentas (como os simuladores ns-2 \ne ns-3 bastante utilizados), que permitem que outros \npesquisadores tenham sucesso. Ela se aposentou em \n2009, mas sua influência no campo será sentida duran-\nte anos.\nQuais são suas recomendações para alunos que \ndesejam seguir carreira em ciência da computação \ne redes?\nRedes é um campo inerentemente interdisciplinar. A \naplicação de técnicas de outras disciplinas aos problemas \nde rede é uma ótima maneira de levar o campo adiante. \nTemos visto inovações tremendas no uso de redes, vindas \nde áreas tão diversificadas quanto teoria de filas, teorias \nde jogos, teoria de controle, sistemas distribuídos, otimi-\nzação de redes, linguagens de programação, aprendizado \nde máquina, algoritmos, estruturas de dados e assim por \ndiante. Creio que familiarizar-se com um campo rela-\ncionado, ou colaborar de perto com especialistas nessas \n \náreas, seja um modo maravilhoso de preparar um alicerce \nmais forte para as redes, de modo que possamos apren-\nder como montar redes que sejam dignas de confiança \nda sociedade. Além das disciplinas teóricas, o campo das \nredes é interessante porque criamos artefatos reais, que \npessoas reais utilizam. Dominar o modo como projeta-\nmos e montamos sistemas — ganhando experiência em \nsistemas operacionais, arquitetura de computador etc. — \né outro modo fantástico de ampliar seus conhecimentos \nem redes, ajudando a mudar o mundo.\nUma nota sobre URLs. Nas referências a seguir, fornecemos URLs para páginas Web, documentos apenas da Web \ne outros materiais que não foram publicados em conferências ou periódicos (quando pudemos localizar um URL \npara tal material). Não fornecemos URLs para publicações de configuração e periódico, pois esses documentos \nnormalmente podem ser localizados por um mecanismo de busca, pelo site da conferência (por exemplo, artigos \nem todas as conferências e seminários ACM SIGCOMM podem ser localizados por http://www.acm.org/sig-\ncomm), ou por meio de uma assinatura de biblioteca digital. Embora todos os URLs fornecidos a seguir fossem \nválidos (e testados) em janeiro de 2012, URLs podem se tornar desatualizados. Por favor, consulte a versão on-line \ndeste livro (http://www.awl.com/kurose-ross) para obter uma bibliografia atualizada.\nUma nota sobre Solicitações de Comentários (RFCs) da Internet: Cópias de RFCs da Internet estão disponíveis em \nmuitos sites. O RFC Editor da Internet Society (o órgão que supervisiona as RFCs) mantém o site, em <http://www.\nrfc-editor.org>. Esse site lhe permite procurar um RFC específico por título, número ou autores, e mostrará atua-\nlizações de quaisquer RFCs listadas. As RFCs da Internet podem ser atualizadas ou podem se tornar obsoletas por \noutras RFCs mais recentes. Nosso site favorito para obter RFCs é a fonte original — <http://www.rfc-editor.org>.\n3COM ADDRESSING 2012. “White paper: Understanding IP addressing: Everything you ever wanted to know”\n. <http://\nwww.3com.com/other/pdfs/infra/corpinfo/en_US/501302.pdf>.\n3GPP 2012. Third Generation Partnership Project homepage, <http://www.3gp.org/>.\n3GPP NETWORK ARCHITECTURE 2012. 3GPP, “TS 23.002: Network Architecture: Digital Cellular Telecommunica-\ntions System (Phase 2+); Universal Mobile Telecommunications System (UMTS); LTE”. <http://www.3gp.org/ftp/Specs/\nhtml-info/23002.htm>.\nALBITZ P. & LIU C. DNS and BIND. Petaluma,: O’Reilly & Associates, 1993.\nABRAMSON, N. “The Aloha System—Another Alternative for Computer Communications”. Proc. 1970 Fall Joint Compu-\nter Conference, AFIPS Conference, p. 37, 1970.\nABRAMSON, N. “Development of the Alohanet”. IEEE Transactions on Information Theory, v.IT-31, n.3 (mar. 1985), \np. 119–123.\nABRAMSON, N. “The Alohanet – Surfing for Wireless Data”. IEEE Communications Magazine, v.47, n.12, p. 21–25.\nABU-LIBDEH, H.; COSTA, P.; ROWSTRON, A.; O’SHEA, G.; DONNELLY, A. “Symbiotic Routing in Future Data Cen-\nters”. Proc. 2010 ACM SIGCOMM.\nReferências\nReferências  581 \nADHIKARI, V. K.; JAIN, S.; CHEN, Y.; ZHANG, Z. L. “Vivisecting YouTube: An Active Measurement Study”\n. Technical \nReport, University of Minnesota, 2011.\nADHIKARI, V. K.; GAO, Y.; HAO, F.; VARVELLO, M.; HILT, V.; STEINER, M.; ZHANG, Z. L. “Unreeling Netflix: Unders-\ntanding and Improving Multi-CDN Movie Delivery”. Technical Report, University of Minnesota, 2012.\nAFANASYEV, A.; TILLEY, N.; REIHER, P.; KLEINROCK, L. “Host-to-Host Congestion Control for TCP”. IEEE Commu-\nnications Surveys & Tutorials, v.12, n.3, p. 304–342. \nAGARWAL, S.; LORCH, J. “Matchmaking for Online Games and Other Latency-sensitive P2P Systems”. Proc. 2009 ACM \nSIGCOMM.\nAHN, J. S.; DANZIG, P. B.; LIU, Z.; YAN, Y. “Experience with TCP Vegas: Emulation and Experiment”. Proc. 1995 ACM \nSIGCOMM (Boston, ago. 1995), p. 185–195.\nAKAMAI homepage, http://www.akamai.com.\nAKELLA, A.; SESHAN, S.; SHAIKH, A. “An Empirical Evaluation of Wide-Area Internet Bottlenecks”. Proc. 2003 ACM \nInternet Measurement Conference (nov. 2003).\nAKHSHABI, S.; BEGEN, A. C.; DOVROLIS, C. “An Experimental Evaluation of Rate-Adaptation Algorithms in Adaptive \nStreaming over HTTP”\n. Proc. 2011 ACM Multimedia Systems Conf.\nAKYILDIZ, I.; GUTIERREX-ESTEVEZ, D.; REYES, E. “The Evolution to 4G Cellular Systems, LTE Advanced”\n. Physical \nCommunication. Elsevier, 3 (2010), 217–244.\nALCATEL-LUCENT. “Introduction to Evolved Packet Core”. <http://downloads.lightreading.com/wplib/alcatellucent/\nALU_WP_Intro_to_EPC.pdf>.\nAL-FARES, M.; LOUKISSAS, A.; VAHDAT, A. “A Scalable, Commodity Data Center Network Architecture”. Proc. 2008 \nACM SIGCOMM.\nALIZADEH, M.; GREENBERG, A.; MALTZ, D.; PADHYE, J.; PATEL, P.; PRABHAKAR, B.; SENGUPTA, S.; SRIDHA-\nRAN, M. “Data Center TCP (DCTCP)”\n. Proc. 2010 ACM SIGCOMM.\nALLMAN, E. “The Robustness Principle Reconsidered: Seeking a Middle Ground”. Communications of the ACM, v.54, n.8 \n(ago. 2011), p. 40–45.\nANDERSEN, J. B.; RAPPAPORT, T. S.; YOSHIDA, S. “Propagation Measurements and Models for Wireless Communica-\ntions Channels”\n. IEEE Communications Magazine, (jan.1995), p. 42–49.\nANDREWS, M.; SHEPHERD, M.; SRINIVASAN, A.; WINKLER, P.; ZANE, F. “Clustering and Server Election Using Pas-\nsive Monitoring”\n. Proc. 2002 IEEE INFOCOM.\nANDROUTSELLIS-THEOTOKIS, S.; SPINELLIS, D. “A Survey of Peer-to-Peer Content Distribution Technologies”\n. ACM \nComputing Surveys, v.36, n.4 (dez. 2004), p. 335–371.\nAPERJIS, C.; FREEDMAN, M. J.; JOHARI, R. “Peer-Assisted Content Distribution with Prices”. Proc. ACM CoNEXT’08 \n(Madri, dez. 2008).\nAPPENZELLER, G.; KESLASSY, I.; MCKEOWN, N. “Sizing Router Buffers”. Proc. 2004 ACM SIGCOMM (Portland, ago. \n2004).\nASH, G. R. Dynamic Routing in Telecommunications Networks. Nova York: McGraw Hill, 1998.\nASO-ICANN 2012. The Address Supporting Organization home page, <http://www.aso.icann.org>.\nAT&T. “AT&T High Speed Internet Business Edition Service Level Agreements”. <http://www.att.com/gen/gene-\nral?pid=6622>.\nATHEROS COMMUNICATIONS INC. “Atheros AR5006 WLAN Chipset Product Bulletins”. <http://www.atheros.com/\npt/AR5006Bulletins.htm>.\nAUGUSTIN, B.; KRISHNAMURTHY, B.; WILLINGER, W. “IXPs: Mapped?”. Proc. Internet Measurement Conference \n(IMC), nov. 2009.\nAYANOGLU, E.; PAUL, S.; LA PORTA, T. F.; SABNANI, K. K.; GITLIN, R. D. “AIRMAIL: A Link-Layer Protocol for \nWireless Networks”\n. ACM ACM/Baltzer Wireless Networks Journal, 1: 47–60, fev. 1995.\n   Redes de computadores e a Internet\n582\nBAKRE, A.; BADRINATH, B. R. “I-TCP: Indirect TCP for Mobile Hosts”. Proc. 1995 Int. Conf. on Distributed Computing \nSystems (ICDCS) (maio 1995), p. 136–143.\nBALAKRISHNAN, H.; PADMANABHAN, V.; SESHAN, S.; KATZ, R. “A Comparison of Mechanisms for Improving TCP \nPerformance Over Wireless Links”\n. IEEE/ACM Transactions on Networking v.5, n.6 (dez. 1997).\nBALAKRISHNAN, H.; KAASHOEK, F.; KARGER, D.; MORRIS, R.; STOICA, I. “Looking Up Data in P2P Systems”\n. Com-\nmunications of the ACM, v.46, n.2 (fev. 2003), p. 43–48.\nBALDAUF, M.; DUSTDAR, S.; ROSENBERG, F. “A Survey on Context-Aware Systems”. Int. J. Ad Hoc and Ubiquitous \nComputing, v.2, n.4 (2007), p. 263–277.\nBALLANI, H.; FRANCIS, P.; RATNASAMY, S. “A Measurement-based Deployment Proposal for IP Anycast”. Proc. 2006 \nACM Internet Measurement Conf.\nBALLANI, H.; COSTA, P.; KARAGIANNIS, T.; ROWSTRON, A. “Towards Predictable Datacenter Networks”\n. Proc. 2011 \nACM SIGCOMM.\nBARAN, P. “On Distributed Communication Networks”. IEEE Transactions on Communication Systems, mar. 1964. Rand \nCorporation Technical report with the same title (Memorandum RM-3420-PR, 1964). <http://www.rand.org/publica-\ntions/RM/RM3420/>.\nBARDWELL, J. “You Believe You Understand What You Think I Said... The Truth About 802.11 Signal And Noise Metrics: \nA Discussion Clarifying Often-Misused 802.11 WLAN Terminologies”. <http://www.connect802.com/download/tech-\npubs/2004/you_believe_D100201.pdf>.\nBARFORD, P.; DUFFIELD, N.; RON, A.; SOMMERS, J. “Network Performance Anomaly Detection and Localization”\n. \nProc. 2009 IEEE INFOCOM (abr. 2009).\nBARONTI, P.; PILLAI, P.; CHOOK, V.; CHESSA, S.; GOTTA, A.; HU, Y. “Wireless Sensor Networks: A Survey on the State \nof the Art and the 802.15.4 and ZigBee Standards”. Computer Communications, v.30, n.7 (2007), p. 1655–1695.\nBASSET, S. A.; SCHULZRINNE, H. “An analysis of the Skype peer-to-peer Internet Telephony Protocol”. Proc. 2006 IEEE \nINFOCOM (Barcelona, abr. 2006).\nBBC NEWS ONLINE. “A Small Slice of Design”. abr. 2001, <http://news.bbc.co.uk/2/hi/science/nature/1264205.stm>.\nBBC. “Multicast”\n. <http://www.bbc.co.uk/multicast/>.\nBEHESHTI, N.; GANJALI, Y.; GHOBADI, M.; MCKEOWN, N.; SALMON, G. “Experimental Study of Router Buffer \nSizing”. Proc. ACM Internet Measurement Conference (Vouliagmeni, out. 2008).\nBENDER, P.; BLACK, P.; GROB, M.; PADOVANI, R.; SINDHUSHAYANA, N.; VITERBI, A. “CDMA/HDR: A band-\nwidth-efficient high-speed wireless data service for nomadic users”. IEEE Commun.Mag., v.38, n.7 (jul. 2000) p. 70–77.\nBERNERS-LEE, T.; CERN, “Information Management: A Proposal”. mar. 1989, maio 1990. <http://www.w3.org/History/1 \n989/proposal.html>.\nBERNERS-LEE, T.; CAILLIAU, R.; LUOTONEN, A.; FRYSTYK NIELSEN, H.; SECRET, A. “The World-Wide Web”\n. Com-\nmunications of the ACM, v.37, n.8 (ago. 1994), p. 76–82.\nBERTSEKAS, D.; GALLAGHER, R. Data Networks, 2.ed. Englewood Cliffs: Prentice Hall, 1991.\nBIDDLE, P.; ENGLAND, P.; PEINADO, M.; WILLMAN, B. “The Darknet and the Future of Content Distribution”\n. 2002 \nACM Workshop on Digital Rights Management, (Washington, nov. 2002, ) <http://crypto.stanford.edu/DRM2002/dark-\nnet5.doc>.\nBIERSACK, E. W. “Performance evaluation of forward error correction in ATM networks”. Proc. 1999 ACM SIGCOMM \n(Baltimore, ago. 1992), p. 248–257.\nBIND 2012. Internet Software Consortium page on BIND, <http://www.isc.org/bind.html>.\nBISDIKIAN, C. “An Overview of the Bluetooth Wireless Technology”. IEEE Communications Magazine, n.12 (dez. 2001), \np. 86–94.\nBISHOP, M. Computer Security: Art and Science. Boston: Addison Wesley, 2003.\nBLACK, U. ATM Volume I: Foundation for Broadband Networks. Prentice Hall, 1995.\nReferências  583 \nBLACK, U. ATM Volume II: Signaling in Broadband Networks. Prentice Hall, 1997.\nBLUMENTHAL, M.; CLARK, D. “Rethinking the Design of the Internet: the End-to-end Arguments vs. the Brave New \nWorld”. ACM Transactions on Internet Technology, v.1, n.1 (ago. 2001), p. 70–109. \nBOCHMANN, G. V.; SUNSHINE, C. A. “Formal methods in communication protocol design”. IEEE Transactions on Com-\nmunications, v.28, n.4 (abr. 1980) p. 624–631.\nBOLOT, J-C.; TURLETTI, T. “A rate control scheme for packet video in the Internet”. Proc. 1994 IEEE INFOCOM, p. \n1216–1223.\nBOLOT, J-C.; VEGA-GARCIA, A. “Control Mechanisms for Packet Audio in the Internet”. Proc. 1996 IEEE INFOCOM, \np. 232–239.\nBRADNER, S.; MANKIN, A. IPng: Internet Protocol Next Generation. Reading: Addison-Wesley, 1996.\nBRAKMO, L.; PETERSON, L. “TCP Vegas: End to End Congestion Avoidance on a Global Internet”. IEEE Journal of Se-\nlected Areas in Communications, v.13, n.8 (out. 1995), p. 1465–1480.\nBRESLAU, L.; KNIGHTLY, E.; SHENKER, S.; STOICA, I.; ZHANG, H. “Endpoint Admission Control: Architectural \nIssues and Performance”\n. Proc. 2000 ACM SIGCOMM (Estocolmo, ago. 2000).\nBRYANT, B. “Designing an Authentication System: A Dialogue in Four Scenes”. <http://web.mit.edu/kerberos/www/dia-\nlogue.html>.\nBUSH, V. “As We May Think”\n. The Atlantic Monthly, jul. 1945. <http://www.theatlantic.com/unbound/flashbks/computer/\nbushf.htm>.\nBYERS, J.; LUBY, M.; MITZENMACHER, M.; REGE, A. “A digital fountain approach to reliable distribution of bulk data”\n. \nProc. 1998 ACM SIGCOMM (Vancouver, ago. 1998), p. 56–67.\nCABLELABS 2012. <http://www.cablelabs.com>.\nCACHELOGIC 2012. <http://www.cachelogic.com>.\nCAESAR, M.; CALDWELL, D.; FEAMSTER, N.; REXFORD, J.; SHAIKH, A.; VAN der MERWE, J. “Design and imple-\nmentation of a Routing Control Platform”\n. Proc. Networked Systems Design and Implementation (maio 2005a).\nCAESAR, M.; REXFORD, J. “BGP Routing Policies in ISP Networks”. IEEE Network Magazine, v.19, n.6 (nov. 2005b).\nCALDWELL, C. “The Prime Pages”. <http://www.utm.edu/research/primes/prove>.\nCARDWELL, N.; SAVAGE, S.; ANDERSON, T. “Modeling TCP Latency”. Proc. 2000 IEEE INFOCOM (Tel-Aviv, mar. \n2000).\nCASA 2012. Center for Collaborative Adaptive Sensing of the Atmosphere, <http://www.casa.umass.edu>.\nCASADO, M.; FREEDMAN, M.; PETTIT, J.; LUO, J.; GUDE, N.; MCKEOWN, N.; SHENKER, S. “Rethinking Enterprise \nNetwork Control”\n. IEEE/ACM Transactions on Networking (ToN), v.17, n.4 (ago. 2009), p. 1270–1283.\nCASADO, M.; FREEDMAN, M.; PETTIT, J.; LUO, J.; MCKEOWN, N.; SHENKER, S. “Ethane: Taking Control of the En-\nterprise”. Proc. 2007 ACM SIGCOMM (Kioto, ago. 2007).\nCASNER, S.; DEERING, S. “First IETF Internet Audiocast”. ACM SIGCOMM Computer Communications Review, v.22, n.3 \n(jul. 1992), p. 92–97.\nCEIVA 2012. <http://www.ceiva.com/>.\nCENS 2012. Center for Embedded Network Sensing, <http://www.cens.ucla.edu/>.\nCERF, V.; KAHN, R. “A Protocol for Packet Network Interconnection”. IEEE Transactions on Communications Technology, \nv.COM-22, n.5, p. 627–641.\nCERT 2001–09. “Advisory 2001–09: Statistical Weaknesses in TCP/IP Initial Sequence Numbers”. <http://www.cert.org/\nadvisories/CA-2001-09.html>.\nCERT 2003–04. “CERT Advisory CA-2003-04 MS-SQL Server Worm”. <http://www.cert.org/advisories/CA-2003-04.\nhtml>.\nCERT 2012. <http://www.cert.org/advisories>.\n   Redes de computadores e a Internet\n584\nCERT FILTERING 2012. “Packet Filtering for Firewall Systems”. <http://www.cert.org/tech_tips/packet_filtering.html>.\nCERT SYN 1996. “Advisory CA-96.21: TCP SYN Flooding and IP Spoofing Attacks”. <http://www.cert.org/advisories/\nCA-1998-01.html>.\nCHAO, H. J.; LAM, C.; OKI, E. Broadband Packet Switching Technologies — A Practical Guide to ATM Switches and IP \nRouters. John Wiley & Sons, 2001.\nCHAO, H. J.; ZHANG, C.; DUNGHEL, P.; WU, D.; ROSS, K. W. “Unraveling the BitTorrent Ecosystem”. IEEE Transactions \non Parallel and Distributed Systems, v.22, n.7 (jul. 2011).\nCHEN, G.; KOTZ, D. “A Survey of Context-Aware Mobile Computing Research”. Technical Report TR2000-381, Dept. of \nComputer Science, Dartmouth College, nov. 2000. <http://www.cs.dartmouth.edu/reports/TR2000-381.pdf>.\nCHEN, K.-T.; HUANG, C.-Y.; HUANG, P.; LEI, C.-L. “Quantifying Skype User Satisfaction”. Proc. 2006 ACM SIGCOMM \n(Pisa, Itália, set. 2006).\nCHEN, K.; GUO, C.; WU, H.; YUAN, J.; FENG, Z.; CHEN, Y.; LU, S.; WU, W. “Generic and Automatic Address Configu-\nration for Data Center Networks”\n. Proc. 2010 ACM SIGCOMM.\nCHEN, Y.; JAIN, S.; ADHIKARI, V. K.; ZHANG, Z. “Characterizing Roles of Front-End Servers in End-to-End Perfor-\nmance of Dynamic Content Distribution”\n. Proc. 2011 ACM Internet Measurement Conference (Berlim, nov. 2011).\nCHENOWETH, T.; MINCH, R.; TABOR, S. “Wireless Insecurity: Examining User Security Behavior on Public Networks”\n. \nCommunications of the ACM, v.53, n.2 (fev. 2010), p. 134–138.\nCHESWICK, B.; BURCH, H.; BRANIGAN, S. “Mapping and Visualizing the Internet”. Proc. 2000 Usenix Conference (San \nDiego, jun. 2000).\nCHIU, D.; JAIN, R. “Analysis of the Increase and Decrease Algorithms for Congestion Avoidance in Computer Networks”\n. \nComputer Networks and ISDN Systems, v.17, n.1, p. 1–14. <http://www.cs.wustl.edu/~jain/papers/cong_av.htm>.\nCHRISTIANSEN, M.; JEFFAY, K.; OTT, D.; SMITH, F. D. “Tuning Red for Web Traffic”. IEEE/ACM Transactions on \nNetworking, v.9, n.3 (jun. 2001), p. 249–264.\nCHU, Y.; RAO, S.; SESHAN, S.; ZHANG, H. “A Case for End System Multicast”. IEEE J. Selected Areas in Communications, \nv.20, n.8 (out. 2002), p. 1456–1471.\nCHUANG, S.; IYER, S.; MCKEOWN, N. “Practical Algorithms for Performance Guarantees in Buffered Crossbars”\n. Proc. \n2005 IEEE INFOCOM.\nCICCONETTI, C.; LENZINI, L.; MINGOZI, A.; EKLUND, K. “Quality of Service Support in 802.16 Networks”\n. IEEE \nNetwork Magazine (mar./abr. 2006), p. 50–55.\nCISCO 12000 2012. “Cisco XR 12000 Series and Cisco 12000 Series Routers”. <http://www.cisco.com/en/US/products/\nps6342/index.html>.\nCISCO 8500 2012. “Catalyst 8500 Campus Switch Router Architecture”. <http://www.cisco.com/univercd/cc/td/doc/pro-\nduct/l3sw/8540/rel_12_0/w5_6f/softcnfg/1cfg8500.pdf>.\nCISCO 2011. Cisco Visual Networking Index: Forecast and Methodology, 2010–2015. White Paper, 2011.\nCISCO 2012. http://www.cisco.com/go/dce.\nCISCO NAT 2012. “How NAT Works”. <http://www.cisco.com/en/US/tech/tk648/tk361/technologies_tech_no-\nte09186a0080094831.shtml>.\nCISCO QoS 2012. “Advanced QoS Services for the Intelligent Internet”. <http://www.cisco.com/warp/public/cc/pd/iosw/\nioft/ioqo/tech/qos_wp.htm>.\nCISCO QUEUE 2012. “Congestion Management Overview”. <http://www.cisco.com/en/US/docs/ios/12_2/qos/configu-\nration/guide/qcfconmg.html>.\nCISCO SWITCHES 2012. “Multiservice Switches”. <http://www.cisco.com/warp/public/cc/pd/si/index.shtml>.\nCISCO SYN 2012. “Defining Strategies to Protect Against TCP SYN Denial of Service Attacks”. <http://www.cisco.com/\nen/US/tech/tk828/technologies_tech_ note09186a00800f67d5.shtml>.\nReferências  585 \nCISCO VNI 2011. “Visual Networking Index”. <http://www.cisco.com/web/solutions/sp/vni/vni_forecast_highlights/in-\ndex.html>.\nCLARK, D. “The Design Philosophy of the DARPA Internet Protocols”. Proc. 1988 ACM SIGCOMM (Stanford, ago. 1988).\nCLARKE, I.; HONG, T. W.; MILLER, S. G.; SANDBERG, O.; WILEY, B. “Protecting Free Expression Online with Freenet”\n. \nIEEE Internet Computing (jan.–fev. 2002), p. 40–49.\nCOHEN, D. “Issues in Transnet Packetized Voice Communication”. Proc. Fifth Data Communications Symposium (Snow-\nbird, set. 1977), p. 6–13.\nCOHEN, B. “Incentives to Build Robustness in BitTorrent”. First Workshop on the Economics of Peer-to-Peer Systems \n(Berkeley, jun.2003).\nCOOKIE CENTRAL 2012. <http://www.cookiecentral.com/n_cookie_faq.htm>.\nCORMEN, T. H. Introduction to Algorithms. 2.ed., Cambridge: MIT Press, 2001.\nCROW, B.; WIDJAJA, I.; KIM, J.; SAKAI, P. “IEEE 802.11 Wireless Local Area Networks”. IEEE Communications Magazine \n(set. 1997), p. 116–126.\nCROWCROFT, J.; WANG, Z.; SMITH, A.; ADAMS, J. “A Comparison of the IETF and ATM Service Models”\n. IEEE \nCommunications Magazine (nov./dez. 1995), p. 12–16.\nCROWCROFT, J.; HANDLEY, M.; WAKEMAN, I. Internetworking Multimedia. San Francisco: Morgan-Kaufman, 1999.\nCURTIS, A. R.; MOGUL, J. C.; TOURRILHES, J.; YALAGANDULA, P.; SHARMA, P.; BANERJEE, S. “DevoFlow: Scaling \nFlow Management for High-Performance Networks”. Proc. 2011 ACM SIGCOMM.\nCUSUMANO, M. A.; YOFFIE, D. B. Competing on Internet Time: Lessons from Netscape and its Battle with Microsoft. Nova \nYork: Free Press, 1998. \nDAHLMAN, E.; GUDMUNDSON, B.; NILSSON, M.; SKÖLD, J. “UMTS/IMT-2000 Based on Wideband CDMA”\n. IEEE \nCommunications Magazine (set. 1998), p. 70–80.\nDAIGLE, J. N. Queuing Theory for Telecommunications. Reading: Addison-Wesley, 1991.\nDALAL, Y.; METCALFE R., “Reverse Path Forwarding of Broadcast Packets”. Communications of the ACM, v.21, n.12 (dez. \n1978), p. 1040–1048.\nDAVIE B.; REKHTER, Y. MPLS: Technology and Applications. Morgan Kaufmann Series in ­\nNetworking, 2000.\nDAVIES, G.; KELLY, F. “Network Dimensioning, Service Costing, and Pricing in a Packet-Switched Environment”\n. Tele-\ncommunications Policy, v.28, n.4, p. 391–412.\nDEC 1990. “In Memoriam: J. C. R. Licklider 1915–1990”. SRC Research Report 61, ago. 1990. <http://www.memex.org/\nlicklider.pdf>.\nDECLERCQ, J.; PARIDAENS, O. “Scalability Implications of Virtual Private Networks”. IEEE Communications Magazine, \nv.40, n.5 (maio 2002), p. 151–157.\nDEMERS, A.; KESHAV, S.; SHENKER, S. “Analysis and Simulation of a Fair Queuing Algorithm”. Internetworking: \nResearch and Experience, v.1, n.1 (1990), p. 3–26.\nDENNING, D. (Ed.), DENNING, P. (Pref.). Internet Besieged: Countering Cyberspace Scofflaws. Reading: Addison-Wesley, \n1997.\nDHC 2012 IETF. Dynamic Host Configuration working group homepage. <http://www.ietf. org/html.charters/dhc-char-\nter.html>.\nDHUNGEL, P.; ROSS, K. W.; STEINER., M.; TIAN, Y.; HEI, X. “Xunlei: Peer-Assisted Download Acceleration on a \nMassive Scale”\n. Passive and Active Measurement Conference (PAM) 2012, Viena, 2012.\nDHUNGEL, P.; WU, D.; SCHONHORST, B.; ROSS, K. W. “\nA Measurement Study of Attacks on BitTorrent Leechers”\n. 7th Inter-\nnational Workshop on Peer-to-Peer Systems (IPTPS 2008) (Tampa Bay, fev. 2008).\nDIFFIE, W.; HELLMAN, M. E. “New Directions in Cryptography”. IEEE Transactions on Information Theory, v.IT-22 \n(1976), p. 644–654.\n   Redes de computadores e a Internet\n586\nDIGGAVI, S. N.; AL-DHAHIR, N.; STAMOULIS, A.; CALDERBANK, R. “Great Expectations: The Value of Spatial Diver-\nsity in Wireless Networks”\n. Proceedings of the IEEE, v.92, n.2 (fev. 2004).\nDILLEY, J.; MAGGS, B.; PARIKH, J.; PROKOP, H.; SITARAMAN, R.; WEIHL, B. “Globally Distributed Content Delivert”\n. \nIEEE Internet Computing (set.–out. 2002).\nDING, Y.; DU, Y.; HU, Y.; LIU, Z.; WANG, L.; ROSS, K. W.; GHOSE, A. “Broadcast Yourself: Understanding YouTube \nUploaders”. Proc. 2011 ACM Internet Measurement Conference (Berlim).\nDIOT, C.; LEVINE, B. N.; LYLES, B.; KASSEM, H.; BALENSIEFEN, D. “Deployment Issues for the IP Multicast Service \nand Architecture”\n. IEEE Network, v.14, n.1 (jan./fev. 2000) p. 78–88.\nDISCHINGER, M.; HAEBERLEN, A.; GUMMADI, K.; SAROIU, S. “Characterizing residential broadband networks”\n. \nProc. 2007 ACM Internet Measurement Conference, p. 24–26.\nDMITIROPOULOS, X.; KRIOUKOV\n, D.; FOMENKOV\n, M.; HUFFAKER, B.; HYUN, Y.; CLAFFY, K. C.; RILEY, G. “\nAS Relation-\nships: Inference and Validation”\n. ACM Computer Communication Review (jan. 2007).\nDOCSIS 2004. Data-over-cable service interface specifications: Radio-frequency interface specification.ITU-T J.112, 2004.\nDOCSIS 2011. Data-Over-Cable Service Interface Specifications, DOCSIS 3.0: MAC and Upper Layer Protocols Interface \nSpecification, CM-SP-MULPIv3.0-I16-110623, 2011.\nDODGE, M. “An Atlas of Cyberspaces”\n. <http://www.cybergeography.org/atlas/isp_maps.html>.\nDONAHOO, M.; CALVERT, K. TCP/IP Sockets in C: Practical Guide for Programmers. Morgan Kaufman, 2001.\nDOUCEUR, J. R. “The Sybil Attack”\n. First International Workshop on Peer-to-Peer Systems (IPTPS ’02) (Cambridge, mar. \n2002).\nDSL 2012. DSL Forum homepage, <http://www.dslforum.org/>.\nDROMS, R.; LEMON, T. The DHCP Handbook. 2.ed. SAMS Publishing, 2002.\nEDNEY, J.; ARBAUGH, W. A. Real 802.11 Security: Wi-Fi Protected Access and 802.11i. Addison-Wesley Professional, \n2003.\nEDWARDS, W. K.; GRINTER, R.; MAHAJAN, R.; WETHERALL, D. “Advancing the State of Home Networking”\n. Commu-\nnications of the ACM, v.54, n.6 (jun. 2011), p. 62–71.\nEKLUND, K.; MARKS, R.; STANSWOOD, K.; WANG, S. “IEEE Standard 802.16: A Technical Overview of the Wireless \nMAN Air Interface for Broadband Wireless Access”\n. IEEE Communications Magazine (jun. 2002), p. 98–107.\nELLIS, H. “The Story of Non-Secret Encryption”. http://jya.com/ellisdoc.htm.\nERICSSON 2011. “LTE—An Introduction”\n. <www.ericsson.com/res/docs/2011/lte_an_introduction.pdf>.\nERICSSON 2012. “The Evolution of Edge”\n. <http://www.ericsson.com/technology/whitepapers/broadband/evolution_of_\nEDGE.shtml>.\nESTRIN, D.; HANDLEY, M.; HELMY, A.; HUANG, P.; THALER, D. “A Dynamic Bootstrap Mechanism for Rendez-\nvous-Based Multicast Routing”\n. Proc. 1998 IEEE INFOCOM (Nova York, abr. 1998).\nFALKNER, J.; PIATEK, M.; JOHN, J. P.; KRISHNAMURTHY, A.; ANDERSON, T. “Profiling a Million Sser DHT”\n. Proc. \n2007 ACM Internet Measurement Conference.\nFALOUTSOS, C.; FALOUTSOS, M.; FALOUTSOS, P. “What Does the Internet Look Like? Empirical Laws of the Internet \nTopology”. Proc. 1999 ACM SIGCOMM (Boston, ago. 1999).\nFARRINGTON, N.; PORTER, G.; RADHAKRISHNAN, S.; BAZZAZ, H.; SUBRAMANYA, V.; FAINMAN, Y.; PAPEN, \nG.; VAHDAT, A. “Helios: A Hybrid Electrical/Optical Switch Architecture for Modular Data Centers”. Proc. 2010 ACM \nSIGCOMM.\nFEAMSTER, N.; WINICK, J.; REXFORD, J. “A Model for BGP Routing for Network Engineering”. Proc. 2004 ACM \nSIGMETRICS (Nova York, jun. 2004).\nFEAMSTER, N.; BALAKRISHNAN, H. “Detecting BGP Configuration Faults with Static Analysis”. NSDI (maio 2005).\nFELDMAN, M.; CHUANG, J. “Overcoming Free-Riding Behavior in Peer-to-peer Systems”. ACM SIGecom Exchanges (jul. \n2005).\nReferências  587 \nFELDMEIER, D. “Fast Software Implementation of Error Detection Codes”. IEEE/ACM Transactions on Networking, v.3, \nn.6 (dez. 1995), p. 640–652.\nFIPS 1995. Federal Information Processing Standard, “Secure Hash Standard”. FIPS Publication 180-1. <http://www.itl.\nnist.gov/fipspubs/fip180-1.htm>.\nFLOYD, S.; FALL, K. “Promoting the Use of End-to-End Congestion Control in the Internet”\n. IEEE/ACM Transactions on \nNetworking, v.6, n.5 (out. 1998), p. 458–472.\nFLOYD, S.; HANDLEY, M.; PADHYE, J.; WIDMER, J. “Equation-Based Congestion Control for Unicast Applications”\n. \nProc. 2000 ACM SIGCOMM (Estocolmo, ago. 2000).\nFLOYD, S. “A Report on Some Recent Developments in TCP Congestion Control”. IEEE Communications Magazine (abr. \n2001).\nFLOYD, S. “References on RED (Random Early Detection) Queue Management”. <http://www.icir.org/floyd/red.html>.\nFLOYD, S.; JACOBSON, V. “Synchronization of Periodic Routing Messages”. IEEE/ACM Transactions on Networking, v.2, \nn.2 (abr. 1997) p. 122–136.\nFLOYD, S. “TCP and Explicit Congestion Notification”. ACM SIGCOMM Computer Communications Review, v.24, n.5 \n(out. 1994), p. 10–23.\nFLUHRER, S.; MANTIN, I.; SHAMIR, A. “Weaknesses in the Key Scheduling Algorithm of RC4”. Eighth Annual Workshop \non Selected Areas in Cryptography, (Toronto, ago. 2002).\nFORTZ, B.; THORUP, M. “Internet Traffic Engineering by Optimizing OSPF Weights”. Proc. 2000 IEEE INFOCOM (Tel \nAviv, abr. 2000).\nFORTZ, B.; REXFORD, J.; THORUP, M. “Traffic Engineering with Traditional IP Routing Protocols”. IEEE Communica-\ntion Magazine (out. 2002).\nFRALEIGH, C.; TOBAGI, F.; DIOT, C. “Provisioning IP Backbone Networks to Support Latency Sensitive Traffic”\n. Proc. \n2003 IEEE INFOCOM (San Francisco, mar. 2003).\nFREEDMAN, M. J.; FREUDENTHAL, E.; MAZIRES, D. “Democratizing Content Publication with Coral”. USENIX NSDI, \n2004.\nFRIEDMAN, T.; TOWSLEY, D. “Multicast Session Membership Size Estimation”. Proc. 1999 IEEE INFOCOM (Nova York, \nmar. 1999).\nFROST, J. “BSD Sockets: A Quick and Dirty Primer”\n. <http://world.std.com/~jimf/papers/sockets/sockets.html>.\nFTTH COUNCIL 2011a. “NORTH AMERICAN FTTH STATUS—MARCH 31, 2011” (mar. 2011). <www.ftthcouncil.\norg>.\nFTTH COUNCIL 2011b. “2011 Broadband Consumer Research” (jun. 2011), <www.ftthcouncil.org>.\nGALLAGHER, R. G.; HUMBLET, P. A.; SPIRA, P. M. “A Distributed Algorithm for Minimum Weight-Spanning Trees”\n. \nACM Trans. on Programming Languages and Systems, v.1, n.5 (jan. 1983), p. 66–77.\nGAO, L.; REXFORD, J. “Stable Internet Routing Without Global Coordination”. IEEE/ACM Transactions on Networking, \nv.9, n.6 (dez. 2001), p. 681–692.\nGARCES-ERCE, L.; ROSS, K. W.; BIERSACK, E.; FELBER, P.; URVOY-KELLER, G. “TOPLUS: Topology Centric Lookup \nService”. Fifth Int. Workshop on Networked Group Communications (NGC 2003) (Munique, set. 2003) <http://cis.poly.\nedu/~ross/papers/TOPLUS.pdf>.\nGARTNER, F. C. “\nA Survey of Self-Stabilizing Spanning-Tree Construction Algorithms”\n. Technical Report IC/2003/38, Swiss \nFederal Institute of Technology (EPFL), School of Computer and Communication Sciences, jun.10, 2003. <http://ic2.epfl.ch/\npublications/documents/IC_TECH_REPORT_200338.pdf>.\nGAUTHIER, L.; DIOT, C.; KUROSE, J. “End-to-end Transmission Control Mechanisms for Multiparty Interactive Appli-\ncations on the Internet”\n. Proc. 1999 IEEE INFOCOM (Nova York, abr. 1999).\nGIRARD, A. Routing and Dimensioning in Circuit-Switched Networks. Reading: Addison-Wesley, 1990.\n   Redes de computadores e a Internet\n588\nGLITHO, R. “Contrasting OSI Systems Management to SNMP and TMN”. Journal of Network and Systems Management, \nv.6, n.2 (jun. 1998), p. 113–131.\nGNUTELLA 2009. “The Gnutella Protocol Specification, v0.4” <http://www9.limewire.com/developer/gnutella_proto-\ncol_0.4.pdf>.\nGOODMAN, D. J. Wireless Personal Communications Systems. Prentice-Hall, 1997.\nGOOGLE LOCATIONS 2012. Google data centers. <http://www.google.com/corporate/datacenter/locations.html>.\nGORALSKI, W. Frame Relay for High-Speed Networks. Nova York: John Wiley, 1999.\nGORALSKI, W. Optical Networking and WDM. Berkeley: Osborne/McGraw-Hill, 2001.\nGREENBERG, A.; HAMILTON, J.; MALTZ, D.; PATEL, P. “The Cost of a Cloud: Research Problems in Data Center \nNetworks”. ACM Computer Communications Review (jan. 2009a).\nGREENBERG, A.; JAIN, N.; KANDULA, S.; KIM, C.; LAHIRI, P.; MALTZ, D.; PATEL, P.; SENGUPTA, S. “VL2: A Scala-\nble and Flexible Data Center Network”\n. Proc. 2009 ACM SIGCOMM. (2009b)\nGREENBERG, A.; HAMILTON, J.; JAIN, N.; KANDULA, S.; KIM, C.; LAHIRI, P.; MALTZ, D.; PATEL, P.; SENGUPTA, \nS. “VL2: A Scalable and Flexible Data Center Network”. Communications of the ACM, v.54, n.3 (mar. 2011), p. 95–104.\nGRIFFIN, T. “Interdomain Routing Links”\n. <http://www.cl.cam.ac.uk/~tgg22/interdomain/>.\nGUHA, S.; DASWANI, N.; JAIN, R. “An Experimental Study of the Skype Peer-to-Peer VoIP System”. Proc. Fifth Int. \nWorkshop on P2P Systems (Santa Barbara, 2006).\nGUO, L.; CHEN, S.; XIAO, Z.; TAN, E.; DING, X.; ZHANG, X. “Measurement, Analysis, and Modeling of BitTorrent-Like \nSystems”. Proc. 2005 ACM Internet Measurement Conference.\nGUO, C.; LU, G.; LI, D.; WU, H.; ZHANG, X.; SHI, Y.; TIAN, C.; ZHANG, Y.; LU, S. “BCube: A High Performance, Server-cen-\ntric Network Architecture for Modular Data Centers”\n. Proc. 2009 ACM SIGCOMM.\nGUPTA, P.; MCKEOWN, N. “Algorithms for Packet Classification”. IEEE Network Magazine, v.15, n.2 (mar./abr. 2001), \np. 24–32.\nHA, S., RHEE, I., XU, L. “CUBIC: A New TCP-Friendly High-Speed TCP Variant”. ACM SIGOPS Operating System Review, \n2008.\nHALABI, S. Internet Routing Architectures. 2.ed., Cisco Press, 2000.\nHALPERIN, D.; HEYDT-BENJAMIN, T.; RANSFORD, B.; CLARK, S.; DEFEND, B.; MORGAN, W\n.; FU, K.; KOHNO, T.; MAI-\nSEL, W\n. “Pacemakers and implantable cardiac defibrillators: Software radio attacks and zero-power defenses”\n. Proc. 29th Annual \nIEEE Symposium on Security and Privacy (maio 2008).\nHALPERIN, D.; KANDULA, S.; PADHYE, J.; BAHL, P.; WETHERALL, D. “Augmenting Data Center Networks with \nMulti-Gigabit Wireless Links”\n. Proc. 2011 ACM SIGCOMM.\nHANBALI, A. A.; ALTMAN, E.; NAIN, P. “A Survey of TCP over Ad Hoc Networks”. IEEE Commun.Surveys and Tutorials, \nv.7, n.3 (2005), p. 22–36.\nHEI, X.; LIANG, C.; LIANG, J.; LIU, Y.; ROSS, K. W. “A Measurement Study of a Large-scale P2P IPTV System”\n. IEEE \nTrans. on Multimedia (dez. 2007).\nHEIDEMANN, J.; OBRACZKA, K.; TOUCH, J. “Modeling the Performance of HTTP over Several Transport Protocols”\n. \nIEEE/ACM Transactions on Networking, v.5, n.5 (out. 1997), p. 616–630.\nHELD, G. Data Over Wireless Networks: Bluetooth, WAP, and Wireless LANs. McGraw-Hill, 2001.\nHERSENT, O.; GURLE, D.; PETIT, J-P. IP Telephony: Packet-Based Multimedia Communication Systems. Edinburgh: \nPearson Education Ltd., 2000.\nHOLLAND, G.; VAIDYA, N.; BAHL, V\n. “\nA Rate-Adaptive MAC Protocol for Multi-Hop Wireless Networks”\n. Proc. 2001 ACM Int. \nConference of Mobile Computing and Networking (Mobicom01) (Roma, jul. 2001).\nHOLLOT, C. V.; MISRA, V.; TOWSLEY, D.; GONG, W. “\nAnalysis and design of controllers for AQM routers supporting TCP \nflows”\n. IEEE Transactions on Automatic Control, v.47, n.6 (jun. 2002), p. 945–959.\nReferências  589 \nHUANG, C.; SHARMA, V.; OWENS, K.; MAKAM, V. “Building Reliable MPLS Networks Using a Path Protection Mecha-\nnism”. IEEE Communications Magazine, v.40, n.3 (mar. 2002), p. 156–162.\nHUANG, Y.; GUERIN, R. “Does Over-Provisioning Become More or Less Efficient as Networks Grow Larger?”. Proc. IEEE \nInt. Conf. Network Protocols (ICNP) (Boston, nov. 2005).\nHUANG, C.; LI, J.; ROSS, K. W. “Can Internet VoD Be Profitable?”. Proc 2007 ACM SIGCOMM (Kioto, ago. 2007).\nHUANG, C.; LI, J.; WANG, A.; ROSS, K. W. “Understanding Hybrid CDN-P2P: Why Limelight Needs its Own Red \nSwoosh”. Proc. 2008 NOSSDAV, Braunschweig.\nHUANG, C.; HOLT, N.; WANG, Y. A.; GREENBERG, A.; LI, J.; ROSS, K. W. “A DNS Reflection Method for Global Traffic \nManagement”\n. Proc. 2010 USENIX, Boston.\nHUITEMA, C. IPv6: The New Internet Protocol. 2.ed. Englewood Cliffs: Prentice Hall, 1998.\nHUSTON, G. “Interconnection, Peering, and Settlements — Part I”. The Internet Protocol Journal, v.2, n.1 (mar. 1999a).\nHUSTON, G. “NAT Anatomy: A Look Inside Network Address Translators”. The Internet Protocol Journal, v.7, n.3 (set. \n2004). \nHUSTON, G. “Confronting IPv4 Address Exhaustion”. <http://www.potaroo.net/ispcol/2008-10/v4depletion.html>.\nHUSTON, G.; MICHAELSON, G. “IPv6 Deployment: Just where are we?”. <http://www.potaroo.net/ispcol/2008-04/ipv6.\nhtml>.\nHUSTON, G. “\nA Rough Guide to Address Exhaustion”\n. The Internet Protocol Journal, v.14, n.1 (mar. 2011).\nHUSTON, G. “Transitioning Protocols”\n. The Internet Protocol Journal, v.14, n.1 (mar. 2011).\nIAB 2012. <http://www.iab.org/>.\nIANA 2012a. <http://www.iana.org/>.\nIANA 2012b. “Private Enterprise Numbers”\n. <http://www.iana.org/assignments/enterprise-numbers>.\nIANA PROTOCOL NUMBERS 2012. <http://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml>.\nIANA TLD 2012. <http://www.iana.org/domains/root/db/>.\nICANN 2012. <http://www.icann.org>.\nIEC OPTICAL 2012. “Optical Access”\n. <http://www.iec.org/online/tutorials/opt_acc/>.\nIEEE 802 2012. <http://www.ieee802.org/>.\nIEEE 802.11 1999. “1999 Edition (ISO/IEC 8802-11: 1999) IEEE Standards for Information Technology — Telecommuni-\ncations and Information Exchange Between Systems — Local and Metropolitan Area Network — Specific Requirements — \nPart 11: Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specification”\n. <http://standards.ieee.org/\ngetieee802/download/802.11-1999.pdf>.\nIEEE 802.11n 2012. “IEEE P802.11 — Task Group N — Meeting Update: Status of 802.11n”. <http://grouper.ieee.org/\ngroups/802/11/Reports/tgn_update.htm>.\nIEEE 802.15 2012. IEEE 802.15 Working Group for WPAN homepage. <http://grouper.ieee. org/groups/802/15/>.\nIEEE 802.15.4 2012. IEEE 802.15 WPAN Task Group 4. <http://www.ieee802.org/15/pub/TG4.html>.\nIEEE 802.16d 2004. “IEEE Standard for Local and Metropolitan Area Networks, Part 16: Air Interface for Fixed Broad-\nband Wireless Access Systems”\n. <http://standards.ieee.org/getieee802/download/802.16-2004.pdf>.\nIEEE 802.16e 2005. “IEEE Standard for Local and Metropolitan Area Networks, Part 16: Air Interface for Fixed and \nMobile Broadband Wireless Access Systems, Amendment 2: Physical and Medium Access Control Layers for Combined \nFixed and Mobile Operation in Licensed Bands and Corrigendum 1”. <http://standards.ieee.org/getieee802/downloa-\nd/802.16e-2005.pdf>.\nIEEE 802.1q 2005. “IEEE Standard for Local and Metropolitan Area Networks: Virtual Bridged Local Area Networks”\n. \n<http://standards.ieee.org/getieee802/download/802.1Q-2005.pdf>.\nIEEE 802.1X. IEEE Std 802.1X-2001 Port-Based Network Access Control, <http://standards.ieee.org/reading/ieee/std_\npublic/description/lanman/802.1x-2001_desc.html>.\n   Redes de computadores e a Internet\n590\nIEEE 802.3 2012. “IEEE 802.3 CSMA/CD (Ethernet)”\n. <http://grouper.ieee.org/groups/802/3/>.\nIEEE 802.5 2012. <http://www.ieee802.org/5/www8025org/>.\nIETF 2012. <http://www.ietf.org>.\nIHM, S.; PAI, V. S. “Towards Understanding Modern Web Traffic”. Proc. 2011 ACM Internet Measurement Conference \n(Berlim).\nIMAP 2012. The IMAP Connection. <http://www.imap.org/>.\nINTEL 2012. “Intel® 82544 Gigabit Ethernet Controller”. <http://www.intel.com/design/network/products/lan/\ndocs/82544_docs.htm>.\nINTEL WIMAX 2012. “WiMax Technology”. <http://www.intel.com/technology/wimax/index.htm>.\nINTERNET2 MULTICAST 2012. <http://www.internet2.edu/multicast/>.\nIPv6 2012. <http://www.ipv6.com/>.\nISC 2012. <http://www.isc.org>.\nISI 1979. “DoD Standard Internet Protocol”\n. Internet Engineering Note 123 (dez. 1979). <http://www.isi.edu/in-notes/ien/\nien123.txt>.\nISO 2012. <http://www.iso.org/>.\nISO X.680 2002. “X.680: ITU-T Recommendation X.680 (2002) Information Technology — Abstract Syntax Notation One \n(ASN.1): Specification of Basic Notation”\n. <http://www.itu.int/ITU-T/studygroups/com17/languages/X.680-0207.pdf>.\nITU 1999. Asymmetric Digital Subscriber Line (ADSL) Transceivers. ITU-T G.992.1, 1999.\nITU 2003. Asymmetric Digital Subscriber Line (ADSL) Transceivers — Extended Bandwidth ADSL2 (ADSL2Plus). ITU-T \nG.992.5, 2003.\nITU 2005a. “ITU-T X.509, The Directory: Public-key and attribute certificate frameworks” (ago. 2005).\nITU 2005b. The Internet of Things. 2005. <http://www.itu.int/osg/spu/publications/internetofthings/InternetofThings_\nsummary.pdf>.\nITU 2012. <http://www.itu.int/>.\nITU Statistics 2012. International Telecommunications Union, “ICT Statistics”. <http://www.itu.int/ITU-D/icteye/Re-\nports.aspx>.\nITU 2011. ITU, “Measuring the Information Society, 2011”. <http://www.itu.int/ITU-D/ict/publications/idi/2011/index.\nhtml>.\nITU 2011. “The World in 2010: ICT Facts and Figures”. <http://www.itu.int/ITU-D/ict/material/Telecom09_flyer.pdf>.\nITU-T Q.2931 1995. “Recommendation Q.2931 (02/95) - Broadband Integrated Services Digital Network (B-ISDN) — \nDigital subscriber signalling system no. 2 (DSS 2) — User-network interface (UNI) — Layer 3 specification for basic call/\nconnection control”\n.\nIYER, S.; ZHANG, R.; MCKEOWN, N. “Routers with a Single Stage of Buffering”\n. Proc. 2002 ACM SIGCOMM (Pittsburgh, \nago. 2002).\nIYER, S.; KOMPELLA, R. R.; MCKEOWN, N. “Designing Packet Buffers for Router Line Cards”. IEEE Transactions on \nNetworking, v.16, n.3 (jun. 2008), p. 705–717.\nJACOBSON, V. “Congestion Avoidance and Control”. Proc. 1988 ACM SIGCOMM (Stanford, ago. 1988), p. 314–329.\nJAIN, R. “A timeout-based congestion control scheme for window flow-controlled networks”. IEEE Journal on Selected \nAreas in Communications SAC-4, 7 (out. 1986).\nJAIN, R. “A Delay-Based Approach for Congestion Avoidance in Interconnected Heterogeneous Computer Networks”\n. \nACM SIGCOMM Computer Communications Review, v.19, n.5 (1989), p. 56–71.\nJAIN, R. FDDI Handbook: High-Speed Networking Using Fiber and Other Media. Reading: Addison-Wesley, 1994.\nJAIN, R.; KALYANARAMAN, S.; FAHMY, S.; GOYAL, R.; KIM, S. “Tutorial Paper on ABR Source Behavior”. ATM \nForum/96-1270, out. 1996. <http://www.cse.wustl.edu/~jain/atmf/ftp/atm96-1270.pdf>.\nReferências  591 \nJAISWAL, S.; IANNACCONE, G.; DIOT, C.; KUROSE, J.; TOWSLEY, D. “Measurement and Classification of Out-of-Se-\nquence Packets in a Tier-1 IP backbone”\n. Proc. 2003 IEEE INFOCOM.\nJI, P.; GE, Z.; KUROSE, J.; TOWSLEY, D. “A Comparison of Hard-State and Soft-State Signaling Protocols”. Proc. 2003 \nACM SIGCOMM (Karlsruhe, ago. 2003).\nJIANG, W\n.; LENNOX, J.; SCHULZRINNE, H.; SINGH, K. “Towards Junking the PBX: Deploying IP Telephony”\n. NOSSDAV’01 \n(Port Jefferson, jun. 2001).\nJIMENEZ, D. “Outside Hackers Infiltrate MIT Network, Compromise Security”. The Tech, v.117, n.49 (out. 1997), p. 1, \n<http://www-tech.mit.edu/V117/N49/hackers.49n.html>.\nJIN, C.; WE, D. X.; LOW, S. “FAST TCP: Motivation, architecture, algorithms, performance”. Proc. 2004 IEEE INFOCOM \n(Hong Kong, mar. 2004).\nKAARANEN, H.; NAGHIAN, S.; LAITINEN, L.; AHTIAINEN, A.; NIEMI, V. Networks: Architecture, Mobility and Servi-\nces. Nova York: John Wiley & Sons, 2001.\nKAHN, D. The Codebreakers: The Story of Secret Writing. The Macmillan Company, 1967.\nKAHN, R. E.; GRONEMEYER, S.; BURCHFIEL, J.; KUNZELMAN, R. “Advances in Packet Radio Technology”. Proc. 1978 \nIEEE INFOCOM, 66, 11 (nov. 1978).\nKAMERMAN, A.; MONTEBAN, L.; “WaveLAN-II: A High–Performance Wireless LAN for the Unlicensed Band”\n. Bell \nLabs Technical Journal (Verão 1997), p. 118–133.\nKANGASHARJU, J.; ROSS, K. W.; ROBERTS, J. W. “Performance Evaluation of Redirection Schemes in Content Distribution \nNetworks”. Proc. 5th Web Caching and Content Distribution Workshop (Lisboa, maio 2000).\nKAR, K.; KODIALAM, M.; LAKSHMAN, T. V. “Minimum Interference Routing of Bandwidth Guaranteed Tunnels with \nMPLS Traffic Engineering Applications”\n. IEEE J. Selected Areas in Communications (dez. 2000).\nKARN, P.; PARTRIDGE, C. “Improving Round-Trip Time Estimates in Reliable Transport Protocols”. Proc. 1987 ACM \nSIGCOMM.\nKAROL, M.; HLUCHYJ, M.; MORGAN, A. “Input Versus Output Queuing on a Space-Division Packet Switch”\n. IEEE \nTransactions on Communications, v.35, n.12 (dez. 1987), p. 1347–1356.\nKATABI, D.; HANDLEY, M.; ROHRS, C. “Internet Congestion Control for Future High Bandwidth-Delay Product Envi-\nronments”. Proc. 2002 ACM SIGCOMM (Pittsburgh, ago. 2002).\nKATZELA, I.; SCHWARTZ. M.; “Schemes for Fault Identification in Communication Networks”. IEEE/ACM Transactions \non Networking, v.3, n.6 (dez. 1995), p. 753–764.\nKAUFMAN, C.; PERLMAN, R.; SPECINER, M. Network Security, Private Communication in a Public World. Englewood \nCliffs: Prentice Hall, 1995.\nKELLY, F. P.; MAULLOO, A.; TAN, D. “Rate control for communication networks: Shadow prices, proportional fairness \nand stability”\n. J. Operations Res. Soc., v.49, n.3 (mar. 1998), p. 237–252.\nKELLY, T. “Scalable TCP: improving performance in high speed wide area networks”. ACM SIGCOMM Computer Commu-\nnications Review, v.33, n.2 (abr. 2003), pp 83–91.\nKILKKI, K. Differentiated Services for the Internet. Indianapolis: Macmillan Technical Publishing, 1999.\nKIM, H.; RIXNER, S.; PAI, V. “Network Interface Data Caching”. IEEE Transactions on Computers, v.54, n.11 (nov. 2005), \np. 1394–1408.\nKIM, C.; CAESAR, M.; REXFORD, J. “Floodless in SEATTLE: A Scalable Ethernet Architecture for Large Enterprises”\n. \nProc. 2008 ACM SIGCOMM (Seattle, ago. 2008).\nKLEINROCK, L. “Information Flow in Large Communication Networks”. RLE Quarterly Progress Report, jul. 1961.\nKLEINROCK, L. 1964 Communication Nets: Stochastic Message Flow and Delay. Nova York: McGraw-Hill, 1964.\nKLEINROCK, L. Queuing Systems, v.1. Nova York: John Wiley, 1975.\n   Redes de computadores e a Internet\n592\nKLEINROCK, L.; TOBAGI, F. A. “Packet Switching in Radio Channels: Part I — Carrier Sense Multiple-Access Modes \nand Their Throughput-Delay Characteristics”. IEEE Transactions on Communications, v.23, n.12 (dez. 1975), p. 1400–1416.\nKLEINROCK, L. Queuing Systems, v.2. Nova York: John Wiley, 1976.\nKLEINROCK, L. “The Birth of the Internet”\n. <http://www.lk.cs.ucla.edu/LK/Inet/birth.html>.\nKOHLER, E.; HANDLEY, M.; FLOYD, S. “DDCP: Designing DCCP: Congestion Control Without Reliability”. Proc. 2006 \nACM SIGCOMM (Pisa, set. 2006).\nKOLDING, T.; PEDERSEN, K.; WIGARD, J.; FREDERIKSEN, F.; MOGENSEN, P. “High Speed Downlink Packet Access: \nWCDMA Evolution”\n. IEEE Vehicular Technology Society News (fev. 2003), p. 4–10.\nKOPONEN, T.; SHENKER, S.; BALAKRISHNAN, H.; FEAMSTER, N.; GANICHEV, I.; GHODSI, A.; GODFREY, P. B.; \nMCKEOWN, N.; PARULKAR, G.; RAGHAVAN, B.; REXFORD, J.; ARIANFAR, S.; KUPTSOV, D. “Architecting for Inno-\nvation”. ACM Computer Communications Review, 2011.\nKORHONEN, J. Introduction to 3G Mobile Communications. 2.ed. Artech House, 2003.\nKOZIOL, J. Intrusion Detection with Snort. Sams Publishing, 2003.\nKRISHNAMURTHY, B.; REXFORD, J. Web Protocols and Practice: HTTP/1.1, Networking Protocols, and Traffic Measure-\nment. Boston: Addison-Wesley, 2001a.\nKRISHNAMURTHY, B.; WILLS, C.; ZHANG, Y. “On the Use and Performance of Content Distribution Networks”\n. Proc. \n2001 ACM Internet Measurement Conference. (2001b)\nKRISHNAN, R.; MADHYASTHA, H.; SRINIVASAN, S.; JAIN, S.; KRISHNAMURTHY, A.; ANDERSON, T.; GAO, J. \n“Moving Beyond End-to-end Path Information to Optimize CDN Performance”. Proc. 2009 ACM Internet Measurement \nConference.\nKULKARNI, S.; ROSENBERG, C. “Opportunistic Scheduling: Generalizations to Include Multiple Constraints, Multiple \nInterfaces, and Short Term Fairness”. Wireless Networks, 11 (2005), 557–569.\nKUMAR, R.; ROSS, K.W. “Optimal Peer-Assisted File Distribution: Single and Multi-Class Problems”. IEEE Workshop on \nHot Topics in Web Systems and Technologies (Boston, 2006).\nLABOVITZ, C.; MALAN, G. R.; JAHANIAN, F. “Internet Routing Instability”. Proc. 1997 ACM SIGCOMM (Cannes, set. \n1997), p. 115–126.\nLABOVITZ, C.; IEKEL-JOHNSON, S.; MCPHERSON, D.; OBERHEIDE, J.; JAHANIAN, F. “Internet Inter-Domain Tra-\nffic”. Proc. 2010 ACM SIGCOMM.\nLABRADOR, M.; BANERJEE, S. “Packet Dropping Policies for ATM and IP Networks”. IEEE Communications Surveys, \nv.2, n.3 (Third Quarter 1999), p. 2–14.\nLACAGE, M.; MANSHAEI, M. H.; TURLETTI, T. “IEEE 802.11 Rate Adaptation: A Practical Approach”. ACM Int. Sym-\nposium on Modeling, Analysis, and Simulation of Wireless and Mobile Systems (MSWiM) (Veneza, out. 2004).\nLAKHINA, A.; CROVELLA, M.; DIOT, C. “Diagnosing Network-Wide Traffic Anomalies”. Proc. 2004 ACM SIGCOMM.\nLAKHINA, A.; CROVELLA, M.; DIOT, C. “Mining Anomalies Using Traffic Feature Distributions”. Proc. 2005 ACM SI-\nGCOMM.\nLAKSHMAN, T. V.; MADHOW, U. “The Performance of TCP/IP for Networks with High Bandwidth-Delay Products and \nRandom Loss”\n. IEEE/ACM Transactions on Networking, v.5, n.3 (1997), p. 336–350.\nLAM, S. “A Carrier Sense Multiple Access Protocol for Local Networks”. Computer Networks, v.4 (1980), p. 21–32.\nLARMOUTH, J. Understanding OSI, International Thomson Computer Press 1996. O Capítulo 8 deste livro trata de ASN.1 e \nestá disponível on-line em <http://www.salford.ac.uk/iti/books/osi/all.html#head8>.\nLARMOUTH, J. Understanding OSI, <http://www.business.salford.ac.uk/legacy/isi/books/osi/osi.html>.\nLAWTON, G. “Is IPv6 Finally Gaining Ground?” IEEE Computer Magazine (ago. 2001), p. 11–15.\nLEBLOND, S.; ZHANG, C.; LEGOUT, A.; ROSS, K. W\n.; DABBOUS, W\n. “Exploring the Privacy Limits of Real-Time Communica-\ntion Applications”\n. Proc. 2011 ACM Internet Measurement Conference (Berlim, 2011).\nReferências  593 \nLEBLOND, S.; ZHANG, C.; LEGOUT, A.; ROSS, K. W.; DABBOUS, W. “I Know Where You and What You Are Sharing: \nExploiting P2P Communications to Invade Users Privacy”. Proc. 2011 ACM Internet Measurement Conference (Berlim).\nLEIGHTON, T. “Improving Performance on the Internet”. Communications of the ACM, v.52, n.2 (fev. 2009), p. 44–51.\nLEINER, B.; CERF, V.; CLARK, D.; KAHN, R.; KLEINROCK, L.; LYNCH, D.; POSTEL, J.; ROBERTS, L.; WOOLF, S. “A \nBrief History of the Internet”\n. <http://www.isoc.org/internet/history/brief.html>.\nLEUNG, K.; V.; LI, O. K. “TCP in Wireless Networks: Issues, Approaches, and Challenges”. IEEE Commun.Surveys and \nTutorials, v.8, n.4 (2006), p. 64–79.\nLI, L.; ALDERSON, D.; WILLINGER, W.; DOYLE, J. “A First-Principles Approach to Understanding the Internet’s Router\n-Level Topology”\n. Proc. 2004 ACM SIGCOMM (Portland, ago. 2004).\nLI, J.; GUIDERO, M.; WU, Z.; PURPUS, E.; EHRENKRANZ, T. “BGP Routing Dynamics Revisited”. ACM Computer \nCommunication Review (abr. 2007).\nLIANG, J.; NAOUMOV, N.; ROSS, K. W. “The Index Poisoning Attack in P2P File- Sharing Systems”. Proc. 2006 IEEE \nINFOCOM (Barcelona, abr. 2006).\nLIN, Y.; CHLAMTAC, I. Wireless and Mobile Network Architectures. Nova York: John Wiley and Sons, 2001.\nLIOGKAS, N.; NELSON, R.; KOHLER, E.; ZHANG, L. “Exploiting BitTorrent For Fun (But Not Profit)”. 6th International \nWorkshop on Peer-to-Peer Systems (IPTPS 2006).\nLIU, B.; GOECKEL, D.; TOWSLEY, D. “TCP-Cognizant Adaptive Forward Error Correction in Wireless Networks”\n. Proc. \n2002 Global Internet.\nLIU, J.; MATTA, I.; CROVELLA, M. “End-to-End Inference of Loss Nature in a Hybrid Wired/Wireless Environment”\n. \nProc. WiOpt’03: Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks.\nLIU, Z.; DHUNGEL, P.; WU, D.; ZHANG, C.; ROSS, K. W. “Understanding and Improving Incentives in Private P2P \nCommunities”\n. ICDCS (Gênova, 2010).\nLOCHER, T.; MOOR, P.; SCHMID, S.; WATTENHOFER, R. “Free Riding in BitTorrent is Cheap”\n. Proc. ACM HotNets \n2006 (Irvine, nov. 2006).\nLUI, J.; MISRA, V.; RUBENSTEIN, D. “On the Robustness of Soft State Protocols”. Proc. IEEE Int. Conference on Network \nProtocols (ICNP ’04), p. 50–60.\nLUOTONEN, A. Web Proxy Servers. Englewood Cliffs: Prentice Hall, 1998.\nLYNCH, D.; ROSE, M. Internet System Handbook. Reading: Addison-Wesley, 1993.\nMACEDONIA, M.; BRUTZMAN, D. “MBone Provides Audio and Video Across the Internet”. IEEE Computer Magazine. \nv.27, n.4 (abr. 1994), p. 30–36.\nMAHDAVI, J.; FLOYD, S. “TCP-Friendly Unicast Rate-Based Flow Control”. Nota não publicada (jan.1997).\nMALWARE 2006. Computer Economics, “2005 Malware Report: The Impact of Malicious Code Attacks”\n. <http://www.\ncomputereconomics.com>.\nMANET 2012. IETF Mobile Ad-hoc Networks Working Group. <http://www.ietf.org/html.charters/manet-charter.html>.\nMAO, Z. M.; CRANOR, C.; BOUDLIS, F.; RABINOVICH, M.; SPATSCHECK, O.; WANG, J. “A Precise and Efficient \nEvaluation of the Proximity Between Web Clients and Their Local DNS Servers”. Proc. 2002 USENIX ATC.\nMAXMIND 2012. <http://www.maxmind.com/app/ip-location>.\nMAYMOUNKOV, P.; MAZIÈRES, D. “Kademlia: A Peer-to-Peer Information System Based on the XOR Metric”\n. Proceedings \nof the 1st International Workshop on Peerto-Peer Systems (IPTPS ’02) (mar. 2002), p. 53–65.\nMCKEOWN, N.; IZZARD, M.; MEKKITTIKUL, A.; ELLERSICK, W.; HOROWITZ, M. “The Tiny Tera: A Packet Switch \nCore”\n. IEEE Micro Magazine (jan.–fev. 1997a).\nMCKEOWN, N. “A Fast Switched Backplane for a Gigabit Switched Router”. Business Communications Review, v.27, n.12. \n<http://tiny-tera.stanford.edu/~nickm/papers/cisco_fasts_wp.pdf>. (1997b)\n   Redes de computadores e a Internet\n594\nMCKEOWN, N.; ANDERSON, T.; BALAKRISHNAN, H.; PARULKAR, G.; PETERSON, L.; REXFORD, J.; SHENKER, S.; \nTURNER, J. “OpenFlow: Enabling Innovation in Campus Networks”. ACM SIGCOMM Computer Communication Review, \nv.38, n.2 (abr. 2008).\nMCQUILLAN, J.; RICHER, I.; ROSEN, E. “The New Routing Algorithm for the Arpanet”. IEEE Transactions on Commu-\nnications, v.28, n.5 (maio 1980), p. 711–719.\nMEDHI, D.; TIPPER D. (eds.). “Special Issue: Fault Management in Communication Networks”. Journal of Network and \nSystems Management, v.5. n.2 (jun.1997).\nMETCALFE, R. M.; BOGGS, D. R. “Ethernet: Distributed Packet Switching for Local Computer Networks”. Communica-\ntions of the Association for Computing Machinery, v.19, n.7 (jul. 1976), p. 395–404.\nMEYERS, A.; NG, T.; ZHANG, H. “Rethinking the Service Model: Scaling Ethernet to a Million Nodes”. ACM Hotnets \nConference, 2004.\nMFA FORUM 2012. IP/MPLS Forum homepage, <http://www.ipmplsforum.org/>.\nMIRKOVIC, J.; DIETRICH, S.; DITTRICH, D.; REIHER, P. Internet Denial of Service: Attack and Defense Mechanisms. \nPrentice Hall, 2005.\nMOCKAPETRIS, P. V.; DUNLAP, K. J. “Development of the Domain Name System”. Proc. 1988 ACM SIGCOMM (Stan-\nford, ago. 1988).\nMOCKAPETRIS, P. Sigcomm Award Lecture. Vídeo disponível em <http://www.postel.org/sigcomm>.\nMOGUL, J. “TCP offload is a dumb idea whose time has come”\n. Proc. HotOS IX: The 9th Workshop on Hot Topics in Operating \nSystems (2003), USENIX Association.\nMOLINARO-FERNANDEZ, P.; MCKEOWN, N.; ZHANG, H. “Is IP Going to Take Over the World (of Communica-\ntions)?”. Proc. 2002 ACM Hotnets.\nMOLLE, M. L.; SOHRABY, K.; VENETSANOPOULOS, A. N. “Space-Time Models of Asynchronous CSMA Protocols for \nLocal Area Networks”\n. IEEE Journal on Selected Areas in Communications, v.5, n.6 (1987), p. 956–968.\nMOORE, D.; VOELKER, G.; SAVAGE, S. “Inferring Internet Denial of Service Activity”. Proc. 2001 USENIX Security \nSymposium (Washington, ago. 2001).\nMOORE, D.; PAXSON, V.; SAVAGE, S.; SHANNON, C.; STANIFORD, S.; WEAVER, N. “Inside the Slammer Worm”\n. 2003 \nIEEE Security and Privacy Conference.\nMOSHCHUK, A.; BRAGIN, T.; GRIBBLE, S.; LEVY, H. “A Crawler-based Study of Spyware on the Web”. Proc. 13th Annu-\nal Network and Distributed Systems Security Symposium (NDSS 2006) (San Diego, fev. 2006).\nMOTOROLA 2007. “Long Term Evolution (LTE): A Technical Overview”. <http://www.motorola.com/staticfiles/\nBusiness/Solutions/Industry%20Solutions/Service%20P \nroviders/Wireless%20Operators/LTE/_Document/Static%20\nFiles/6834_MotDoc_New.pdf>.\nMOULY, M.; PAUTET, M. The GSM System for Mobile Communications, Cell and Sys, Palaiseau, França, 1992.\nMOY, J. OSPF: Anatomy of An Internet Routing Protocol. Reading: Addison-Wesley, 1998.\nMUDIGONDA, J.; YALAGANDULA, P.; MOGUL, J. C.; STIEKES, B.; POUFFARY, Y. “NetLord: A Scalable Multi-Tenant \nNetwork Architecture for Virtualized Datacenters”. Proc. 2011 ACM SIGCOMM.\nMUKHERJEE, B. Optical Communication Networks. McGraw-Hill, 1997.\nMUKHERJEE, B. Optical WDM Networks. Springer, 2006.\nMYSORE, R. N.; PAMBORIS, A.; FARRINGTON, N.; HUANG, N.; MIRI, P.; RADHAKRISHNAN, S.; SUBRAMANYA, \nV.; VAHDAT, A. “PortLand: A Scalable Fault-Tolerant Layer 2 Data Center Network Fabric”. Proc. 2009 ACM SIGCOMM.\nNADEL, B. “4G shootout: Verizon LTE vs. Sprint WiMax”. Computerworld, 3 fev. 2011.\nNAHUM, E.; BARZILAI, T.; KANDLUR, D. “Performance Issues in WWW Servers”\n. IEEE/ACM Transactions on Networking, \nv.10, n.1 (fev. 2002).\nNAOUMOV, N.; ROSS, K. W. “Exploiting P2P Systems for DDoS Attacks”. Intl Workshop on Peer-to-Peer Information \nManagement (Hong Kong, maio 2006).\nReferências  595 \nNEGLIA, G.; REINA, G.; ZHANG, H.; TOWSLEY, D.; VENKATARAMANI, A.; DANAHER, J. “Availability in BitTorrent \nSystems”. Proc. 2007 IEEE INFOCOM.\nNEUMANN, R. “Internet Routing Black Hole”. The Risks Digest: Forum on Risks to the Public in Computers and Related \nSystems, v.19, n.12 (maio 1997). <http://catless.ncl.ac.uk/Risks/19.12.html#subj1.1>.\nNEVILLE-NEIL, G. “Whither Sockets?” Communications of the ACM, v.52, n.6 (jun. 2009), p. 51–55.\nNICHOLSON, A.; CHAWATHE, Y.; CHEN, M.; NOBLE, B.; WETHERALL, D. “Improved Access Point Selection”\n. Proc. \n2006 ACM Mobisys Conference (Uppsala, 2006).\nNIELSEN, H. F.; GETTYS, J.; BAIRD-SMITH, A.; PRUD’HOMMEAUX, E.; LIE, H. W.; LILLEY, C. “Network Performance \nEffects of HTTP/1.1, CSS1, and PNG”\n. W3C Document, 1997 (veja também em Proc. 1997 ACM SIGCOM (Cannes, set. \n1997), p. 155–166.\nNIST 2001. National Institute of Standards and Technology, “Advanced Encryption Standard (AES)”. Federal Information \nProcessing Standards 197, nov. 2001, <http://csrc.nist.gov/publications/fips/fips197/fips-197.pdf>.\nNIST IPv6 2012. National Institute of Standards, “Estimating IPv6 & DNSSEC Deployment SnapShots”. <http://usgv6-de-\nploymon.antd.nist.gov/snap-all.html>.\nNMAP 2012. Nmap homepage. <http://www.insecure.com/nmap>.\nNONNENMACHER, J.; BIERSAK, E.; TOWSLEY, D. “Parity-Based Loss Recovery for Reliable Multicast Transmission”\n. \nIEEE/ACM Transactions on Networking, v.6, n.4 (ago. 1998), p. 349–361.\nNTIA 1998. National Telecommunications and Information Administration (NTIA), US Department of Commerce, \n“Management of Internet names and addresses”. Docket Number: 980212036-8146-02. <http://www.ntia.doc.gov/ntia-\nhome/domainname/6_5_98dns.htm>.\nO’DELL, M. “Network Front-End Processors, Yet Again”. Communications of the ACM, v.52, n.6 (jun. 2009), p. 46–50.\nOID Repository 2012. <http://www.oid-info.com/>.\nOSI 2012. <http://www.iso.org/iso/en/ISOOnline.frontpage>.\nOSS 2012. OSS Nokalva, “ASN.1 Resources”\n. <http://www.oss.com/asn1/>.\nPADHYE, J.; FIROIU, V\n.; TOWSLEY, D.; KUROSE, J. “Modeling TCP Reno Performance: A Simple Model and its Empirical \nValidation”\n. IEEE/ACM Transactions on Networking, v.8 n.2 (abr. 2000), p. 133–145.\nPADHYE, J.; FLOYD, S. “On Inferring TCP Behavior”\n. Proc. 2001 ACM SIGCOMM (San Diego, ago. 2001).\nPAN, P.; SCHULZRINNE, H. “Staged Refresh Timers for RSVP”. Proc. 2nd Global Internet Conference (Phoenix, dez. \n1997).\nPAREKH, A.; GALLAGHER, R. “A generalized processor sharing approach to flow control in integrated services \nnetworks: the single-node case”\n. IEEE/ACM Transactions on Networking, v.1, n.3 (jun.1993), p. 344–357.\nPARTRIDGE, C.; PINK, S. “An Implementation of the Revised Internet Stream Protocol (ST-2)”. Journal of Internetwork-\ning: Research and Experience, v.3, n.1 (mar. 1992).\nPARTRIDGE, C. et al. “A Fifty Gigabit per second IP Router”. IEEE/ACM Transactions on Networking, v.6, n.3 (jun.1998), \np. 237–248.\nPATHAK, A.; WANG, Y. A.; HUANG, C.; GREENBERG, A.; HU, Y. C.; LI, J.; ROSS, K. W. “Measuring and Evaluating TCP \nSplitting for Cloud Services”\n. Passive and Active Measurement (PAM) Conference (Zurich, 2010).\nPAXSON, V. “End-to-End Internet Packet Dynamics”. Proc. 1997 ACM SIGCOMM (Cannes, set. 1997).\nPERKINS, A. “Networking with Bob Metcalfe”. The Red Herring Magazine (nov. 1994).\nPERKINS, C.; HODSON, O.; HARDMAN, V. “A Survey of Packet Loss Recovery Techniques for Streaming Audio”\n. IEEE \nNetwork Magazine (set./out. 1998), p. 40–47.\nPERKINS, C. Mobile IP: Design Principles and Practice. Reading: Addison-Wesley, 1998.\nPERKINS, C. Ad Hoc Networking. Reading: Addison-Wesley, 2000.\n   Redes de computadores e a Internet\n596\nPERLMAN, R. Interconnections: Bridges, Routers, Switches, and Internetworking Protocols. 2. ed. Reading: Addison-Wesley \nProfessional Computing Series, 1999.\nPGPI 2012. <http://www.pgpi.org>.\nPHIFER, L. “The Trouble with NAT”\n. The Internet Protocol Journal, v.3, n.4 (dez. 2000). <http://www.cisco.com/warp/\npublic/759/ipj_3-4/ipj_3-4_nat.html>.\nPIATEK, M.; ISDAL, T.; ANDERSON, T.; KRISHNAMURTHY, A.; VENKATARAMANI, A. “Do Incentives Build Robust-\nness in Bittorrent?”\n. Proc. NSDI (2007). \nPIATEK, M.; ISDAL, T.; KRISHNAMURTHY, A.; ANDERSON, T. “One hop Reputations for Peer-to-peer File Sharing \nWorkloads”\n. Proc. NSDI (2008).\nPICKHOLTZ, R.; SCHILLING, D.; MILSTEIN, L. “Theory of Spread Spectrum Communication — a Tutorial”. IEEE Tran-\nsactions on Communications, v.30, n.5 (maio 1982), p. 855–884.\nPINGPLOTTER 2012. PingPlotter homepage, <http://www.pingplotter.com>.\nPISCATELLO, D.; LYMAN CHAPIN, A. Open Systems Networking. Reading: Addison-Wesley, 1993.\nPOINT TOPIC 2006. Point Topic Ltd., World Broadband Statistics Q1 2006. <http://www.pointtopic.com>.\nPOTAROO 2012. “Growth of the BGP Table–1994 to Present”. <http://bgp.potaroo.net/>.\nPPLIVE 2012. PPLive homepage. <http://www.pplive.com>.\nQUAGGA 2012. “Quagga Routing Suite”\n. <http://www.quagga.net/>.\nQUITTNER, J.; SLATALLA, M. Speeding the Net: The Inside Story of Netscape and How it Challenged Microsoft. Atlantic \nMonthly Press, 1998.\nQUOVA 2012. <www.quova.com>.\nRAICIU, C.; BARRE, S.; PLUNTKE, C.; GREENHALGH, A.; WISCHIK, D.; HANDLEY, M. “Improving Datacenter \nPerformance and Robustness with Multipath TCP”. Proc. 2011 ACM SIGCOMM.\nRAMAKRISHNAN, K. K.; JAIN, R. “A Binary Feedback Scheme for Congestion Avoidance in Computer Networks”\n. ACM \nTransactions on Computer Systems, v.8, n.2 (maio 1990), p. 158–181.\nRAMAN, S.; MCCANNE, S. “A Model, Analysis, and Protocol Framework for Soft State-based Communication”\n. Proc. \n1999 ACM SIGCOMM (Boston, ago. 1999).\nRAMAN, B.; CHEBROLU, K. “Experiences in using WiFi for Rural Internet in India”. IEEE Communications Magazine, \nSpecial Issue on New Directions in Networking Technologies in Emerging Economies (jan. 2007).\nRAMASWAMI, R.; SIVARAJAN, K.; SASAKI, G. Optical Networks: A Practical Perspective. Morgan Kaufman Publishers, \n2010.\nRAMJEE, R.; KUROSE, J.; TOWSLEY, D.; SCHULZRINNE, H. “Adaptive Playout Mechanisms for Packetized Audio \nApplications in Wide-Area Networks”\n. Proc. 1994 IEEE INFOCOM.\nRAO, K. R.; HWANG, J. J. Techniques and Standards for Image, Video and Audio Coding. Englewood Cliffs: Prentice Hall, \n1996.\nRAO, A. S.; LIM, Y. S.; BARAKAT, C.; LEGOUT, A.; TOWSLEY, D.; DABBOUS, W. “Network Characteristics of Video \nStreaming Traffic”\n. Proc. 2011 ACM CoNEXT (Tóquio).\nRAT 2012. Robust Audio Tool, <http://www-mice.cs.ucl.ac.uk/multimedia/software/rat/>.\nRATNASAMY, S.; FRANCIS, P.; HANDLEY, M.; KARP, R.; SHENKER, S. “A Scalable Content-Addressable Network”\n. \nProc. 2001 ACM SIGCOMM (San Diego, ago. 2001).\nREN, S.; GUO L.; ZHANG, X. “ASAP: an AS-aware peer-relay protocol for high quality VoIP”. Proc. 2006 IEEE ICDCS \n(Lisboa, jul. 2006).\nRESCORLA, E. SSL and TLS: Designing and Building Secure Systems. Boston: Addison-Wesley, 2001.\nRFC 001. CROCKER, S. “Host Software”\n. RFC 001 (the very first RFC!).\nRFC 768. POSTEL, J. “User Datagram Protocol”. RFC 768, ago. 1980.\nReferências  597 \nRFC 789. ROSEN, E. “Vulnerabilities of Network Control Protocols”. RFC 789.\nRFC 791. POSTEL, J. “Internet Protocol: DARPA Internet Program Protocol Specification”. RFC 791, set. 1981.\nRFC 792. POSTEL, J. “Internet Control Message Protocol”. RFC 792, set. 1981.\nRFC 793. POSTEL, J. “Transmission Control Protocol”. RFC 793, set. 1981.\nRFC 801. POSTEL, J. “NCP/TCP Transition Plan”. RFC 801, nov. 1981.\nRFC 826. PLUMMER, D. C. “\nAn Ethernet Address Resolution Protocol — or — Converting Network Protocol Addresses to 48 bit \nEthernet Address for Transmission on Ethernet Hardware”\n. RFC 826, nov. 1982.\nRFC 829. CERF, V. “Packet Satellite Technology Reference Sources”. RFC 829, nov. 1982.\nRFC 854. POSTEL, J.; REYNOLDS, J. “TELNET Protocol Specification”. RFC 854, maio 1993.\nRFC 950. MOGUL, J.; POSTEL, J. “Internet Standard Subnetting Procedure”. RFC 950, ago. 1985.\nRFC 959. POSTEL J.; REYNOLDS, J. “File Transfer Protocol (FTP)”. RFC 959, out. 1985.\nRFC 977. KANTOR, B.; LAPSLEY, P. “Network News Transfer Protocol”. RFC 977, fev. 1986.\nRFC 1028. DAVIN, J.; CASE, J. D.; FEDOR, M.; SCHOFFSTALL, M. “A Simple Gateway Monitoring Protocol”. RFC 1028, \nnov. 1987.\nRFC 1034. MOCKAPETRIS, P. V. “Domain Names — Concepts and Facilities”. RFC 1034, nov. 1987.\nRFC 1035 MOCKAPETRIS, P. V. “Domain Names — Implementation and Specification”. RFC 1035, nov. 1987.\nRFC 1058. HENDRICK, C. L. “Routing Information Protocol”. RFC 1058, jun.1988.\nRFC 1071. BRADEN, R.; BORMAN D.; PARTRIDGE, C. “Computing The Internet Checksum”. RFC 1071, set. 1988.\nRFC 1075. WAITZMAN, D.; PARTRIDGE, C.; DEERING, S. “Distance Vector Multicast Routing Protocol”. RFC 1075, \nnov. 1988.\nRFC 1112. DEERING, S. “Host Extension for IP Multicasting”. RFC 1112, ago. 1989.\nRFC 1122. BRADEN, R. “Requirements for Internet Hosts — Communication Layers”. RFC 1122, out. 1989.\nRFC 1123. BRADEN, R. (ed.). “Requirements for Internet Hosts — Application and Support”\n. RFC-1123, out. 1989.\nRFC 1142. ORAN, D. “OSI IS-IS Intra-Domain Routing Protocol”. RFC 1142, fev. 1990.\nRFC 1190. TOPOLCIC, C. “Experimental Internet Stream Protocol: Version 2 (ST-II)”\n. RFC 1190, out. 1990.\nRFC 1191. MOGUL, J.; DEERING, S. “Path MTU Discovery”. RFC 1191, nov. 1990.\nRFC 1213. MCCLOGHRIE, K.; ROSE, M. T. “Management Information Base for Network Management of TCP/IP-based \ninternets: MIB-II”\n. RFC 1213, mar. 1991. \nRFC 1256. DEERING, S. “ICMP Router Discovery Messages”. RFC 1256, set. 1991.\nRFC 1320. RIVEST, R. “The MD4 Message-Digest Algorithm”. RFC 1320, abr. 1992.\nRFC 1321. RIVEST, R. “The MD5 Message-Digest Algorithm”\n. RFC 1321, abr. 1992.\nRFC 1323. JACOBSON, V.; BRADEN, S.; BORMAN, D. “TCP Extensions for High Performance”. RFC 1323, maio 1992.\nRFC 1422. KENT, S. “Privacy Enhancement for Internet Electronic Mail: Part II: Certificate-Based Key Management”. \nRFC 1422.\nRFC 1546. PARTRIDGE, C.; MENDEZ, T.; MILLIKEN, W. “Host Anycasting Service”. RFC 1546, 1993.\nRFC 1547. PERKINS, D. “Requirements for an Internet Standard Point-to-Point Protocol”. RFC 1547, dez. 1993.\nRFC 1584. MOY, J. “Multicast Extensions to OSPF”. RFC 1584, mar. 1994.\nRFC 1633. BRADEN, R.; CLARK, D.; SHENKER, S. “Integrated Services in the Internet Architecture: an Overview”\n. RFC \n1633, jun. 1994.\nRFC 1636. BRADEN, R.; CLARK, D.; CROCKER, S.; HUITEMA, C. “Report of IAB Workshop on Security in the Internet \nArchitecture”\n. RFC 1636, nov. 1994.\nRFC 1661. SIMPSON, W. (ed.), “The Point-to-Point Protocol (PPP)”. RFC 1661, jul. 1994.\n   Redes de computadores e a Internet\n598\nRFC 1662. SIMPSON, W. (ed.), “PPP in HDLC-Like Framing”. RFC 1662, jul. 1994.\nRFC 1700. REYNOLDS, J.; POSTEL, J. “Assigned Numbers”. RFC 1700, out. 1994.\nRFC 1752. BRADNER, S.; MANKIN, A. “The Recommendations for the IP Next Generation Protocol”. RFC 1752, \njan. 1995.\nRFC 1918. REKHTER, Y.; MOSKOWITZ, B.; KARRENBERG, D.; DE GROOT, G. J.; LEAR, E. “Address Allocation for \nPrivate Internets”\n. RFC 1918, fev. 1996. \nRFC 1930. HAWKINSON, J.; BATES, T. “Guidelines for Creation, Selection, and Registration of an Autonomous System \n(AS)”. RFC 1930, mar. 1996.\nRFC 1938. HALLER, N.; METZ, C. “A One-Time Password System”. RFC 1938, maio 1996.\nRFC 1939. MYERS J.; ROSE, M. “Post Office Protocol—Version 3”. RFC 1939, maio 1996.\nRFC 1945. BERNERS-LEE, T.; FIELDING, R.; FRYSTYK, H. “Hypertext Transfer Protocol — HTTP/1.0”. RFC 1945, \nmaio 1996.\nRFC 2003. PERKINS, C. “IP Encapsulation within IP”. RFC 2003, out. 1996.\nRFC 2004. PERKINS, C. “Minimal Encapsulation within IP”. RFC 2004, out. 1996.\nRFC 2018. MATHIS, M.; MAHDAVI, J.; FLOYD, S.; ROMANOW, A. “TCP Selective Acknowledgment Options”\n. RFC \n2018, out. 1996.\nRFC 2050. HUBBARD, K.; KOSTERS, M.; CONRAD, D.; KARRENBERG, J.; POSTEL, D. “Internet Registry IP Allocation \nGuidelines”\n. RFC 2050, nov. 1996.\nRFC 2104. KRAWCZYK, H.; BELLARE, M.; CANETTI, R. “HMAC: Keyed-Hashing for Message Authentication”\n. RFC \n2104, fev. 1997.\nRFC 2131. DROMS, R. “Dynamic Host Configuration Protocol”. RFC 2131, mar. 1997.\nRFC 2136. VIXIE, P.; THOMSON, S.; REKHTER, Y.; BOUND, J. “Dynamic Updates in the Domain Name System”\n. RFC \n2136, abr. 1997.\nRFC 2153. SIMPSON, W. “PPP Vendor Extensions”. RFC 2153, maio 1997.\nRFC 2205. BRADEN, R.; ZHANG, L.; BERSON, S.; HERZOG, S.; JAMIN, S. “Resource ReSerVation Protocol (RSVP)—\nVersion 1 Functional Specification”\n. RFC 2205, set. 1997.\nRFC 2210. WROCLAWSKI, J. “The Use of RSVP with IETF Integrated Services”. RFC 2210, set. 1997.\nRFC 2211. WROCLAWSKI, J. “Specification of the Controlled-Load Network Element Service”. RFC 2211, set. 1997.\nRFC 2215. SHENKER, S.;WROCLAWSKI, J. “General Characterization Parameters for Integrated Service Network Ele-\nments”. RFC 2215, set. 1997.\nRFC 2326. SCHULZRINNE, H.; RAO, A.; LANPHIER, R. “Real Time Streaming Protocol (RTSP)”. RFC 2326, abr. 1998.\nRFC 2328. MOY, J. “OSPF Version 2”\n. RFC 2328, abr. 1998.\nRFC 2420. KUMMERT, H. “The PPP Triple-DES Encryption Protocol (3DESE)”. RFC 2420, set. 1998.\nRFC 2453. MALKIN, G. “RIP Version 2”\n. RFC 2453, nov. 1998.\nRFC 2460. DEERING, S.; HINDEN, R. “Internet Protocol, Version 6 (IPv6) Specification”. RFC 2460, dez. 1998.\nRFC 2475. BLAKE, S.; BLACK, D.; CARLSON, M.; DAVIES, E.; WANG, Z.; WEISS, W. “An Architecture for Differentiated \nServices”. RFC 2475, dez. 1998.\nRFC 2578. McCLOGHRIE, K.; PERKINS, D.; SCHOENWAELDER, J. “Structure of Management Information Version 2 \n(SMIv2)”. RFC 2578, abr. 1999.\nRFC 2579. MCCLOGHRIE, K.; PERKINS, D.; SCHOENWAELDER, J. “Textual Conventions for SMIv2”. RFC 2579, \nabr. 1999.\nRFC 2580. MCCLOGHRIE, K.; PERKINS, D.; SCHOENWAELDER, J. “Conformance Statements for SMIv2”. RFC 2580, \nabr. 1999.\nReferências  599 \nRFC 2597. HEINANEN, J.; BAKER, F.; WEISS, W.; WROCLAWSKI, J. “Assured Forwarding PHB Group”. RFC 2597, jun. \n1999.\nRFC 2616. FIELDING, R.; GETTYS, J.; MOGUL, J.; FRYSTYK, H.; MASINTER, L.; LEACH, P.; BERNERS-LEE, T.; FIEL-\nDING, R. “Hypertext Transfer Protocol — HTTP/1.1”. RFC 2616, jun. 1999.\nRFC 2663. SRISURESH, P.; HOLDREGE, M. “IP Network Address Translator (NAT) Terminology and Considerations”\n. \nRFC 2663.\nRFC 2702. AWDUCHE, D.; MALCOLM, J.; AGOGBUA, J.; O’DELL, M.; MCMANUS, J. “Requirements for Traffic Engi-\nneering Over MPLS”\n. RFC 2702, set. 1999.\nRFC 2827. FERGUSON, P.; SENIE, D. “Network Ingress Filtering: Defeating Denial of Service Attacks which Employ IP \nSource Address Spoofing”\n. RFC 2827, maio 2000.\nRFC 2865. RIGNEY, C.; WILLENS, S.; RUBENS, A.; SIMPSON, W. “Remote Authentication Dial In User Service (RA-\nDIUS)”. RFC 2865, jun. 2000.\nRFC 2961. BERGER, L.; GAN, D.; SWALLOW, G.; PAN, P.; TOMMASI, F.; MOLENDINI, S. “RSVP Refresh Overhead \nReduction Extensions”\n. RFC 2961, abr. 2001.\nRFC 3007. WELLINGTON, B. “Secure Domain Name System (DNS) Dynamic Update”. RFC 3007, nov. 2000.\nRFC 3022. SRISURESH, P.; EGEVANG, K. “Traditional IP Network Address Translator (Traditional NAT)”. RFC 3022, \njan. 2001.\nRFC 3022. SRISURESH, P.; EGEVANG, K. “Traditional IP Network Address Translator (Traditional NAT)”. RFC 3022, \njan. 2001.\nRFC 3031. ROSEN, E.; VISWANATHAN, A.; CALLON, R. “Multiprotocol Label Switching Architecture”. RFC 3031, jan. \n2001.\nRFC 3032. ROSEN, E.; TAPPAN, D.; FEDORKOW, G.; REKHTER, Y.; FARINACCI, D.; LI, T.; CONTA, A. “MPLS Label \nStack Encoding”\n. RFC 3032, jan. 2001.\nRFC 3052. EDER, M.; NAG, S. “Service Management Architectures Issues and Review”. RFC 3052, jan. 2001.\nRFC 3139. SANCHEZ, L.; MCCLOGHRIE, K.; SAPERIA, J. “Requirements for Configuration Management of IP-Based \nNetworks”. RFC 3139, jun. 2001.\nRFC 3168. RAMAKRISHNAN, K.; FLOYD, S.; BLACK, D. “The Addition of Explicit Congestion Notification (ECN) to \nIP”. RFC 3168, set. 2001.\nRFC 3209. AWDUCHE, D.; BERGER, L.; GAN, D.; LI, T.; SRINIVASAN, V.; SWALLOW, G. “RSVP-TE: Extensions to \nRSVP for LSP Tunnels”\n. RFC 3209, dez. 2001.\nRFC 3221. HUSTON, G. “Commentary on Inter-Domain Routing in the Internet”\n. RFC 3221, dez. 2001.\nRFC 3232. REYNOLDS, J. “Assigned Numbers: RFC 1700 is Replaced by an On-line Database”. RFC 3232, jan. 2002.\nRFC 3246. DAVIE, B.; CHARNY, A.; BENNET, J. C. R.; BENSON, K.; LE BOUDEC, J. Y.; COURTNEY, W.; DAVARI, S.; \nFIROIU, V.; STILIADIS, D. “An Expedited Forwarding PHB (Per-Hop Behavior)”. RFC 3246, mar. 2002.\nRFC 3260. GROSSMAN, D. “New Terminology and Clarifications for Diffserv”. RFC 3260, abr. 2002.\nRFC 3261. ROSENBERG, J.; SCHULZRINNE, H.; CARMARILLO, G.; JOHNSTON, A.; PETERSON, J.; SPARKS, R.; \nHANDLEY, M.; SCHOOLER, E. “SIP: Session Initiation Protocol”. RFC 3261, jul. 2002.\nRFC 3272. BOYLE, J.; GILL, V.; HANNAN, A.; COOPER, D.; AWDUCHE, D.; CHRISTIAN, B.; LAI, W. S. “Overview and \nPrinciples of Internet Traffic Engineering”\n. RFC 3272, maio 2002.\nRFC 3286. ONG, L.; YOAKUM, J. “An Introduction to the Stream Control Transmission Protocol (SCTP)”. RFC 3286, \nmaio 2002.\nRFC 3346. BOYLE, J.; GILL, V.; HANNAN, A.; COOPER, D.; AWDUCHE, D.; CHRISTIAN, B.; LAI, W. S. “Applicability \nStatement for Traffic Engineering with MPLS”. RFC 3346, ago. 2002.\nRFC 3376. CAIN, B.; DEERING, S.; KOUVELAS, I.; FENNER, B.; THYAGARAJAN, A. “Internet Group Management \nProtocol, Version 3”\n. RFC 3376, out. 2002. \n   Redes de computadores e a Internet\n600\nRFC 3390. ALLMAN, M.; FLOYD, S.; PARTRIDGE, C. “Increasing TCP’s Initial Window”. RFC 3390, out. 2002.\nRFC 3410. CASE, J.; MUNDY, R.; PARTAIN, D. “Introduction and Applicability Statements for Internet Standard Mana-\ngement Framework”\n. RFC 3410, dez. 2002.\nRFC 3411. HARRINGTON, D.; PRESUHN, R.; WIJNEN, B. “An Architecture for Describing Simple Network Manage-\nment Protocol (SNMP) Management Frameworks”. RFC 3411, dez. 2002.\nRFC 3414. BLUMENTHAL U.; WIJNEN, B. “User-based Security Model (USM) for Version 3 of the Simple Network \nManagement Protocol (SNMPv3)”\n. RFC 3414, dez. 2002.\nRFC 3415. WIJNEN, B.; PRESUHN, R.; MCCLOGHRIE, K. “View-based Access Control Model (VACM) for the Simple \nNetwork Management Protocol (SNMP)”\n. RFC 3415, dez. 2002.\nRFC 3416. PRESUHN, R.; CASE, J.; MCCLOGHRIE, K.; ROSE, M.; WALDBUSSER, S. “Version 2 of the Protocol Opera-\ntions for the Simple Network Management Protocol (SNMP)”. dez. 2002.\nRFC 3439. BUSH, R.; MEYER, D. “Some internet architectural guidelines and philosophy”. RFC 3439, dez. 2003.\nRFC 3447. JONSSON, J.; KALISKI, B. “Public-Key Cryptography Standards (PKCS) #1: RSA Cryptography Specifications \nVersion 2.1”\n. RFC 3447, fev. 2003.\nRFC 3468. ANDERSSON, L.; SWALLOW, G. “The Multiprotocol Label Switching (MPLS) Working Group Decision on \nMPLS Signaling Protocols”\n. RFC 3468, fev. 2003.\nRFC 3469. SHARMA, V., (ed.), HELLSTRAND, F. (ed.), “Framework for Multi-Protocol Label Switching (MPLS)-based \nRecovery”. RFC 3469, fev. 2003. <ftp://ftp.rfc-editor.org/in-notes/rfc3469.txt>.\nRFC 3501. CRISPIN, M. “Internet Message Access Protocol — Version 4rev1”. RFC 3501, mar. 2003.\nRFC 3550. SCHULZRINNE, H.; CASNER, S.; FREDERICK, R.; JACOBSON, V. “RTP: A Transport Protocol for Real-Time \nApplications”\n. RFC 3550, jul. 2003.\nRFC 3569. BHATTACHARYYA, S. (ed.). “An Overview of Source-Specific Multicast (SSM)”. RFC 3569, jul. 2003.\nRFC 3588. CALHOUN, P.; LOUGHNEY, J.; GUTTMAN, E.; ZORN, G.; ARKKO, J. “Diameter Base Protocol”. RFC 3588, \nset. 2003.\nRFC 3618. FENNER, B.; MEYER, D. (ed.). “Multicast Source Discovery Protocol (MSDP)”. RFC 3618, out. 2003.\nRFC 3649. FLOYD, S. “High Speed TCP for Large Congestion Windows”. RFC 3649, dez. 2003.\nRFC 3748. ABOBA, B.; BLUNK, L.; VOLLBRECHT, J.; CARLSON, J.; LEVKOWETZ, H. Ed., “Extensible Authentication \nProtocol (EAP)”\n. RFC 3748, jun.2004.\nRFC 3782. FLOYD, S.; HENDERSON, T.; GURTOV, A. “The NewReno Modification to TCP’s Fast Recovery Algorithm”\n. \nRFC 3782, abr. 2004.\nRFC 3973. ADAMS, A.; NICHOLAS, J.; SIADAK, W. “Protocol Independent Multicast—Dense Mode (PIM-DM): Protocol \nSpecification (Revised)”\n. RFC 3973, jan. 2005.\nRFC 4022. RAGHUNARAYAN, R. (ed.), “Management Information Base for the Transmission Control Protocol (TCP)”\n. \nRFC 4022, mar. 2005.\nRFC 4113. FENNER, B.; FLICK, J. “Management Information Base for the User Datagram Protocol (UDP)”. RFC 4113, \njun. 2005.\nRFC 4213. NORDMARK, E.; GILLIGAN, R. “Basic Transition Mechanisms for IPv6 Hosts and Routers”. RFC 4213, out. \n2005.\nRFC 4271. REKHTER, Y.; LI, T.; HARES, S. Ed., “A Border Gateway Protocol 4 (BGP-4)”. RFC 4271, jan. 2006.\nRFC 4272. MURPHY, S. “BGP Security Vulnerabilities Analysis”. RFC 4274, jan. 2006.\nRFC 4274. MEYER, D.; PATEL, K. “BGP-4 Protocol Analysis”. RFC 4274, jan. 2006. \nRFC 4291. HINDEN, R.; DEERING, S. “IP Version 6 Addressing Architecture”. RFC 4291, fev. 2006.\nRFC 4293. ROUTHIER, S. (ed.). “Management Information Base for the Internet Protocol (IP)”. RFC 4293, abr. 2006.\nRFC 4301. KENT, S.; SEO, K. “Security Architecture for the Internet Protocol”. RFC 4301, dez. 2005.\nRFC 4302. KENT, S. “IP Authentication Header”. RFC 4302, dez. 2005.\nReferências  601 \nRFC 4303. KENT, S. “IP Encapsulating Security Payload (ESP)”. RFC 4303, dez. 2005.\nRFC 4305. EASTLAK D. “Cryptographic Algorithm Implementation Requirements for Encapsulating Security Payload \n(ESP) and Authentication Header (AH)”\n. RFC 4305, dez. 2005.\nRFC 4340. KOHLER, E.; HANDLEY, M.; FLOYD, S. “Datagram Congestion Control Protocol (DCCP)”. RFC 4340, \nmar. 2006.\nRFC 4443. CONTA, A.; DEERING, S.; GUPTA, M. Ed., “Internet Control Message Protocol (ICMPv6) for the Internet \nProtocol Version 6 (IPv6) Specification”\n. RFC 4443, mar. 2006.\nRFC 4346. DIERKS, T.; RESCORLA, E. “The Transport Layer Security (TLS) Protocol Version 1.1”. RFC 4346, abr. 2006.\nRFC 4502. WALDBUSSER, S. “Remote Network Monitoring Management Information Base Version 2”. RFC 4502, \nmaio 2006.\nRFC 4514. ZEILENGA, K. (ed.), “Lightweight Directory Access Protocol (LDAP): String Representation of Distinguished \nNames”. RFC 4514, jun. 2006.\nRFC 4601. FENNER, B.; HANDLEY, M.; HOLBROOK, H.; KOUVELAS, I. “Protocol Independent Multicast — Sparse \nMode (PIM-SM): Protocol Specification (Revised)”. RFC 4601, ago. 2006.\nRFC 4607. HOLBROOK, H.; CAIN, B. “Source-Specific Multicast for IP”. RFC 4607, ago. 2006.\nRFC 4611. MCBRIDE, M.; MEYLOR, J.; MEYER, D. “Multicast Source Discovery Protocol (MSDP) Deployment Scena-\nrios”. RFC 4611, ago. 2006.\nRFC 4632. FULLER, V.; LI, T. “Classless Inter-domain Routing (CIDR): The Internet Address Assignment and Aggrega-\ntion Plan”. RFC 4632, ago. 2006.\nRFC 4960. STEWART, R. (ed.), “Stream Control Transmission Protocol”. RFC 4960, set. 2007.\nRFC 4987. EDDY, W. “TCP SYN Flooding Attacks and Common Mitigations”. RFC 4987, ago. 2007.\nRFC 5000. RFC (ed.). “Internet Official Protocol Standards”. RFC 5000, maio 2008.\nRFC 5109. LI, A. (ed.). “RTP Payload Format for Generic Forward Error Correction”\n. RFC 5109, dez. 2007.\nRFC 5110. SAVOLA, P. “Overview of the Internet Multicast Routing Architecture”. RFC 5110, jan. 2008.\nRFC 5216. SIMON, D.; ABOBA, B.; HURST, R. “The EAP-TLS Authentication Protocol”. RFC 5216, mar. 2008.\nRFC 5218. THALER, D.; ABOBA, B. “What Makes for a Successful Protocol?”. RFC 5218, jul. 2008.\nRFC 5321. KLENSIN, J. “Simple Mail Transfer Protocol”. RFC 5321, out. 2008.\nRFC 5322. RESNICK, P. (ed.). “Internet Message Format”. RFC 5322, out. 2008.\nRFC 5348. FLOYD, S.; HANDLEY, M.; PADHYE, J.; WIDMER, J. “TCP Friendly Rate Control (TFRC): Protocol Specifi-\ncation”. RFC 5348, set. 2008.\nRFC 5411. ROSENBERG, J. “A Hitchhiker’s Guide to the Session Initiation Protocol (SIP)”. RFC 5411, fev. 2009.\nRFC 5681. ALLMAN, M.; PAXSON, V.; BLANTON, E. “TCP Congestion Control”\n. RFC 5681, set. 2009.\nRFC 5944. PERKINS, C. (ed.). “IP Mobility Support for IPv4, Revised”. RFC 5944, nov. 2010.\nRFC 5996. KAUFMAN, C.; HOFFMAN, P.; NIR, Y.; ERONEN, P. “Internet Key Exchange Protocol Version 2 (IKEv2)”\n. \nRFC 5996, set. 2010.\nRFC 6071. FRANKEL, S.; KRISHNAN, S. “IP Security (IPsec) and Internet Key Exchange (IKE) Document Roadmap”\n. \nRFC 6071, fev. 2011.\nRFC 6265. BARTH, A. “HTTP State Management Mechanism”. RFC 6265, abr. 2011.\nRFC 6298. PAXSON, V.; ALLMAN, M.; CHU, J.; SARGENT, M. “Computing TCP’s Retransmission Timer”. RFC 6298, \njun. 2011.\nRHEE, I. “Error Control Techniques for Interactive Low-Bit Rate Video Transmission over the Internet”. Proc. 1998 ACM \nSIGCOMM (Vancouver, ago. 1998).\nRIVEST, R.; SHAMIR, A.; ADELMAN, L. “A Method for Obtaining Digital Signatures and Public-key Cryptosystems”\n. \nCommunications of the ACM, v.21, n.2 (fev. 1978), p. 120–126.\n   Redes de computadores e a Internet\n602\nROBERTS, L.; MERRIL, T. “Toward a Cooperative Network of Time-Shared Computers”. AFIPS Fall Conference (out. \n1966).\nROBERTS, J. “Internet Traffic, QoS and Pricing”. Proc. 2004 IEEE INFOCOM, v.92, n.9 (set. 2004), p. 1389–1399.\nRODRIGUES, R.; DRUSCHEL, P. “Peer-to-Peer Systems”. Communications of the ACM, v.53, n.10 (out. 2010), p. 72–82.\nROHDE; SCHWARZ. “UMTS Long Term Evolution (LTE) Technology Introduction”. Application Note 1MA111.\nROM, R.; SIDI, M. Multiple Access Protocols: Performance and Analysis. Nova York: Springer-Verlag, 1990.\nROOT SERVERS 2012. <http://www.root-servers.org/>.\nROSE, M. The Simple Book: An Introduction to Internet Management, Revised Second Edition., Englewood Cliffs: Prentice \nHall, 1996.\nROSS, K. W\n. Multiservice Loss Models for Broadband Telecommunication Networks. Berlim: Springer, 1995.\nROWSTON, A.; DRUSCHEL, P. “Pastry: Scalable, Distributed Object Location and Routing for Large-Scale Peer-to-Peer \nSystems”. Proc. 2001 IFIP/ACM Middleware (Heidelberg, 2001).\nRSA Fast 2012. “How Fast is RSA?” <http://www.rsa.com/rsalabs/node.asp?id=2215>.\nRSA Key 2012. RSA Laboratories. “How large a key should be used in the RSA Crypto system?” <http://www.rsa.com/\nrsalabs/node.asp?id=2218>.\nRUBENSTEIN, D.; KUROSE, J.; TOWSLEY, D. “Real-Time Reliable Multicast Using Proactive Forward Error Correction”\n. \nProceedings of NOSSDAV ’98 (Cambridge, jul. 1998).\nRUBIN, A. White-Hat Security Arsenal: Tackling the Threats. Addison-Wesley, 2001.\nRUIZ-SÁNCHEZ, M.; BIERSACK, E.; DABBOUS, W. “Survey and Taxonomy of IP Address Lookup Algorithms”\n. IEEE \nNetwork Magazine, v.15, n.2 (mar./abr. 2001), p. 8–23.\nSALTZER, J.; REED, D.; CLARK, D. “End-to-End Arguments in System Design”. ACM Transactions on Computer Systems \n(TOCS), v.2, n.4 (nov. 1984).\nSANDVINE 2011. “Global Internet Phenomena Report, Spring 2011”. <http://www.sandvine. com/news/global broa-\ndband trends.asp, 2011>.\nSARDAR, B.; SAHA, D. “A Survey of TCP Enhancements for Last-Hop Wireless Networks”. IEEE Commun.Surveys and \nTutorials, v.8, n.3 (2006), p. 20–34.\nSAROIU, S.; GUMMADI, P. K.; GRIBBLE, S. D. “A Measurement Study of Peer-to-Peer File Sharing Systems”\n. Proc. of \nMultimedia Computing and Networking (MMCN) (2002a).\nSAROIU, S.; GUMMADI, K. P.; DUNN, R. J.; GRIBBLE S. D.; LEVY, H. M. “An Analysis of Internet Content Delivery \nSystems”. USENIX OSDI (2002b).\nSAYDAM, T.; MAGEDANZ, T. “From Networks and Network Management into Service and Service Management”\n. Jour-\nnal of Networks and System Management, v.4, n.4 (dez. 1996), p. 345–348.\nSCHILLER, J. Mobile Communications 2.ed. Addison Wesley, 2003.\nSCHNEIER, B. Applied Cryptography: Protocols, Algorithms, and Source Code in C. John Wiley and Sons, 1995.\nSCHULZRINNE, H. “A Comprehensive Multimedia Control Architecture for the Internet”. NOSSDAV’97 (Network and \nOperating System Support for Digital Audio and Video) (St. Louis, maio 1997).\nSCHULZRINNE-RTP 2012. <http://www.cs.columbia.edu/~hgs/rtp>.\nSCHULZRINNE-RTSP 2012. <http://www.cs.columbia.edu/~hgs/rtsp>.\nSCHULZRINNE-SIP 2012. <http://www.cs.columbia.edu/~hgs/sip>.\nSCHWARTZ, M. Computer-Communication Network Design and Analysis. Englewood Cliffs: Prentice-Hall, 1997.\nSCHWARTZ, M. Information, Transmission, Modulation, and Noise. Nova York: McGraw Hill, 1980. \nSCHWARTZ, M. “Performance Analysis of the SNA Virtual Route Pacing Control”\n. IEEE Transactions on Communica-\ntions, v.30, n.1 (jan.1982), p. 172–184.\nReferências  603 \nSCOURIAS, J. “Overview of the Global System for Mobile Communications: GSM”. <http://www.privateline.com/PCS/\nGSM0.html>.\nSEGALLER, S. Nerds 2.0.1, A Brief History of the Internet. Nova York: TV Books, 1998.\nSHACHAM, N.; MCKENNEY, P. “Packet Recovery in High-Speed Networks Using Coding and Buffer Management”\n. Proc. \n1990 IEEE INFOCOM (San Francisco, abr. 1990), p. 124–131.\nSHAIKH, A.; TEWARI, R.; AGRAWAL, M. “On the Effectiveness of DNS-based Server Selection”. Proc. 2001 IEEE INFO-\nCOM.\nSHARMA, P.; PERRY, E.; MALPANI, R. “IP Multicast Operational Network management: Design, Challenges, and Expe-\nriences”. IEEE Network Magazine (mar. 2003), p. 49–55.\nSINGH, S. The Code Book: The Evolution of Secrecy from Mary, Queen of Scotsto Quantum Cryptography. Doubleday Press, \n1999. \nSIP SOFTWARE 2012. H. Schulzrinne Software Package site. <http://www.cs.columbia.edu/IRT/software>.\nSKOUDIS, E.; ZELTSER, L. Malware: Fighting Malicious Code. Prentice Hall, 2004.\nSKOUDIS, E.; LISTON, T. Counter Hack Reloaded: A Step-by-Step Guide to Computer Attacks and Effective Defenses (2nd \nEdition), Prentice Hall, 2006.\nSKYPE 2012. <www.skype.com>.\nSMIL 2012. W3C Synchronized Multimedia homepage, <http://www.w3.org/AudioVideo>.\nSMITH, J. “Fighting Physics: A Tough Battle”\n. Communications of the ACM, v.52, n.7 (jul. 2009), p. 60–65.\nSNORT 2012. Sourcefire Inc., Snort homepage, <http://http://www.snort.org/>.\nSOLARI, S. J. Digital Video and Audio Compression. Nova York: McGraw Hill, 1997.\nSOLENSKY, F. “IPv4 Address Lifetime Expectations”. in IPng: Internet Protocol Next Generation. (S. Bradner, A. Mankin, \ned.), Reading: Addison-Wesley, 1996.\nSPRAGINS, J. D. Telecommunications Protocols and Design. Reading: Addison-Wesley, 1991.\nSRIKANT, R. The Mathematics of Internet Congestion Control. Birkhauser, 2004.\nSRIPANIDKULCHAI, K.; MAGGS B.; ZHANG, H. “An analysis of live streaming workloads on the Internet”. Proc. 2004 \nACM Internet Measurement Conference (Taormina) p. 41–54.\nSTALLINGS, W. SNMP, SNMP v2, and CMIP The Practical Guide to Network Management Standards. Reading: Addison­\n‑Wesley, 1993.\nSTALLINGS, W. SNMP, SNMPv2, SNMPv3, and RMON 1 and 2. Reading: Addison-Wesley, 1999.\nSTEINDER, M.; SETHI, A. “Increasing robustness of fault localization through analysis of lost, spurious, and positive \nsymptoms”\n. Proc. 2002 IEEE INFOCOM.\nSTEVENS, W. R. Unix Network Programming. Englewood Cliffs: Prentice-Hall.\nSTEVENS, W. R. TCP/IP Illustrated, v.1: The Protocols. Reading: Addison-Wesley, 1994.\nSTEVENS, W. R. Unix Network Programming, v.1: Networking APIs-Sockets and XTI, 2.ed. Englewood Cliffs: Prentice\n-Hall, 1997.\nSTEVENS, W. R. BGP4: Interdomain Routing in the Internet. Addison-Wesley, 1999.\nSTOICA, I.; MORRIS, R.; KARGER, D.; KAASHOEK, M. F.; BALAKRISHNAN, H. “Chord: A Scalable Peer-to-Peer Lookup Ser-\nvice for Internet Applications”\n. Proc. 2001 ACM SIGCOMM (San Diego, ago. 2001).\nSTONE, J.; GREENWALD, M.; PARTRIDGE, C.; HUGHES, J. “Performance of Checksums and CRC’s Over Real Data”\n. \nIEEE/ACM Transactions on Networking, v.6, n.5 (out. 1998), p. 529–543.\nSTONE, J.; PARTRIDGE, C. “When Reality and the Checksum Disagree”. Proc. 2000 ACM SIGCOMM. (Estocolmo, ago. \n2000).\nSTRAYER, W. T.; DEMPSEY, B.; WEAVER, A. XTP: The Xpress Transfer Protocol. Reading: Addison-Wesley, 1992.\n   Redes de computadores e a Internet\n604\nSTUBBLEFIELD, A.; IOANNIDIS, J.; RUBIN, A. “Using the Fluhrer, Mantin, and Shamir Attack to Break WEP”\n. Procee-\ndings of 2002 Network and Distributed Systems Security Symposium (2002), p. 17–22.\nSUBRAMANIAN, M. Network Management: Principles and Practice. Reading: Addison-Wesley, 2000.\nSUBRAMANIAN, L.; AGARWAL, S.; REXFORD, J.; KATZ, R. “Characterizing the Internet Hierarchy from Multiple \nVantage Points”\n. Proc. 2002 IEEE INFOCOM.\nSUNDARESAN, K.; PAPAGIANNAKI, K. “The Need for Cross-layer Information in Access Point Selection”. Proc. 2006 \nACM Internet Measurement Conference (Rio de Janeiro, out. 2006).\nSU, A.-J.; CHOFFNES, D.; KUZMANOVIC A.; BUSTAMANTE, F. “Drafting Behind Akamai” Proc. 2006 ACM SIGCOMM.\nSUH, K.; FIGUEIREDO, D. R.; KUROSE J.; TOWSLEY, D. “Characterizing and detecting relayed traffic: A case study \nusing Skype”\n. Proc. 2006 IEEE INFOCOM (Barcelona, abr. 2006).\nSUNSHINE, C.; DALAL, Y. “Connection Management in Transport Protocols”. Computer Networks. North-Holland, Ams-\nterdam, 1978.\nTARIQ, M.; ZEITOUN, A.; VALANCIUS, V.; FEAMSTER, N.; AMMAR, M. “Answering What-If Deployment and Confi-\nguration Questions with WISE”\n. Proc. 2008 ACM SIGCOMM (ago. 2008).\nTECHNONLINE 2012. “Protected Wireless Networks”. online webcast tutorial, <http://www.techonline.com/communi-\nty/tech_topic/internet/21752>.\nTEIXEIRA, R.; REXFORD, J. “Managing Routing Disruptions in Internet Service Provider Networks”. IEEE Communica-\ntions Magazine (mar. 2006).\nTHALER, D.; RAVISHANKAR, C. “Distributed Center-Location Algorithms”. IEEE Journal on Selected Areas in Commu-\nnications, v.15, n.3 (abr. 1997), p. 291–303.\nTHINK 2012. Technical History of Network Protocols, “Cyclades”. <http://www.cs.utexas.edu/users/chris/think/Cycla-\ndes/index.shtml>.\nTIAN, Y.; DEY, R.; LIU, Y.; ROSS, K. W. “China’s Internet: Topology Mapping and Geolocating”. IEEE INFOCOM Mini-\nConference 2012 (Orlando, 2012).\nTOBAGI, F. “Fast Packet Switch Architectures for Broadband Integrated Networks”. Proc. 1990 IEEE INFOCOM, v.78, n.1 \n(jan. 1990), p. 133–167.\nTOR 2012. TOR: Anonymity Online, <http://www.torproject.org>.\nTORRES, R.; FINAMORE, A.; KIM, J. R.; MUNAFO, M. M.; RAO, S. “Dissecting Video Server Selection Strategies in the \nYouTube CDN”\n. Proc. 2011 Int. Conf. on Distributed Computing Systems.\nTURNER, J. S. “Design of a Broadcast packet switching network”. IEEE Transactions on Communications, v.36, n.6 \n(jun.1988), p. 734–743.\nTURNER, B. (2012) “2G, 3G, 4G Wireless Tutorial”. <http://blogs.nmscommunications.com/communications/2008/ \n10/2g-3g-4g-wireless-tutorial.html>.\nUPnP FORUM 2012. <http://www.upnp.org/>.\nVAN DER BERG, R. “How the ‘Net works: an introduction to peering and transit”. <http://arstechnica.com/guides/other/\npeering-and-transit.ars>.\nVARGHESE, G.; LAUCK, A. “Hashed and Hierarchical Timing Wheels: Efficient Data Structures for Implementing a Timer \nFacility”\n. IEEE/ACM Transactions on Networking, v.5, n.6 (dez. 1997), p. 824–834.\nVASUDEVAN, S.; DIOT, C.; KUROSE, J.; TOWSLEY, D. “Facilitating Access Point Selection in IEEE 802.11 Wireless \nNetworks”. Proc. 2005 ACM Internet Measurement Conference (San Francisco, out. 2005).\nVERIZON FIOS 2012. “Verizon FiOS Internet: FAQ”. <http://www22.verizon.com/residential/fiosinternet/faq/faq.htm>.\nVERIZON SLA 2012. “Global Latency and Packet Delivery SLA”. <http://www. verizonbusiness.com/terms/global_laten-\ncy_sla.xml>.\nVERMA, D. C. Content Distribution Networks: An Engineering Approach. John Wiley, 2001.\nReferências  605 \nVILLAMIZAR, C.; SONG. C. “High performance tcp in ansnet”. ACM SIGCOMM Computer Communications Review, \nv.24, n.5 (1994), p. 45–60.\nVITERBI, A. CDMA: Principles of Spread Spectrum Communication. Reading: Addison-Wesley, 1995.\nVIXIE, P. “What DNS Is Not”\n. Communications of the ACM, v.52, n.12 (dez. 2009), p. 43–47.\nW3C 1995. The World Wide Web Consortium, “A Little History of the World Wide Web” (1995), <http://www.w3.org/\nHistory.html>.\nWAKEMAN, I.; CROWCROFT, J.; WANG, Z.; SIROVICA, D. “Layering Considered Harmful”. IEEE Network (jan.1992), \np. 20–24.\nWALDROP, M. “Data Center in a Box”. Scientific American (jul. 2007).\nJ. WALKER, “IEEE P802.11 Wireless LANs, Unsafe at Any Key Size; An Analysis of the WEP Encapsulation”. out. 2000, \nhttp://www.drizzle.com/~aboba/IEEE/0-362.zip.\nWALL, D. Mechanisms for Broadcast and Selective Broadcast, tese de doutorado, Stanford University, jun. 1980.\nWANG, B.; KUROSE, J.; SHENOY, P.; TOWSLEY, D. “Multimedia Streaming via TCP: An Analytic Performance Study”\n. \nProc. 2004 ACM Multimedia Conference (Nova York, out. 2004).\nWANG, B.; KUROSE, J.; SHENOY, P.; TOWSLEY, D. “Multimedia Streaming via TCP: An Analytic Performance Study”. \nACM Transactions on Multimedia Computing Communications and Applications (TOMCCAP), v.4, n.2 (abr. 2008), p. \n16:1–22.\nWANG, G.; ANDERSEN, D. G.; KAMINSKY, M.; PAPAGIANNAKI, K.; NG, T. S. E.; KOZUCH, M.; RYAN, M. “c-Throu-\ngh: Part-time Optics in Data Centers”\n. Proc. 2010 ACM SIGCOMM.\nWEATHERSPOON, S. “Overview of IEEE 802.11b Security”. Intel Technology Journal (2.nd Quarter 2000), <http://down-\nload.intel.com/technology/itj/q22000/pdf/art_5.pdf>.\nWEI, W\n.; WANG, B.; ZHANG, C.; KUROSE, J.; TOWSLEY, D. “Classification of Access Network Types: Ethernet, Wireless LAN, \nADSL, Cable Modem or Dialup?”\n. Proc. 2005 IEEE INFOCOM (abr. 2005).\nWEI, W.; WANG, B.; ZHANG, C.; KUROSE, J.; TOWSLEY, D. “Inference and Evaluation of Split-Connection Approaches \nin Cellular Data Networks”\n. Proc. Active and Passive Measurement Workshop (Adelaide, mar. 2006).\nWEI, D. X.; JIN, C.; LOW, S. H.; HEGDE, S. “FAST TCP: Motivation, Architecture, Algorithms, Performance”. IEEE/ACM \nTransactions on Networking (2007).\nWEISER, M. “The Computer for the Twenty-First Century”. Scientific American (set. 1991), p. 94–10. <http://www.ubiq.\ncom/hypertext/weiser/SciAmDraft3.html>.\nWHITE, A.; SNOW, K.; MATTHEWS, A.; MONROSE, F. “Hookt on fon-iks: Phonotactic Reconstruction of Encrypted \nVoIP Conversations”\n. IEEE Symposium on Security and Privacy, Oakland, 2011.\nWIGLE.NET 2012. <http://www.wigle.net>.\nWILLIAMS, R. “A Painless Guide to CRC Error Detection Algorithms”. <http://www.ross.net/crc/crcpaper.html>.\nWILSON, C.; BALLANI, H.; KARAGIANNIS, T.; ROWSTRON, A. “Better Never than Late: Meeting Deadlines in Data-\ncenter Networks”\n. Proc. 2011 ACM SIGCOMM.\nWIMAX FORUM 2012. <http://www.wimaxforum.org>.\nWIRESHARK 2012. <http://www.wireshark.org>.\nWISCHIK, D.; MCKEOWN, N. “Part I: Buffer Sizes for Core Routers”. ACM SIGCOMM Computer Communications \nReview, v.35, n.3 (jul. 2005).\nWOO, T.; BINDIGNAVLE, R.; SU, S.; LAM, S. “SNP: an interface for secure network programming”. Proc. 1994 Summer \nUSENIX (Boston, jun. 1994), p. 45–58. \nWOOD, L. “Lloyds Satellites Constellations”\n. <http://www.ee.surrey.ac.uk/Personal/L.Wood/constellations/iridium.html>.\nWU, J.; MAO, Z. M.; REXFORD, J.; WANG, J. “Finding a Needle in a Haystack: Pinpointing Significant BGP Routing \nChanges in an IP Network”\n. Proc. USENIX NSDI (2005).\n   Redes de computadores e a Internet\n606\nXANADU 2012. <http://www.xanadu.com/>.\nXIAO, X.; HANNAN, A.; BAILEY, B.; NI, L. “Traffic Engineering with MPLS in the Internet”. IEEE Network (mar./abr. \n2000).\nXIE, H.; YANG, Y. R.; KRISHNAMURTHY, A.; LIU, Y.; SILBERSCHATZ, A. “P4P: Provider Portal for Applications”. Proc. \n2008 ACM SIGCOMM (Seattle, ago. 2008).\nYANNUZZI, M.; MASIP-BRUIN, X.; BONAVENTURE, O. “Open Issues in Interdomain Routing: A Survey”. IEEE \nNetwork Magazine (nov./dez. 2005).\nYAVATKAR, R.; BHAGWAT, N. “Improving End-to-End Performance of TCP over Mobile Internetworks”. Proc. Mobile 94 \nWorkshop on Mobile Computing Systems and Applications (dez. 1994).\nYOUTUBE 2009. YouTube 2009, Google container data center tour, 2009.\nYU, H.; KAMINSKY, M.; GIBBONS P. B.; FLAXMAN, A. “SybilGuard: Defending Against Sybil Attacks via Social \nNetworks”. Proc. 2006 ACM SIGCOMM (Pisa, set. 2006).\nZEGURA, E.; CALVERT, K.; DONAHOO, M. “\nA Quantitative Comparison of Graph-based Models for Internet Topology”\n. IEEE/\nACM Transactions on Networking, v.5, n.6, (dez. 1997). Ver também <http://www.cc.gatech.edu/projects/gtim> para um pacote \nde software que gera redes com uma estrutura transit-stub.\nZHANG, L.; DEERING, S.; ESTRIN, D.; SHENKER, S.; ZAPPALA, D. “RSVP: A New Resource Reservation Protocol”\n. \nIEEE Network Magazine, v.7, n.9 (set. 1993), p. 8–18.\nZHANG, L. “A Retrospective View of NAT”. The IETF Journal, v.3, Issue 2 (out. 2007).\nZHANG, M.; JOHN, W.; CHEN, C. “Architecture and Download Behavior of Xunlei: A Measurement-Based Study”\n. Proc. \n2010 Int. Conf. on Educational Technology and Computers (ICETC).\nZHANG, X.; LIU, J.; LI B.; YUM, T.-S. P. “CoolStreamingDONet/: A Data-driven Overlay Network for Peer-to-Peer Live \nMedia Streaming”\n. Proc. 2005 IEEE INFOCOM (Miami, mar. 2005).\nZHANG, X.; XU, Y.; LIU, Y.; GUO, Z.; WANG, Y. “Profiling Skype Video Calls: Rate Control and Video Quality”\n. IEEE \nINFOCOM (mar. 2012).\nZHAO, B. Y.; HUANG, L.; STRIBLING, J. S.; RHEA, C.; JOSEPH, A. D.; KUBIATOWICZ, J. “Tapestry: A Resilient \nGlobal-scale Overlay for Service Deployment”. IEEE Journal on Selected Areas in Communications, v.22, n.1 (jan. 2004).\nZIMMERMAN, H. “OS1 Reference Model-The ISO Model of Architecture for Open Systems Interconnection”\n. IEEE \nTransactions on Communications, v.28, n.4 (abr. 1980), p. 425–432.\nZIMMERMANN, P. “Why do you need PGP?” <http://www.pgpi.org/doc/whypgp/en/>.\nZINK, M.; SUH, K.; GU, Y.; KUROSE, J. “Characteristics of YouTube Network Traffic at a Campus Network — Measure-\nments, Models, and Implications”\n. Computer Networks, v.53, n.4 (2009), p. 501–514.\n10BASE-2, 351\n10BASE-T, 351\n10GBASE-T, 351, 352\n2G, arquitetura de redes de celular, 406\n3DES, 501\n3G, rede de núcleo, 407\n3G, redes de acesso a rádio, 408\n3G, redes celulares de dados, 406\n3G, redes, 669\n3G, sistemas celulares móveis versus LANs sem fio, 405\n3G, sistemas, 547\n3G, UMTS e DS-WCDMA (Direct Sequence Wideband \nCDMA), 408\n3GPP (3rd Generation Partnership Project), 266, 407, 409\n4G, sistemas, 18, 553–554\n802.11, LANs sem fio, 12, 389, 390,  \nAPs (pontos de acesso), 390, 398 \narquitetura, 390 \nassociação, 391 \nautenticação, 392, 393, 536 \nbanda de frequência, 384 \nBSS (conjunto básico de serviço), 390 \ncampos de endereço, 398 \ncanais, 391 \nCSMA/CA (CSMA com impedimento de colisão), 393,    \n  394, 396, 404 \ndetecção de colisão, 393, 394 \nendereços MAC, 390,  \nestação-base, 390 \nprotocolos MAC, 393 \nquadros da camada de enlace, 390 \nreduzindo a taxa de transmissão, 390, 401 \ntaxas de dados, 390, 391 \ntransmitindo quadros, 392\n802.11i, 536\n802.11n, rede sem fio, 390\n802.15.1, 402, 403\n802.1Q, quadros, 359-359\nA\nAAC (Advanced Audio Coding), 435\nAbordagem top-down, 37\nABR (Available Bit-Rate), 190, 229 \nvantagem da largura de banda disponível, 196\nABR, serviço de rede ATM, 229\nAbramson, Norman, 46, 333, 335, 336\nAbstract Syntax Notation One. Veja ASN.1\nAcesso à Internet por cabo, 10 \nprotocolos da camada de enlace, 341 \nprotocolo DOCSIS, 38, 341\nAcesso com fios, 9\nAcesso discado, 9\nAcesso remoto sem fio, 13\nACK (reconhecimentos positivos), 152, 154, 155, 151, 152, \n154-160\nACK, recebimento, 163\nACKs duplicados, 181\nACKs ou NAKs corrompido, 154\nAções de controle de congestionamento específicas da origem, \n195\nActive Optical Networks. Veja AONs\nAdaptação da taxa, 401\nAdaptador de recepção, 348-349\nAdaptador de rede, 324\nAdaptadores, 343-345 \nendereços MAC, 343 \nroteadores, 345\nAddress Resolution Protocol. Veja ARP\nAddress Supporting Organization da ICANN, 255\nAdleman, Leonard, 504\nAdmissão de chamada, 481\nAdvanced Audio Coding. Veja AAC\nAdvanced Encryption Standard. Veja AES\nAdvanced Research Projects Agency. Veja ARPA\nAES (Advanced Encryption Standard), 501\nAgente de gerenciamento de rede, 560\nÍndice\n   Redes de computadores e a Internet\n608\nAgente externo âncora, 416\nAgente externo, 412-418 \nCOA (Care-Of-Address), 419 \nIP móvel, 417 \nrecebendo e desencapsulando datagrama, 414, 416 \nregistro com agente nativo, 418, 419\nAgentes do usuário, 87, 89\nAgentes nativos, 412-421, 425\nAgregação de endereço, 253\nAgregação de rota, 253\nAH (Authentication Header), protocolo, 530\nAIMD (Additive-Increase, Multiplicative-Decrease), algoritmo, \n204-206\nAkamai, 81, 97, 202, 444, 445, 450 \nAlgoritmo de backoff exponencial binário, 339\nAlgoritmo de caminho de menor custo de Dijkstra, 271, 274, \n275, 286 \nOSPF (Open-Shortest Path First), 283\nAlgoritmo de chave pública, 527\nAlgoritmo de chave simétrica \ncifras de bloco, 500 \ncifra de César, 498 \ncifras de fluxo, 500 \ncifra monoalfabética, 498 \ncriptografia polialfabética, 500\nAlgoritmo de criptografia de chave pública, 507\nAlgoritmo de criptografia de dados e WEP (Wired Equivalent \nPrivacy), 535\nAlgoritmo de decriptografia, 498 \nAlgoritmo de roteamento descentralizado, 270\nAlgoritmo de roteamento dinâmico, 270\nAlgoritmo de roteamento estático, 270\nAlgoritmo de roteamento global, 270\nAlgoritmo de roteamento insensível à carga, 270\nAlgoritmo de roteamento para comutação de circuitos, 280\nAlgoritmo de vetor de distâncias. Veja DV, algoritmo\nAlgoritmo simétrico, 527\nAlgoritmos de gerenciamento ativo de fila. Veja AQM, \nalgoritmos\nAlgoritmos de roteamento de estado do enlace, 270\nAlgoritmos de roteamento multicast da camada de rede, 287\nAlgoritmos de roteamento, 225, 268 \nARPAnet, 270 \ncaminhos de menor custo, 269 \ncomutação de circuitos, 280 \ncaminho do roteador de origem ao destino, 268 \ncomutadores, 365 \ndescentralizados, 270 \ndinâmicos, 270 \nDV (vetor de distâncias), algoritmo, 270, 274 \nescala de roteadores,280 \nestáticos, 270 \nglobais, 270 \nLS (estado do enlace), algoritmos, 270 \nroteamento hierárquico, 280 \nsensíveis a carga, 270 \ntabelas de repasse, 268 \nvisualização do fluxo de tráfego de pacotes, 280\nALOHA, protocolo, 46, 333-336\nALOHAnet, 46, 336, 350\nAmbientes com fio e analisador de pacotes, 43\nAnalisador de pacote, 43, 58\nAndreessen, Marc, 133\nAnonimato, 543\nAntenas de entrada múltipla, saída múltipla. Veja MIMO, \nantenas\nAnúncios de estado do enlace. Veja LSAs\nAnycast, 263\nAONs (Active Optical Networks), 11\nAP (pontos de acesso), 390\nApache, servidor Web, 71, 78, 115\nApelido de hospedeiro, 102\nAPI (Application Programming Interface), 4, 66\nAplicação cliente-servidor \ndesenvolvimento, 116-123 \nnúmero de porta bem conhecido, 116 \nTCP, 116 \nUDP, 116\nAplicações da Internet, 61, 73 \nprotocolos da camada de aplicação, 71\nAplicações de nuvem e redes de centro de dados, 362-365\nAplicações de rede proprietárias, 115\nAplicações de rede, 61 \narquitetura, 62 \ncomunicação entre cliente e servidor, 114, 115 \ncriação, 115-123 \nprincípios, 62-72 \ncomunicação entre processos, 65 \nprograma cliente, 115 \nprograma servidor, 115 \nproprietárias, 115 \nprotocolos da camada de aplicação, 71 \nWeb, 71-85\nAplicações de tempo real \ntemporização, 68 \nUDP (User Datagram Protocol), 146\nAplicações distribuídas, 4\nAplicações elásticas, 68\nAplicações interativas de tempo real \nprotocolos, 460 \nRTP (Real-Time Transport Protocol), 460-463 \nSIP (Session Initiation Protocol), 463-467\nAplicações multimídia, 434 \náudio e vídeo de fluxo contínuo ao vivo, 433, 437 \náudio e vídeo de fluxo contínuo armazenado, 433, 436 \nmelhorando a qualidade, 468 \npropriedades de áudio, 435 \npropriedades de vídeo, 434 \nsensíveis à largura de banda, 67 \nTCP (Transmission Control Protocol), 168 \ntelefonia da Internet, 436 \ntipos, 436­\n-437 \nvoz conversacional, 587, 592-593 \nvídeo sobre IP, 437 \nUDP (User Datagram Protocol), 145, 208\nAplicações multimídia, tipo 436-437\nAplicações não de tempo real, 92-93\nAplicações sensíveis à largura de banda, 67\nAplicações sensíveis ao tempo, 68\nAplicações individuais e pacotes RTP, 461 \nÍndice  609 \nApplication Programming Interface. Veja API\nApresentação (handshake), 89, 169, 186, 187, 525, 533\nApresentação de três vias, 75, 120, 170, 186, 370, 541\nAQM (Active Queue Management), algoritmos, 243\nÁrea de backbone, 288\nÁrea de cobertura, 382\nAritmética de módulo, 504\nARP (Address Resolution Protocol), 345-348, 364 \nARPAnet, 336, 350, 378 \nARP de resposta, 346\nARP, mensagens, 346, 369\nARP, tabelas, 345-346, 353\nARPA (Advanced Research Projects Agency), 46, 378\nARQ (Automatic Repeat reQuest), protocolos, 152, 426\nArquitetura de aplicação, 62-64\nArquitetura em camadas, 35-39\nArquivo de manifesto, 443, 447, 450\nArquivo-base HTML, 72\nÁrvores multicast e pacotes RTP, 461\nASN (Número de Sistema Autônomo), 289\nASN.1 (Abstract Syntax Notation One), 563, 564, 566, 572, \n574-576\nAS-PATH, atributo, 290\nASs (sistemas autônomos), 280-283\nAssinaturas digitais, 510 \ncertificação de chave pública, 513-515 \ncomparação com MAC (Message Authentication Code), \n572 \ncriptografia de chave pública, 503-504\nexcessos de criptografia e decriptografia, 512 \nfunções hash, 508-509 \nintegridade de mensagem, 508 \nPGP (Pretty Good Privacy), 522\nAssociação de segurança. Veja SA (Security Association)\nAssociação, 391-393\nAtaque com texto aberto conhecido, 499\nAtaque de envenenamento, 143\nAtaque de mutilação, 717\nAtaque de repetição de conexão, 527\nAtaque de texto aberto conhecido, 499\nAtaque do homem no meio e DNS (Domain Name System), \n105\nAtaque exclusivo a texto cifrado, 498\nAtaques à rede, 41-44\nAtaques de pacotes maliciosos, 262\nAtaques de reprodução, 518, 572\nAtaques de vulnerabilidade, 42\nAtaques distribuídos. Veja ataques DDoS\nATM (Asynchronous Transfer Mode), 190, 379 \ncomplexidade e custo, 348 \nmodelos de serviço múltiplos, 228 \nQ2931b, protocolo, 483  \nserviços, 228\nATM ABR (Available Bit-Rate), controle de \ncongestionamento,196, 230\nAtrasando a reprodução, 454-456\nAtraso de acesso, 82\nAtraso de fim a fim, 31, 453 \nDiffserv, 478 \nTraceroute, programa, 31\nAtraso de pacote, 18, 75, 468-469\nAtraso de reprodução adaptativo, 454\nAtraso de reprodução fixo, 454\nAtraso de reprodução, 454\nAtraso nodal total, 26\nAtrasos de fila, 18, 26, 27, 29, 44\nAtrasos de ida e volta, 31\nAtrasos de processamento, 27\nAtrasos de propagação, 27, 337\nAtrasos de transmissão, 27\nAtrasos \ncomparação de atrasos de transmissão e propagação, 28 \nde fila, 18, 26, 27, 29, 44 \nempacotamento, 32 \nfim a fim, 31 \nnodais totais, 26 \npacotes, 26 \nprocessamento nodal, 26 \nprocessamento, 26 \npropagação, 27 \nredes comutadas de pacote, 26-29 \nsistemas finais, 32 \ntransmissão, 26-27\nAtributos de rota, 290\nÁudio analógico, 435\nÁudio digital, 435\nÁudio \nAAC (Advanced Audio Coding), 435 \nglitches, 591 \nMP3 (MPEG 1 camada 3), 435 \nPCM (Pulse Code Modulation), 435 \npropriedades, 435 \nquantização, 435 \neliminação da variação de atraso no receptor, 453 \nvoz humana, 435\nAumento aditivo, diminuição multiplicativa. Veja AIMD\nAutenticação da origem, 268\nAutenticação de receptor, 520\nAutenticação do remetente, 520\nAutenticação do ponto final, 44, 496, 515 \nAutenticação do servidor, 524\nAutenticação simples, 287\nAutenticação \n802.11i, 536-537 \nestação sem fio, 390 \nLANs sem fio 802.11, 389 \nMD5, 287 \nponto final, 515-519 \nredes, 515-519 \nsenha secreta, 517 \nSNMPv3, 562 \ntécnicas criptográficas, 497 \nWEP (Wired Equivalent Privacy), 534\nAutoaprendizagem, 353, 400\nAutoescalabilidade, 64\nAutomatic Repeat reQuest, protocolos. Veja ARQ\nAutonomous System Number. Veja ASN\nAutorreplicação, 56\nB\nBalanceador de carga, 364\nBancos de dados, implementando na rede P2P, 11\n   Redes de computadores e a Internet\n610\nBaran, Paul, 45\nBarramento, comutação via, 240\nBase Station System. Veja BSS\nBase Transceiver Station. Veja BTS\nBasic Encoding Rules. Veja BER\nBasic Service Set. Veja BSS\nBellman-Ford, equação, 274\nBellovin, Steven M., 553-554\nBER (Basic Encoding Rules), 574\nBER (Bit Error Rate), 385\nBerners-Lee, Tim, 47, 72\nBGP (Border Gateway Protocol), 288-295, 305 \nASN (Autonomous System Number), 289 \nanúncio de rota, 290-296 \natributos, 289 \nbits de prefixo, 250-252 \ncomplexidade, 288, 295 \nconexões TCP, 288, 295 \nDV (Vetor de distância), algoritmo, 274 \neBGP (external BGP), sessão, 289-290 \niBGP (internal BGP), sessão, 289-290 \npares, 288 \npares BGP, 289 \npolítica de roteamento, 293 \nprotocolos de roteamento entre ASs, 294, 304 \nregras de eliminação para rotas, 293 \nrotas, 289 \nseleção de rota, 290 \nsessão, 289 \nsessões BGP, 289 \ntabelas de roteamento, 295\nBGP externo, sessão. Veja eBGP, sessão\nBIND (Berkeley Internet Name Domain), 96\nBit de indicação de congestionamento. Veja CI, bit\nBit Error Rate. Veja BER\nBITNET, 46\nBits, 14 \natraso de propagação, 26 \naceitando conexões de outros hospedeiros, 352 \nalgoritmo de troca inteligente, 110\nBitTorrent, 64, 109 \ndesenvolvimento, 132 \nKademlia DHT, 114 \nP2P (peer-to-peer), protocolo, 106, 109, 111, 114 \npedaços, 109 \nprincípios de swarming de dados, 183\nBloqueio de cabeça de fila. Veja HOL, bloqueio\nBluetooth, 402, 493\nBoggs, David, 348, 350, 352\nBorder Gateway Protocol. Veja BGP\nBotnet, 41\nBroadcast de caminho de reserva. Veja RPB\nBroadcast de estado do enlace, 295\nBroadcast de spanning tree, 303\nBroadcasting (camada de enlace), 321 \natraso de propagação de canal, 337 \ncanais, 321, 330 \nenlaces, 329, 341 \nquadro, 343\nBroadcasting (camada de rede) \nalgoritmos de roteamento de broadcast, 306 \nbroadcast por spanning tree, 303 \ninundação, 296-299 \nnúmero de sequência, 296 \ntempestade de broadcast, 296 \nunicast de n caminhos, 296\nBSC (Base Station Controller), 406\nBSS (Base Station System), 406\nBSS (Basic Service Set), 390 \nendereço MAC, 393 \nmobilidade entre, 400\nBTS (Base Transceiver Station), 406\nBuffer de aplicação cliente, 439-440\nBuffer de envio, 170\nBuffer de recepção, 171\nBuffer do cliente, 438\nBuffering \npacotes, 161 \nvídeo de fluxo contínuo, 434\nBuffers de interfaces de saída do comutador, 352\nBuffers de saída, 18\nBuffers \nconexão TCP, 171 \ninterfaces de saída de comutador, 352 \nperda de pacotes, 18\nC\nCA (Certification Authority), 514 \ncertificando chaves públicas, 522\nCabeçalho IP, 148, 245\nCabeçalho IPv4, 470\nCabeçalho IPv6, 263\nCable Modem Termination System. Veja CMTS\nCabo coaxial, 15, 321, 351\nCaches proxy da Web, 76\nCaches Web,  81-83\nCaixa de correio, 87Caixas do meio, 235\nCamada de apresentação, 39\nCamada de enlace de dados, 38-40 \nCamada de enlace, 38, 322 \ncanais de broadcast, 322 \ncomutadores, 342 \nCRC (Cyclic Redundancy Check), códigos, 328-330 \ndetecção e correção de erro em nível de bit, 325-330 \nenlace de comunicação ponto a ponto, 323 \nenlaces sem fio, 14, 384-386 \nimplementação, 324 \nprotocolos de acesso múltiplo, 330-342 \nprotocolos IEEE, 329 \nquadro da camada de enlace, 323 \nredes, 359-362 \nserviços, 323 \ntécnicas de detecção e correção de erro, 325-330\nCamada de rede, 35-41, 138-139 \ncaminho entre remetente e destinatário, 232 \ncomplexidade, 224 \ncomunicação hospedeiro a hospedeiro, 224 \nconfidencialidade, 528 \nentrega garantida, 228 \nÍndice  611 \nentrega de pacotes na ordem, 228 \nencapsulamento do segmento da camada de transporte no \ndatagrama IP, 145 \nestabelecimento de conexão, 228 \nextração do segmento da camada de transporte do \ndatagrama, 137 \nICMP (Internet Control Message Protocol), 260 \nInternet, 245 \nmobilidade, 410 \npassando segmentos para, 139-145 \nprotocolo IP, 38, 245, 260 \nprotocolos de broadcast, 299 \nprotocolos de roteamento da Internet, 283 \nredes de datagrama, 230 \nredes VC (circuito virtual), 230 \nrelação com camada de transporte, 135-139 \nrelato de erros em datagramas, 245 \nrepasse, 224-226, 232 \nreserva de recursos, 232 \nroteamento, 38, 224-228 \nsegurança, 519, 528-534 \nserviço com conexão, 230 \nserviço de datagrama, 268 \nserviço de entrega processo a processo, 139 \nserviço de melhor esforço, 229 \nserviço sem conexão, 230 \nserviços hospedeiro a hospedeiro, 230 \nserviços oferecidos por, 225–230\nCamada de sessão, 39\nCamada de transporte do lado receptor, 40\nCamada de transporte, 38-39, 135 \ncomunicação entre processos, 224, 230 \ncontrole de congestionamento, 196 \nmensagem da camada de aplicação, 40 \ndatagrama passado, 337 \ndemultiplexação, 139-145 \ndesignando o número de porta automaticamente, 141 \nentregando dados ao socket, 139 \nhost de destino, 139 \nmultiplexação, 139-145 \nrelação com camada de rede, 135-139 \nresponsabilidade de entregar dados à aplicação  \n  apropriada, 139 \nsegmentos, 138 \nserviço de multiplexação/demultiplexação, 145 \nserviço orientado para conexão, 230 \nserviço sem conexão, 230 \nserviços, 135 \nsoma de verificação, 328 \ntransferência confiável de dados, 149 \nverificação de erro, 148 \nvisão geral, 138-139\nCamada física, 39\nCaminhos com múltiplos saltos, 263-265\nCaminho de menor custo, 269-274 \nBellman-Ford, equação, 274\nCaminhos mais curtos, 269\nCaminhos, 3, 269 \nmenor custo, 269 \nmúltiplos saltos, 263-265\nCampo de flag, 172, 246, 248\nCampo de identificação (IP), 248\nCampo de janela de recepção, 171\nCampo de método, 76\nCampo de opções, 172\nCampo de ponteiro para dados urgentes, 172\nCampos de cabeçalho, 40\nCampos de carga útil, 40\nCanais de rádio, 15\nCanais \natraso de propagação, 337 \ntransferência confiável de dados por um canal com erros \n   \n  de bit, 207-212 \nLANs sem fio 802.11, 389, 393 \nperda de pacotes, 154\nCanal confiável, 149\nCanal perfeitamente confiável, 151-152\nCare-Of-Address. Veja COA\nCarga oferecida, 192 \nCanal remetente-destinatário, 154\nCarrier Sense Multiple Access Protocol. Veja CSMA, \nprotocolo\nCarrier Sense Multiple Access with Collision Detection. Veja \nCSMA/CD\nCBC (Cipher Block Chaining), 502\nCBR (Constant Bit Rate), serviço de rede ATM, 229\nCDMA (Code Division Multiple Access), 387-389\nCDNs (Content Distribution Networks) de terceiros, 444\nCDNs (Content Distribution Networks) privadas, 444\nCDNs (Content Distribution Networks), 436, 444 \nanycast IP, 448 \ncentros de dados, 445 \nestratégia de seleção de cluster, 447 \ninterceptação/redirecionamento de DNS, 445 \nmedidas de desempenho de atraso e perda, 447 \nNetflix, 449 \noperação, 445 \nredes de acesso de ISPs (Internet Service Providers), 444 \nreplicação o conteúdo entre clusters, 444\nCélulas de gerenciamento de recursos. Veja RM, células\nCentral telefônica, 11\nCentro de comutação móvel. Veja MSC\nCentros de dados modulares. Veja MDCs\nCentros de dados, 62 \narquiteturas de interconexão e protocolos de rede, 366 \nbalanceamento de carga, 364 \nCDNs (Content Distribution Networks), 444 \ncustos, 362 \nhierarquia de roteadores e comutadores, 363 \nhospedeiros, 362 \nMDCs (Modular Data Centers) baseados em conteúdo, 366 \nredes do datacenter, 362 \nroteadores de borda, 363 \nserviços de Internet, 62 \nservidores, 8\nCerf, Vinton G., 46, 170, 319, 378\nCERT Coordination Center, 497\nCertificação de chaves públicas, 513-515\nCertificados, 514, 525\nCertification Authority. Veja CA\nChave criptográfica de sessão, 525\nChave de autenticação, 510\n   Redes de computadores e a Internet\n612\nChave privada, 503-505, 511, 520 \nsenhas, 522\nChave simétrica, 519-522\nChaves de sessão, 506, 520-521\nChaves públicas, 503-507, 510-515, 520-522, 525, 527 \nalgoritmos de criptografia/decriptografia, 503, 510 \ncertificar, 522 \nvinculação a uma entidade particular, 514\nCI (Congestion Indication), bit, 197\nCIDR (Classless Interdomain Routing), 251\nCifras de bloco da tabela completa, 501\nCifras de fluxo contínuo, 500\nCifras em bloco, 500-502\nCipher Block Chaining. Veja CBC\nCircuito virtual. Veja VC (circuito virtual)\nCisco Systems, 48, 222, 238, 240, 538, 544 \ndominando o núcleo da rede, 222 \nroteadores e comutadores, 239, 240\nClark, Dave, 302, 370, 432, \nClassificação de pacotes, 479\nClassless Interdomain Routing. Veja CIDR\nClear to Send, quadro de controle. Veja CTS, quadro de \ncontrole\nClientes de correio, 71, 91\nClientes, 8, 62-65, 115 \ncaches Web como, 83 \ncombinando respostas recebidas com solicitações  \n  enviadas, 103 \ncriação de socket TCP, 120 \nendereços IP, 119 \nHTTP, 145 \niniciando contato com servidor, 119 \nnúmero de porta, 117 \npré-busca de vídeo, 440 \nrecebimento e processamento de pacotes, 119\nCMTS (Cable Modem Termination System), 10, 341\nCNAME, registro, 102\nCOA (Care-Of-Address), 413, 417\nCode Division Multiple Access. Veja CDMA\nCódigo de autenticação de mensagem. Veja MAC\nCódigos polinomiais, 328\nColisões, eliminação de comutador, 354\nComcast, 367, 560-561\nComércio na Internet, 47-48, 62 \nnuvem, 48\nCompartilhamento de arquivos, 62\nCompartimentos bem-sucedidos, 451\nCompartimentos de tempo, 332\nComponentes da camada de rede, 196\nComportamento por salto. Veja PHB\nComputação nômade, 59\nComunicação lógica entre processos, 135\nComunicação segura, 496-497\nComunicação individual e endereços IP, 300\nComunicação vizinho a vizinho, 372-375\nComutação de pacotes, 20-21 \nalternativa à comutação de circuitos, 44 \natrasos de enfileiramento, 18 \nmétodo CV (circuito virtual), 196 \nperda de pacotes, 18 \nprotocolos de roteamento, 19 \ntabelas de encaminhamento, 219 \nteoria de filas, 44-45 \ntransmissão segura de voz pelas redes militares, 45 \ntransmissão store-and-forward, 16\nComutação e roteadores, 239\nComutador crossbar, 240\nComutador da camada superior, 364\nComutador de camadas 4, 364\nComutador de pacotes da camada 2, 355\nComutadores da camada de enlace, 2, 16, 39, 228, 352-359\nComutadores plug-and-play, 356\nComutadores, 62 \nmonitoramento do comportamento dos remetentes, 196 \nalgoritmos de roteamento, 366 \naltas taxas de filtragem e repasse, 352 \nautoaprendizado, 353,  \ncamada de enlace, 342-343, 354-357 \ncolhendo estatísticas, 355 \ndifusão de quadros, 344 \ndispositivos plug-and-play, 356 \neliminando colisões, 356 \nendereços da camada de enlace, 343 \nendereços MAC, 343 \nEthernet, 330 \nenlaces heterogêneos, 356 \nfiltragem, 356 \ngerenciamento, 357 \nhierarquia do centro de dados, 364 \ninformação relacionada a congestionamento, 196 \npequenas redes, 357 \nporta de tronco para interconexão, 358 \nprocessando quadros, 356 \nquadro de filtragem, 352 \nquadros da camada de enlace, 352 \nrepasse, 352-353 \nsegurança avançada, 355 \ntabela de comutação, 353 \ntempestades de difusão, 356 \ntempo de envelhecimento, 354 \ntransparentes, 353 \ntrocas de pacotes store-and-forward, 16  \nversus roteadores, 355-356 \nVLANs (Virtual Local Area Networks), 357\nConexão de controle, 85\nConexão TCP de socket do servidor, 119\nConexões não persistentes, 73-76, 145\nConexões persistentes e não persistentes, 73-76\nConexões persistentes, 73-76\nConfidencialidade, 496, 520, 537\nCongestionamento \ncausas e custos, 190-195 \ncapacidade de transmissão desperdiçada, 195 \ngrandes atrasos de fila, 191 \nperda de pacotes, 195 \nretransmissões desnecessárias pelo transmissor, 190\nConsulta \ninformações sobre, 103 \nmensagem ARP, 467\nconsultas iterativas, 101\nÍndice  613 \nConsultas recursivas e servidores de DNS, 101-104\nContent Distribution Networks. Veja CDNs\nControlador de estação-base. Veja BSC\nControle de acesso e SNMPv3, 562-563\nControle de congestionamento adaptativo, 147\nControle de congestionamento assistido pela rede, 196-198\nControle de congestionamento fim a fim, 196\nControle de congestionamento, 175, 190, 184, 439 \nABR (Available Bit-Rate), serviço, 190 \nAIMD (Additive-Increase, Multiplicative-Decrease), 204 \nATM (Asynchronous Transfer Mode), redes, 190 \nassistido pela rede, 195, 198 \nfim a fim, 195, 198 \nmensagem de extinção de origem, 353 \nprincípios, 190-198 \nTCP (Transmission Control Protocol), 198-208 \ntécnicas, 194-195 \nUDP (User Datagram Protocol), 207\nControle de enlace de dados de alto nível. Veja HDLC\nControle de fluxo, 176 \ndiferente do controle de congestionamento, 161 \nTCP, 198\nCookies, 79-81\nCorrespondente, 412, 414-418\nCRC (Cyclic Redundancy Check), códigos, 328\nCriptografia de chave pública, 503-508\nCriptografia de chave pública, 520 \nassinaturas digitais, 510-515 \nchave privada, 510 \nchave pública, 510, 513 \nsistema de e-mail seguro, 520-522\nCriptografia de chave simétrica e CBC (Cipher Block \nChaining), 500-503\nCriptografia polialfabética, 500\nCriptografia, 497-507 \nalgoritmo de decriptografia, 498 \nalgoritmo de encriptação, 497-498 \nchaves, 498 \nconfidencialidade, 497 \ncriptografia de chave pública, 503-508 \ncriptografia de chave simétrica, 498-503 \nfunções hash criptográficas, 508-509 \ntexto cifrado, 498 \ntexto claro, 498 \ntexto limpo, 498\nCSMA (Carrier Sense Multiple Access), 337 \ncolisões, 337 \ndetecção de portadora, 337\nCSMA/CA (Carrier Sense Multiple Access with Collision \nAvoidance), 393, 394, 404 \nCSMA/CD (Carrier Sense Multiple Access with Collision \nDetection), 337 \ndetecção de colisão, 337 \neficiência, 339 \nEthernet, 339\nCSNET (Computer Science Network), 46\nCTS (Clear to Send), quadro de controle, 395\nCV (circuito virtual), 196, 231-232 \nraízes no mundo da telefonia, 235 \nencerramento, 232\nCV, redes, 231-234, 235, \nD\nDados nomeados, 432\nDaemons de roteamento, 497\nDARPA (US Department of Defense Advanced Research \nProjects Agency), 46, 319\nDASH (Dynamic Adaptive Streaming over HTTP), 443\nData Encryption Standard. Veja DES\nDATA, comando SMTP, 90-91\nDATA, quadro, 396\nDatagrama da camada de rede, 41\nDatagrama, 41, 138 \ncomprimento, 246 \ndetectando erros de bit, 246 \nendereços IP de origem e destino, 246-247 \nenvio para fora da sub-rede, 352 \nestendendo cabeçalho IP, 245 \nformato do Internet Protocol (IP) v4, 244-247 \nfragmentação, 247-249 \nmovimentação entre hospedeiros, 38 \npassado à camada de transporte, 248 \nremontando nos sistemas finais, 247 \nrotulação, 360 \nsegmentos da camada de transporte, 177 \nTOS (Type Of Service), bits, 245 \nTTL (Time-To-Live), campo, 246\nDatagrama IPv4, 244-247 \nformato, 245\nDatagrama IPv6, 263-265\nData-Over-Cable Service Interface Specifications. Veja \nDOCSIS\nDDoS (Distributed Denial-of-Service), ataques, 42, 105\nDECnet, arquitetura, 195\nDeering, Steve, 432\nDemultiplexação, 139-145 \norientada para conexão, 142-143 \nnão orientada para conexão, 141-142\nDerivação de chave, 525-526\nDES (Data Encryption Standard), 501\nDES triplo, 522\nDescarte do final da fila, 243\nDescoberta de agente, 417-419\nDetecção de colisão, 337-339\nDetecção de erro \nARQ (Automatic Repeat reQuest), protocolos, 152 \nbits de paridade, 326-328 \nCRC (Cyclic Redundancy Check), códigos, 328-330 \nesquema de paridade bidimensional, 327-328 \nmétodos de soma de verificação, 203, 328-329\nDetecção de invasão, 557\nDetecção de portadora, 337\nDetecção e correção de erro em nível de bit, 325\nDHCP (Dynamic Host Configuration Protocol), 255-260 \nendereços IP designados dinamicamente, 465\nDHCP, mensagem de solicitação, 257, 367\nDHCP, servidores, 256-257, 258, 367\nDHTs (Distributed Hash Tables), 111-115\nDIAMETER, protocolo, 393, 537\nDiferenças entre enlaces com e sem fio, 384\nDiffie-Hellman, algoritmo, 503-507, 549\nDiffserv, 478\n   Redes de computadores e a Internet\n614\nDIFS (Distributed Interframe Space), 394\nDigital Subscriber Line Access Multiplexer. Veja DSLAM\nDigital Subscriber Line. Veja DSL\nDimensionamento de rede, 468-469\nDimensionando redes de melhor esforço, 468-469\nDirect Sequence Wideband CDMA. Veja DS-WCDMA\nDisciplina de enfileiramento por prioridade não preemptiva, \n475\nDisciplina de enfileiramento por varredura cíclica, 475\nDisciplina de varredura cíclica de conservação de trabalho, \n475\nDisciplinas de escalonamento de enlace \ndisciplina de enfileiramento por varredura cíclica, 475 \ndisciplina de varredura cíclica de conservação de \ntrabalho, 475 \nenfileiramento prioritário, 474 \nFIFO (First-In-First-Out), 473-475 \nWFQ (weighted fair queuing), 476\nDispositivo gerenciado, 559\nDispositivos móveis, 59 \ngerenciamento de energia, 402 \nInternet, 404-409\nDispositivos sem fio, 48\nDistance-Vector Multicast Routing Protocol. Veja DVMRP\nDistribuição de carga, 97\nDistribuidores com buffer, 352\nDistributed Inter-frame Space. Veja DIFS\nDMZ (Demilitarized Zone), 544\nDNS (Domain Name System), 38, 42, 61, 95-106, 368 \napelidos do hospedeiro, 96-97 \nataque de DDoS contra hospedeiro direcionado, 105 \nataques e vulnerabilidades, 105 \natraso adicional para aplicações da Internet, 96 \nbanco de dados distribuído e hierárquico, 96, 98 \ncache, 101 \ncomunicação segura, 497 \nconsultas iterativas, 101 \nconsultas recursivas, 101 \ndistribuição de carga, 97 \nmelhorando o desempenho quanto ao atraso, 100 \nmensagens de consulta, 100, 102-104, 368 \nmensagens de consulta e reprodução, 100 \nmensagens de resposta, 100, 102-104 \nobtenção de presença no, 291 \nopção UPDATE, 104 \nparadigma cliente-servidor, 97 \nprotocolo de transporte fim a fim subjacente, 97 \nprotocolos da camada de aplicação, 97 \nrequisições de CDN, 446 \nrotação, 97 \nRRs (Resource Records), 102, 369 \nserviço de tradução entre nome de hospedeiro e endereço \nIP, 100 \nserviços fornecidos pelo, 96 \nservidores, 98 \ntradução de nomes de hospedeiro para endereços IP, 100 \nvisão geral operacional, 97-104\nDNS, banco de dados, 98, 102\nDNS, servidores, 96-99, 367 \nbanco de dados centralizado distante, 98 \nBIND (Berkeley Internet Name Domain), 96 \nDDoS, ataque de inundação de largura de banda, 105 \ndescarte de informações em cache, 102 \nhierarquia, 98 \nrecursão, 103 \nservidor DNS local, 99 \nservidores DNS autoritativos, 99, 102 \nservidores DNS raiz, 99 \nTLD (Top-Level Domain), servidores DNS, 99 \núnico ponto de falha, 98\nDOCSIS (Data-Over-Cable Service Interface Specifications), \n11, 38, 341\nDomínios de alto nível, 98\nDoS (Denial-of-Service), ataques, 42, 105 \nataque de inundação de SYN, 185-189 \nfragmentação e, 249\nDSL (Digital Subscriber Line), 9-15, 23, 48\nDSLAM (Digital Subscriber Line Access Multiplexer), 9-15\nDS-WCDMA (Direct Sequence Wideband CDMA), 408\nDV (Distance-Vector), algoritmo, 270, 274\nDVMRP (Distance-Vector Multicast Routing Protocol), 304\nDynamic Adaptive Streaming over HTTP. Veja DASH\nDynamic Host Configuration Protocol. Veja DHCP\nE\nEAP (Extensible Authentication Protocol), 537\neBGP (external BGP), 289, 292\nEC2, 48\nEFCI (Explicit Forward Congestion Indication), bit, 197\nEliminação da variação de atraso (Jitter), 453\nE-mail baseado na Web, 62, 95\nE-mail, 47, 62, 71, 87-95 \nprotocolos de acesso, 93-95 \nprotocolos da camada de aplicação, 71 \nsegurança, 519-523 \nsistema de e-mail baseado na Web, 95\nEmissor \ndefinição de operação, 151 \ndetectar e recuperar  pacotes perdidos, 157 \nenvio de múltiplos pacotes sem reconhecimentos, 161 \nestado mais à direita, 153 \nestado mais à esquerda, 153 \njanela de recepção, 184 \nnúmero de sequência do pacote, 154 \ntemporizador de contagem regressiva, 157 \nutilização, 159\nEmparelhamento, 25\nEmpresas e redes de acesso, 8\nEncapsulamento IP-em-IP, 419\nEncapsulamento, 39-41,416-419\nEncapsulamento/desencapsulamento e IP móvel, 416-417\nEncapsulation Security Payload, protocolo. Veja ESP\nEncriptação, 524 \ncriptografia, 497-507 \nSNMPv3, 563\nEndereçamento com classes, 252\nEndereçamento da camada de enlace, 343-348\nEndereçamento IPv4, 248-260\nEndereçamento \nÍndice  615 \nInternet, 244-268 \nprocessos, 66\nEndereço externo, 413\nEndereço físico, 343\nEndereço indireto, 300\nEndereço IP temporário, 255\nEndereço permanente, 413\nEndereços da camada de rede, 341-345\nEndereços IP, 19, 66, 95 \nadaptadores, 345 \nagregação de endereço, 253 \nalocação, 254-255 \naumento de tamanho, 262 \nbloco, 254 \nCIDR (Classless Interdomain Routing), 251 \nconhecidos, estabelecendo chamada para, 463-467 \ndestino, 247 \ndifusão, 253 \ndistinção entre dispositivos, 251 \nDNS, 95-106 \nendereçamento com classes, 252 \nendereços privados, 258 \nestrutura hierárquica, 95, 344 \nfaixa, 291 \nglobalmente exclusivos, 250 \ninterfaces, 250 \nmobilidade, 410-417 \nnotação decimal pontuada, 250 \nobtenção, 450 \norigem, 247 \nprefixo, 251-253 \nredes domésticas, 256 \nroteadores, 343, 355 \nservidor DNS autorizado, 102 \nservidor DNS local, 100 \nsub-redes, 250-256 \ntempo de concessão, 257 \ntraduzindo nomes de host, 131-139\nEndereços IPv4, 244, 263\nEndereços IPv6, 263, 379\nEndereços MAC, 343-344, 368 \n802.11, LAN sem fio, 390 \nadaptadores, 343-345, 348-350 \nAP (ponto de acesso), 390 \nBSS, 390, 400 \ncomutadores, 352, 355 \ndois adaptadores não têm os mesmos, 343 \nestrutura plana, 343 \npermanentes, 343\nEndereços de grupo, 263\nEndereços individuais, 263\nEnfileiramento justo ponderado. Veja WFQ\nEnfileiramento prioritário, 474\nEnfileiramento, 241-244\nEngenharia de tráfego, 362\nEnlace de comunicações ponto a ponto, 397\nEnlace de gargalo, 33, 205-206\nEnlaces de acesso múltiplos, 330-342\nEnlaces de comunicação sem fio, 385\nEnlaces de difusão cabeados, 386\nEnlaces de satélite, 12, 29\nEnlaces ponto a ponto, 250, 321, 323\nEnlaces sem fio \nciente de emissor TCP, 426 \ncolisões não detectáveis, 386 \nerros de bit, 386 \ndiferenças de enlaces com fio, 386 \ndiminuindo a força do sinal, 386 \ninterferência de outras fontes, 386 \nproblema do terminal oculto, 386 \npropagação por caminhos múltiplos, 386 \nredução da força do sinal, 386\nEnlaces, 322 \ndifusão, 330 \nformato de cabeçalho MPLS, 361 \nheterogêneos, 354 \nponto a ponto, 330 \ntaxas de transmissão, 3\nEntidade gerenciadora, 559\nEntrega confiável, 323\nEntrega de pacotes na ordem, 228\nEntrega garantida, 228\nEnvenenamento do comutador, 355\nEPC (Evolved Packet Core), 409\nER (Explicit Rate), campo, 197\nErros de bit não detectados, 326\nEscalabilidade de arquitetura P2P, 106\nEscalonador de pacotes, 242\nESP (Encapsulation Security Payload), 530 \nEsquemas de paridade ímpar, 326\nEsquemas de recuperação de perda, 456\nEstabelecimento de conexão, 228\nESTABLISHED, estado, 186\nEstação sem fio, 390\nEstações-base, 282, 390 \ntransferência entre, 423\nEstações terrestres, 16\nEstações, 393-396\nEstado da conexão, 169, 232\nEstado, 86\nEstratégia de seleção de cluster, 447-449\nEstrin, Deborah, 303, 431\nEthernet comutada, 348\nEthernet, 14, 43, 46, 321, 330, 348 \n10BASE-2, 351 \n10BASE-T, 351 \n10GBASE-T, 351-352 \nacesso aleatório por detecção de portadora, 393 \nalgoritmo de recuo exponencial binário, 339 \nalgoritmo de detecção de colisão, 394 \ncamada de enlace e especificação de camada física, 351 \ncomutação de pacotes de armazenamento e \nencaminhamento, 351 \ncomutadores, 13, 348-352, 496 \nCSMA/CD, protocolo, 352 \nenlace de difusão, 352 \nformato de quadro, 352 \nMSS (tamanho máximo de segmento), 170 \nMTU (unidade máxima de transmissão), 349 \nmultiplexação de protocolos da camada de rede, 349 \n   Redes de computadores e a Internet\n616\npadronizado, 352 \nprotocolos da camada física, 39, 351 \nredes domésticas, 12 \nredes locais, 348 \ntopologia de estrela baseada em um comutador, 354\nEWMA (Exponential Weighted Moving Average), 240\nExemplos de protocolo de autenticação, 516-519\nExplicit Forward Congestion Indication, bit. Veja EFCI\nExtensible Authentication Protocol. Veja EAP\nF\nFCFS (First-Come-First-Served), escalonamento, 242, 473\nFDDI (Fiber Distributed Data Interface), 340, 348\nFDM (Frequency-Division Multiplexing), 21, 332, 406\nFDM/TDM, sistemas, 406\nFEC (Forward Error Correction), 327, 426, 453, 456\nFeedback do receptor, 400\nFHSS (Frequency-Hopping Spread Spectrum), 402\nFiber-to-the-home. Veja FTTH\nFibra óptica, 15 \n100 Mbits Ethernet, 355\nFIFO (First-In-First-Out), 473\nFila de mensagens, 87\nFila de saída, 18\nFilas \ndisciplina de enfileiramento por varredura cíclica, 475 \ndisciplina de varredura cíclica de conservação de \ntrabalho, 475 \nFIFO (First-In-First-Out), 473 \nenfileiramento prioritário, 474 \nmáximo atraso provável, 477 \npolítica de descarte de pacote, 473 \nWFQ (Weighted Fair Queuing), 475\nFiltragem de pacotes, 539-540\nFiltragem, 352-353\nFiltros de pacotes com controle do estado, 540-541\nFiltros de pacotes tradicionais, 539\nFIN, bit, 172, 186\nFio de cobre de par trançado, 14, 355\nFIOS, serviço, 11\nFirewalls, 495, 538 \nataques de pacotes maliciosos, 262 \nbloqueio de pacotes, 262  \nfiltrando, 539-540 \ngateways de aplicação, 538, 541-542 \ntabela de conexão, 538 \ntráfego autorizado, 538\nFirst-Come-First-Served, escalonamento. Veja FCFS\nFirst-In-First-Out, ordem. Veja FIFO\nFluxo (de pacotes), 241, 263-264\nFluxo contínuo adaptativo, 449-451\nFluxo contínuo de vídeos previamente gravados, 436\nFluxo contínuo HTTP adaptativo, 438\nFluxo contínuo, 436 \náudio e vídeo ao vivo, 436, 438 \náudio e vídeo armazenado, 436 \nvídeo, 434\nFluxo de vídeo, 434, 462\nFones móveis e estações base de LAN sem fio, 405\nFora da banda, 85\nFragmentação de datagramas, 247\nFragmentos superpostos, 338\nFrequency-Hopping Spread Spectrum. Veja FHSS\nFSM (Finite-State Machine), 151 \nestado inicial, 151 \nestendida, 162\nFSM (máquina de estado finito) estendida, 162\nFTP (File Transfer Protocol), 37, 62, 85, 97\nFTTH (Fiber To The Home), 11\nFull-duplex, serviço, 169\nFunção de medição, 480\nFunção de repasse, 235\nFunções do plano de controle do roteador, 236\nFunções hash criptográficas, 508-509\nFunções hash, 112, 521, 526 \nassinaturas digitais, 695-696\nG\nGarantia flexível, 468\nGarantia rígida, 468\nGarantias de temporização, 68\nGateway GPRS Support Nodes. Veja GGSNs\nGateways de aplicação, 538, 541-542 \ninspeção profunda de pacote, 544\nGBN (Go-Back-N), protocolo, 161\nGeneralized Packet Radio Service. Veja GPRS\nGeograficamente mais próximo, 447\nGerador, 328\nGerenciamento de chaves, 534\nGerenciamento de configuração, 558, 561\nGerenciamento de contabilização, 558, 561\nGerenciamento de desempenho, 557, 561\nGerenciamento de energia, 402\nGerenciamento de falhas, 557-558, 561\nGerenciamento de rede \nComcast, estudo de caso,  560-561 \ndefinição, 557 \ndetecção de invasão, 557 \ndispositivo gerenciado, 559 \ngerenciamento contábil, 558 \ngerenciamento de configuração, 558 \ngerenciamento de desempenho, 557 \ngerenciamento de falhas, 557 \ngerenciando entidades, 559 \ninfraestrutura, 558-560 \nMIB (Management Information Base), 559 \nmonitoramento de hospedeiro, 556 \nmonitorando o tráfego, 556 \ngerenciamento de segurança, 558 \nobjetos gerenciados, 559 \npadrões, 560 \nprotocolo de gerenciamento de rede, 560 \nSLAs (Service Level Agreements), 556\nGerenciamento de temporizador de retransmissão, 177\nGET condicional, 83\nGET, método, 76-77, 83\nGGSNs (Gateway GPRS Support Nodes), 407\nGigabit Ethernet, 352\nÍndice  617 \nGlobal System for Mobile Communications, padrões. Veja \nGSM\nGMSC (Gateway Mobile services Switching Center), 421\nGo-Back-N, protocolo. Veja GBN, protocolo\nGoogle, 25, 48, 62, 83, 89, 202, 362, 445\nGPRS (Generalized Packet Radio Service), 407\nGrafo, 269\nGrupo multicast, 299\nGSM (Global System for Mobile Communications), 405-408, \n420 \nBTS (Base Transceiver Station), 406 \ncélulas, 406 \ncodificando áudio, 463 \nmobilidade, 425 \nMSC âncora, 424 \nPLMN (Public Land Mobile Network) nativa, 420 \nrede nativa, 420 \nrede visitada, 420 \nroteamento indireto, 420 \nroteando células para usuário móvel, 422-423 \ntransferências, 423\nH\nHDLC (High-level Data Link Control), 330\nHFC (Hybrid Fiber Coax), 10\nHíbrida fibra-coaxial. Veja HFC\nHierarquia multinível, 24\nHipertexto, 47\nHLR (Home Location Register), 420\nHMAC, padrão, 510\nHOL (Head-Of-the-Line), bloqueio, 243\nHospedeiros sem fio, 382, 392\nHospedeiros, 1, 3, 7 \naliasing, 96 \narmazenando informações de roteamento, 280 \nbalanceador de carga, 364 \ndatacenter, 364 \nconectados na rede, 250 \nconexão por modem discado, 360 \ndesignando endereços IP a, 250 \nendereços da camada de enlace, 343 \nendereços da camada de rede, 343 \nendereços IP, 19, 66, 95, 145, 343 \nendereços MAC, 343 \ninterfaces, 250 \nmensagem de descoberta DHCP, 392 \nmonitorando, 555 \nmovendo datagramas entre, 38 \nmudando a estação-base, 381-382 \nnome canônico de hospedeiro, 96 \nnomes de hospedeiros, 85 \nprotocolos da camada de rede, 349 \nroteador default, 269 \nsem fio, 382 \nserviço de entrega processo a processo, 138-139 \nservidor DNS local, 99 \ntabela ARP, 345\nHost remoto, transferindo arquivos, 85\nHSP (High Speed Packet Access), 409\nHTML, 47, 72, 74\nHTTP (HyperText Transfer Protocol), 38, 71, 93-95 \ncaches Web, 81-83 \nprograma cliente, 74 \ncomparação de SMTP com, 91 \nconexões não persistentes, 74 \nconexões persistentes, 75 \ncookies, 79 \nformato de mensagem, 76 \nGET condicional, 84 \nlinha de cabeçalho If-Modified-Since, 84 \nmensagem de resposta, 77, 98, 463 \nmétodo POST, 77 \nmétodo PUT, 77 \nprograma servidor, 72, 74 \nprotocolo de recuperação (pull protocol), 91 \nprotocolos sem estado, 73  \nrequisição GET, 77, 369, 569 \nsegurança SSL, 523 \nsem estado, 81 \nTCP e, 73, 85, 137\nHTTP persistente, 145\nHTTP, fluxo contínuo, 439,  \npré-busca de vídeo, 440\nHubs, 348\nHulu, vídeo armazenado de fluxo contínuo, 436\nHyperText Transfer Protocol. Veja HTTP\nI\niBGP (internal BGP), 289, 290, 292\nIBM SNA, arquitetura, 46, 195\nICANN (Internet Corporation for Assigned Names and \nNumbers), 104, 255\nICMP (Internet Control Message Protocol), 260 \ndatagrama, 189 \nIPv6, 265 \nmensagens, 261\nIDEA, 552\nIDSs (Intrusion Detection Systems) baseados em anomalias, \n544\nIDSs (Intrusion Detection Systems), 262, 544\nIDSs (sistemas de detecção de intrusão) baseados em \nassinatura, 544\nIEEE 802 LAN/MAN Standards Committee, 4\nIEEE 802.3 CSMA/CD (Ethernet), grupo de trabalho, 351\nIEEE controlando o espaço de endereços MAC, 340\nIETF (Internet Engineering Task Force), 5, 493 \nExpectativa de Tempo de Vida dos Endereços, grupo de \n   \n  trabalho, 263 \npadrões para CAs, 493\nIf-Modified-Since: linha de cabeçalho, 84\nIGMP (Internet Group Management Protocol), 265, 300\nIKE (Internet Key Exchange), 533\nIMAP (Internet Mail Access Protocol), 93, 94-95\nIMAP, servidor, 94\nImpedimento de congestionamento, 198\nImplantação de túnel, 266, 414\nInformações na banda, 85\nInspeção profunda de pacote, 544\n   Redes de computadores e a Internet\n618\nIntegridade de dados, 268, 524\nIntegridade de mensagem, 495, 496, 507 \nassinaturas digitais, 507 \nsistema de e-mail seguro, 520 \ntécnicas criptográficas, 497\nIntel 8254x, controlador, 324\nInteração cliente-servidor Web, 369\nInteração usuário-servidor e HTTP (HyperText Transfer \nProtocol), 79\nInteratividade, 436\nIntercalação, 456\nInterface de dados distribuída de fibra. Veja FDDI\nInterface de socket no lado do cliente, 73\nInterfaces \ndesignando endereços IP a, 250 \nendereços IP, 240 \nroteadores, 348\nIntermediate-System-to-Intermediate-System, algoritmo de \nroteamento. Veja IS-IS, algoritmo de roteamento\nInternational Organization for Standardization. Veja ISO\nInternet API, 4, 524\nInternet Control Message Protocol. Veja ICMP\nInternet Corporation for Assigned Names and Numbers. Veja \nICANN\nInternet Engineering Task Force. Veja IETF\nInternet eXchange Points. Veja IXPs\nInternet Group Management Protocol. Veja IGMP\nInternet Key Exchange. Veja IKE\nInternet Mail Access Protocol. Veja IMAP\nInternet Protocol. Veja IP\nInternet Service Providers. Veja ISPs\nInternet \nacesso por celular, 404 \nacesso sem fios, 13 \naplicações distribuídas, 4 \narquitetura em camadas, 35-39 \ncamada de rede, 245 \nclasses de serviço, 469 \ncomercialização, 47-48 \ncomutadores da camada de enlace, 2, 16, 39, 228, 352-359 \nconectando sistemas finais, 4 \nconectividade, 291 \ndescrição de serviços, 4 \ndia da conversão, 265 \ndificuldade para fazer mudanças na, 303 \ndispositivos móveis, 404-409 \ne-mail, 47, 62, 71, 87-95, 520 \nendereçamento, 244-268 \nentregando data, 6 \nestratégia de designação de endereço, 250 \nhistória, 44-48 \nhospedeiros, 1, 3 \nISPs (Internet Service Providers), 3, 23 \nIXPs (Internet eXchange Points), 25 \nmodelo de serviço de melhor esforço, 138, 229, 468-469 \nobtendo presença, 291 \npadrões, 4 \npilha de protocolos, 37 \nprotocolo intradomínios, 369 \nprotocolos, 3, 5 \nprotocolos de roteamento, 19, 283 \nrede de redes, 1, 23 \nredes de acesso, 8 \nredes de celular 3G e 4G, 48, 405-410 \nredes privadas de provedor de serviço, 49 \nrepasse, 244-268 \nrepasse de pacotes, 19 \nroteadores, 3 \nroteamento dentro do AS, 283 \nroteamento entre ASs, 283 \nroteamento multicast, 287 \nserviços de transporte, 68 \nsistemas finais, 1, 3, 7 \nservidores de DNS raiz, 99 \nTLD (Top-Level Domain), servidores, 99 \ntráfego global, 3 \nvisão geral da camada de enlace, 321 \nvisão geral da camada de transporte, 138 \nWeb, 48, 72, 81\nInternet, aplicação de telefone, 452, 458\nInternet, hospedeiros. Veja hospedeiros\nInternet, protocolos de roteamento, 283\nInternet, protocolos de transporte, 68\nInternet Standard Management Framework, 562\nInter-redes, 62\nInundação controlada pelo número de sequência, 296\nInundação controlada, 296-299\nInundação de largura de banda, 42\nInundação não controlada, 296\nInundação por escopo limitado, 299\nInundação, 296-299\nIP (Internet Protocol), 3, 38, 138, 224, 244-268,  283 \ncamada de rede, 224, 244 \nconfinar o desenvolvimento da Internet, 319 \ndatagramas, 177, 244 \nendereçamento e repasse na Internet, 244 \nmulticasting, 436 \npadrão de facto para inter-redes, 512 \npenetração, 431 \nroteadores, 38 \nsegurança, 267-268 \nseparação do TCP, 46 \nserviço de entrega pelo melhor esforço, 138, 229, 468-469\nIP móvel, 258, 381, 411, 417-420\nIP de grupo (multicast), 305\nIP spoofing, 44\nIP, anycast, 448\nIP, datagramas, 171, 367 \nataques e, 262 \nencriptação de payloads, 363 \nfragmentação, 247-249 \nMTU (Maximum Transmission Unit), 349 \nprotocolos da camada de transporte, 246 \nquadros Ethernet, 348 \nsegurança, 528\nIP, endereço de broadcast, 295\nIPng (Next Generation IP), 263\nIPS (Intrusion Prevention System), 262, 544\nIPsec, 267, 500, 528\nIPTV, 64\nÍndice  619 \nIPv4 (protocolo IP versão 4), 120, 224,  244 \nIPsec, 267 \ntransição para IPv6, 265\nIPv6 (protocolo IP versão 6), 245, 247, 263 \nabordagem pilha dupla, 265 \nendereços IP, 263 \nrotulação de fluxo e prioridade, 263 \ntransição de IPv4, 265 \ntunelamento, 265\nIS-IS (Intermediate-System-to-Intermediate-System), 283, 369\nIS-IS, protocolo, 369\nISO (International Organization for Standardization), 39, 557 \nIsolamento de tráfego, 356, 471\nISP de acesso, 24\nISP local, 291\nISPs (Internet Service Providers) comerciais, 47\nISPs (Internet Service Providers) de trânsito global, 24\nISPs (Internet Service Providers), 24 \nacordos de emparelhamento, 399 \nASs (Autonomous Systems), 283 \ncamada inferior e camada superior, 3 \nconvenções de nomeação e endereçamento, 3 \nmulti-homing, 25 \nobtenção de um bloco de endereços de, 254 \npares, 25 \nredes de acesso, 8-14 \nrelacionamento cliente-provedor, 24 \nserviço fim-a-fim, 481 \ntrânsito global, 24\nISPs de nível 1, 24\nISPs de acesso com fio \nníveis de serviço em camadas, 469\nISPs do cliente, 25\nISPs regionais, 24\nISPs residenciais, 63\nITU-T (International Telecommunication Union), 233, 566\nIXPs (Internet eXchange Points), 25\nJ\nJacobson, Van, 222, 432\nJanela de congestionamento, 198-205, 426\nJanela de recepção, 184\nJava e programação cliente-servidor, 115\nJogos on-line com jogadores múltiplos, 68\nJustiça \nalgoritmo AIMD, 204 \nconexões TCP paralelas, 208 \nmecanismo de controle de congestionamento, 203 \nTCP (Transmission Control Protocol), 203 \nTDM (Time-Division Multiplexing), 332 \nUDP (User Datagram Protocol), 206\nK\nKademlia DHT, 114\nKahn, Robert, 6245, 170, 497\nKanKan, 64, 433, 449, 451\nKleinrock, Leonard, 29, 44, 58, 378\nL\nLam, Simon S., 378\nLâminas, 263\nLANs (Local Area Networks), 12 \ncomutadores, 12, 342, 348, 357 \nendereço de difusão, 344 \nendereço MAC, 343 \nEthernet, 348 \nhierarquicamente configuradas, 357 \ntopologia de estrela baseada em um hub, 348 \nVLANs (Virtual Local Area Networks), 357-360\nLANs comutadas \nARP (Address Resolution Protocol), 343-348 \ncomutadores da camada de enlace, 352-360 \nendereçamento da camada de enlace, 343-348 \nendereços MAC, 343 \nenvenenamento de comutador, 480 \nEthernet, 348 \nVLANs (Virtual Local Area Networks), 357-360\nLANs sem fio e padrões 802.11, 389\nLANs sem fio, 330 \nDHCP (Dynamic Host Configuration Protocol), 255 \nestações-base de LAN, 404 \nponto de acesso, 12 \nsegurança, 534-538 \ntecnologia IEEE 802.11, 12 \nversus sistemas celulares móveis 3G, 405 \nWi-Fi, 12\nLargura de banda, 21, 205 \nalocação em nível de enlace, 469 \nmínima garantida, 228 \nrecurso “useou perca”, 473 \nvídeo, 434, 440\nLast-Modified:, linha de cabeçalho, 78, 85\nLDNS (Local DNS Server), 97, 446-447\nLei µ do PCM, 463\nLEO (Low-Earth Orbiting), satélites, 16\nLimelight, 83, 445, 450\nLinguagem de definição de dados, 562\nLinha de estado em mensagens de resposta HTTP, 78\nListas de controle de acesso, 538\nLocal DNS Server. Veja LDNS\nLong-Term Evolution. Veja LTE\nLoop de roteamento, 278\nLow-Earth Orbiting, satélites. Veja LEO, satélites\nLSAs (Link-State Advertisements), 299\nLTE (Long-Term Evolution), 13, 408\nM\nMAC (Message Authentication Code), 509-510, 572 \ncomparação com assinaturas digitais, 512-513\nMAC (Multiple Access Protocols), 323, 343, 393 \nLANs sem fio 802.11, 393-397\nMais raro primeiro, 110\nMalware, 41-42\nManagement Information Base. Veja MIB\nMANETs (redes ad hoc móveis), 384\nMapeamento de rede, 544\n   Redes de computadores e a Internet\n620\nMáquina de estado finito. Veja FSM\nMarcação de pacote, 471 \nMarca de tempo, 453-456, 461\nMáscara de sub-rede, 250\nMaster Key. Veja MK\nMaster Secret. Veja MS\nMBone, rede multicast, 304\nMCR (Minimum Cell transmission Rate), 229\nMD5, 522 \nautenticação, 287\nMD5, algoritmo de hash, 522\nMDCs (Modular Data Centers), 366\nMDCs baseados em contêiner (centros de dados modulares), \n366\nMecanismo de “balde furado”, 471, 476\nMecanismos de escalonamento, 473-476\nMédia móvel exponencial ponderada. Veja EWMA\nMedições em tempo real de desempenho de atraso e perda, \n447\nMeio compartilhado, 15\nMensagem de descoberta do roteador, 418-419\nMensagem de repressão da origem, 261\nMensagem instantânea, 47, 61, 466\nMensagem MAP, 341\nMensagens da camada de aplicação, 40\nMensagens de consulta, 104\nMensagens de estado de enlace, 508\nMensagens de requisição e HTTP, 76\nMensagens de resposta e DNS (Domain Name System), 104\nMensagens de resposta e HTTP, 77\nMensagens de sinalização, 233\nMensagens trap, 569\nMensagens de adesão à árvore, 298\nMensagens, 40 \nautenticação, 508 \nbisbilhotando, 673 \ncamada de aplicação, 40 \nconfidencialidade, 496 \ncriptografadas, 496 \ndivisão em segmentos menores, 40 \nformato HTTP, 76-79 \nintegridade, 496 \nsegmentação, 56 \nsemântica de campos, 71 \nsintaxe, 71 \ntransmissão  e recebimento, 4\nMetcalfe, Bob, 336,348, 350\nMIB (Management Information Base), 559-563, 566-568\nMIB, módulos, 565, 568\nMIB, objetos, 565\nMicrosoft, 47, 83\nMeios físicos, 14 \ncabo coaxial, 15 \ncanais de rádio, 15 \ncustos, 14 \ncanais de rádio por satélite, 16 \nfibra ótica, 15 \nfio de cobre de par trançado, 14 \nmeios guiados, 14 \nmeios não guiados, 14\nMeio guiado, 14\nMeio não guiado, 14\nMIMO (Multiple-Input, Multiple Output), antenas, 409\nMinitel, projeto, 43\nMK (Master Key), 537\nMobilidade, 380 \nendereçamento, 412-413 \nem uma sub-rede IP, 400 \nescalabilidade, 558 \nfuncionalidade exigida na camada de rede, 414 \ngerenciamento de rede celular, 420-425 \ngerenciamento, 410-417 \nGSM (Global System for Mobile Communications),    \n  424-425 \nIP móvel, 417 \nredes sem fio, 417 \nroteamento para um nó móvel, 413-417\nModelo de carga de trabalho, 469\nModelo de serviço, 35\nModelos de serviço da camada de rede, 235\nModelos de serviço de rede, 228-230\nModems a cabo, 10, 15, 341\nModems, 9\nModo comando-resposta, 568\nModo de transferência assíncrono. Veja ATM\nModo de transporte, 531\nModo túnel, 531\nMonitorar, 496\nMP3 (MPEG 1 layer 3), 435\nMPEG, 461-462\nMPLS (Multiprotocol Label Switching), 360-362\nMPLS, caminhos, 303\nMS (Master Secret), 525\nMSC (Mobile Switching Center), 408\nMSC de âncora, 424\nMSC nativa, 424\nMSC visitado, 424\nMSDP (Multicast Source Discovery Protocol), 305\nMSRN (Mobile Station Roaming Number), 422\nMSS (Maximum Segment Size), 170\nMTU (Maximum Transmission Unit), 170 \ndatagramas IP, 349 \nEthernet, 349\nMudanças de custo de enlace e algoritmo DV (vetor de \ndistâncias), 274\nMulticast específico da origem. Veja SSM\nMulticast Source Discovery Protocol. Veja MSDP\nMultimídia \nmétodo de remessa em nível de sistema, 467 \nsuporte de rede para, 467 \nvídeo de fluxo contínuo armazenado, 433, 436 \nVoIP (Voice-over-IP), 452\nMúltiplas classes de serviço, 469-478\nMúltiplas interconexões, 21\nMultiplexação da camada de transporte, 139\nMultiplexação ortogonal por divisão de frequência. Veja \nOFDM\nMultiplexação por divisão de frequência. Veja FDM\nMultiplexação por divisão de tempo. Veja TDM\nMultiplexação, 139 \nÍndice  621 \norientada para conexão, 141 \nredes de comutação de circuitos, 20-23 \nnão orientada para a conexão, 141-142\nMultiprotocol Label Switching. Veja MPLS\nMX, registro, 102\nN\nNAKs (reconhecimentos negativos), 152\nNão repúdio e técnicas criptográficas, 497\nNão sufocado, 110\nNapster, 47\nNAT (Network Address Translation), 224, 239, 250, 258 \nSkype, 458\nNavegadores Web, 71 \ninterfaces GUI, 47 \nlado cliente do HTTP, 73\nNavegadores, 73 \nprocessos cliente, 65 \nsolicitações HTTP direcionadas ao cache Web, 81-83\nNCP (Network-Control Protocol), 45\nNegação de serviço, ataques. Veja DoS, ataques\nNetflix, 48, 61, 433, 449-450\nNetscape Communications Corporation, 47\nNEXT-HOP, atributo, 290, 292\nNI (No Increase), bit, 197\nNIC (Network Interface Card), implementação da camada de \nenlace, 324\nNmap, ferramenta de varredura de porta, 144, 189-190\nNo Increase, bit. Veja NI, bit\nNOC (Network Operations Center), 555\nNome de servidor default, 99\nNomes canônicos de hospedeiros, 96\nNomes de hospedeiros, 96 \napelido, 96 \ncanônicos, 96 \ntradução para endereços IP, 96-98, 100\nNonces, 527, 546, 572\nNós IPv6/IPv4, 263\nNós móveis, 407-410 \nCOA (Care-Of-Address), 413 \ncorrespondentes, envio de datagramas para, 415 \nendereço externo, 413 \nendereço permanente, 413 \nprotocolo de localização, 416 \nrede externa, 413 \nrede nativa, 413 \nregistrando com agente externo, 418 \nresidência permanente, 412 \nroteamento direto, 415 \nroteamento indireto, 413 \nroteando para, 413\nNotação decimal separada por pontos, 250\nNslookup, programa, 104\nNúcleo da rede \ncomutação de circuitos, 20-23 \ncomutação de pacotes, 16 \nrede de redes, 24\nNúmero de portas bem conhecido, 140\nNúmero de roaming, 422\nNúmero de versão, 238\nNúmero roaming da estação móvel. Veja MSRN\nNúmeros de canal, 391\nNúmeros de porta de destino, 140, 171\nNúmeros de porta de origem, 140, 171, 259\nNúmeros de porta, 66  \nbem conhecidos, 140 \ndestino, 171 \nendereçando processos, 260 \norigem, 171 \nprotocolos, 66 \nservidores Web, 143-145\nNúmeros de sequência, 154, 156, 161, 163-169, 171, 172-174, 453 \nIPsec, 528 \npacotes RTP, 460 \nSSL (Secure Sockets Layer), 523 \nsegmento SYN, 185-189 \nsegmentos TCP, 171-175 \nTCP (Transmission Control Protocol), 179-183 \nTelnet, 174\nNuvem da Amazon, 449-450\nO\nOBJECT IDENTIFIER, tipo de dado, 563\nObjetos gerenciados, 559\nObjetos, 72,76\nOC (Optical Carrier), enlace padrão, 15\nOcultação de erro, 458\nOLT (Optical Line Terminator), 11\nONT (Optical Network Terminator), 11\nOpen Systems Interconnection model. Veja OSI, modelo\nOpen-Shortest Path First. Veja OSPF\nOptical Carrier, enlace padrão. Veja OC, enlace padrão\nOrigem \natraso total até o destino, 31 \nhospedeiro e roteador de origem, 269\nOS, ataques de vulnerabilidade, 544\nOSI (Open Systems Interconnection), modelo, 39\nOSPF (Open-Shortest Path First), 271, 283, 286-288, 539 \nautenticação de roteador, 286 \nLSAs (Link-State Advertisements), 299\nP\nP2P (peer-to-peer) \naplicações, 106 \narquitetura, 106 \naplicações de fluxo contínuo de vídeo, 439 \nBitTorrent, 109 \nDHTs (Distributed Hash Tables), 111-115 \ncompartilhamento de arquivo, 62, 64-67 \nNAT, 224, 239, 250, 258 \nreversão de conexão, 260 \nSkype, 458\nPacket Satellite, 378\nPacote de congestionamento, 196\nPacotes da camada de rede, 38\nPacotes da camada de transporte, 136\n   Redes de computadores e a Internet\n622\nPacotes de dados duplicados, 154-157\nPacotes multicast, 299-305\nPacotes perdidos, 193\nPacotes, 43, 16 \nACKs duplicados, 176 \natraso de fim a fim, 452 \natrasos, 26-28 \natrasos de fila, 26, 27, 29 \natrasos de ida e volta, 21 \natrasos de processamento, 27 \nbuffering, 163 \ncaminho, 26 \ncampos de cabeçalho, 40 \ncampos de carga útil, 40 \ncomutação, 239 \ndescarte, 30, 243 \ndestino, 26 \ndetectando perda, 156 \nduplicados, 154, 157 \nendereço de destino do prefixo, 234 \nendereço de origem, 116 \nendereço de origem do transmissor, 116 \nendereço IP de destino, 116 \nenfileiramento por varredura cíclica, 475 \nenfileiramento prioritário, 474 \nentrega, 149 \nenvio, 16 \nenvios múltiplos, 161 \nerros de bit, 152 \nFIFO (First-In-First-Out), 473-475 \nformato de, 3 \ninundação controlada, 296-299 \ninundação não controlada, 296 \nIP spoofing, 44 \nmesma classe de prioridade, 468 \nmovimento entre nós, 52 \nnúmeros de sequência, 154, 156, 161, 163-169 \nnúmero de VC, 230 \nonde ocorre a formação de fila, 241 \no que fazer quando há perda, 157 \norigem, 26 \nrastreamento, 31 \nreconhecimento cumulativo, 163 \nrepasse, 3, 225, 235, 479 \nroteamento, 225 \nRTT (Round-Trip Time), 75 \nsockets, 114 \ntamanho de rajada, 478 \ntaxa de pico, 477 \ntaxa média, 476 \ntransmissão, 151  \nvariabilidade de atraso (jitter), 453 \nWFQ (Weighted Fair Queuing), 474\nPadrão IP móvel, 415\nPaginação, 406\nPáginas Web, 71 \nexibição de, 74 \nrequisições de, 366\nPar trançado não blindado. Veja UTP\nPar, 25\nPares, 63, 106 \nDHT, 114 \ncompartilhamento de arquivos, 106-111 \ntorrent, 109\nParidade bidimensional, esquema, 327-328\nPBXs (Private Branch Exchanges), 627\nPCM (Pulse Code Modulation), 463\nPDU e SNMP, aplicações, 569\nPedaços, 111 \natraso na reprodução, 451-456\nPedidos de comentários. Veja RFCs\nPeer churn e DHTs (Distributed Hash Tables), 114\nPeer-to-peer, aplicações. Veja P2P, aplicações\nPerda de pacotes, 18, 30,  190, 437 \nFEC (Forward Error Correction), 453, 456 \niminente, prevenção, 205 \nintercalação, 456 \nocultação de erro, 458 \nrecuperação de, 154, 456\nPerfil de tráfego, 545\nPeríodos de silêncio, 21\nPesos de enlace, 286-288\nPesquisas, 340\nPGP (Pretty Good Privacy), 500, 513, 520, 522\nPHB (Per-Hop Behavior), 479-481\nPHB de repasse acelerado, 481\nPHB de repasse assegurado, 481\nPiconet, 403\nPilha de protocolos, 37\nPIM (Protocol-Independent Multicast), 304, 432\nPing, programa, 261\nPipelining (tubulação), 111 \nconexões persistentes, 74 \nTCP (Transmission Control Protocol), 4, 138\nPlaca de interface de rede. Veja NIC\nPlano de controle de roteamento, 244\nPlano de controle de software, 244\nPlano de dados de hardware, 244\nPlano de dados e hardware, 244\nPlano de repasse do roteador, 244\nPlano de repasse, 244\nPMS (Pre-Master Secret), 527\nPoint-to-Point Protocol. Veja PPP\nPoliciamento de tráfego, 638-639\nPoliciando disciplinas, 645-648\nPoliciando mecanismos, 640\nPolítica de descarte de pacote, 473\nPolítica de importação, 290\nPONs (Passive Optical Networks), 11\nPonto a ponto, 169\nPonto de encontro, 298\nPontos de acesso. Veja AP\nPontos de presença. Veja PoPs\nPOP3 (Post Office Protocol-Version 3), 93\nPOP3, agente do usuário, 93\nPOP3, servidor, 93\nPoPs (pontos de presença), 25\nPortas de entrada, 235 \ncomutação para portas de saída, 236 \nfilas de pacotes, 241 \nÍndice  623 \nprocessamento, 236\nPortas de saída, 236 \nescalonador de pacotes, 242 \nfilas de pacotes, 241 \nprocessamento, 236\nPost Office Protocol-Version 3. Veja POP3\nPOST, método, 76-77\nPPP (Point-to-Point Protocol), 330, \nPPstream, 64\nPPTV e entrega de P2P, 451\nPré-busca, 436 \nvídeo, 440\nPrefixo de rede, 251\nPrefixos, 251, 253, 289 \natributos BGP, 289 \nconscientização de, 390 \nroteadores, 288 \ntabela de repasse, 289-290\nPre-Master Secret. Veja PMS\nPretty Good Privacy. Veja PGP\nPrincípio de fim a fim, 149\nPrivacidade \ncookies, 79 \nservidores proxy, 543 \nsites Web, 738QQ, 623 \nSkype, 458 \nSSL (Secure Sockets Layer), 523\nPrivate Branch eXchanges. Veja PBXs\nProblema de acesso múltiplo, 330\nProblema de roteamento triangular, 415\nProcessamento de entrada, 237-239\nProcessamento de nó, 27\nProcessos servidores, 65, 119, 169\nProcessos, 65-71 \napresentação (handshaking), 169 \ncomunicação lógica entre, 135 \ncomunicação pelo envio de mensagens a sockets, 116 \ncomunicando usando sockets UDP, 116 \nsockets de conexão, 145 \nsockets, 139\nPrograma especial de servidor de socket, 119\nPrograma servidor, 114, 119\nProgramação baseada em eventos, 164\nPropagação de multicaminhos, 384\nProtocol-Independent Multicast. Veja PIM\nProtocolo de controle da rede. Veja NCP\nProtocolo de estabelecimento, 483\nProtocolo de gerenciamento de rede, 558\nProtocolo de janela deslizante, 161\n Protocolo de nó móvel ao agente externo, 414\nProtocolo de passagem de permissão (token), 340\nProtocolo de transferência confiável de dados. Veja rdt\nProtocolo IP versão 4. Veja IPv4\nProtocolo IP versão 6. Veja IPv6\nProtocolo plug-and-play, 255\nProtocolo pull, 91\nProtocolo push, 91\nProtocolo slotted ALOHA, 333 \ndecisão de transmitir do nó, 336\nProtocolos confiáveis de transferência de dados em tubulação, \n159\nProtocolos da camada de aplicação, 37 \naplicação de e-mail da Internet, 81 \ncorreio eletrônico, 85 \nDNS (Domain Name System), 95-97 \ndomínio público, 83 \nFTP (File Transfer Protocol), 37 \nHTTP (HyperText Transfer Protocol), 37, 81 \nproprietários, 81 \nsegurança, 519 \nSMTP (Simple Mail Transfer Protocol), 37, 81, 88 \nTelnet, 174\nProtocolos da camada de enlace, 38 \nacesso à Internet a cabo, 341 \ndetecção e correção de erro, 324 \nserviços, 324\nProtocolos da camada de rede \ncomunicação lógica entre hospedeiros, 135-138 \ndificuldade para mudar, 267 \nIP (Internet Protocol), 139 \nmúltiplos, hospedeiros suportando, 356 \nrestritos por modelo de serviço, 138\nProtocolos da camada de transporte, 38, 139 \ncomunicação lógica entre processos, 135-138 \ndatagramas IP, 245 \nentrega confiável, 323 \nimplementação de sistemas finais, 135 \nresidindo em sistemas finais, 138 \nsegurança, 68, 519 \nTCP (Transmission Control Protocol), 138 \ntemporização, 68 \ntransferência confiável de dados, 66 \nUDP (User Datagram Protocol), 138 \nvazão, 67\nProtocolos de acesso a correio, 92-95\nProtocolos de acesso aleatório, 332, 333, 350 \nAloha, protocolo, 333 \nCSMA (Carrier Sense Multiple Access), protocolo, 333 \nCSMA/CD (Carrier Sense Multiple Access with Collision \nDetection), 336 \nprotocolo slotted ALOHA, 333\nProtocolos de acesso múltiplos, 330-342 \ncaracterísticas, 330-332 \nCDMA (Code Division Multiple Access), 333 \nFDM (Frequency-Division Multiplexing), 332 \nprotocolo ALOHA, 46 \nprotocolos de acesso aleatório, 332 \nprotocolos de particionamento de canal, 332 \nprotocolos de revezamento, 332, 340 \nTDM (Time-Division Multiplexing), 333\nProtocolos de bit alternante, 158\nProtocolos de camada superiores e redes sem fio e mobilidade, \n425-427\nProtocolos de estado do enlace, 286, 295\nProtocolos de estado flexível, 301\nProtocolos de divisão de canal, 332 \nCDMA (Code Division Multiple Access), protocolo, 333,  \n  351 \nFDM (Frequency-Division Multiplexing), 332 \nTDM (Time-Division Multiplexing), 332\nProtocolos de polling, 340\n   Redes de computadores e a Internet\n624\nProtocolos de rede seguros e integridade de mensagem, 508\nProtocolos de rede, 5\nProtocolos de repetição seletiva. Veja SR, protocolos\nProtocolos de reserva de recursos, 267\nProtocolos de revezamento, 332, 340\nProtocolos de roteadores internos, 283\nProtocolos de roteamento entre ASs, 283 \nBGP (Border Gateway Protocol), 288-295Protocolos de \nroteamento intra-AS (sistema autônomo), 283, 286, 294\nProtocolos de roteamento, 19, 38 \nBGP (Border Gateway Protocol), 288-295 \ndentro do AS, 283 \nDV (vetor de distâncias), algoritmos, 279 \nentre ASs, 283 \nexecução, 282 \nInternet, 282 \nIS-IS, 283 \nmensagens, 427 \nOSPF (Open-Shortest Path First), 283 \nRIP (Routing Information Protocol), 283\nProtocolos de sinalização, 233\nProtocolos de transporte \naplicações da Internet, 81 \nserviços, 138 \nSSL (Secure Sockets Layer), 523 \nTCP, 38 \nUDP, 38\nProtocolos humanos, 5\nProtocolos implementados pelo hardware, 6\nProtocolos multicast, 267\nProtocolos pare e espere, 153, 159\nProtocolos sem estado, 73\nProtocolos, 4, 49 \nanalogia humana, 5 \naplicações interativas em tempo real, 460 \nbit alternante, 158 \ncamada de aplicação, 38 \ncamada de transporte, 38 \ncontrole de congestionamento, 6 \ndefinição, 5 \ndisposição em camadas, 36 \nestado flexível, 301 \nimplementados pelo hardware, 6 \nroteador interno, 283 \nInternet, 6 \nIP (Internet Protocol), 4 \nnonce, 527, 546, 572 \nnúmeros de porta, 66 \npare e espere, 153, 159 \nplug-and-play, 255 \nroteamento, 38 \nRTP, 460 \nsem estado, 73 \nSIP (Session Initiation Protocol), 463-467 \nSR (Selective Repeat), 164-168 \ntamanhos de pacote, 245 \nTCP, 4 \ntransmissão e recebimento de mensagens, 5 \nUDP, 38\nProvedor, 23\nProvedores de serviços e redes privadas, 48\nProvisão de largura de banda, 467\nPulse Code Modulation. Veja PCM (Pulse Code Modulation)\nPUT, método, 77\nPython, 115, 118, 141\nQ\nQ2931b, protocolo, 483\nQAM16, modulação, 385\nQoS (Quality-of-Service), 461, 468, 475, 476, 481-484\nQoS (Quality-of-Service), garantias por conexão, 468, 481-\n484\nQQ, 437, 460\nQuadros da camada de enlace, 41, 323-325, 331-335\nQuadros de sinalização, 392\nQuadros, 39, 323, 331\nQuality-of-Service. Veja QoS\nQuantização, 435\nQuarta geração de redes remotas sem fio. Veja 4G\nR\nRadio Network Controller. Veja RNC\nRADIUS, protocolo, 393, 537\nRandom Early Detection, algoritmo. Veja RED, algoritmo\nRastreador, 109\nRC4, algoritmo, 535\nRCP (Routing Control Platform), 578\nRdt (protocolo de transferência confiável de dados), 150 \ncamada não confiável abaixo, 151 \ncriação, 151-154 \nem tubulação, 160-161 \nreordenação de pacotes, 167 \nTCP (Transmission Control Protocol), 149\nReal-Time Streaming Protocol. Veja RTSP\nReal-Time Transport Protocol. Veja RTP\nRecebimento de endereços de processos, 66\nReceptores \ndefinição de operação, 146 \nnúmero de sequência de pacote reconhecido por \nmensagem ACK, 154 \npolítica de geração de ACK, 181\nReconhecimento de carona, 175\nReconhecimento positivo. Veja ACK\nReconhecimento seletivo, 183\nReconhecimentos cumulativos, 163, 173, 182\nReconhecimentos da camada de enlace, 393, 394\nReconhecimentos negativos. Veja NAKs\nReconhecimentos, 152, 169 \ncarona, 175 \nTCP (Transmission Control Protocol), 169, 178 \nTelnet, 171\nRecuperação baseada em receptor, 458\nRecursos \nadmição ou bloqueio de fluxos, 482 \nreservas, 481 \nuso eficiente, 473\nRED (Random Early Detection), algoritmo, 243\nÍndice  625 \nRede de computadores \nhistória, 44-48\nRede de comutação, 320, 322, 327, 329-330\nRede de confiança, 523\nRede de distribuição ótica, 11\nRede de interconexão, 240\nRede de redes, 24, 46\nRede de sobreposição, 113, 297, 459\nRede social, 61-62\nRede stub com múltiplas interconexões, 293\nRede stub, 293-295 \ncom múltiplas interconexões, 293\nRede telefônica, 20 \nbanda de frequência, 21 \ncomplexidade, 235 \ncomutação de circuitos, 44 \ncomutação de pacotes, 16\nRedes ad hoc móveis. Veja MANETs\nRedes ad hoc, 383 \n802.15.1 \nhospedeiros sem fio, 382 \nredes pessois, 402\nRedes com fio, 384\nRedes comutadas por circuito \nconexão fim a fim, 20 \nenviando pacotes, 20 \nmultiplexação, 21 \nredes de telefone, 20 \nreserva de compartimentos de tempo, 23 \nversus comutação de pacotes, 22\nRedes comutadas por pacotes, 3, 18 \nARPAnet, 45 \natrasos, 26 \natrasos de fim a fim, 31 \natrasos de enfileiramento, 29 \natrasos de processamento, 27 \natrasos de propagação, 27 \natrasos de transmissão, 27 \ncomparação atraso de transmissão e propagação, 28 \nenvio pacotes, 18 \nperda de pacotes, 29-30\nRedes comutadas, 352\nRedes de acesso, 8-14 \nacesso discado, 12 \nacesso doméstico, 9 \nacesso por Internet a cabo, 10, 341 \nacesso remoto sem fio, 14 \ncomutadores da camada de enlace, 3 \nDSL (Digital Subscriber Line), 9 \nempresas, 12 \nenlaces de satélite, 12 \nEthernet, 12 \nFTTH (Fiber To The Home), 12 \nLANs sem fio, 13 \nLTE (Long-Term Evolution), 14, 409 \nrede de distribuição ótica, 13 \nredes remotas sem fio 2G, 3G, 4G (geração), 13, 405-410\nRedes de celulares \narquitetura, 405-407 \ncélulas, 405 \ngerenciando a mobilidade, 420-425 \nredes 2G, 3G, 4G, 405-410 \nroteando chamadas para usuário de celular, 421 \ntransferências em GSM, 420-422\nRedes de centro de dados, 362-369\nRedes de computadores, 1\nRedes de comutação de pacotes, 48\nRedes de datagramas, 230\nRedes de malha sem fio, 384\nRedes de melhor esforço, 468-469\nRedes de pacote de rádio, 46, 378\nRedes de provedor de conteúdo, 25\nRedes de tempo compartilhado, 44\nRedes domésticas, 9  \nagente nativo, 412 \nendereços IP, 256 \nEthernet, 12 \nGMSC (Gateway Mobile services Switching Center), 421 \nGSM (Global System for Mobile Communications), 421 \nHLR (Home Location Register), 421 \nnós móveis, 413 \nPLMN (Public Land Mobile Network), 421 \nWi-Fi, 12\nRedes externas, 414-417\nRedes locais virtuais. Veja VLANs\nRedes locais. Veja LANs\nRedes óticas passivas. Veja PONs\nRedes pessoais sem fio. Veja WPAN\nRedes pessoais \nBluetooth, 402 \nZigbee, 403\nRedes privadas virtuais. Veja VPNs\nRedes privadas, 48, 528\nRedes sem fio com múltiplos saltos, 384\nRedes sem fio de infraestrutura, 384, 390\nRedes sem fio, 381 \ncamada de aplicação, 426 \ncamada de enlace, 426 \ncamada de rede, 426 \ncaracterísticas, 384-389 \nCDMA (Code Division Multiple Access), protocolo,  \n  287-389 \nenlaces de comunicação sem fio, 382 \nestação-base, 382 \nhosts sem fio, 382 \ninfraestrutura de rede, 383 \nLANs sem fio 802.11, 389 \nmobilidade, 426 \nsaltos múltiplos, baseadas na infraestrutura, 383 \nsaltos múltiplos, sem infraestrutura, 383 \ntaxas de enlace, 382 \nTCP (Transmission Control Protocol), 426 \nUDP (User Datagram Protocol), 426 \núnico salto, baseadas em infraestrutura, 383 \núnico salto, sem infraestrutura, 518\nRedes sociais, 48, 61\nRedes visitadas, 412, 420\nRedes, 224 \nataques, 42 \natraso e perda de pacotes, 26-35, 468, 469 \n   Redes de computadores e a Internet\n626\ncelular, 404-410 \nclasses de serviço múltiplas, 468 \ncomo camada de enlace, 359-362 \ncomponentes, 3 \ncomutação de circuitos, 20-23 \ncomutação de pacotes, 3, 16-20 \ncrescimento, 46 \ncrossbar, 243 \nCV (circuito virtual), 231-233 \ndatagrama, 233 \ndimensionando pelo melhor esforço, 468-469 \nDMZ (zona desmilitarizada), 544 \nenlaces, 468 \nexternas, 412 \ngarantias de QoS (qualidade de serviço) por conexão, 481 \ninterconectando, 46 \nLANS sem fio, 381, 389-404 \nligando universidades, 46 \nmecanismos de escalonamento, 678-679 \nmeios físicos, 14-16 \nmobilidade, 381 \nprogramas se comunicando, 62 \nprivadas, 528 \nredes de acesso, 8-14 \nredes locais comutadas, 342 \nredes MPLS (Multiprotocol Label Switching), 360 \nrotas, 2 \nsegurança, 41-42, 495-497 \nserviço diferenciado, 468, 476-481 \nsockets, 65 \nvisitadas, 412\nRedundância espacial, 434\nRegistradora da Internet, 392\nRegistradoras, 102\nRegistro com agente nativo, 418\nRegistro de localização de visitantes. Veja VLR\nRegistro nativo de localização. Veja HLR\nRegistros de recursos. Veja RRs\nRegistros, inserindo em banco de dados de DNS, 102, 104\nRegra da concordância de prefixo mais longo, 234\nRelação sinal-ruído. Veja SNR\nRelacionamento cliente-provedor, 24\nRelays, 459\nRepasse de pacotes, 19\nRepasse pelo caminho inverso. Veja RPF\nRepasse, 225-228, 235, 352\nRepetição de pacotes, 458\nRepetidor, 348\nReposicionamento de vídeo, 443\nReprodução contínua, 436\nRequest to Send, quadro de controle. Veja RTS, quadro de \ncontrole\nRequisição de estabelecimento de conexão, 143\nResumo de rotas, 253\nResumos de mensagem, 520-521\nRetransmissão, 152-155 \nRetransmissão de dados, 175, 178\nRetransmissão de pacotes, 190, 456\nReversão envenenada, 278-279\nRexford, Jennifer, 579-579\nRFCs (Requests For Comments), 4\nRIP, anúncios, 283\nRIP, mensagem de requisição, 283\nRIP, mensagem de resposta, 146, 283\nRIP, roteadores, 284 \naspectos de implementação, 283 \natualizações de roteamento, 284 \nISPs da camada mais baixa, 283 \nmensagens RIP, 283 \nmodificação da tabela de roteamento local e informação \nde propagação, 283 \nprotocolo da camada de rede IP, 283 \nRIP (Routing Information Protocol), 283, 305 \nsaltos, 283 \ntabela RIP, 284 \nUDP, protocolo da camada de transporte, 285 \nUNIX, implementação, 283\nRivest, Ron, 504, 509\nRM (Resource-Management, células), 197-198\nRNC (Radio Network Controller), 408\nRoberts, Larry, 45, 378\nRotas, 2, 289\nRoteador de borda, 8-9\nRoteador de destino, 269\nRoteador de origem, 269\nRoteador default, 269\nRoteadores de acesso, 364\nRoteadores de borda de área, 288\nRoteadores de borda, 363\nRoteadores de gateway, 280 \nfiltragem de pacotes, 539 \npolítica de importação, 290 \nprefixos, 289\nRoteadores por comutação de rótulos, 360\nRoteadores, 2, 8, 12, 16, 35,  228 \nadaptadores, 343 \nASs (Autonomous Systems), 280 \natributo AS-PATH, 290 \nautonomia administrativa, 280 \nautossincronismo, 74 \nborda de área, 288 \nbuffer de bits do pacote, 18 \nbuffers finitos, 192 \ncamada 2, 322 \ncamada de enlace e endereços MAC, 343  \ncanal autenticado e criptografado entre, 533 \ncomutação, 235, 239-341 \ncomutação de rótulos, 361 \nconectados à rede, 250 \nCV, estabelecimento, 231 \ndecisões de encaminhamento de pacotes, 269 \ndefault, 269 \ndestino, 269 \ndimensionamento do buffer, 242 \nendereço de, 31 \nendereços da camada de rede, 343, 345 \nendereços IP, 290, 348  \nenfileiramento, 241 \nenlaces incidentes, 16 \nenlaces físicos entre, 269 \nÍndice  627 \nescala, 280 \nfirewalls, 262, 356 \nfunção de repasse, 235 \nfunções de controle, 236 \ngateway, 280 \nhierarquia do datacenter, 364 \nimplementando camadas de 1 até 3, 53 \ninformação de estado de conexão, 232 \ninterfaces, 250, 348 \nlistas de controle de acesso, 540 \nlooping de anúncios, 289 \nmódulos ARP, 345 \nnúcleo da rede, 2 \norigem, 269 \npapel primordial, 225 \nperda de pacotes, 241 \npesquisa, 323–324 \nplano de controle implementado, 243 \nplano de controle de roteamento, 243 \nplug-and-play, 346 \nportas de entrada, 235 \nportas de saída, 236 \nprimeiro salto, 269 \nprocessador de roteamento, 236 \nprocessamento de entrada, 237-239 \nprocessamento de saída, 241 \nprocessamento de datagramas, 352 \nprocessamento de pacotes, 249 \nprotocolo IP, 38 \nprotocolos, 4 \nprotocolos de roteamento dentro do AS, 288 \nregra da concordância com o prefixo mais longo, 234 \nroteamento de pacotes, 288 \nspanning tree, 356 \nstore-and-forward, 16, 18 \ntabela de encaminhamento, 26, 308-309, 317-318,  \n  322-323, 394, 396-397, 469 \ntabelas de roteamento, 284 \ntempos de acesso à memória, 239 \nterminando enlace físico de entrada, 241 \ntroca de pacotes, 355 \ntrocas de pacotes armazena-e-repassa, 355 \nversus comutadores, 355\nRoteamento da batata quente, 282\nRoteamento direto, 415\nRoteamento entre ASs, 283-288\nRoteamento hierárquico, 280-283\nRoteamento indireto, GSM (Global System for Mobile \nCommunications), 420 \nnó móvel, 413-416 \npadrão IP móvel, 415 \nproblema do roteamento triangular, 415\nRoteamento intra-AS, 286-288\nRoteamento intradomínio e servidores DNS, 369-371\nRoteamento para grupos, 299-305\nRoteamento, 225-228 \narmazenando informação, 280 \nbatata quente, 282 \nbroadcast, 295-299 \nchamadas para usuário móvel, 422 \nhierárquico, 280-283 \ninformação de anúncio, 282 \nmulticast, 299-305 \npara nó móvel, 413-416 \nvetor de distâncias, 283\nRótulos de tamanho fixo, 360\nRouting Control Platform. Veja RCP\nRouting Information Protocol. Veja RIP\nRPB (Reverse Path Broadcast), 297\nRPF (Reverse Path Forwarding), 297\nRRs (Resource Records), 102\nRSA, algoritmo, 503, 522\nRST, bit de flag e segmento, 172, 189\nRSVP, RSVP-TE, protocolo, 362, 483\nRTP (Real-Time Transport Protocol), 439, 460, 462, 484  \nfluxo contínuo UDP, 439\nRTP, pacotes, 461\nRTS (Request to Send), quadro de controle, 396\nRTS, quadro, 396\nRTS/CTS, troca, 396\nRTSP (Real-Time Streaming Protocol),  439,\nRTT (Round-Trip Time), 75 \nEWMA (média móvel exponencial), 176 \nTCP (Transmission Control Protocol), 175-177\nRuído eletromagnético, 284\nS\nSA (Security Association), 530\nSAD (Security Association Database), 531\nSatélite de comunicação, 16\nSatélites geoestacionários, 16\nScanners de porta, 196, 740\nSchulzrinne, Henning, 461, 466, 492\nSDN (Software Defined Networking), 578\nSecure Hash Algorithm. Veja SHA-1\nSecure Network Programming, 378\nSecure Sockets Layer. Veja SSL\nSecurity Association Database. Veja SAD\nGerenciamento de segurança, 558, 561\nSecurity Policy Database. Veja SPD\nSegmentos da camada de transporte, 40, 136 \ncampos, 138 \ndatagramas, 177 \nentregando dados ao socket correto, 138 \nnão confiabilidade, 177\nSegmentos fora de ordem, 236\nSegmentos, 38, 136138 \ncampo de número de porta de destino, 140 \ncampo de número de porta de origem, 140 \ncampos, 140 \nfora de ordem, 236 \nidentificadores exclusivos, 140 \nnúmero de reconhecimento, 171 \nnúmeros de sequência, 171 \nreconhecimento de carona, 175 \nretransmissão rápida, 181 \nTCP (Transmission Control Protocol), 169\nSegurança baseada no usuário, 572\nSegurança e administração, capacidades de, 570\n   Redes de computadores e a Internet\n628\nSegurança operacional, 496, 538-546\nSegurança, 41 \narquitetura P2P, 106 \nassinaturas digitais, 507-508 \nataques, 499 \nautenticação de ponto final, 515 \nbaseada no usuário, 572 \ncamada de enlace de dados, 519 \ncamada de rede, 519, 528-534 \ncomutadores, 352 \nconexão TCP, 523-528 \ncriptografia, 497-507 \ncriptografia de chave pública, 503-507 \ndatagramas IP, 528 \ne-mail, 519-523 \nIEEE 802.11i, 536-538 \nintegridade de mensagem, 507-515 \nIP (Internet Protocol), 267-268 \nIP móvel, 413 \nIPsec, 267 \nLANs sem fio, 534-538 \noperacional, 496, 538-546 \nOSPF (Open-Shortest Path First), 286 \nprotocolo em nível de aplicação, 519 \nprotocolos da camada de transporte, 68, 519 \nredes, 496-497 \nRSA, 503 \nSNMPv3, 570-572 \nserviços de transporte, 68 \nWEP (Wired Equivalent Privacy), 534-536\nSem fio, 380\nSenhas, 518, 522 \nSensível a atraso, 437\nService Level Agreements. Veja SLAs\nService Set Identifier. Veja SSID\nServiço de apresentação, 572\nServiço de compatibilização de velocidades, 184\nServiço de datagrama e camada de rede, 268\nServiço de diretório, 71\nServiço de entrega de melhor esforço, 138\nServiço de melhor esforço, 138, 229, 452-453, 467\nServiço de multiplexação/demultiplexação e camada de \ntransporte, 139, 140\nServiço de transferência confiável de dados, 171, 172\nServiço de transporte confiável, 198\nServiço diferenciado, 468\nServiço não confiável, 138\nServiço orientado para a conexão, 69, 230\nServiço sem conexão, 230\nServiços de transporte \ndisponíveis às aplicações, 66-68 \nprovidos pela Internet, 68-71 \nsegurança, 68 \nserviço orientado para a conexão, 69 \nserviços TCP, 69 \ntemporização, 66-68 \ntransferência confiável de dados, 66 \nUDP, 69 \nvazão, 67\nServiços relacionados a mobilidade, 380\nServiços, 35 \ncamada de transporte, 135 \ndescrição da Internet, 3 \nDNS (Domain Name System), 97-102 \nfluxo de pacotes, 228 \nprotocolos de transporte, 138\nServidor SMTP, 88\nServidores de cache da rede, 78\nServidores de correio, 71, 86-89, 91-92\nServidores DNS com autoridade, 98-99, 369 \nendereços IP, 97-98 \nnomes de hospedeiro, 96 \nnomes, 96-97\nServidores DNS de domínio de alto nível. Veja TLD, \nservidores DNS\nServidores DNS raiz, 98-99\nServidores proxy, 81, 132, 466\nServidores replicados, 97\nServidores Web, 65, 73 \nendereços IP, 290 \nextração de objetos, 78 \ngeração de novos processos para conexões, 145 \nlado servidor do HTTP, 72 \nnúmeros de porta, 140 \nprocessos do servidor, 65 \nTCP (Transmission Control Protocol), 140-141 \nupload de objetos para, 105 \nversões iniciais, 47\nServidores, 1, 6-7, 65-66 \nataques de rede, 42 \ncaches Web como, 81 \nconexões não persistentes, 141 \ncriação de socket TCP, 120 \nendereços IP, 64, 119, 120, 122 \nHTTP persistente, 145 \nnome de hospedeiro, 119 \nnúmero de porta, 120-123 \nsempre em funcionamento, 62 \nsocket dedicado, 119\nServing GPRS Support Nodes. Veja SGSNs\nSessão BGP interna. Veja iBGP, sessão\nSession Initiation Protocol. Veja SIP\nSGMP (Simple Gateway Monitoring Protocol), 562\nSGSNs (Serving GPRS Support Nodes), 407\nSHA, 522\nSHA-1 (Secure Hash Algorithm), 510\nShamir, Adi, 504\nShannon, Claude, 59-60\nSIFS (Shorter Inter-Frame Spacing), 394\nSimple Gateway Monitoring Protocol. Veja SGMP\nSimple Mail Transfer Protocol. Veja SMTP\nSimple Network Management Protocol. Veja SNMP\nSIP (Session Initiation Protocol), 460, 463-467, 492-493\nSistema de e-mail seguro, 520-522\nSistema de nome de domínio. Veja DNS\nSistema de prevenção de intrusão. Veja IPS\nSistemas autônomos. Veja ASs\nSistemas de chave pública, 498\nSistemas de detecção de intrusão. Veja IDSs\nSistemas finais, 1, 3-4, 7 \nÍndice  629 \nAPI (Application Programming Interface), 4 \naplicações rodando em, 4-5 \natrasos, 32 \nconectando, 3-4 \nendereços IP, 19 \nenlaces de comunicação, 3 \nhospedeiros, 3, 7 \nprocessos, 62, 65 \nprotocolo TCP, 169 \nprotocolos da camada de transporte, 137 \nremontagem de datagrama, 264-265\nSites Web, 79 \nanonimato, 543 \nprivacidade, 543\nSkype, 48, 61, 70, 437, 452, 458-460 \nprotocolos proprietários da camada de aplicação, 71 \nUDP (User Datagram Protocol), 458 \nvoz conversacional e voz, 436-437\nSLAs (Service Level Agreements), 556-557\nSmall Office, Home Office, sub-redes. Veja SOHO, sub-redes\nSMI (Structure of Management Information), 562, 563-566\nSMTP (Simple Mail Transfer Protocol), 88, 71, 87, 88-91\nSMTP, clientes, 88-91\nSMTP, servidores, 88\nSNMP (Simple Network Management Protocol), 557, 560, \n562-563, 566\nSNMP, aplicações, 570-571\nSNMP, mensagens, 571\nSNMPv2 (Simple Network Management Protocol, versão 2), \n562563, 568, 569, 570\nSNMPv3, 562, 563, 570, 573\nSnort IDS, sistema, 544, 546\nSNR (Signal-to-Noise Ratio), 385\nSocket do cliente, 119, 120, 123\nSocket, interface, 73\nSocket, módulo, 119\nSocket, programação \nTCP (Transmission Control Protocol), 115, 119-123  \nUDP, 115-116\nSockets, 66-71,  139 \ndesignando número de porta, 120 \nnúmero de porta, 119\nSoftware de plano de controle, 244\nSoftware Defined Networking. Veja SDN\nSOHO (Small Office, Home Office), sub-redes \ne endereços IP, 258\nSolicitação de agente, 418\nSoma de verificação da Internet, 148, 246, 328\nSoma de verificação do cabeçalho, 246\nSomas de verificação, 148, 328 \ncálculo, 148 \nfunção de hash criptográfica fraca, 508 \nInternet, 148 \npacotes ACK/NAK, 152 \nTCP (Transmission Control Protocol), 246 \nUDP (User Datagram Protocol), 148, 246\nSocket e processos da conexão, 145\nSpam, 41\nSpanning tree mínima, 298\nSpanning trees, 297-298, 356\nSPD (Security Policy Database), 533\nSPI (Security Parameter Index), 532\nSprint, 3, 24, 556\nSpyware, 41\nSR (Selective Repeat), protocolos, 165\nSRAM, 237\nSSH, protocolo, 174\nSSID (Service Set Identifier), 391\nSSL (Secure Sockets Layer), 523 \nalgoritmos criptográficos, 527 \nanonimato, 543 \nAPI (Application Programmer Interface) com sockets, 524 \napresentação, 523-524, 527 \ncertificação de chave pública, 513 \ncifras de bloco, 500 \nderivação de chave, 523-524 \nfechamento de conexão, 527 \nnonces, 527 \nnúmeros de sequência, 526 \nquebra do fluxo de dados em registros, 524 \npopularidade, 523 \nprivacidade, 543 \nprojetado pela Netscape, 524 \nprotocolos de transporte, 524 \nsegurança de transações HTTP, 524 \nSSL, classes/bibliotecas, 524 \nSSL, registro, 527 \ntransferência de dados, 523-524\nSSM (Source-Specific Multicast), 305\nStructure of Management Information. Veja SMI\nSub-rede IP, 400\nSub-redes, 250 \nanunciando existência à Internet, 288 \nárvore de caminho mais curto, 286 \ndefinição pelo IP, 249-251 \ndefinição, 251 \nDHCP, mensagem de oferta, 255 \nDHCP, servidores, 255 \nendereços IP, 249, 251, 254 \nenvio de datagramas de, 343 \nprefixos, 289 \nredes classe A, B e C, 344\nSYN segmentos, 185-189 \nSYN, ataque de inundação, 185, 186, 189\nSYN, bit, 172, 186\nSYN, cookies, 189\nSYN, pacote,  189\nSYN_SENT, estado, 186\nSYNACK, segmento, 186, 189\nT\nTabela de comutação, 353\nTabelas de repasse, 19-20, 226, 233, 237 \nalgoritmos de roteamento, 268 \nconfiguraçãoo, 227, 268 \ndirecionando datagramas para rede externa, 412 \ninclusão de entradas, 289 \nredes de datagramas, 235 \nredes de CV, 235\n   Redes de computadores e a Internet\n630\nTamanho de janela, 161\nTamanho de rajada, 477\nTamanho máximo de segmento. Veja MSS\nTaxa de bits constante, serviço de rede ATM. Veja CBR, \nserviço de rede ATM\nTaxa de bits disponível. Veja ABR\nTaxa de pico, 476-477\nTaxa explícita, campo. Veja ER (Explicit Rate), campo\nTaxa mínima de transmissão celular. Veja MCR\nTaxas de emissão, 190\nTaxas de enlace, 382\nTaxas de transmissão, 2, 33\nTCAMs (Ternary Content Address Memories), 238\nTCP (Transmission Control Protocol), 4, 38, 68, 138, 230, 258 \nACK do receptor ao emissor, 426 \nACK duplicado, 181-182 \naplicação cliente-servidor, 115 \naplicações de multimídia, 147 \napresentação de três vias, 170, 186, 189, 228 \natrasos no estabelecimento de conexão, 147 \nbuffer de recepção, 198 \nbuffer e segmentos fora de ordem, 184 \ncaminhos com alta largura de banda, 205 \ncifras de bloco, 500 \ncomportamento de estado constante, 198 \ncontrole de fluxo, 176, 184-185 \ncontrole de congestionamento, 69, 139, 146, 176, 181,  \n  184, 190-208  \ncontrole de congestionamento baseado em hospedeiro, 47 \ncontrole de congestionamento de fim a fim, 195, 198 \ndescrição macroscópica da vazão, 204-205 \nestado da conexão, 145-146, 169 \nestados, 186 \nestendendo o serviço de entrega do IP, 138 \nestouro de buffer, 184 \nevolução contínua, 205 \nfluxo de bytes, 177 \nfull-duplex, 184 \nGBN (Go-Back-N), protocolo, 183-185 \nHTTP e, 85, 145, 146 \nimpedimento de congestionamento, 272–276 \nintervalo de temporização de retransmissão, 175 \njanela de congestionamento, 198, 205, 426 \njanela de recepção, 184 \njustiça, 279–282 \nmecanismo de temporização/retransmissão, 175 \nmecanismo NAK implícito, 176 \nmídia de fluxo contínuo, 300 \nMSS (Maximum Segment Size), 169 \nMTU (Maximum Transmission Unit), 169 \nnúmero de sequência de 32 bits, 162 \nnúmeros de reconhecimento, 179 \nnúmeros de sequência, 179, 183 \norientado para a conexão, 69, 119, 169-171 \nperda de pacotes, 181, 452 \npipelining, 161 \nprogramação de socket, 114-119 \nreconhecimento perdido, 177 \nreconhecimento seletivo, 183 \nreconhecimentos cumulativos, 163, 164, 173, 183 \nreconhecimentos negativos, 182 \nreconhecimentos positivos, 176 \nredes sem fio, 425-427 \nreenvio de segmento até ser reconhecido, 146 \nretransmissão rápida, 181 \nretransmissão de dados, 348 \nretransmissão de segmentos, 175, 180,  426 \nRST, segmento, 189 \nRTT (Round-Trip Time), estimativa, 175 \nsegmentos, 138 \nsegmentos perdidos, 172 \nseparação do IP, 46 \nserviço de transferência confiável de dados, 66, 73, 88,  \n  120, 146, 171, 177-184 \nserviços, 66 \nserviços de segurança, 66 \nservidores Web, 141 \nsoma de verificação, 246 \nsoma de verificação da Internet, 328 \nSYNACK, segmento, 186 \nSYN, segmentos, 185 \ntamanho de janela, 195 \ntaxa de transmissão, 199 \ntaxa de transmissão do servidor ao cliente, 439 \nTCP Reno, 203TCP, segmentos, 171 \nTCP Tahoe, 203 \nTCP Vegas, 204 \ntemporização (timeout), 175-188 \ntemporizador de retransmissão, 177 \ntransferência confiável de dados, 66, 120, 146, 171,  \n  177-184 \nvariável de estado, 179 \nverificação de integridade, 138 \nversões anteriores, 46 \nvídeo de fluxo contínuo, 441\nTCP buffers, 439-440\nTCP Reno, 203\nTCP SYN, segmento, 370\nTCP SYNACK, segmento, 370\nTCP Tahoe, 203\nTCP Vegas, 203\nTCP, algoritmo de controle de congestionamento, 199-209 \nTCP, cabeçalho, 170-171\nTCP, clientes, 139, 185-190\nTCP, conexões, 42, 69 \ntécnicas de conexão dividida, 426 \nalocando buffers e variáveis, 186 \napresentação de três vias, 75, 120, 170 \nbuffers, 171 \nbuffer de envio, 170 \nbuffer de recepção, 171, 184 \nconexão de socket com processo, 171 \nencerramento, 186 \nenlace de gargalo, 205-207 \nentre cliente e servidor, 119 \nestabelecimento, 171, 186, 525 \ngerenciamento, 185-190 \nlargura de banda, 205 \nparalelas e justas, 208 \nperda de pacotes, 205 \nÍndice  631 \nponto a ponto, 169 \nprocesso cliente, 169 \nprocesso servidor, 169 \nprocessos de envio de dados, 169 \nregulação da taxa de tráfego, 190 \nsegmento com conexão concedida, 185 \nsegmentos fora de ordem, 173 \nsegurança, 523-527 \nserviço full-duplex, 169 \nservidor HTTP, 439 \nsocket do cliente, 119 \nsocket de servidor, 119 \nTCP no lado cliente enviando segmento ao lado servidor,  \n  185-186 \ntransporte de mensagem de requisição e mensagem de  \n  resposta, 74 \nvariáveis, 169, 171  \nvazão, 204\nTCP, divisão, 202\nTCP, emissor, 177, 198-199 \nciente de enlaces sem fio, 426 \ncontrole de congestionamento, 184\nTCP, fluxo contínuo e busca prévia de vídeo, 439\nTCP, portas, 171\nTCP, segmentos, 169-171, 185-186 \ncom diferentes endereços IP de origem, 140 \nestrutura, 169-171 \noverhead de cabeçalho, 146 \nperda, 198 \nreordenação, 526\nTCP, servidor, 119, 141\nTCP, sockets, 119, 368-369 \nsocket de conexão no lado do servidor, 115 \nsocket de boas vindas, 115\nTCP/IP (Transmission Control Protocol/Internet Protocol), 4, \n38, 68, 138, 230, 258\nTCPClient.py, programa cliente, 120-121\nTCPServer.py, programa servidor, 121-122\nTDM (Time-Division Multiplexing), 21, 169, 332, 371, 402, \n406\nTécnica de pilha dupla, 265\nTécnicas criptográficas, 497\nTécnicas de conexão dividida, 426\nTelco. Veja companhia telefônica\nTelefones celulares \nacesso à Internet, 48, 404-410 \ncrescimento, 380\nTelefonia da Internet, 64, 452-456 \nSkype, 458-460\nTelenet, 46\nTelnet, 65 \nbloqueado, 539 \nenvio de mensagem ao servidor de correio, 91 \nSMTP, servidor, 91 \nTCP, exemplo, 171, 174-175\nTempo de envelhecimento, 354\nTempo de ida e volta. Veja RTT\nTempo de transferência, 22\nTempo de vida, campo. Veja TTL (Time-To-Live), campo\nTemporização (timeout) \ncomprimento dos intervalos, 175 \ndefinição e gerenciando intervalo, 175 \n intervalo dobrado, 177 \nevento, 163, 165, 178 \nTCP (Transmission Control Protocol), 171, 175, 178\nTemporizador de contagem regressiva, 157\nTerminais ocultos, 386, 395-397\nTerminal de linha ótica. Veja OLT\nTerminal de rede ótica. Veja ONT\nTernary Content Address Memories. Veja TCAMs\nTexto aberto, 498\nTexto cifrado, 498\nTexto claro, 498\nTipo de serviço, bits. Veja TOS, bits\nTLD (Top-Level Domain), servidores DNS, 98-99, 103 \nservidores DNS, 98-99\nTLS (Transport Layer Security), 524\nTLV (Type, Length, Value), abordagem, 575\nTolerância a perdas, 91, 437\nTop of Rack, comutador. Veja TOR, comutador\nTopologia totalmente conectada, 366\nTOR (Top Of Rack), comutador, 363\nTOR, serviço de anonimato e privacidade, 543\nTorrents, 109\nTOS (Type Of Service), bits, 245\nTraceroute, programa, 20, 261-262 \natrasos de fim a fim, 31\nTradução de endereço de rede. Veja NAT\nTradução de nomes e SIP (Session Initiation Protocol),  \n463-464\nTráfego em rajadas, 44-45\nTráfego \ncondicionando, 479 \nintensidade, 29 \nrajada, 44-45\nTransferência confiável de dados, 66, 138 \ncamada de aplicação, 149 \ncamada de enlace, 149 \ncamada de transporte, 149 \ncanal com erros de bit, 151-156 \ncanal com perdas com erros de bit, 156-158 \ncanal confiável, 149 \ncanal perfeitamente confiável, 150 \nprincípios, 149 \nprotocolos da camada de transporte, 66 \nTCP (Transmission Control Protocol), 168, 175, 177-184\nTransferência de arquivos, 71, 85-87\nTransferência de dados bidirecional, 151\nTransferência de dados não confiável, 151\nTransferência de dados unidirecional, 151\nTransferência de dados, 525 \nbidirecional, 151 \nconfiável, 66, 149-168 \nnão confiável, 151 \nredes CV (Circuito Virtual), 231 \nSSL (Secure Sockets Layer), 526-527 \nunidirecional, 151\nTransferência, 383 \nGSM, 423-425\nTransferência de arquivos, 85-86\n   Redes de computadores e a Internet\n632\nTransmission Control Protocol. Veja TCP (Transmission \nControl Protocol/Internet Protocol)\nTransmitindo \npacotes em redes de datagramas, 233 \nquadros, 393\nTransport Layer Security. Veja TLS\nTransporte orientado para a conexão e TCP (Transmission \nControl Protocol), 168-190\nTransporte não orientado para conexão e UDP (User Datagram \nProtocol), 145-149\nTrocas de pacote, 2, 228 \nbuffers de saída, 18 \ncomutadores da camada de enlace, 16, 39, 227-228 \nfacilitando a troca de dados, 4 \nroteadores, 16, 39, 226-227 \ntransmissão store-and-forward (armazena-e-reenvia), 16,  \n  18\nTrocas de pacotes store-and-forward (armazena-e-reenvia), \n16, 18, 352\nTTL (Time-To-Live), campo, 102, 246\nTúneis IP, 266\nTwitter, 48, 61, 133\nType, Length, Value, abordagem. Veja TLV, abordagem\nU\nUDP (User Datagram Protocol), 38, 69, 138, 285 \naplicação cliente-servidor, 115 \naplicações de telefonia na Internet, 69-70 \naplicações de tempo real, 146 \naplicações multimídia, 147, 116 \napresentação, 199 \natrasos, 146 \natualizações da tabela de roteamento RIP, 146 \ncontrole de congestionamento, 146, 116 \ncontrole de fluxo, 184-185 \ncontrole mais detalhado dos dados em nível de aplicação, \n   \n  146 \ndados de gerenciamento de rede, 146 \ndatagramas, 138 \nDNS e, 145-146 \ndescarte do segmento danificado, 149 \ndesenvolvimento, 46 \ndetecção de erro, 148-149 \nestabelecimento de conexão, 145 \nestado da conexão, 146 \nestendendo o serviço de entrega do IP, 138 \n“falando” diretamente com IP, 145 \nfunção de multiplexação/demultiplexação, 145 \njustiça, 282 \nlacunas nos dados, 350 \nnão confiabilidade, 69, 138 \nnúmero de porta de destino, 145 \noverhead de cabeçalho, 146 \npassando segmento danificado à aplicação, 149 \npequeno overhead de cabeçalho de pacote, 146 \nperda de pacotes, 452 \nprincípio de fim a fim, 149 \nprogramação de socket, 115-116 \nredes sem fio, 425-426, 208 \nRTP e, 460-461 \nsegmentos, 138 \nserviço simples de entrega de segmento, 199 \nserviços de transporte, 68 \nsoma de verificação, 152, 246 \nsoma de verificação da Internet, 328 \ntransferência confiável de dados, 145-146 \ntransporte sem conexão, 68, 145-148 \nvazão de fim a fim, 68 \nverificação de erro, 145 \nverificação de integridade, 138\nUDP, cabeçalho, 148\nUDP, fluxo contínuo, 437-439 \nUDP, pacote, 185, 256, 440\nUDP, portas, 185\nUDP, segmentos, 148, 367\nUDP, sockets \ncomunicando com processos, 114-115 \ncriação, 114-115 \nidentificação, 139 \nnúmeros de porta, 139-140\nUDP, soma de verificação, 148\nUDPClient.py, programa cliente, 116-118\nUDPServer.py, programa servidor, 116, 118-119, 142\nUMTS (Universal Mobile Telecommunications Service), \npadrões 3G, 406\nUnicast de N caminhos, 295\nÚnico salto, redes sem fio, 383\nUnidade máxima de transmissão. Veja MTU\nUniversal Plug and Play. Veja UPnP\nUNIX \nprograma nslookup, 104 \nRIP implementado no, 283 \nSnort, 546 \nversão BSD (Berkeley Software Distribution), 283\nUPnP (Universal Plug and Play), 260\nURL, campo, 76\nURLs, 72\nUS Department of Defense Advanced Research Projects \nAgency. Veja DARPA\nUtilização, 159\nUTP (Unshielded Twisted Pair), 14\nV\nVANET (Vehicular Ad hoc Network), 384\nVarredura passiva, 392\nVariáveis e conexão TCP, 171\nVazão de fim a fim, 32\nVazão instantânea, 32\nVazão média, 32\nVazão por conexão, 191\nVazão servidor-cliente, 33-34\nVazão, 191 \nconexão TCP, 205 \ndescrição macroscópica para TCP, 204 \nfim a fim, 32 \nflutuações na, 92 \ninstantânea, 32 \nmédia, 32 \nÍndice  633 \nprotocolos da camada de transporte, 66 \nservidor ao cliente, 32 \ntaxas de transmissão de enlaces, 27 \nvídeo de fluxo contínuo, 436-438 \nzero no limite de tráfego pesado, 194\nVehicular Ad hoc Network. Veja VANET\nVelocidade de chipping, 387\nVerificação de erro \ncamada de transporte, 135 \nprotocolos da camada de enlace, 149-150 \nverificações de paridade, 326\nVerificação de redundância cíclica, códigos. Veja CRC, \ncódigos\nVerificações de paridade, 326\nVerizon, 556 \nserviço FIOS e PONs (Passive Optical Networks), 11\nVetor de distâncias, 270\nVídeo de fluxo contínuo armazenado, 437-452 \natrasos de fim a fim, 438 \nbuffering do cliente, 438 \nCDNs (Content Distribution Networks), 444-449 \nDASH (Dynamic Adaptive Streaming over HTTP), 443 \nfluxo contínuo, 436 \nfluxo contínuo adaptativo, 443 \nHTTP de fluxo contínuo, 439  \nHTTP de fluxo contínuo adaptativo, 439 \ninteratividade, 436 \nKanKan, 451 \nlargura de banda, 438 \nNetflix, 449 \nreprodução contínua, 436 \nUDP de fluxo contínuo, 439 \nYouTube, 450\nVídeo em fluxo contínuo, 434 \nTCP (Transmission Control Protocol), 439-441\nVídeo pré-gravado, 436\nVídeo sobre IP, 436-437\nVídeo, 433 \natravessando firewalls e NATs, 440 \nconsiderações de temporização e tolerância de perda de \ndados, 433, 436 \nentrega P2P, 451 \nfluxo contínuo armazenado, 437, 458 \npré-busca, 440 \npré-gravado, 435-437 \nreposicionando, 438, 442\nVideoconferência, 48, 50, 61\nVirtualização de enlace, 359-360\nVírus, 41-42, 544\nVizinhos, 269\nVLAN baseada em porta, 357-358\nVLAN, entroncamento, 358\nVLAN, tag, 358-359\nVLANs (redes locais virtuais) baseadas em MAC, 359\nVLANs (Virtual Local Area Networks), 357-362\nVLR (Visitor Location Register), 421\nVoIP (Voice-over-IP), 61, 452 \natraso de fim a fim, 452-455 \natraso de reprodução adaptativo, 454 \natraso de reprodução fixo, 454 \natrasos de empacotamento de mídia, 32 \nmarca de tempo, 454 \nvariação do atraso e áudio, 453 \naumento na qualidade pela rede de melhor esforço, 457,  \n  467 \nnúmeros de sequência, 453 \nperda de pacotes, 452 \nrecuperando-se da perda de pacotes, 456-458 \nsistemas sem fios, 493\nVoz e vídeo, aplicações, 61\nVoz humana, 435\nVPNs (Virtual Private Networks), 362 \nconfidencialidade, 496-497IPsec, 528-530 \nIPv4, 529-532 \nmodo túnel, 531 \nMPLS (Multiprotocol Label Switching), 360-361 \npontos finais, 533 \nSA (Security Association), 530\nW\nWeb, 47-48, 62, 71 \naplicações de rede, 72-95 \narquitetura de aplicação cliente-servidor, 73 \nHTTP (HyperText Transfer Protocol), 71-73 \noperando por demanda, 72 \nplataforma para aplicações surgindo após 2003, 72 \nterminologia, 72-73\nWeb, aplicações, 71-72 \narquitetura cliente-servidor, 62 \nprocessos cliente e servidor, 65\nWEP (Wired Equivalent Privacy), 534-536\nWFQ (Weighted Fair Queuing), 242, 476-478 \nbalde furado, 476-478\nWi-Fi, 12-14, 37-38, 389-404 \nacesso público, 381 \nalta velocidade, 48 \nhotspots, 404 \nredes domésticas, 8-9\nWiMAX (World Interoperability for Microwave Access), 410, \n493\nWindows \nanalisador de pacote Wireshark, 58 \nprograma nslookup, 104 \nSnort, 544, 546\nWired Equivalent Privacy. Veja WEP\nWireless Philadelphia, 381\nWireshark labs, 43, 58 \nWorld Wide Web. Veja Web\nWorms, 41-42, 544\nWPAN (Wireless Personal Area Network), 402\nX\nX-25, 379\nXNS (Xerox Network Systems), arquitetura, 283\n   Redes de computadores e a Internet\n634\nY\nYahoo!, 48, 62, 95\nYouTube, 48, 436, 449-450 \nfluxo contínuo HTTP (sobre TCP), 439 \nvídeo armazenado de fluxo contínuo, 436 \nvídeo, 444\nZ\nZigbee, 403-404\nZona desmilitarizada. Veja DMZ\n6a edição\nISBN 978-85-8143-677-7\nComputação\nw w w . p e a r s o n . c o m . b r\nsv.pearson.com.br\nA Sala Virtual oferece, para professores, apresentações em PowerPoint \ne manual de soluções (em inglês). Para estudantes, exercícios interativos, \nmaterial de aprendizagem interativo (applets), tarefas extras de programação \nem Python e Java (em inglês) e Wireshark (em inglês). \nEste livro também está disponível para compra em formato e-book.  \nPara adquiri-lo, acesse nosso site.\nRedes de computadores \ne a internet\numa abordagem top-down\nSeguindo o sucesso da abordagem top-down de suas edições ante-\nriores, Redes de computadores e a Internet tem como foco camadas \nde aplicação e interfaces de programação propondo ao leitor \numa experiência prática com os conceitos de protocolo e redes de \ncomputadores antes de trabalhar com mecanismos de transmissão \nde informação das camadas inferiores das pilhas de protocolos. \nAlém de manter essa característica inovadora, esta 6a edição adota o \nPython em suas aplicações e explora temas recentes e importantes, \ncomo as redes 4G e os serviços em nuvem. É indicada para alunos de \ngraduação em ciências da computação, engenharia da computação, \nanálise e desenvolvimento de sistemas e sistemas de informação.\nKurose  \n Ross\n6a edição\n"
    }
]